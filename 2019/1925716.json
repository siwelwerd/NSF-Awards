{
 "awd_id": "1925716",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC* Compute: Shared Computing Infrastructure for Large-scale Science Problems",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-07-17",
 "awd_max_amd_letter_date": "2019-07-17",
 "awd_abstract_narration": "Advances in scientific instrumentation over the past decade have led to phenomenal growth in both the size and complexity of data across a wide variety of scientific domains. Researchers at the University of Connecticut are expanding the capabilities of the existing shared high-performance computer facility to meet this challenge by the addition of a 28-node cluster, each node with 40 Intel cores and 192 GB of memory, and 1000 TB of shared storage.\r\n\r\nThis balance of resources has been chosen to accommodate applications in the areas of particle physics, astrophysics, geophysics, and statistics in use by UConn researchers, with a view to meet a rising need for data-intensive computing across all of science and engineering at the University. Furthermore, the cluster will be configured as a shared scientific computing resource on the Open Science Grid (OSG). OSG is a consortium of universities and national research facilities who are using grid technology to aggregate their individual computer clusters into a single unified national compute infrastructure for science. In joining this cluster to the OSG, UConn researchers will see a much higher throughput than they would see running on local resources alone, granting them faster turn-around and access to bigger data than could be stored and processed locally. The cluster will enable the introduction of a new big data component within an existing course on scientific computing at the graduate level. It will also be used to produce visualizations of astronomical observations that will be used in K-12 outreach.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Jones",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Richard T Jones",
   "pi_email_addr": "Richard.T.Jones@uconn.edu",
   "nsf_id": "000255491",
   "pi_start_date": "2019-07-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vernon",
   "pi_last_name": "Cormier",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Vernon F Cormier",
   "pi_email_addr": "vernon.cormier@uconn.edu",
   "nsf_id": "000462494",
   "pi_start_date": "2019-07-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kyungseon",
   "pi_last_name": "Joo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kyungseon Joo",
   "pi_email_addr": "kjoo@phys.uconn.edu",
   "nsf_id": "000424539",
   "pi_start_date": "2019-07-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Yan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Yan",
   "pi_email_addr": "jun.yan@uconn.edu",
   "nsf_id": "000483974",
   "pi_start_date": "2019-07-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Cara",
   "pi_last_name": "Battersby",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Cara D Battersby",
   "pi_email_addr": "cara.battersby@uconn.edu",
   "nsf_id": "000603862",
   "pi_start_date": "2019-07-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Connecticut",
  "inst_street_address": "438 WHITNEY RD EXTENSION UNIT 1133",
  "inst_street_address_2": "",
  "inst_city_name": "STORRS",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "8604863622",
  "inst_zip_code": "062699018",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CT02",
  "org_lgl_bus_name": "UNIVERSITY OF CONNECTICUT",
  "org_prnt_uei_num": "",
  "org_uei_num": "WNTPS995QBM7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Connecticut",
  "perf_str_addr": "2152 Hillside Road",
  "perf_city_name": "Storrs",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "062693046",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CT02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-617a69cf-7fff-e4cb-6947-7ce73dd04076\"> </span></p>\n<p dir=\"ltr\"><span>The NSF project entitled ?Shared Computing Infrastructure for Large-scale Science Problems? enabled a significant expansion of the UConn HPC cluster on the Storrs campus of the University of Connecticut. The expansion consists of 38 nodes with 64 cores (AMD EPYC) and 256GB of memory each, and 1.1PB of shared data storage. The chosen design matches the requirements for high-throughput computing, where the principal requirements are good single-thread floating point performance matched with few-GB/core memory and fast local scratch space. The new nodes were able to take advantage of the existing 100Gb/s low-latency infiniband network, which enabled multi-threaded applications that use the MPI library to run on multiple nodes at once. scale up to using 2000 cores at once. The Geophysics research group led by co-PI Cormier was able to demonstrate linear scaling up to 1088 cores spread out over 17 nodes. During its first two years of operation, the PI and 4 co-PI?s successfully deployed a range of scientific applications on the cluster, including Monte Carlo simulations of nuclear physics experiments, &nbsp; nuclear shell model calculations, simulation of seismic wave propagation in the earth?s interior, and solution of intense laser field dynamics. Work on porting astrophysics simulations of star formation to the cluster is now underway, and a challenging new application in real-time satellite image analysis is under active development using a combination of compute and shared storage resources on the new cluster.</span></p>\n<p dir=\"ltr\"><span>One requirement under the NSF CC* campus infrastructure grant was that the new cluster be configured as a computational resource of the Open Science Grid (OSG) and that 20% of the available time on the cluster be allocated for OSG jobs. With support from the OSG administrators, the new cluster nodes were configured as OSG resource site UConn-HPC soon after they were put into production in fall 2020. Figure 2 shows the total number of core-hours per month that were accumulated by OSG jobs running on the UConn-HPC resource. The full 2432 cores offer 1.75M core-hours per month, of which 20% would be 350k core-hr/mo. As the graph shows, UConn-HPC has provided several times that amount of time to OSG jobs every month of its first two years of operation. Figure 3 shows the same information broken down according to the field of science to which the OSG jobs belong, demonstrating the breadth of science that is supported. It is worth noting that the total 8M core-hours combined for nuclear science is only a fraction of the total work performed on nuclear physics simulations performed on the OSG by the scientific collaborations supported by PI Jones and co-PI Joo. Under the high-throughput computing paradigm implemented by the OSG, jobs from any one submitter spread out across all available sites to provide the fastest possible completion, with the implicit understanding that participating sites contribute to work coming from any qualified submitter. This table shows that the UConn-HPC site is a well-rounded participant in the OSG high-throughput paradigm.</span></p>\n<p dir=\"ltr\"><span>In addition to pursuing their own scientific research at UConn using the cluster and sharing it with outside scientific researchers via the OSG, the PI?s have also carried out a number of activities aimed at broadening the impact of this facility across the Storrs campus. The first of these was the participation of co-PI Battersby in the Bridge+ program, an equity-based excellence collaboration between the UConn School of Engineering and Vergnano Institute for Inclusion, the College of Liberal Arts and Sciences, and the School of Pharmacy. This program aims to support incoming graduate students that have been historically excluded, underrepresented, or marginalized in STEM. Figure 4 shows one of the slides from the training module presented by Prof. Battersby to Bridge+ students in August, 2021, where she shows students how a large collection of seismic data can be mined to extract the radius of the outer core - mantle boundary deep inside the earth?s interior.</span></p>\n<p dir=\"ltr\"><span>The second such effort is a new website (see figure 1) featuring Data Science Tutorials created by the PI and students, implementing an interactive learning platform on the cluster.&nbsp;</span></p>\n<p dir=\"ltr\"><span>A third outreach effort was the Cluster Working Group meeting series initiated by the PI&nbsp; in winter 2021. These meetings were held online once per month to bring together faculty, postdocs, and graduate students to share their experience working on the cluster.. News about these meetings spread by word of mouth, reaching a typical attendance by 7-8 research groups and 15-20 researchers over the past year. Figures 5 and 6 show slides from talks presented at these meetings within the past few months, both of them presented by postdocs from research groups that were not part of the original team of co-PIs on the original proposal. In just two years this cluster has achieved the status of a shared university-wide research facility.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2022<br>\n\t\t\t\t\tModified by: Richard&nbsp;T&nbsp;Jones</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669668599103_DataScienceTutorials--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669668599103_DataScienceTutorials--rgov-800width.jpg\" title=\"Data Science Tutorials website on UConn-HTC\"><img src=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669668599103_DataScienceTutorials--rgov-66x44.jpg\" alt=\"Data Science Tutorials website on UConn-HTC\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Image taken of a new website hosting a series of student-created tutorials that provide hands-on activities to teach skills in big data analysis using interactive python notebooks and real-world data hosted on UConn-HTC.</div>\n<div class=\"imageCredit\">University of Connecticut</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Richard&nbsp;T&nbsp;Jones</div>\n<div class=\"imageTitle\">Data Science Tutorials website on UConn-HTC</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669667147622_osg_payload_per_month--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669667147622_osg_payload_per_month--rgov-800width.jpg\" title=\"Open Science Grid job cpu-hours on UConn-HTC\"><img src=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669667147622_osg_payload_per_month--rgov-66x44.jpg\" alt=\"Open Science Grid job cpu-hours on UConn-HTC\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Accounting statistics for OSG jobs completed on the UConn-HTC cluster per month since the start of operations in October, 2020 broken down according to project.</div>\n<div class=\"imageCredit\">OSG accounting</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;T&nbsp;Jones</div>\n<div class=\"imageTitle\">Open Science Grid job cpu-hours on UConn-HTC</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669667882319_osg_fields_of_science(1)--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669667882319_osg_fields_of_science(1)--rgov-800width.jpg\" title=\"Fields of science served by UConn-HTC\"><img src=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669667882319_osg_fields_of_science(1)--rgov-66x44.jpg\" alt=\"Fields of science served by UConn-HTC\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Number of core hours dedicated to OSG jobs over the first two years of UConn-HTC operation, broken down according to the field of science.</div>\n<div class=\"imageCredit\">OSG accounting</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Richard&nbsp;T&nbsp;Jones</div>\n<div class=\"imageTitle\">Fields of science served by UConn-HTC</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669669078627_Bridges2021presentationbyCara--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669669078627_Bridges2021presentationbyCara--rgov-800width.jpg\" title=\"Bridges+ program presentation 8/2021\"><img src=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669669078627_Bridges2021presentationbyCara--rgov-66x44.jpg\" alt=\"Bridges+ program presentation 8/2021\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Slide taken from a presentation by Prof. Cara Battersby (coPI) explaining how to mine seismic data to extract an estimate for the radius of the outer core - mantle boundary (August 19, 2021).</div>\n<div class=\"imageCredit\">Cara Battersby, University of Connecticut</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Richard&nbsp;T&nbsp;Jones</div>\n<div class=\"imageTitle\">Bridges+ program presentation 8/2021</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669670549454_satelliteimageanalysis--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669670549454_satelliteimageanalysis--rgov-800width.jpg\" title=\"Satellite images track landscape changes\"><img src=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669670549454_satelliteimageanalysis--rgov-66x44.jpg\" alt=\"Satellite images track landscape changes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Data-intensive project preparing to take advantage of UConn-HTC for real-time analysis of time-series satellite imagery of the continental US, tracking systematic changes arising from land development and deforestation by fires.</div>\n<div class=\"imageCredit\">Ye Su, University of Connecticut</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Richard&nbsp;T&nbsp;Jones</div>\n<div class=\"imageTitle\">Satellite images track landscape changes</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669671268469_electrondensityquantum-classical--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669671268469_electrondensityquantum-classical--rgov-800width.jpg\" title=\"Atomic excitations by intense laser fields\"><img src=\"/por/images/Reports/POR/2022/1925716/1925716_10622224_1669671268469_electrondensityquantum-classical--rgov-66x44.jpg\" alt=\"Atomic excitations by intense laser fields\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In this study of strong field laser ionization of atoms and molecules, UConn Prof. A.T. Le and colleagues compare the excited electron density maps from an exact quantum calculation with the results from an approximate semi-classical method.</div>\n<div class=\"imageCredit\">Phi-Hung Tran, University of Connecticut</div>\n<div class=\"imageSubmitted\">Richard&nbsp;T&nbsp;Jones</div>\n<div class=\"imageTitle\">Atomic excitations by intense laser fields</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThe NSF project entitled ?Shared Computing Infrastructure for Large-scale Science Problems? enabled a significant expansion of the UConn HPC cluster on the Storrs campus of the University of Connecticut. The expansion consists of 38 nodes with 64 cores (AMD EPYC) and 256GB of memory each, and 1.1PB of shared data storage. The chosen design matches the requirements for high-throughput computing, where the principal requirements are good single-thread floating point performance matched with few-GB/core memory and fast local scratch space. The new nodes were able to take advantage of the existing 100Gb/s low-latency infiniband network, which enabled multi-threaded applications that use the MPI library to run on multiple nodes at once. scale up to using 2000 cores at once. The Geophysics research group led by co-PI Cormier was able to demonstrate linear scaling up to 1088 cores spread out over 17 nodes. During its first two years of operation, the PI and 4 co-PI?s successfully deployed a range of scientific applications on the cluster, including Monte Carlo simulations of nuclear physics experiments,   nuclear shell model calculations, simulation of seismic wave propagation in the earth?s interior, and solution of intense laser field dynamics. Work on porting astrophysics simulations of star formation to the cluster is now underway, and a challenging new application in real-time satellite image analysis is under active development using a combination of compute and shared storage resources on the new cluster.\nOne requirement under the NSF CC* campus infrastructure grant was that the new cluster be configured as a computational resource of the Open Science Grid (OSG) and that 20% of the available time on the cluster be allocated for OSG jobs. With support from the OSG administrators, the new cluster nodes were configured as OSG resource site UConn-HPC soon after they were put into production in fall 2020. Figure 2 shows the total number of core-hours per month that were accumulated by OSG jobs running on the UConn-HPC resource. The full 2432 cores offer 1.75M core-hours per month, of which 20% would be 350k core-hr/mo. As the graph shows, UConn-HPC has provided several times that amount of time to OSG jobs every month of its first two years of operation. Figure 3 shows the same information broken down according to the field of science to which the OSG jobs belong, demonstrating the breadth of science that is supported. It is worth noting that the total 8M core-hours combined for nuclear science is only a fraction of the total work performed on nuclear physics simulations performed on the OSG by the scientific collaborations supported by PI Jones and co-PI Joo. Under the high-throughput computing paradigm implemented by the OSG, jobs from any one submitter spread out across all available sites to provide the fastest possible completion, with the implicit understanding that participating sites contribute to work coming from any qualified submitter. This table shows that the UConn-HPC site is a well-rounded participant in the OSG high-throughput paradigm.\nIn addition to pursuing their own scientific research at UConn using the cluster and sharing it with outside scientific researchers via the OSG, the PI?s have also carried out a number of activities aimed at broadening the impact of this facility across the Storrs campus. The first of these was the participation of co-PI Battersby in the Bridge+ program, an equity-based excellence collaboration between the UConn School of Engineering and Vergnano Institute for Inclusion, the College of Liberal Arts and Sciences, and the School of Pharmacy. This program aims to support incoming graduate students that have been historically excluded, underrepresented, or marginalized in STEM. Figure 4 shows one of the slides from the training module presented by Prof. Battersby to Bridge+ students in August, 2021, where she shows students how a large collection of seismic data can be mined to extract the radius of the outer core - mantle boundary deep inside the earth?s interior.\nThe second such effort is a new website (see figure 1) featuring Data Science Tutorials created by the PI and students, implementing an interactive learning platform on the cluster. \nA third outreach effort was the Cluster Working Group meeting series initiated by the PI  in winter 2021. These meetings were held online once per month to bring together faculty, postdocs, and graduate students to share their experience working on the cluster.. News about these meetings spread by word of mouth, reaching a typical attendance by 7-8 research groups and 15-20 researchers over the past year. Figures 5 and 6 show slides from talks presented at these meetings within the past few months, both of them presented by postdocs from research groups that were not part of the original team of co-PIs on the original proposal. In just two years this cluster has achieved the status of a shared university-wide research facility.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/28/2022\n\n\t\t\t\t\tSubmitted by: Richard T Jones"
 }
}