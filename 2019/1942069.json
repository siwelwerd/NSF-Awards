{
 "awd_id": "1942069",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Deep Architectures for Ppredicting 3D Object Motion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 175406.0,
 "awd_amount": 175406.0,
 "awd_min_amd_letter_date": "2019-08-16",
 "awd_max_amd_letter_date": "2019-08-16",
 "awd_abstract_narration": "Our everyday living environments are populated with lots of functional objects with which we can interact through their moving parts (e.g., swivel chairs, laptops, bikes and cars, to name just a few). For autonomous agents to correctly interact with these objects in real-world settings, the agents must be equipped with algorithms that are able to parse the objects into their moving parts. But that is not enough.  Through the widespread use of commodity 3D sensors and modern 3D modeling techniques, large repositories (such as ShapeNet) containing millions of digital representations of everyday objects are now available, but these representations are for the most part currently static, that is to say they represent single snapshots of objects. To make use of these object representations in dynamic, virtual environments and in animation applications, methods that automatically segment them into moving parts and synthesize plausible motions for them are needed. This project will explore the design, implementation, and testing of new deep learning architectures that accomplish this, and thereby bring large portions of static 3D datasets \"to life.\" The new algorithms will have broad industrial impact by advancing 3D modeling and animation software, while the generated motion data will be useful for training new computer vision algorithms for object motion recognition and tracking in videos.\r\n\r\nAchieving the project goals will require development of new algorithms to convert static digital representations of 3D objects into dynamic ones by automatically recognizing their moving parts and animating them based on input reference videos of similar objects from the real world and through incorporation of novel methods for estimating partial 2D-3D correspondences, for lifting 2D motion cues to 3D, and for inferring motion rigs for 3D shapes. The project will be organized into two main thrusts, each of which will present its own research challenges.  The first thrust will investigate new deep learning architectures for performing mobility-based segmentation of 3D objects and predicting the underlying motion of their parts. The architecture will be applied to man-made objects with rigidly moving parts.  This part of the research will be carried out in the first year of the project.  The second thrust will extend the previous work to animate 3D models representing living organisms such as quadrupeds, birds and fish (i.e., animals from the DigitalLife dataset). These models undergo non-rigid deformations, so the architecture will have to be modified to estimate and control more sophisticated deformation primitives. This part of the work will be executed in the second year of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Evangelos",
   "pi_last_name": "Kalogerakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Evangelos Kalogerakis",
   "pi_email_addr": "kalo@cs.umass.edu",
   "nsf_id": "000630222",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010359450",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 175406.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With the emergence of 3D social media, massive online games, and virtual worlds, the need for diverse, high-quality, animation-ready objects and avatars is greater than ever. Despite the fact that we currently have huge repositories of 3D models (e.g., ShapeNet), the vast majority of these object representations are currently static, or in other words, represent single snapshots of 3D objects without any motion-related attributes or information. This proposal aimed to develop new algorithms that are able to synthesize both motion and animation-related primitives for input static models of objects and characters, so that they are brought&nbsp; \"into life\" i.e., they are animated automatically.<br /><br />The traditional animation workflow in computer graphics involves \"rigging\" an object with animation primitives, then manipulating the primitives to move the object parts. A commonly used type of animation primitive is an \"animation skeleton\". Animation skeletons consist of a series of virtual \"bones\" connected at joints, acting as handles used for moving articulated parts of the object. Rigging involves creating these animation skeletons, and also attaching the surface of the 3D model to the bones (a process called \"skinning\"), such that when bones move, the surface of the object also moves accordingly. Rigging and animation are&nbsp; both time-consuming and costly: artists need to manually design the animation skeleton, specify bone influences (\"skinning weights\") on the surface of the 3D model, then manipulate the animation skeleton at key frames (e.g., specify desired joint rotations or desired joint positions). <br /><br />The proposal led to the development of two new algorithms that significantly automate character rigging and animation:<br /><br />(a) the PI&rsquo;s research team and collaborators developed RigNet (Figure 1), a new neural network method that automatically predicts both animation skeletons and skinning for input 3D meshes of characters, effectively yielding an end-to-end solution to the fundamentally important and challenging problem of character rigging.&nbsp; In contrast to prior work that fits pre-defined skeletal templates with hand-tuned objectives, RigNet is able to automatically rig diverse characters, such as humanoids, bipeds, quadrupeds, toys, birds, with varying articulation structure and geometry, producing state-of-the-art rigging results. RigNet is published at SIGGRAPH 2020, a top-tier computer graphics conference, and has attracted significant attention from both animation communities and industry. We also refer readers to our supplementary video demonstrating our method: https://youtu.be/J90VETgWIDg<br /><br />(b) the PI&rsquo;s research team and collaborators also developed a method, called MakeItTalk (Figure 2), that is able to generate talking-head animations from a single image driven only by human speech as input. MakeItTalk can animate a wide variety of image inputs, including photos, artistic paintings, sketches, 2D cartoon characters, and stylized caricatures. The generated talking heads have significantly higher quality compared to previous methods. The paper was published at SIGGRAPH ASIA 2020,&nbsp; a top-tier computer graphics conference. Part of this research project is already incorporated in popular animation software (Adobe's Character Animator). We also refer readers to our supplementary video demonstrating our method: <br />https://youtu.be/vUMGKASgbf8</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2021<br>\n\t\t\t\t\tModified by: Evangelos&nbsp;Kalogerakis</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1942069/1942069_10634963_1640752322743_img1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1942069/1942069_10634963_1640752322743_img1--rgov-800width.jpg\" title=\"RigNet\"><img src=\"/por/images/Reports/POR/2021/1942069/1942069_10634963_1640752322743_img1--rgov-66x44.jpg\" alt=\"RigNet\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Given a 3D object, RigNet produces an animation skeleton tailored to the articulation structure of the input object. From left to right: input examples of test 3D meshes, predicted skeletons for each of them, and resulting 3D characters under different skeletal poses.</div>\n<div class=\"imageCredit\">Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, Karan Singh</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Evangelos&nbsp;Kalogerakis</div>\n<div class=\"imageTitle\">RigNet</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1942069/1942069_10634963_1640752461308_img2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1942069/1942069_10634963_1640752461308_img2--rgov-800width.jpg\" title=\"MakeItTalk\"><img src=\"/por/images/Reports/POR/2021/1942069/1942069_10634963_1640752461308_img2--rgov-66x44.jpg\" alt=\"MakeItTalk\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Given an audio speech signal and a single portrait image as input (left), MakeItTalk generates speaker-aware talking-head animations (right). Our method creates both non-photorealistic cartoon animations (top)and natural human face videos (bottom).</div>\n<div class=\"imageCredit\">Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, Dingzeyu Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Evangelos&nbsp;Kalogerakis</div>\n<div class=\"imageTitle\">MakeItTalk</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWith the emergence of 3D social media, massive online games, and virtual worlds, the need for diverse, high-quality, animation-ready objects and avatars is greater than ever. Despite the fact that we currently have huge repositories of 3D models (e.g., ShapeNet), the vast majority of these object representations are currently static, or in other words, represent single snapshots of 3D objects without any motion-related attributes or information. This proposal aimed to develop new algorithms that are able to synthesize both motion and animation-related primitives for input static models of objects and characters, so that they are brought  \"into life\" i.e., they are animated automatically.\n\nThe traditional animation workflow in computer graphics involves \"rigging\" an object with animation primitives, then manipulating the primitives to move the object parts. A commonly used type of animation primitive is an \"animation skeleton\". Animation skeletons consist of a series of virtual \"bones\" connected at joints, acting as handles used for moving articulated parts of the object. Rigging involves creating these animation skeletons, and also attaching the surface of the 3D model to the bones (a process called \"skinning\"), such that when bones move, the surface of the object also moves accordingly. Rigging and animation are  both time-consuming and costly: artists need to manually design the animation skeleton, specify bone influences (\"skinning weights\") on the surface of the 3D model, then manipulate the animation skeleton at key frames (e.g., specify desired joint rotations or desired joint positions). \n\nThe proposal led to the development of two new algorithms that significantly automate character rigging and animation:\n\n(a) the PI\u2019s research team and collaborators developed RigNet (Figure 1), a new neural network method that automatically predicts both animation skeletons and skinning for input 3D meshes of characters, effectively yielding an end-to-end solution to the fundamentally important and challenging problem of character rigging.  In contrast to prior work that fits pre-defined skeletal templates with hand-tuned objectives, RigNet is able to automatically rig diverse characters, such as humanoids, bipeds, quadrupeds, toys, birds, with varying articulation structure and geometry, producing state-of-the-art rigging results. RigNet is published at SIGGRAPH 2020, a top-tier computer graphics conference, and has attracted significant attention from both animation communities and industry. We also refer readers to our supplementary video demonstrating our method: https://youtu.be/J90VETgWIDg\n\n(b) the PI\u2019s research team and collaborators also developed a method, called MakeItTalk (Figure 2), that is able to generate talking-head animations from a single image driven only by human speech as input. MakeItTalk can animate a wide variety of image inputs, including photos, artistic paintings, sketches, 2D cartoon characters, and stylized caricatures. The generated talking heads have significantly higher quality compared to previous methods. The paper was published at SIGGRAPH ASIA 2020,  a top-tier computer graphics conference. Part of this research project is already incorporated in popular animation software (Adobe's Character Animator). We also refer readers to our supplementary video demonstrating our method: \nhttps://youtu.be/vUMGKASgbf8\n\n\t\t\t\t\tLast Modified: 12/29/2021\n\n\t\t\t\t\tSubmitted by: Evangelos Kalogerakis"
 }
}