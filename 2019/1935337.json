{
 "awd_id": "1935337",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Coordination of Dyadic Object Handover for Human-Robot Interactions",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032927557",
 "po_email": "amedinab@nsf.gov",
 "po_sign_block_name": "Alexandra Medina-Borja",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 760262.0,
 "awd_amount": 760262.0,
 "awd_min_amd_letter_date": "2019-08-23",
 "awd_max_amd_letter_date": "2019-08-23",
 "awd_abstract_narration": "The research objective of this project is to improve the fluidity of human-robot interactions wherein robots and humans can seamlessly pass objects between one another. Object handover is critical to everyday interactions, whether in ordinary environments or in high-stakes circumstances such as operating rooms. Seemingly effortless object handover results from successful inference and anticipation of shared intentions and actions. Manual object transfer between humans and robots will become increasingly important as robots become more common in the workplace and at home. The project team will perform human subject experiments investigating human-human and human-robot interactions within the context of object handover tasks to identify characteristics of dyadic coordination that allow people to understand their collaborator's intentions, to anticipate their actions, and to coordinate movements leading to task success. The team will use that new knowledge to develop robots that people can collaborate with on physical tasks as readily as they do other humans. Broader Impacts of the project include training opportunities for high school, undergraduate, and graduate students, with efforts to increase participation of underrepresented groups.\r\n\r\nThis project explores human and robotic perception, behavior, and intent inference in a bidirectional and integrated manner within the context of object handover. Three specific aims are planned. The first uses motion capture, eye tracking, and electroencephalography data to build models of human intent and action during object handover. A novelty of the models is that they are sensitive to an individual's role (giver vs. receiver; leader vs. follower), to the presence or absence of communicative gaze, and to the degree of predictability of certain aspects of handover, such as grasp type, locus of handover, gaze conditions, and dyadic role. The second aim will develop a real-time intent inference engine that uses recursive Bayesian state estimation to obtain a probabilistic assessment of human intent as the handover action evolves in time. The output of this model will inform the robot's trajectory planner, thereby enabling it to make short time horizon predictions of human actions and to adjust robot motion plans accordingly in real-time. The third aim will examine how specific choices in the high-level planning and low-level control of robot motion impact human inference of robotic intent and action during object handover. If successful, this project will advance understanding of robot manipulation during human-robot handover and yield algorithms for achieving advanced autonomy during human collaboration with humanoid robots.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eugene",
   "pi_last_name": "Tunik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eugene Tunik",
   "pi_email_addr": "e.tunik@northeastern.edu",
   "nsf_id": "000734645",
   "pi_start_date": "2019-08-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Deniz",
   "pi_last_name": "Erdogmus",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Deniz Erdogmus",
   "pi_email_addr": "erdogmus@ece.neu.edu",
   "nsf_id": "000483728",
   "pi_start_date": "2019-08-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Taskin",
   "pi_last_name": "Padir",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Taskin Padir",
   "pi_email_addr": "t.padir@northeastern.edu",
   "nsf_id": "000531489",
   "pi_start_date": "2019-08-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mathew",
   "pi_last_name": "Yarossi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mathew Yarossi",
   "pi_email_addr": "m.yarossi@northeastern.edu",
   "nsf_id": "000802208",
   "pi_start_date": "2019-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave.",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "058Y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "070E",
   "pgm_ref_txt": "INTEG OF HUMAN & COGNITIVE"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 760262.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to bring human-robot collaboration for object handover to new levels of performances, where the bi-directional interaction between the pair is holistic and intuitive. To achieve this goal, we: (i) performed a systematic investigation to identify the spatiotemporal characteristics of dyadic coordination that allow us to understand our collaborator&rsquo;s intentions, anticipate their actions, and coordinate our movements with theirs during object handover tasks, and (ii) operationalized this knowledge to develop robots with whom we can collaborate on physical tasks as readily as we do with humans. We performed empirical data analysis of hand kinematics and kinetics, eye tracking, and EEG data, along with computer vision and computational modeling approaches, across three aims to study object handover between human dyads, as well as between human-robot dyads when the human had to infer the human&rsquo;s intent. The principal finding between human-human dyads was that the interaction for object handover was extremely precise, coming down to a smooth hand-off of the object within 50 ms at the time when each individual&rsquo;s hands approached the other&rsquo;s. There was some variability in this timing depending on whether the giver or the receiver initiated the movement, suggesting that there are likely different cues used for motor planning under each of these conditions. Second, we found that the kinematics of initiator&rsquo;s actions could be decomposed into a primary movement that brought the hand most of the way to the location of handover, and between 1-4 submovements that adjusted the rest of the trajectory to the final handover location. We leveraged these kinematic features to build a Gaussian Process inference model that used a prior history of handover trials, and a variable kernel &lsquo;lengthscale&rsquo;. The lengthscale parameter controlled the width of submovements that were predicted in the initiator giver&rsquo;s velocity profile. Larger lengthscales resulted in the model predicting broad, smooth submovements, while smaller lengthscales resulted in the model predicting narrow, sharp submovements. We also incorporated a spacing between submovements to determine the width of submovements predicted by the inference model. A sigmoidal function was fit to the submovement spacing as a function of distance to the object. Finally, we incorporated a coordinate transformation that transformed all marker coordinates into the local coordinate system of the initiator. This ego-centric coordinate system is centered at the midpoint between the initiator&rsquo;s C7 vertebra and sternum, with the positive X axis projecting forwards out of the subject&rsquo;s chest. This transformation made the human inference model generalizable to different handover environments. The trajectory model (for the robot) was tested using the output of the human inference model. The model trajectories aligned spatially with ground truth follower trajectories within 5cm for all target locations. Ground truth follower trajectories exhibited rightward and upwards biases early in the movement while model trajectories followed a straight-line path to the handover location. These differences resulted in small deviations between corresponding trajectories. Model trajectories also aligned with ground truth follower trajectories in time when broken down into X, Y, and Z components. Deviations between model and ground truth trajectories were less than 5cm in every component. The largest differences occurred at the beginning and end of each trajectory component. Ground truth follower trajectories tended to begin earlier and end later than model trajectories, however these patterns depended on handover location. For example, in the Y dimension for right side handover locations, model trajectories started later than ground truth trajectories but aligned before handover time. For left side handover locations, the model and ground truth were aligned at the beginning of the trial, but model trajectories arrived at the handover location earlier. In all, the robot&rsquo;s real-time trajectory planner performed quite well in adjusting to the proper location of handover even when the human collaborator decided to switch the hand path to another location mid-way into the movement, indicating an impressive degree of resilience in the planner.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/23/2024<br>\nModified by: Eugene&nbsp;Tunik</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project was to bring human-robot collaboration for object handover to new levels of performances, where the bi-directional interaction between the pair is holistic and intuitive. To achieve this goal, we: (i) performed a systematic investigation to identify the spatiotemporal characteristics of dyadic coordination that allow us to understand our collaborators intentions, anticipate their actions, and coordinate our movements with theirs during object handover tasks, and (ii) operationalized this knowledge to develop robots with whom we can collaborate on physical tasks as readily as we do with humans. We performed empirical data analysis of hand kinematics and kinetics, eye tracking, and EEG data, along with computer vision and computational modeling approaches, across three aims to study object handover between human dyads, as well as between human-robot dyads when the human had to infer the humans intent. The principal finding between human-human dyads was that the interaction for object handover was extremely precise, coming down to a smooth hand-off of the object within 50 ms at the time when each individuals hands approached the others. There was some variability in this timing depending on whether the giver or the receiver initiated the movement, suggesting that there are likely different cues used for motor planning under each of these conditions. Second, we found that the kinematics of initiators actions could be decomposed into a primary movement that brought the hand most of the way to the location of handover, and between 1-4 submovements that adjusted the rest of the trajectory to the final handover location. We leveraged these kinematic features to build a Gaussian Process inference model that used a prior history of handover trials, and a variable kernel lengthscale. The lengthscale parameter controlled the width of submovements that were predicted in the initiator givers velocity profile. Larger lengthscales resulted in the model predicting broad, smooth submovements, while smaller lengthscales resulted in the model predicting narrow, sharp submovements. We also incorporated a spacing between submovements to determine the width of submovements predicted by the inference model. A sigmoidal function was fit to the submovement spacing as a function of distance to the object. Finally, we incorporated a coordinate transformation that transformed all marker coordinates into the local coordinate system of the initiator. This ego-centric coordinate system is centered at the midpoint between the initiators C7 vertebra and sternum, with the positive X axis projecting forwards out of the subjects chest. This transformation made the human inference model generalizable to different handover environments. The trajectory model (for the robot) was tested using the output of the human inference model. The model trajectories aligned spatially with ground truth follower trajectories within 5cm for all target locations. Ground truth follower trajectories exhibited rightward and upwards biases early in the movement while model trajectories followed a straight-line path to the handover location. These differences resulted in small deviations between corresponding trajectories. Model trajectories also aligned with ground truth follower trajectories in time when broken down into X, Y, and Z components. Deviations between model and ground truth trajectories were less than 5cm in every component. The largest differences occurred at the beginning and end of each trajectory component. Ground truth follower trajectories tended to begin earlier and end later than model trajectories, however these patterns depended on handover location. For example, in the Y dimension for right side handover locations, model trajectories started later than ground truth trajectories but aligned before handover time. For left side handover locations, the model and ground truth were aligned at the beginning of the trial, but model trajectories arrived at the handover location earlier. In all, the robots real-time trajectory planner performed quite well in adjusting to the proper location of handover even when the human collaborator decided to switch the hand path to another location mid-way into the movement, indicating an impressive degree of resilience in the planner.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 12/23/2024\n\n\t\t\t\t\tSubmitted by: EugeneTunik\n"
 }
}