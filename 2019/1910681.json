{
 "awd_id": "1910681",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF:  RI: Small: Barriers in Adversarially Robust Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-06-27",
 "awd_max_amd_letter_date": "2019-06-27",
 "awd_abstract_narration": "Learning algorithms are increasingly taking on roles that were previously held by humans. Examples include face recognition, malware detection, making decisions about loans or bail, etc. Learning algorithms, however, are usually sensitive to adversarial manipulations happening during training or decision time. Due to the sensitivity of the contexts in which these algorithms are used, it is crucial to understand the power and limitations of provably robust methods in such adversarial contexts. The goal of this project is to study adversarial robustness from a provable perspective and identify the barriers that might exist against it. The project will build connections to other areas such as computational complexity as well as cryptography. The project also involves mentoring PhD students. The findings will be incorporated into newly designed courses and will be disseminated via workshops, conferences, and journals.\r\n\r\nThe project, more specifically, will focus on two parts that enable the main goals outlined above. The first part is to model adversarially robust learning formally to enable a provable approach. Indeed, Cryptography has benefited tremendously from such mathematically rigorous approach to security, and to reach similar results, adversarially robust learning needs a similar definitional approach that models subtle aspects of the attack such as: the computational complexity of the attacker, its precise knowledge, and the role of randomness. The second part of this project aims at identifying barriers that exist against provable robustness for adversarial learning. This project will study barriers against both information theoretic (a.k.a. statistic) as well as computational security. Information theoretic security models the adversary as an all powerful entity, while the more realistic model of computational security, which is widely used in Cryptography, models the attacker as a polynomial-time algorithm. Identifying these barriers is an essential part of designing optimally robust learning methods.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohammad",
   "pi_last_name": "Mahmoody Ghidary",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohammad Mahmoody Ghidary",
   "pi_email_addr": "mohammad@virginia.edu",
   "nsf_id": "000649947",
   "pi_start_date": "2019-06-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Virginia",
  "perf_str_addr": "PO BOX 400740 85 Engineer's Way",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044740",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-643960e6-7fff-d493-a0ce-6dcb93032a38\"><br /><br /> </span></p>\n<p dir=\"ltr\"><span>The main goal of this project was to study adversarial robustness from a provable perspective and identify possible barriers against it. Below, we elaborate on some of the key findings of the projects supported by this grant. Before that, we mention the summary of the outcomes.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Students and mentoring:&nbsp;</span></p>\n<p dir=\"ltr\"><span>With the help of this grant, the PI supported the research of three PHD students and one postdoctoral researcher.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Publications:</span></p>\n<p dir=\"ltr\"><span>With the valuable help of this grant, the PI published 12 papers in top conferences: SODA &lsquo;20, ICMLA &lsquo;20, NeurIPS &lsquo;19, ALT &lsquo;20, UAI &lsquo;21, IEEE S&amp;P &lsquo;21, PETS &lsquo;22, NeurIPS &lsquo;21 (spotlight paper), TCC&rsquo;21, NeurIPS &lsquo;22 (two papers, one selected for oral presentation) and CCS &lsquo;23.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Now, we mention the key research outcomes.</span></p>\n<p>&nbsp;</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Building connections to cryptography: A key question of this project was to understand the power of data poisoning attacks in machine learning. We first identified a deep connection between such attacks on measuring concentration in high dimensions. Then, in a series of works (SODA &lsquo;20, NeurIPS &rsquo;19, ALT &lsquo;20, TCC &lsquo;21) we studied the computational aspects of this connection, leading to studies in computational complexity of attacks on collective coin flipping, which is a classical problem in cryptography. In a nutshell, improvement on such attacks (of specific forms) would be translated into poisoning attacks on machine learning models.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Finding optimal parameters. Another line of work supported by this project (ICMLA &lsquo;20, UAI &lsquo;21, NeurIPS &lsquo;22) we identified the optimal error (vs sample complexity) parameter of learning under data poisoning, in the &ldquo;targeted&rdquo; setting. In targeted poisoning, the adversary plans their whole attack focus on the specific test instance that they care about).</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Privacy of ML: In our works (PETS &lsquo;22, S&amp;P &lsquo;21 and a PPML workshop paper), we presented attacks on privacy of heuristic (or standard) learning methods that aim to improve upon what differentially private mechanisms can offer with the goal of more utility and efficiency.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Beyond original goals: we also established connections between robustness of models to overparameterization (NeurIPS &lsquo;22) and also studied the robustness of training to adversarial behavior (proof of learning &ndash; CCS &lsquo;23).</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In summary, the above outcomes build the foundation for many possible follow-up studies at the core of robust machine learning, with the goal of building models and processes (e.g., for training models) that can be further trusted.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/29/2024<br>\nModified by: Mohammad&nbsp;Mahmoody Ghidary</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n \n\n\nThe main goal of this project was to study adversarial robustness from a provable perspective and identify possible barriers against it. Below, we elaborate on some of the key findings of the projects supported by this grant. Before that, we mention the summary of the outcomes.\n\n\n\n\n\nStudents and mentoring:\n\n\nWith the help of this grant, the PI supported the research of three PHD students and one postdoctoral researcher.\n\n\n\n\n\nPublications:\n\n\nWith the valuable help of this grant, the PI published 12 papers in top conferences: SODA 20, ICMLA 20, NeurIPS 19, ALT 20, UAI 21, IEEE S&P 21, PETS 22, NeurIPS 21 (spotlight paper), TCC21, NeurIPS 22 (two papers, one selected for oral presentation) and CCS 23.\n\n\n\n\n\nNow, we mention the key research outcomes.\n\n\n\n\n\n\n\nBuilding connections to cryptography: A key question of this project was to understand the power of data poisoning attacks in machine learning. We first identified a deep connection between such attacks on measuring concentration in high dimensions. Then, in a series of works (SODA 20, NeurIPS 19, ALT 20, TCC 21) we studied the computational aspects of this connection, leading to studies in computational complexity of attacks on collective coin flipping, which is a classical problem in cryptography. In a nutshell, improvement on such attacks (of specific forms) would be translated into poisoning attacks on machine learning models.\n\n\n\n\nFinding optimal parameters. Another line of work supported by this project (ICMLA 20, UAI 21, NeurIPS 22) we identified the optimal error (vs sample complexity) parameter of learning under data poisoning, in the targeted setting. In targeted poisoning, the adversary plans their whole attack focus on the specific test instance that they care about).\n\n\n\n\nPrivacy of ML: In our works (PETS 22, S&P 21 and a PPML workshop paper), we presented attacks on privacy of heuristic (or standard) learning methods that aim to improve upon what differentially private mechanisms can offer with the goal of more utility and efficiency.\n\n\n\n\nBeyond original goals: we also established connections between robustness of models to overparameterization (NeurIPS 22) and also studied the robustness of training to adversarial behavior (proof of learning  CCS 23).\n\n\n\n\n\n\n\nIn summary, the above outcomes build the foundation for many possible follow-up studies at the core of robust machine learning, with the goal of building models and processes (e.g., for training models) that can be further trusted.\n\n\n\t\t\t\t\tLast Modified: 01/29/2024\n\n\t\t\t\t\tSubmitted by: MohammadMahmoody Ghidary\n"
 }
}