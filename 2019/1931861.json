{
 "awd_id": "1931861",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Small: Collaborative Research: RF Sensing for Sign Language Driven Smart Environments",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 133002.0,
 "awd_amount": 133002.0,
 "awd_min_amd_letter_date": "2019-08-05",
 "awd_max_amd_letter_date": "2019-08-05",
 "awd_abstract_narration": "Deaf individuals who rely on American Sign Language (ASL) as their primary mode of communication heavily rely on technology as an assistive device. Yet, many technologies are designed for hearing individuals, which precludes the Deaf community from benefiting from advances, which, if designed to be compatible with ASL, could in fact generate tangible improvements in their quality of life. This proposal aims at transforming ubiquitous sensing technologies through the integration of a new sensing modality - radio frequency (RF) sensing - into smart environments designed to respond to the needs of ASL users. RF sensors are uniquely desirable for this application because they are non-contact, can operate in the dark or through-the-wall, protect privacy, and bring to bear a new type of information that will aid in ASL understanding: namely, the micro-Doppler signature, which is reflective of the time-varying velocity profiles of motion.  Thus, RF sensing is uniquely suited to capture the rapid progression of dynamic sign sequences that is characteristic of ASL usage.  This collaborative project not only brings to bear, for the first time, a linguistic perspective to RF-based motion recognition, but also a physics-based machine learning approach achieved through integration of kinematics with deep learning.  In this way, the project aims at 1) improving ASL recognition technologies and the design of smart environments for deaf individuals, 2) augmenting the tools linguists use to analyze language and related cognitive processes, and 3) advancing machine learning approaches specifically geared towards RF signal classification.\r\n\r\n     The project is focused on developing signal processing algorithms for leveraging the unique aspects of RF sensing towards understanding of ASL and related linguistic features.  More specifically, three aspects of ASL recognition are considered: classification of pre-defined ASL words and phrases, design of RF-sensing based dynamic sequence segmentation algorithms, and differentiation of daily activities from communicative sign language gestures. Novel ways of visualizing and representing RF data in one, two, and three dimensions will be investigated, both for extraction of linguistic features and as inputs to deep neural networks.   Novel techniques will be developed for classification of three-dimensional time-varying data streams, the generation of synthetic RF data samples that have improved kinematic fidelity and realism, sequential classification and segmentation, as well as discrimination of daily motion from communicative signing. The critical experiments conducted during this project will result in a one-of-a-kind dataset of multi-frequency RF sensor network and Kinect(tm) sensor measurements of ASL signs, which will be made publicly available.  The project directly engages the Deaf community through support and interaction of the Alabama Institute of Deaf and Blind (AIDB) and Gallaudet University as part of a needs-driven approach to communicative and assistive technology design, which will ultimately serve personal, professional, and educational needs of the Deaf community.\r\n\r\nThis project is jointly funded by the Cyber Physical Systems Program and the Established Program to Stimulate Competitive Research (EPSCoR).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ali",
   "pi_last_name": "Gurbuz",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Ali C Gurbuz",
   "pi_email_addr": "aligurbuz@ncsu.edu",
   "nsf_id": "000759102",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Mississippi State University",
  "inst_street_address": "245 BARR AVE",
  "inst_street_address_2": "",
  "inst_city_name": "MISSISSIPPI STATE",
  "inst_state_code": "MS",
  "inst_state_name": "Mississippi",
  "inst_phone_num": "6623257404",
  "inst_zip_code": "39762",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MS03",
  "org_lgl_bus_name": "MISSISSIPPI STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTXJM52SHKS7"
 },
 "perf_inst": {
  "perf_inst_name": "Mississippi State University",
  "perf_str_addr": "PO Box 9571",
  "perf_city_name": "Mississippi State",
  "perf_st_code": "MS",
  "perf_st_name": "Mississippi",
  "perf_zip_code": "397629571",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MS03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 133002.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Deaf&nbsp; individuals&nbsp; who&nbsp; rely&nbsp; on&nbsp; American&nbsp; Sign&nbsp; Language&nbsp; (ASL)&nbsp; as&nbsp; their&nbsp; primary&nbsp; mode&nbsp; of&nbsp; communication &nbsp;heavily rely on technology as an assistive device. Yet, many technologies are designed for hearing individuals, which precludes the Deaf community from benefiting from advances, which, if designed to be compatible with ASL, could in fact generate tangible improvements in their quality of life.</p>\n<p>This project aims at transforming ubiquitous sensing&nbsp; technologies&nbsp; through&nbsp; the&nbsp; integration&nbsp; of&nbsp; a&nbsp; new&nbsp; sensing&nbsp; modality&nbsp; -&nbsp; radio&nbsp; frequency&nbsp; (RF) &nbsp;sensing - into smart environments designed to respond to the needs of ASL users. RF sensors are uniquely desirable for this application because they&nbsp; are&nbsp; non-contact,&nbsp; can&nbsp; operate&nbsp; in&nbsp; the&nbsp; dark&nbsp; or&nbsp; through-the-wall, protect privacy, and bring to bear a new type of information that will aid in ASL understanding: namely, the micro-Doppler signature, which is reflective of the time-varying velocity profiles of motion. Thus, RF sensing is uniquely suited to capture the rapid progression of dynamic sign sequences that is characteristic of ASL usage. This collaborative project not only brings to bear, for the first time, a linguistic perspective to RF-based motion recognition,&nbsp; but&nbsp; also&nbsp; a&nbsp; physics-based&nbsp; machine&nbsp; learning&nbsp; approach&nbsp; achieved&nbsp; through&nbsp; integration&nbsp; of &nbsp;kinematics with deep learning. In this way, the project aims at 1) improving ASL recognition technologies and the&nbsp; design&nbsp; of&nbsp; smart&nbsp; environments&nbsp; for&nbsp; deaf&nbsp; individuals,&nbsp; 2)&nbsp; augmenting&nbsp; the&nbsp; tools&nbsp; linguists&nbsp; use&nbsp; to&nbsp; analyze &nbsp;language and related cognitive processes, and 3) advancing machine learning approaches specifically geared towards RF signal classification. In addition to many publications and a patent our work has generated several important and fundamental contributions:</p>\n<p>1)&nbsp;&nbsp;&nbsp; We have developed a novel, physics-aware machine learning approach for the generation of synthetic data, which can augment&nbsp; real&nbsp; data&nbsp; samples,&nbsp; for&nbsp; the&nbsp; training&nbsp; of&nbsp; deep&nbsp; models&nbsp; for&nbsp; sign&nbsp; language&nbsp; recognition.&nbsp; The proposed approach has resulted in ASL recognition rates for 100 ASL signs recorded by RF sensors to 77% top-1 accuracy and 93% top-5 accuracy. These results surpass that attained by high-performing video-based ASL recognition techniques, such as Temporal Graph Convolutional Network (TGCN) and the Sign Language Graph Convolutional Network (SLGCN), which result in 49.5% and 58.28% top-1 accuracy and 81.3% and 90.3% top-5 accuracy when applied to the same 100 ASL signs provided in publicly available video databases.</p>\n<p>2)&nbsp;&nbsp;&nbsp; We have shown that the fluency level of the signer is a significant factor in the training of deep models for ASL recognition. More specifically, the use of copy signers for validation of ASL recognition algorithms can over-optimistically generate results over 20% higher than when validated on data from fluent signers. This is significant as there are works in the literature utilizing video and Kinect data, which rely on copy signing data for validation. We found that the direct physics-aware synthesis techniques outperformed domain adaptation of copy signing data by 14% - underscoring the significance of kinematics and fluency in ASL recognition.</p>\n<p>3)&nbsp;&nbsp;&nbsp; We have shown that ASL-related movements can be discriminated from that of daily activities using fractal complexity. We developed an approach to segment and identify sequences of ASL signs when mixed with daily activities. This is of practical significance when considering the design of an RF-based ASL sensitive personal assistant. We also showed that 15 different trigger signs could be detected at a rate of 98%.</p>\n<p>4)&nbsp;&nbsp;&nbsp; We have shown that modality tuning with RF sensors in a 3-sensor multi-frequency network can boost the ASL recognition rates of 20 ASL signs from 75% (with decision fusion) to 95.5% (proposed approach) using a small number of real samples from fluent signers acquired from each RF sensor.</p>\n<p>The proposed technical solutions pave the way for RF sensing enabled human-computer interactions in a broad range of applications, ranging from non-contact control of user interfaces, CPHS for health and safety, and automotive autonomy.&nbsp; The advancement developed in this project presents RF sensing as a new tool for linguistic studies of sign language. Our results fundamentally demonstrates the initial capabilities of an &nbsp;ASL sensitive radio frequency (RF) sensor-enabled cyber-physical human system(CPHS) to facilitate the interaction of Deaf/Hard-of Hearing individuals fluent in ASL with assistive devices and smart environments. For all project related outputs, the project website at <a href=\"https://ci4r.ua.edu/asl-recognition-with-radar.html\">https://ci4r.ua.edu/asl-recognition-with-radar.html</a> can be visited.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/30/2023<br>\n\t\t\t\t\tModified by: Ali&nbsp;C&nbsp;Gurbuz</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1931861/1931861_10630066_1675055460935_abstractgraphic_orig--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1931861/1931861_10630066_1675055460935_abstractgraphic_orig--rgov-800width.jpg\" title=\"RF Sensing of ASL\"><img src=\"/por/images/Reports/POR/2023/1931861/1931861_10630066_1675055460935_abstractgraphic_orig--rgov-66x44.jpg\" alt=\"RF Sensing of ASL\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Sensing of ASL by radar sensors and time-frequency radar data representation for different signs</div>\n<div class=\"imageCredit\">https://ci4r.ua.edu/asl-recognition-with-radar.html</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Ali&nbsp;C&nbsp;Gurbuz</div>\n<div class=\"imageTitle\">RF Sensing of ASL</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1931861/1931861_10630066_1675055246552_asl-tech--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1931861/1931861_10630066_1675055246552_asl-tech--rgov-800width.jpg\" title=\"ASL Technologies\"><img src=\"/por/images/Reports/POR/2023/1931861/1931861_10630066_1675055246552_asl-tech--rgov-66x44.jpg\" alt=\"ASL Technologies\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Comparison of Sign language Recognition technologies</div>\n<div class=\"imageCredit\">https://ci4r.ua.edu/asl-recognition-with-radar.html</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Ali&nbsp;C&nbsp;Gurbuz</div>\n<div class=\"imageTitle\">ASL Technologies</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nDeaf  individuals  who  rely  on  American  Sign  Language  (ASL)  as  their  primary  mode  of  communication  heavily rely on technology as an assistive device. Yet, many technologies are designed for hearing individuals, which precludes the Deaf community from benefiting from advances, which, if designed to be compatible with ASL, could in fact generate tangible improvements in their quality of life.\n\nThis project aims at transforming ubiquitous sensing  technologies  through  the  integration  of  a  new  sensing  modality  -  radio  frequency  (RF)  sensing - into smart environments designed to respond to the needs of ASL users. RF sensors are uniquely desirable for this application because they  are  non-contact,  can  operate  in  the  dark  or  through-the-wall, protect privacy, and bring to bear a new type of information that will aid in ASL understanding: namely, the micro-Doppler signature, which is reflective of the time-varying velocity profiles of motion. Thus, RF sensing is uniquely suited to capture the rapid progression of dynamic sign sequences that is characteristic of ASL usage. This collaborative project not only brings to bear, for the first time, a linguistic perspective to RF-based motion recognition,  but  also  a  physics-based  machine  learning  approach  achieved  through  integration  of  kinematics with deep learning. In this way, the project aims at 1) improving ASL recognition technologies and the  design  of  smart  environments  for  deaf  individuals,  2)  augmenting  the  tools  linguists  use  to  analyze  language and related cognitive processes, and 3) advancing machine learning approaches specifically geared towards RF signal classification. In addition to many publications and a patent our work has generated several important and fundamental contributions:\n\n1)    We have developed a novel, physics-aware machine learning approach for the generation of synthetic data, which can augment  real  data  samples,  for  the  training  of  deep  models  for  sign  language  recognition.  The proposed approach has resulted in ASL recognition rates for 100 ASL signs recorded by RF sensors to 77% top-1 accuracy and 93% top-5 accuracy. These results surpass that attained by high-performing video-based ASL recognition techniques, such as Temporal Graph Convolutional Network (TGCN) and the Sign Language Graph Convolutional Network (SLGCN), which result in 49.5% and 58.28% top-1 accuracy and 81.3% and 90.3% top-5 accuracy when applied to the same 100 ASL signs provided in publicly available video databases.\n\n2)    We have shown that the fluency level of the signer is a significant factor in the training of deep models for ASL recognition. More specifically, the use of copy signers for validation of ASL recognition algorithms can over-optimistically generate results over 20% higher than when validated on data from fluent signers. This is significant as there are works in the literature utilizing video and Kinect data, which rely on copy signing data for validation. We found that the direct physics-aware synthesis techniques outperformed domain adaptation of copy signing data by 14% - underscoring the significance of kinematics and fluency in ASL recognition.\n\n3)    We have shown that ASL-related movements can be discriminated from that of daily activities using fractal complexity. We developed an approach to segment and identify sequences of ASL signs when mixed with daily activities. This is of practical significance when considering the design of an RF-based ASL sensitive personal assistant. We also showed that 15 different trigger signs could be detected at a rate of 98%.\n\n4)    We have shown that modality tuning with RF sensors in a 3-sensor multi-frequency network can boost the ASL recognition rates of 20 ASL signs from 75% (with decision fusion) to 95.5% (proposed approach) using a small number of real samples from fluent signers acquired from each RF sensor.\n\nThe proposed technical solutions pave the way for RF sensing enabled human-computer interactions in a broad range of applications, ranging from non-contact control of user interfaces, CPHS for health and safety, and automotive autonomy.  The advancement developed in this project presents RF sensing as a new tool for linguistic studies of sign language. Our results fundamentally demonstrates the initial capabilities of an  ASL sensitive radio frequency (RF) sensor-enabled cyber-physical human system(CPHS) to facilitate the interaction of Deaf/Hard-of Hearing individuals fluent in ASL with assistive devices and smart environments. For all project related outputs, the project website at https://ci4r.ua.edu/asl-recognition-with-radar.html can be visited. \n\n \n\n\t\t\t\t\tLast Modified: 01/30/2023\n\n\t\t\t\t\tSubmitted by: Ali C Gurbuz"
 }
}