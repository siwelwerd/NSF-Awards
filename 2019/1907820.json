{
 "awd_id": "1907820",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: New Approaches for Approximation and Online Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 299999.0,
 "awd_amount": 299999.0,
 "awd_min_amd_letter_date": "2019-07-08",
 "awd_max_amd_letter_date": "2019-07-08",
 "awd_abstract_narration": "The area of approximation algorithms focuses on NP-hard optimization problems, and on obtaining fast algorithms that output solutions that are near-optimal, say in polynomial time. In the area of online algorithms, the input is revealed slowly over time, and the goal is to get good algorithms without knowing the future portions of the input. Many questions considered in this project are NP-hard, and often the resultant problems are not static problems but dynamic ones, hence these areas are central to algorithm design. This research project aims to develop new techniques in both these areas, to make progress on some long-standing open problems. For instance, it aims to develop general tools to solve convex optimization problems when the input appears online: given that convex optimization underlies many techniques in algorithms and machine learning, such results have broad applicability both within computer science and beyond. The research component of this project goes hand-in-hand with its educational and training component, which will include training both graduate and undergraduate students, and in developing new courses and materials.\r\n\r\nIn this project, the goal is to bring together hitherto disparate techniques, and use their combination to solve some central questions. For instance, many NP-hard problems have been attacked using, on one hand, the tools of randomization and convex optimization, and on the other, the perspective of fixed-parameter tractability, where the algorithm is allowed to be exponential in some parameter. The project hopes to bring these areas together, and use this synthesis of ideas to further the state-of-the-art on the k-cut partitioning problems and k-means clustering problems, among others. This fine-grained notion of approximation algorithms should lead to new structural insights into these fundamental questions. In online algorithms, the hope is to use ideas from continuous optimization and online learning, in combination with the combinatorial ideas typically used in the design of online algorithms, to make progress on problems like k-server and online convex minimization.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anupam",
   "pi_last_name": "Gupta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anupam Gupta",
   "pi_email_addr": "ag10120@nyu.edu",
   "nsf_id": "000486839",
   "pi_start_date": "2019-07-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 299999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br />The main goals of this project have been to obtain better algorithms for both approximation and online settings. The area of approximation algorithms focuses on NP-hard optimization problems, and on obtaining efficient algorithms to output near-optimal solutions. In the area of online algorithms, the input is revealed slowly over time, and the goal is to get good algorithms without knowing the future portions of the input.</p>\n<p><br />While both these areas have seen considerable successes during the past decades, with new techniques, paradigms, and frameworks being developed, there remained much to to solved. Many other fundamental problems have resisted our attempts at solving them thus far; others have seen progress without a satisfactory resolution. The goal of this project has been to revisit them using new perspectives: mainly the use of fixed-parameter algorithms, and also convex optimization and related techniques to improve the state-of-the-art. Our outcomes are substantial and heartening progress on many proposed directions. Here are some examples:</p>\n<ol>\n<li>The problem of chasing convex functions is an online version of convex optimization, where the algorithm must maintain feasible solutions as convex constraints arrive and depart over time. The goal is for the solutions to be stable, and hence to reduce the cumulative change over time. This project gave the first competitive algorithms for this problem, solving a long-standing open problem.&nbsp;&nbsp;<br /><br /></li>\n<li>For the problem of partitioning planar networks into roughly balanced parts, a long-standing open problem has been to get solutions with sparsity within a constant factor of the optimum. We gave the first algorithm for this problem that runs in close-to-polynomial time, inspired by ideas from fixed-parameter algorithms.&nbsp;<br /><br /></li>\n<li>For problems in online decision making, we considered robust \"Byzantine\" models which allow for some fraction of requests to be adversarially chosen and the rest to be chosen from a stochastic model. We showed performance bounds for these \"semi-random\" settings, using ideas from online learning.</li>\n</ol>\n<p>These and other results make progress on long-standing open problems, and propose new exciting directions for future research. The techniques developed to solve these and other problems are based on a combination of convex and discrete optimization. These will hopefully prove useful for a broad collection of discrete optimization problems, both in the offline (one-shot) and online settings.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/08/2021<br>\n\t\t\t\t\tModified by: Anupam&nbsp;Gupta</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe main goals of this project have been to obtain better algorithms for both approximation and online settings. The area of approximation algorithms focuses on NP-hard optimization problems, and on obtaining efficient algorithms to output near-optimal solutions. In the area of online algorithms, the input is revealed slowly over time, and the goal is to get good algorithms without knowing the future portions of the input.\n\n\nWhile both these areas have seen considerable successes during the past decades, with new techniques, paradigms, and frameworks being developed, there remained much to to solved. Many other fundamental problems have resisted our attempts at solving them thus far; others have seen progress without a satisfactory resolution. The goal of this project has been to revisit them using new perspectives: mainly the use of fixed-parameter algorithms, and also convex optimization and related techniques to improve the state-of-the-art. Our outcomes are substantial and heartening progress on many proposed directions. Here are some examples:\n\nThe problem of chasing convex functions is an online version of convex optimization, where the algorithm must maintain feasible solutions as convex constraints arrive and depart over time. The goal is for the solutions to be stable, and hence to reduce the cumulative change over time. This project gave the first competitive algorithms for this problem, solving a long-standing open problem.  \n\n\nFor the problem of partitioning planar networks into roughly balanced parts, a long-standing open problem has been to get solutions with sparsity within a constant factor of the optimum. We gave the first algorithm for this problem that runs in close-to-polynomial time, inspired by ideas from fixed-parameter algorithms. \n\n\nFor problems in online decision making, we considered robust \"Byzantine\" models which allow for some fraction of requests to be adversarially chosen and the rest to be chosen from a stochastic model. We showed performance bounds for these \"semi-random\" settings, using ideas from online learning.\n\n\nThese and other results make progress on long-standing open problems, and propose new exciting directions for future research. The techniques developed to solve these and other problems are based on a combination of convex and discrete optimization. These will hopefully prove useful for a broad collection of discrete optimization problems, both in the offline (one-shot) and online settings.  \n\n\t\t\t\t\tLast Modified: 12/08/2021\n\n\t\t\t\t\tSubmitted by: Anupam Gupta"
 }
}