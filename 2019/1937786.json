{
 "awd_id": "1937786",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: Towards Robust Machine Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-07-10",
 "awd_max_amd_letter_date": "2019-07-10",
 "awd_abstract_narration": "Machine learning techniques, particularly deep neural networks, are increasingly integrated into safety and security-critical applications such as autonomous driving, precision health care, intrusion detection, malware detection, and spam filtering.  A number of studies have shown that these models can be vulnerable to adversarial evasion attacks where the attacker makes small, carefully crafted changes to normal examples in order to trick the model into making incorrect decisions.  This project's goal is to develop formal understandings of and defenses against these vulnerabilities through characterizing the relationship between adversarial and non-adversarial examples, developing mechanisms that exploit this relationship to support better detection of adversarial examples, and metrics and methods to demonstrate the robustness of machine learning models against them.  Together, the  theories, algorithms, and metrics developed will improve the robustness of machine learning systems, allowing them to be deployed more securely in mission-critical applications.  The team will also make their datasets and source code publicly available and use them in their own courses and research with both graduate and undergraduate students, with particular efforts to include students from underrepresented groups in Science, Technology, Engineering and Math.  The work will also support high school outreach programs and summer camps to attract younger students to study machine learning, security, and computer science.\r\n\r\nThe project is organized around three main thrusts that combine to provide a holistic approach to modeling and defending against evasion attacks.  The first thrust aims to characterize both normal and adversarial examples via systematic measurement studies.  This includes considering different types of regions around specific examples (e.g., metric ball, manifold, and transformation-induced regions) and then characterizing the examples' vulnerability based on a number of algorithms for combining classifications of other examples in the nearby regions.  The second thrust focuses on designing robust defenses against adversarial examples by using representative data points in a region, aggregating multiple data points, and using a diverse set of classifiers to reduce the vulnerability induced by using single data points or algorithms.  The third thrust involves defining metrics for modeling robustness along with theories and algorithms that leverage those metrics to analyze model robustness.  These include lower bounds of adversarial perturbation in metric balls, robustness metrics based on computational costs, analyses of the representativeness of new datasets relative to training data, and methods for leveraging human estimation of adversarialness.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Neil",
   "pi_last_name": "Gong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Neil Gong",
   "pi_email_addr": "neil.gong@duke.edu",
   "nsf_id": "000701784",
   "pi_start_date": "2019-07-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-edc98c14-7fff-2044-3f77-440cff7ca7f6\">\n<p dir=\"ltr\"><span>This project has developed various methods to build machine learning systems that are robust against: 1) poisoning attacks, which compromise the training phase of machine learning, and 2) adversarial examples, which compromise the testing/deployment phase.</span></p>\n<p dir=\"ltr\"><strong>Intellectual Merit</strong><span>:</span><span> This project has uncovered new poisoning attacks on various machine learning systems such as federated learning, foundation models, graph-based machine learning, local differential privacy protocols, recommender systems, and crowdsourcing. For instance, it demonstrated that by sending carefully crafted model updates to the server, malicious clients can poison the model in federated learning, causing it to make incorrect predictions as the attacker desires. An attacker can spoof a recommender system to make attacker-desired recommendations (e.g., recommending an attacker-chosen item to many normal users) by injecting fake users with carefully crafted rating scores. An attacker can make a foundation model like CLIP produce attacker-desired feature vectors for specific inputs by injecting carefully crafted poisoned data into the pre-training dataset or directly modifying the model parameters.</span></p>\n<p dir=\"ltr\"><span>This project has also uncovered new adversarial examples targeting neural network classifiers, multiple object tracking, and watermark-based detectors of AI-generated content. For example, it developed image-transformation-based adversarial examples that are more challenging to defend against than Lp-norm restricted ones, and adversarial examples that are more transferable between deep neural network classifiers. It also uncovered adversarial examples to multiple object tracking via tracker hijacking, and adversarial examples that add a human-imperceptible perturbation to a watermarked image to remove the watermark, evading watermark-based detection of AI-generated content.</span></p>\n<p dir=\"ltr\"><span>Furthermore, this project has developed provably robust defenses against poisoning attacks and adversarial examples. These defenses can guarantee a lower bound of testing accuracy under arbitrary poisoning attacks or adversarial examples within certain constraints. For instance, the project demonstrated that bagging (a well-known ensemble method) and nearest neighbor (a classic machine learning method) have intrinsic provable robustness against poisoning attacks. Bagging was also extended to build federated learning and recommender systems that are provably robust against poisoning attacks. The project has also developed new machine learning classifiers whose top-k predictions are guaranteed to include the correct label for an adversarial example when the adversarial perturbation is bounded; new multi-label classifiers whose predicted labels are guaranteed to substantially overlap with the ground-truth labels for an adversarial example within a bounded perturbation; new graph neural networks whose predicted labels for the nodes in the graph are unaffected by structural perturbations once the number of modified edges is bounded; and new 3D point cloud classifiers whose predicted label for a testing point cloud is unaffected by adversarial perturbations within a bounded number of modified points.</span></p>\n<p dir=\"ltr\"><strong>Broader Impacts</strong><span>:</span><span> This project has supported six Ph.D. students, three of whom have graduated. One has joined Penn State as a tenure-track Assistant Professor, and two have joined Meta as research scientists. Additionally, the project has involved one postdoc, who will join the University of Louisville as a tenure-track Assistant Professor in August 2024, as well as four master's students, five undergraduate students, and two high school students. Both PIs have incorporated some project results into courses, including an undergraduate-level course on computer security and a graduate-level course on Machine Learning in Adversarial Settings. The PIs have also given dozens of talks on robust machine learning, some of which are publicly available on YouTube.</span></p>\n<br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/26/2024<br>\nModified by: Neil&nbsp;Gong</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nThis project has developed various methods to build machine learning systems that are robust against: 1) poisoning attacks, which compromise the training phase of machine learning, and 2) adversarial examples, which compromise the testing/deployment phase.\n\n\nIntellectual Merit: This project has uncovered new poisoning attacks on various machine learning systems such as federated learning, foundation models, graph-based machine learning, local differential privacy protocols, recommender systems, and crowdsourcing. For instance, it demonstrated that by sending carefully crafted model updates to the server, malicious clients can poison the model in federated learning, causing it to make incorrect predictions as the attacker desires. An attacker can spoof a recommender system to make attacker-desired recommendations (e.g., recommending an attacker-chosen item to many normal users) by injecting fake users with carefully crafted rating scores. An attacker can make a foundation model like CLIP produce attacker-desired feature vectors for specific inputs by injecting carefully crafted poisoned data into the pre-training dataset or directly modifying the model parameters.\n\n\nThis project has also uncovered new adversarial examples targeting neural network classifiers, multiple object tracking, and watermark-based detectors of AI-generated content. For example, it developed image-transformation-based adversarial examples that are more challenging to defend against than Lp-norm restricted ones, and adversarial examples that are more transferable between deep neural network classifiers. It also uncovered adversarial examples to multiple object tracking via tracker hijacking, and adversarial examples that add a human-imperceptible perturbation to a watermarked image to remove the watermark, evading watermark-based detection of AI-generated content.\n\n\nFurthermore, this project has developed provably robust defenses against poisoning attacks and adversarial examples. These defenses can guarantee a lower bound of testing accuracy under arbitrary poisoning attacks or adversarial examples within certain constraints. For instance, the project demonstrated that bagging (a well-known ensemble method) and nearest neighbor (a classic machine learning method) have intrinsic provable robustness against poisoning attacks. Bagging was also extended to build federated learning and recommender systems that are provably robust against poisoning attacks. The project has also developed new machine learning classifiers whose top-k predictions are guaranteed to include the correct label for an adversarial example when the adversarial perturbation is bounded; new multi-label classifiers whose predicted labels are guaranteed to substantially overlap with the ground-truth labels for an adversarial example within a bounded perturbation; new graph neural networks whose predicted labels for the nodes in the graph are unaffected by structural perturbations once the number of modified edges is bounded; and new 3D point cloud classifiers whose predicted label for a testing point cloud is unaffected by adversarial perturbations within a bounded number of modified points.\n\n\nBroader Impacts: This project has supported six Ph.D. students, three of whom have graduated. One has joined Penn State as a tenure-track Assistant Professor, and two have joined Meta as research scientists. Additionally, the project has involved one postdoc, who will join the University of Louisville as a tenure-track Assistant Professor in August 2024, as well as four master's students, five undergraduate students, and two high school students. Both PIs have incorporated some project results into courses, including an undergraduate-level course on computer security and a graduate-level course on Machine Learning in Adversarial Settings. The PIs have also given dozens of talks on robust machine learning, some of which are publicly available on YouTube.\n\n\n\n\n\t\t\t\t\tLast Modified: 07/26/2024\n\n\t\t\t\t\tSubmitted by: NeilGong\n"
 }
}