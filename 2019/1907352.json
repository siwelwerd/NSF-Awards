{
 "awd_id": "1907352",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Improved Memory Management for Object-Oriented Big Data Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2018-10-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 270580.0,
 "awd_amount": 270580.0,
 "awd_min_amd_letter_date": "2018-12-14",
 "awd_max_amd_letter_date": "2018-12-14",
 "awd_abstract_narration": "This project seeks to develop runtime system support for improving the performance and scalability of a wide verity of data-intensive systems written in managed, object-oriented languages. It is clear that Big Data analytics has become a key component of modern computing. Popular data processing frameworks such as Hadoop, Spark, Naiad, or Hyracks are all developed in managed languages, such as Java, C#, or Scala, primarily due to the fast development cycles enabled by these languages and their abundance of library suites and community support. However, a great deal of evidence shows that memory management in Big Data systems is prohibitively expensive, severely damaging system performance. This project develops a series of runtime techniques that can automatically reduce the temporal and spatial costs of the managed runtime, allowing Big Data developers to fully enjoy the simplicity of managed languages without having to pay the performance price.\r\n \r\nModern life is relying increasingly on Big Data analytics designed to support many concurrent users and quickly answer their queries. Behind visible services are data-intensive computing systems that need to quickly find useful information from a sea of data records, and therefore, their performance is critically important to our daily lives. This project provides an immediate performance benefit for such data-intensive systems, leading to improved quality, usability, and user satisfaction. The educational component of this project includes creation of new course materials, recruitment of undergraduate students and students from under-represented groups, and education of local programmers on how to develop highly-efficient Big Data applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Harry",
   "pi_last_name": "Xu",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Harry G Xu",
   "pi_email_addr": "harryxu@cs.ucla.edu",
   "nsf_id": "000599637",
   "pi_start_date": "2018-12-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California",
  "perf_str_addr": "10889 Wilshire Boulevard",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 270580.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9804364d-7fff-60f7-d270-660224524700\"> </span></p>\n<p dir=\"ltr\"><span>Modern computing has entered the era of Big Data. Developing systems that can scale to massive amounts of data without significantly increasing resource amounts is a key challenge faced by both researchers and practitioners. Although much work has been done to improve scalability at the architecture level for distributed systems, a common problem in practice is memory pressure on individual nodes -- the execution pushes the heap limit soon after it starts and the system struggles to find memory to allocate new objects throughout the execution. These problems are further exacerbated by the pervasive use of managed languages, which often blow up memory usage in unpredictable ways.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The major goals of the project are to improve the memory management performance for large-scale data analytics systems written in managed languages. Over the course of the project, we have developed a series of static [SOSP&rsquo;19-a, SOSP&rsquo;19-b] and dynamic memory management techniques [OSDI&rsquo;16, ASPLOS&rsquo;18, PLDI&rsquo;19, OSDI&rsquo;20] that have led to considerably improved runtime performance for big data applications.&nbsp; This project also supports development of a set of big data systems, e.g., for graph analytics [USENIX ATC&rsquo;15, USENIX ATC&rsquo;16, ASPLOS&rsquo;17-a, ASPLOS&rsquo;17-b, OSDI&rsquo;18, EuroSys&rsquo;19] and video analytics [SIGCOMM&rsquo;20].&nbsp; We elaborate the two major efforts below.&nbsp;</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>1. Static analysis techniques for improved memory management&nbsp;</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>We developed Gerenuk [SOSP&rsquo;19-a], a compiler and runtime that enables managed big data systems to process data represented in a native format.&nbsp; Managed systems are executed on top of a managed runtime, which requires each data item to be represented as an object before it can be processed. This representation is the direct cause of many kinds of severe inefficiencies. The Gerenuk compiler transforms a set of statements in the target system for direct execution over inlined native bytes. The key insight leading to Gerenuk&rsquo;s success is two-fold: (1) analytics workloads often use immutable and confined data types. If we speculatively optimize the system and user code with this assumption, the transformation can be made tractable; (2) the flow of data starts at a deserialization point where objects are created from a sequence of native bytes and ends at&nbsp; a serialization point where they are turned back into a byte se-quence to be sent to the disk or network. This flow naturally defines a speculative execution region (SER) to be transformed. Gerenuk compiles a SER speculatively into a version that can operate directly over native bytes that come from the disk or network. The&nbsp; Gerenukruntime aborts the SER exe-cution upon violations of the immutability and confinement assumption and switches to the slow path by deserializing the bytes and re-executing the original SER. Applying Gerenuk on Spark and Hadoop improves their performance by 2-3X while simultaneously reducing their memory consumption by 2X.&nbsp;</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>We later found that many performance problems in those systems were due to inefficient handling of multiple languages. Multilingual data-parallel pipelines, such as Microsoft&rsquo;s Scope and Apache Spark, are widely used in real-world ana-lytical tasks. While the involvement of multiple languages(often including both managed and native languages) provides much convenience in data manipulation and transfor-mation, it comes at a performance cost. For example, each switch from a managed to a native runtime (and vice versa) requires marshalling or unmarshalling of an ocean of data objects, taking a large fraction of the executiontime. We developed Niijima [SOSP&rsquo;19-b], an optimizing compiler for Microsoft&rsquo;s Scope/Cosmos, which can consolidate C#-baseduser-defined operators (UDOs) across SQL statements, thereby reducing the number of dataflow vertices that require the managed runtime, and thus the amount of C# computations and the data marshalling cost. We demonstrate that Niijima has reduced job latency by an average of 24% and up to 3.3&times;,on a series of production jobs.</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>2. Dynamic analysis techniques for improved memory management&nbsp;</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>We developed a series of dynamic techniques and incorporated them into the garbage collector for efficient object management.&nbsp; For example, we observed that the lifetimes of objects in a big data system follows a strong epochal behavior. As a result, we developed Yak [OSDI&rsquo;16], a new garbage collector that divides objects into regions and uses user-provided epochal information to efficiently collect objects.&nbsp; Another important observation was big data systems such as Spark spent much time serializing and deserializing objects, while the default Java serializer is slow. We developed Skyway [ASPLOS&rsquo;18], a JVM-based distributed system that transfers an object as is without needing to first turn the object into a sequence of native bytes.&nbsp; We also developed a new managed runtime called Panthera [PLDI&rsquo;19] that efficiently processes big data over hybrid memories (DRAM + Non-volatile memory).&nbsp;</span></p>\n<p dir=\"ltr\">&nbsp;</p>\n<p dir=\"ltr\"><span>Over the course of the project, we have also released a set of tools and systems for graph analytics (Graspan, Grapple, and RSream), big data compilers (Facade), as well as video analytics systems (Reducto). Links to the release of these systems can be found on http://www.cs.ucla.edu/~harryxu.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/03/2020<br>\n\t\t\t\t\tModified by: Harry&nbsp;G&nbsp;Xu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nModern computing has entered the era of Big Data. Developing systems that can scale to massive amounts of data without significantly increasing resource amounts is a key challenge faced by both researchers and practitioners. Although much work has been done to improve scalability at the architecture level for distributed systems, a common problem in practice is memory pressure on individual nodes -- the execution pushes the heap limit soon after it starts and the system struggles to find memory to allocate new objects throughout the execution. These problems are further exacerbated by the pervasive use of managed languages, which often blow up memory usage in unpredictable ways. \nThe major goals of the project are to improve the memory management performance for large-scale data analytics systems written in managed languages. Over the course of the project, we have developed a series of static [SOSP\u201919-a, SOSP\u201919-b] and dynamic memory management techniques [OSDI\u201916, ASPLOS\u201918, PLDI\u201919, OSDI\u201920] that have led to considerably improved runtime performance for big data applications.  This project also supports development of a set of big data systems, e.g., for graph analytics [USENIX ATC\u201915, USENIX ATC\u201916, ASPLOS\u201917-a, ASPLOS\u201917-b, OSDI\u201918, EuroSys\u201919] and video analytics [SIGCOMM\u201920].  We elaborate the two major efforts below. \n \n1. Static analysis techniques for improved memory management \n \nWe developed Gerenuk [SOSP\u201919-a], a compiler and runtime that enables managed big data systems to process data represented in a native format.  Managed systems are executed on top of a managed runtime, which requires each data item to be represented as an object before it can be processed. This representation is the direct cause of many kinds of severe inefficiencies. The Gerenuk compiler transforms a set of statements in the target system for direct execution over inlined native bytes. The key insight leading to Gerenuk\u2019s success is two-fold: (1) analytics workloads often use immutable and confined data types. If we speculatively optimize the system and user code with this assumption, the transformation can be made tractable; (2) the flow of data starts at a deserialization point where objects are created from a sequence of native bytes and ends at  a serialization point where they are turned back into a byte se-quence to be sent to the disk or network. This flow naturally defines a speculative execution region (SER) to be transformed. Gerenuk compiles a SER speculatively into a version that can operate directly over native bytes that come from the disk or network. The  Gerenukruntime aborts the SER exe-cution upon violations of the immutability and confinement assumption and switches to the slow path by deserializing the bytes and re-executing the original SER. Applying Gerenuk on Spark and Hadoop improves their performance by 2-3X while simultaneously reducing their memory consumption by 2X. \n \nWe later found that many performance problems in those systems were due to inefficient handling of multiple languages. Multilingual data-parallel pipelines, such as Microsoft\u2019s Scope and Apache Spark, are widely used in real-world ana-lytical tasks. While the involvement of multiple languages(often including both managed and native languages) provides much convenience in data manipulation and transfor-mation, it comes at a performance cost. For example, each switch from a managed to a native runtime (and vice versa) requires marshalling or unmarshalling of an ocean of data objects, taking a large fraction of the executiontime. We developed Niijima [SOSP\u201919-b], an optimizing compiler for Microsoft\u2019s Scope/Cosmos, which can consolidate C#-baseduser-defined operators (UDOs) across SQL statements, thereby reducing the number of dataflow vertices that require the managed runtime, and thus the amount of C# computations and the data marshalling cost. We demonstrate that Niijima has reduced job latency by an average of 24% and up to 3.3&times;,on a series of production jobs.\n \n2. Dynamic analysis techniques for improved memory management \n \nWe developed a series of dynamic techniques and incorporated them into the garbage collector for efficient object management.  For example, we observed that the lifetimes of objects in a big data system follows a strong epochal behavior. As a result, we developed Yak [OSDI\u201916], a new garbage collector that divides objects into regions and uses user-provided epochal information to efficiently collect objects.  Another important observation was big data systems such as Spark spent much time serializing and deserializing objects, while the default Java serializer is slow. We developed Skyway [ASPLOS\u201918], a JVM-based distributed system that transfers an object as is without needing to first turn the object into a sequence of native bytes.  We also developed a new managed runtime called Panthera [PLDI\u201919] that efficiently processes big data over hybrid memories (DRAM + Non-volatile memory). \n \nOver the course of the project, we have also released a set of tools and systems for graph analytics (Graspan, Grapple, and RSream), big data compilers (Facade), as well as video analytics systems (Reducto). Links to the release of these systems can be found on http://www.cs.ucla.edu/~harryxu.\n\n \n\n\t\t\t\t\tLast Modified: 09/03/2020\n\n\t\t\t\t\tSubmitted by: Harry G Xu"
 }
}