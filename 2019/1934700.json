{
 "awd_id": "1934700",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Advancing Science with Accelerated Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927382",
 "po_email": "vlukin@nsf.gov",
 "po_sign_block_name": "Vyacheslav (Slava) Lukin",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 610000.0,
 "awd_amount": 610000.0,
 "awd_min_amd_letter_date": "2019-09-15",
 "awd_max_amd_letter_date": "2022-12-12",
 "awd_abstract_narration": "In the next generation of big science experiments, the demands for computing resources are expected to outstrip the capabilities of existing computing infrastructure. In light of this, a radical rethinking of the cyberinfrastructure is needed to contend with these developments. With the onset of deep learning, parallelized processing architectures have emerged as a solution. Combined with deep learning algorithms, parallelized processing architectures, in particular, Field Programmable Gate Arrays (FPGAs) have been shown to give large speedups in computing when compared with conventional CPUs. This project aims to bring machine learning based accelerated computing with FPGAs into the scientific community by targeting two big-data physics experiments: the Large Hadron Collider (LHC) and the Laser Interferometer Gravitational-wave  Observatory (LIGO). This project will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. The PIs and their collaborators will build upon their recent work to design and exploit state-of-the-art neural network models for real-time data analytics, reducing overall computing latency. This new computing paradigm aims to significantly increase the processing capability at the LHC and LIGO, leading to an increased scientific output of these devices and,  potentially, foundational discoveries. The students to be mentored and trained in this research will interact closely with industry partners, creating new career opportunities, and strengthening synergies between academia and industry. In addition to sharing algorithms with the community through open source repositories, the team will continue to educate the community regarding credit and citation of scientific software.\r\n\r\nIn this project, the PIs will build upon their recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets using Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms. The team will develop machine learning based acceleration tools focusing on FPGAs to be used within LIGO and the LHC experiments. The team's immediate goal is to take benchmark examples of LHC high level trigger processing and LIGO gravitational wave processing and construct demonstrators in each scenario. For this benchmark, they aim to design and implement an FPGA based accelerator that can perform low latency gravitational wave identification and LHC event reconstruction.  Additionally, the PIs aim to add the capability of graph based neural network accelerators for FPGAs. The open source tools to be developed as part of these activities will be readily shared with LIGO, LHC, and LSST. The project will create an advisory group, including members of large and small projects,  members of the neutrino physics, multi-messenger astronomy community, industry partners, computer scientists, and computational biologists. This project aims to bring together representatives of the different communities that will benefit from and can contribute to this work. The PIs will organize deep learning workshops and boot camps to train students and researchers on how to use and contribute to the framework, creating a wide network of contributors and developers across key science missions.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.\r\n\r\nThis project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Harris",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Philip C Harris",
   "pi_email_addr": "pcharris@mit.edu",
   "nsf_id": "000779160",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Erotokritos",
   "pi_last_name": "Katsavounidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Erotokritos Katsavounidis",
   "pi_email_addr": "kats@mit.edu",
   "nsf_id": "000486692",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Song",
   "pi_last_name": "Han",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Song Han",
   "pi_email_addr": "songhan@MIT.EDU",
   "nsf_id": "000762347",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Ave",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "099y00",
   "pgm_ele_name": "HDR-Harnessing the Data Revolu"
  },
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 311918.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 298082.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-87eba7eb-7fff-c6ee-84cb-94991b8cb37b\"> </span></p>\n<p dir=\"ltr\">Real-time algorithms are critical in the Large Hadron Collider (LHC) experiments at CERN and gravitational wave experiments, including the Laser Interferometer Gravitational-Wave Observatory (LIGO). The challenge for gravitational waves is to quickly identify events and alert the rest of the community in a process known as multi-messenger astronomy; these&nbsp;have proven to be some of the most elucidating astrophysical events ever observed. At the LHC, the data rates are so enormous that we can only save one collision in 40,000 to reconstruct further. As a result, we rely on a real-time system that can only perform a cursory analysis to find interesting events. This analysis leaves critical events behind. By improving our knowledge of particle interactions in real-time, we can recover these events. Our goal in this project is to strengthen the real-time pipelines with better and faster algorithms utilizing deep learning. Faster algorithms lead to lower latency gravitational wave alerts and more LHC collisions analyzed.</p>\n<p dir=\"ltr\">Deep learning algorithms have taken on rapid adoption across many scientific domains due to their incredible ability to provide solutions to very complicated tasks. A significant reason for their effectiveness originates from the rise of heterogeneous computation, which exploits the parallelism of certain processors, particularly GPUs and FPGAs, to perform deep learning calculations. In this project, we developed an end-to-end infrastructure to enable a deep learning computational pipeline for real-time gravitational wave data and high energy physics.&nbsp; Our pipeline can take deep learning algorithms and optimally deploy them into the data acquisition with minimal effort. Moreover, we created new algorithms for these pipelines that will significantly improve the quality of science, allowing for the potential of a discovery.&nbsp;</p>\n<p>With the LHC, we focused on the deployment of algorithms at three tiers of the computational pipeline. The first tier of the pipeline utilizes FPGAs and requires algorithms that run in less than a microsecond. Here, we developed the HLS4ML toolkit, which enables the rapid deployment of deep learning algorithms on FPGA processors. With HLS4ML, we have created deep learning algorithms to perform tau lepton and b-quark identification to be used in the future running of the CMS detector, and we have shown that deploying these algorithms will lead to substantial physics performance improvements, including enhanced sensitivity to the Higgs self-coupling. The community is quickly adopting our toolkit and strategy, and another roughly 20 algorithms are under development for the LHC low latency real-time system.&nbsp;</p>\n<p>In the latter two tiers of the LHC, we instead integrated inference-as-a-service technology into the CPU-based reconstruction workflow of the CMS detector. Our setup can quickly deploy ML and non-ML algorithms on FPGAs or GPUs, enabling optimized computational load balancing to ensure all resources are fully efficient. Our FPGA-as-a-service toolkit is unique, and we have shown it can be particularly effective with specific algorithms. With our setup, we demonstrated how integrating this technology led to significant speed-ups by transferring existing deep learning algorithms to our system. Additionally,&nbsp; we created a new deep learning algorithm that can go from 1000s of raw hits to physics objects in one pass. This work is heralding a new era of LHC computation where single-particle algorithms are replaced by event-level multi-particle deep learning algorithms that work at low latency through heterogeneous computing.&nbsp;</p>\n<p>With Gravitational waves, we created the Hermes software infrastructure to deploy inference-as-a-service technology into the existing gravitational wave computational pipelines. With Hermes, we have developed an end-to-end set of algorithms that goes from raw time series data to denoised data to detect black hole alerts. Our algorithms yield faster inference times than conventional rule-based algorithms and give comparable, if not better, physics performance. By relying on the as-a-service paradigm, Hermes is capable of throughput and latency reductions over an order of magnitude beyond conventional GPU use.&nbsp; This presents the first full deep-learning data processing stream for gravitational waves. &nbsp; Given the rapid advancement of deep learning, this work allows us to take advantage of the significant computational improvements in the next few years.&nbsp;</p>\n<p dir=\"ltr\">The interplay between LHC and gravitational waves has been critical. Communication between the domains and computer scientists has enabled the optimal use of heterogeneous computing. We have shared our experiences and algorithmic approaches, including developing a deep-learning anomaly pipeline in both experiments. The shared experiences have helped to build a knowledge base that is helping to cultivate a new culture of real time deep learning.&nbsp;</p>\n<p>Our work is leading the way toward the integration of real-time AI&nbsp; for scientific experiments. Through this effort, we have developed new concepts in AI deployment on processors and created the infrastructure needed for fast adoption across scientific domains. This grant has trained five undergraduate students, two Ph.D students, supported a postdoc until&nbsp; faculty, held two conferences, developed online tutorials, and helped cultivate a community to deploy low latency, high-throughput, fast machine learning.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/08/2024<br>\nModified by: Philip&nbsp;C&nbsp;Harris</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1934700/1934700_10642185_1691032770463_Screenshot2023-08-02at23.19.01--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1934700/1934700_10642185_1691032770463_Screenshot2023-08-02at23.19.01--rgov-800width.png\" title=\"GW and LHC AI Pipelines\"><img src=\"/por/images/Reports/POR/2023/1934700/1934700_10642185_1691032770463_Screenshot2023-08-02at23.19.01--rgov-66x44.png\" alt=\"GW and LHC AI Pipelines\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Image showing flow of data in Gravitational Waves(GW (Left) and the Large Hadron Collier (Right). With GW the full pipeline is AI based taking multiple raw time series in and outputting a processed stream. The right plot shows the LHC reconstruction workflow where part of the data is processed .</div>\n<div class=\"imageCredit\">https://arxiv.org/abs/1902.08570, https://doi.org/10.1145/3526058.3535454, https://arxiv.org/abs/2007.10359</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Philip&nbsp;C&nbsp;Harris\n<div class=\"imageTitle\">GW and LHC AI Pipelines</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nReal-time algorithms are critical in the Large Hadron Collider (LHC) experiments at CERN and gravitational wave experiments, including the Laser Interferometer Gravitational-Wave Observatory (LIGO). The challenge for gravitational waves is to quickly identify events and alert the rest of the community in a process known as multi-messenger astronomy; thesehave proven to be some of the most elucidating astrophysical events ever observed. At the LHC, the data rates are so enormous that we can only save one collision in 40,000 to reconstruct further. As a result, we rely on a real-time system that can only perform a cursory analysis to find interesting events. This analysis leaves critical events behind. By improving our knowledge of particle interactions in real-time, we can recover these events. Our goal in this project is to strengthen the real-time pipelines with better and faster algorithms utilizing deep learning. Faster algorithms lead to lower latency gravitational wave alerts and more LHC collisions analyzed.\n\n\nDeep learning algorithms have taken on rapid adoption across many scientific domains due to their incredible ability to provide solutions to very complicated tasks. A significant reason for their effectiveness originates from the rise of heterogeneous computation, which exploits the parallelism of certain processors, particularly GPUs and FPGAs, to perform deep learning calculations. In this project, we developed an end-to-end infrastructure to enable a deep learning computational pipeline for real-time gravitational wave data and high energy physics. Our pipeline can take deep learning algorithms and optimally deploy them into the data acquisition with minimal effort. Moreover, we created new algorithms for these pipelines that will significantly improve the quality of science, allowing for the potential of a discovery.\n\n\nWith the LHC, we focused on the deployment of algorithms at three tiers of the computational pipeline. The first tier of the pipeline utilizes FPGAs and requires algorithms that run in less than a microsecond. Here, we developed the HLS4ML toolkit, which enables the rapid deployment of deep learning algorithms on FPGA processors. With HLS4ML, we have created deep learning algorithms to perform tau lepton and b-quark identification to be used in the future running of the CMS detector, and we have shown that deploying these algorithms will lead to substantial physics performance improvements, including enhanced sensitivity to the Higgs self-coupling. The community is quickly adopting our toolkit and strategy, and another roughly 20 algorithms are under development for the LHC low latency real-time system.\n\n\nIn the latter two tiers of the LHC, we instead integrated inference-as-a-service technology into the CPU-based reconstruction workflow of the CMS detector. Our setup can quickly deploy ML and non-ML algorithms on FPGAs or GPUs, enabling optimized computational load balancing to ensure all resources are fully efficient. Our FPGA-as-a-service toolkit is unique, and we have shown it can be particularly effective with specific algorithms. With our setup, we demonstrated how integrating this technology led to significant speed-ups by transferring existing deep learning algorithms to our system. Additionally, we created a new deep learning algorithm that can go from 1000s of raw hits to physics objects in one pass. This work is heralding a new era of LHC computation where single-particle algorithms are replaced by event-level multi-particle deep learning algorithms that work at low latency through heterogeneous computing.\n\n\nWith Gravitational waves, we created the Hermes software infrastructure to deploy inference-as-a-service technology into the existing gravitational wave computational pipelines. With Hermes, we have developed an end-to-end set of algorithms that goes from raw time series data to denoised data to detect black hole alerts. Our algorithms yield faster inference times than conventional rule-based algorithms and give comparable, if not better, physics performance. By relying on the as-a-service paradigm, Hermes is capable of throughput and latency reductions over an order of magnitude beyond conventional GPU use. This presents the first full deep-learning data processing stream for gravitational waves.  Given the rapid advancement of deep learning, this work allows us to take advantage of the significant computational improvements in the next few years.\n\n\nThe interplay between LHC and gravitational waves has been critical. Communication between the domains and computer scientists has enabled the optimal use of heterogeneous computing. We have shared our experiences and algorithmic approaches, including developing a deep-learning anomaly pipeline in both experiments. The shared experiences have helped to build a knowledge base that is helping to cultivate a new culture of real time deep learning.\n\n\nOur work is leading the way toward the integration of real-time AI for scientific experiments. Through this effort, we have developed new concepts in AI deployment on processors and created the infrastructure needed for fast adoption across scientific domains. This grant has trained five undergraduate students, two Ph.D students, supported a postdoc until faculty, held two conferences, developed online tutorials, and helped cultivate a community to deploy low latency, high-throughput, fast machine learning.\n\n\n\t\t\t\t\tLast Modified: 01/08/2024\n\n\t\t\t\t\tSubmitted by: PhilipCHarris\n"
 }
}