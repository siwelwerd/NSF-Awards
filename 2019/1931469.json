{
 "awd_id": "1931469",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Frameworks: Machine learning and FPGA computing for real-time applications in big-data physics experiments",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922656",
 "po_email": "vchandol@nsf.gov",
 "po_sign_block_name": "Varun Chandola",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 904179.0,
 "awd_amount": 904179.0,
 "awd_min_amd_letter_date": "2019-09-17",
 "awd_max_amd_letter_date": "2023-09-11",
 "awd_abstract_narration": "The cyberinfrastructure needs for gravitational wave astrophysics, high energy physics, and large-scale electromagnetic surveys have rapidly evolved in recent years. The construction and upgrade of the facilities used to enable scientific discovery in these disparate fields of research have led to a common pair of computational grand challenges: (i) datasets with ever-increasing complexity and volume; and (ii) data mining analyses that must be performed in real-time with oversubscribed computational resources. Furthermore, the convergence of gravitational wave astrophysics with electromagnetic and astroparticle surveys, the very birth of Multi-Messenger Astrophysics, has already provided a glimpse of the transformational discoveries that it will enable in years to come. Given the unique potential for scientific discovery with the Large Hadron Collider (LHC) and the combination of the Laser Interferometer Gravitational-wave Observatory (LIGO) and the Large Synoptic Survey Telescope (LSST) for Multi-Messenger Astrophysics, the community needs to accelerate the development and exploitation of deep learning algorithms that will outperform existing approaches. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. It will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. Because these methods are also applicable to many other parts of our national and global economy and society, this work will positively impact many fields. The students and junior scientists to be mentored and trained in this research will interact closely with our industry partners, creating new career opportunities, and strengthening synergies between academia and industry. The team will share the algorithms with the community through open source software repositories, and through our tutorials and workshops the team will train the community regarding software credit and software citation.\r\n\r\nIn this project, the PIs will build upon our recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets, as open source software. This work combines scalable deep learning algorithms, trained with TB-size datasets within minutes using thousands of GPUs/CPUs, with state-of-the-art approaches to endow the predictions of deterministic deep learning models with complete posterior distributions. The team will also investigate the use of Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms to minimize the demands of future computing, which is a central goal for Multi-Messenger Astrophysics and particle physics. The open source tools to be developed as part of these activities will be readily shared with and adopted by LIGO, LHC, and LSST as core data analytics algorithms that will significantly increase the speed and depth of existing algorithms, enabling new physics while requiring minimal computational resources for real-time inferences analyses. The team will organize deep learning workshops and bootcamps to train students and researchers on how to use and contribute to our framework, creating a wide network of contributors and developers across key science missions. The team will leverage existing open source and interactive model repositories, such as the Data and Learning Hub for Science (DLHub) at Argonne, to reach out to a large cross-section of communities that analyze open datasets from LIGO, LHC, and LSST, and that will benefit from the use of these technologies that require minimal computational resources for inference tasks.\r\n\r\nThis project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Erotokritos",
   "pi_last_name": "Katsavounidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Erotokritos Katsavounidis",
   "pi_email_addr": "kats@mit.edu",
   "nsf_id": "000486692",
   "pi_start_date": "2019-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Harris",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Philip C Harris",
   "pi_email_addr": "pcharris@mit.edu",
   "nsf_id": "000779160",
   "pi_start_date": "2019-09-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Ave.",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 904179.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-f940f71d-7fff-b6dc-054c-5a065307035d\" style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project contributed to addressing cyberinfrastructure needs in machine learning-based physics analyses within the gravitational-wave and high energy physics (HEP) communities.&nbsp; The focus of this work involved:</span></p>\r\n<ol style=\"margin-top: 0; margin-bottom: 0; padding-inline-start: 48px;\">\r\n<li style=\"list-style-type: lower-alpha; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">developing deep learning algorithms that can analyze data rapidly,</span></p>\r\n</li>\r\n<li style=\"list-style-type: lower-alpha; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">demonstrating these deep learning algorithms can be effective tools in analyzing data</span></p>\r\n</li>\r\n<li style=\"list-style-type: lower-alpha; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">incorporating optimized heterogeneous computing into this workflow to ensure low latency neural network inference, and</span></p>\r\n</li>\r\n<li style=\"list-style-type: lower-alpha; font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\">\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">creating the cyberinfrastructure needed to incorporate heterogeneous Deep Learning computing into both Large Hadron Collider(LHC) experiments and Gravitational-Wave experiments.</span></p>\r\n</li>\r\n</ol>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The goal of this work was to develop a consolidated effort across high energy physics and gravitational-wave physics that enabled a common platform to share ideas, algorithms, and software.&nbsp;</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Within high energy physics, the focus was on the first tier of the real-time system data acquisition, which denoted the level 1 trigger. This tier aims to filter the data from the LHC in real-time using Field Programmable Gate Arrays (FPGAs). In this part of the project, we continued to develop our ML for FPGA software HLS4ML and demonstrated several algorithms that can be run in real-time on the system. We completed our project by deploying an AI-based anomaly detection algorithm, Axolotl, currently running in LHC Run on the Compact Muon Solenoid detector.&nbsp;</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Additionally, within HEP, we developed the Inference-as-a-Service paradigm to allow for the dynamic integration of heterogeneous computing architectures, including both Graphics Processing Units (GPUs), and FPGAs. By building upon industry standard tools, we were able to integrate roughly 50% of the compute budget of the final tier of reconstruction for data from the CMS detector on the LHC. This enabled us to port our computing to GPUs, leading to 30-40% speedups in the overall throughput of the system. Furthermore, we have demonstrated our system on official CMS workflows using the complete production workflow toolkit. With our setup, we have additionally demonstrated that this paradigm is more resource efficient, and we have shown that you can use half the needed GPUs in the CMS High-Level Trigger system (the middle tier of CMS&rsquo;s real-time data acquisition system). Here, both rule-based and deep-learning algorithms were accelerated on GPUs. Our work sets the stage for future LHC running during Run 4. Our infrastructure, denoted Services for Optimized Network Inference in Coprocessors (SONIC), is now being integrated into the ATLAS experiment at the LHC, the DUNE neutrino experiment at Fermilab, and the IceCube experiment at the South Pole. The SONIC infrastructure has led to a design paradigm that enables efficient GPU scale-out and optimized heterogeneous compute. This work was the seed for the ML4GW infrastructure also pursued as part of this grant.&nbsp;</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Using ideas from the high energy physics community, a Machine Learning Inference-as-a-Service cyberinfrastructure for gravitational-wave data analysis was developed. This allows the simple integration and management of machine learning models while using heterogeneous computing resources. The system targets real-time applications, especially those around streaming time series like in gravitational-wave experiments LIGO, Virgo, and KAGRA. It has been deployed in LIGO&rsquo;s real-time computing infrastructure for the purpose of denoising gravitational-wave strain data, which were collected by the experiment and demonstrated sub-second latencies. A collection of machine learning models addressing the end-to-end data analysis challenges in the search for gravitational-wave transients have also been developed and applied. Besides the front-end denoising based on a 1-D convolutional autoencoder, neural networks based on a Residual Network architecture and multiple recurrent autoencoders have been deployed respectively in the search for modeled and unmodeled transients of gravitational radiation. Their application in end-to-end astrophysical searches using archival LIGO data shows comparable sensitivities to the ones reported by the gravitational-wave collaborations but with significantly reduced computational cost and latency.&nbsp;</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A new end-to-end analysis ecosystem for machine learning based gravitational-wave searches (&ldquo;ml4gw&rdquo;) is now available publicly under Git Hub. It addresses the needs of the broader community in exploiting the power of deep learning algorithms as well as in accelerating their development and application in real-time gravitational-wave searches.</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project allowed the organization of the&nbsp; \"Accelerating Physics with ML\" workshop from January 28 to February 2, 2023 at MIT (</span><a style=\"text-decoration: none;\" href=\"https://indico.cern.ch/event/1224718/\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">https://indico.cern.ch/event/1224718/</span></a><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">). Participants from the high energy physics, and multi messenger astrophysics communities as well as representatives from computer science and computational biology shared experiences and challenges in applying machine learning algorithms to data, including addressing issues like real-time, acceleration, and heterogeneous computing. The outcomes of the workshop were summarized in a white paper led by the junior researchers in the project (</span><a style=\"text-decoration: none;\" href=\"https://arxiv.org/abs/2306.08106\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">arxiv.org/abs/2306.08106</span></a><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">) .</span></p><br>\n<p>\n Last Modified: 12/31/2024<br>\nModified by: Erotokritos&nbsp;Katsavounidis</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project contributed to addressing cyberinfrastructure needs in machine learning-based physics analyses within the gravitational-wave and high energy physics (HEP) communities. The focus of this work involved:\r\n\r\n\r\n\n\ndeveloping deep learning algorithms that can analyze data rapidly,\r\n\r\n\r\n\n\ndemonstrating these deep learning algorithms can be effective tools in analyzing data\r\n\r\n\r\n\n\nincorporating optimized heterogeneous computing into this workflow to ensure low latency neural network inference, and\r\n\r\n\r\n\n\ncreating the cyberinfrastructure needed to incorporate heterogeneous Deep Learning computing into both Large Hadron Collider(LHC) experiments and Gravitational-Wave experiments.\r\n\r\n\r\n\n\nThe goal of this work was to develop a consolidated effort across high energy physics and gravitational-wave physics that enabled a common platform to share ideas, algorithms, and software.\r\n\n\nWithin high energy physics, the focus was on the first tier of the real-time system data acquisition, which denoted the level 1 trigger. This tier aims to filter the data from the LHC in real-time using Field Programmable Gate Arrays (FPGAs). In this part of the project, we continued to develop our ML for FPGA software HLS4ML and demonstrated several algorithms that can be run in real-time on the system. We completed our project by deploying an AI-based anomaly detection algorithm, Axolotl, currently running in LHC Run on the Compact Muon Solenoid detector.\r\n\n\nAdditionally, within HEP, we developed the Inference-as-a-Service paradigm to allow for the dynamic integration of heterogeneous computing architectures, including both Graphics Processing Units (GPUs), and FPGAs. By building upon industry standard tools, we were able to integrate roughly 50% of the compute budget of the final tier of reconstruction for data from the CMS detector on the LHC. This enabled us to port our computing to GPUs, leading to 30-40% speedups in the overall throughput of the system. Furthermore, we have demonstrated our system on official CMS workflows using the complete production workflow toolkit. With our setup, we have additionally demonstrated that this paradigm is more resource efficient, and we have shown that you can use half the needed GPUs in the CMS High-Level Trigger system (the middle tier of CMSs real-time data acquisition system). Here, both rule-based and deep-learning algorithms were accelerated on GPUs. Our work sets the stage for future LHC running during Run 4. Our infrastructure, denoted Services for Optimized Network Inference in Coprocessors (SONIC), is now being integrated into the ATLAS experiment at the LHC, the DUNE neutrino experiment at Fermilab, and the IceCube experiment at the South Pole. The SONIC infrastructure has led to a design paradigm that enables efficient GPU scale-out and optimized heterogeneous compute. This work was the seed for the ML4GW infrastructure also pursued as part of this grant.\r\n\n\nUsing ideas from the high energy physics community, a Machine Learning Inference-as-a-Service cyberinfrastructure for gravitational-wave data analysis was developed. This allows the simple integration and management of machine learning models while using heterogeneous computing resources. The system targets real-time applications, especially those around streaming time series like in gravitational-wave experiments LIGO, Virgo, and KAGRA. It has been deployed in LIGOs real-time computing infrastructure for the purpose of denoising gravitational-wave strain data, which were collected by the experiment and demonstrated sub-second latencies. A collection of machine learning models addressing the end-to-end data analysis challenges in the search for gravitational-wave transients have also been developed and applied. Besides the front-end denoising based on a 1-D convolutional autoencoder, neural networks based on a Residual Network architecture and multiple recurrent autoencoders have been deployed respectively in the search for modeled and unmodeled transients of gravitational radiation. Their application in end-to-end astrophysical searches using archival LIGO data shows comparable sensitivities to the ones reported by the gravitational-wave collaborations but with significantly reduced computational cost and latency.\r\n\n\nA new end-to-end analysis ecosystem for machine learning based gravitational-wave searches (ml4gw) is now available publicly under Git Hub. It addresses the needs of the broader community in exploiting the power of deep learning algorithms as well as in accelerating their development and application in real-time gravitational-wave searches.\r\n\n\nThe project allowed the organization of the \"Accelerating Physics with ML\" workshop from January 28 to February 2, 2023 at MIT (https://indico.cern.ch/event/1224718/). Participants from the high energy physics, and multi messenger astrophysics communities as well as representatives from computer science and computational biology shared experiences and challenges in applying machine learning algorithms to data, including addressing issues like real-time, acceleration, and heterogeneous computing. The outcomes of the workshop were summarized in a white paper led by the junior researchers in the project (arxiv.org/abs/2306.08106) .\t\t\t\t\tLast Modified: 12/31/2024\n\n\t\t\t\t\tSubmitted by: ErotokritosKatsavounidis\n"
 }
}