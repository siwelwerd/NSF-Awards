{
 "awd_id": "1846184",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Human-Computer Collaborative Music Making",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 499219.0,
 "awd_amount": 499219.0,
 "awd_min_amd_letter_date": "2019-03-25",
 "awd_max_amd_letter_date": "2022-08-16",
 "awd_abstract_narration": "Music is part of every culture on earth, and the enjoyment of music is nearly universal. Music performance is often highly collaborative; musicians harmonize their pitch, coordinate their timing, and reinforce their expressiveness to make music that strikes the hearts of the audience. This research envisions a human-computer collaborative music making system that allows people to collaborate with machines in a manner similar to that in which we collaborate with each other. This is of great significance, as we live in a world where the interaction between humans and machines is becoming deeper and broader, so developing systems that allow us to collaborate with machines is a primary goal of research into cyber-human systems, robotics, and artificial intelligence. Project outcomes will advance the state of the art in automated accompaniment systems by empowering machines with much stronger music perception skills (audio-visual attending to individual parts in ensemble performances vs. monophonic listening), much more expressive music performance skills (expressive audio-visual rendering vs. timing adaptation of audio only), and much deeper understanding of music theory and composition rules (composition and improvisation skills vs. music theory novice). This project will showcase the powerful connection between music and technology, which has inspired generations of great multidisciplinary thinkers such as Pythagoras, Galilei, Da Vinci, and Franklin. The techniques developed in this project will be applied to augmented concert experiences through collaborations with the Eastman School of Music and the Chinese Choral Society of Rochester. Outreach to pre-college and college students will be accomplished through a variety of activities, including lab visits, a summer mini-course on \"music and math\" and teaching and advising in the unique and interdisciplinary Audio and Music Engineering program at the University of Rochester. \r\n\r\nThe project has four research thrusts with the following expected outcomes: 1) Attending to Human Performances: algorithms for machine listening and visual analysis of multi-instrument polyphonic music performances; 2) Rendering Expressive Machine Performances: computational models for expressiveness and audio-visual rendering techniques for expressive performances; 3) Modeling Music Language for Improvisation: computational models for compositional rules, and algorithms for music generation, harmonization, and improvisation; 4) System Integration: a human-computer collaborative music making system, and a set of design principles backed by subjective evaluations. The research will advance existing interaction mechanisms toward human-computer collaboration. It will also advance the current static-object-displaying type of augmented reality to more intelligent, dynamic and collaborative augmented reality in music performances. The research on audio-visual analysis will advanes both machine listening and visual understanding of audio-visual scenes in the music context. The research on visual rendering of expressive performances will open a new field of computational modeling of visual expressiveness in musical performances. And the research on computational music language models is fundamental for many tasks in music informatics, including transcription, composition, and retrieval. The integration of analysis, performance and music language modeling towards a real-time collaborative system represents a new level of intelligent real-time computing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhiyao",
   "pi_last_name": "Duan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhiyao Duan",
   "pi_email_addr": "zhiyao.duan@rochester.edu",
   "nsf_id": "000662423",
   "pi_start_date": "2019-03-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "720 Computer Studies Building",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270231",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 105136.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 95014.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 97306.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 201763.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In a world where the interaction between humans and computers is becoming deeper and broader, developing intelligent systems that allow us to collaborate with computers is one of the main missions in the research of cyber-human systems, robotics, and artificial intelligence. Music is part of every culture on earth. The highly collaborative nature of music performance is an ideal substance for the research on human-computer collaboration.</p>\n<p>This project aimed to develop a human-computer collaborative music making system, which allows humans to collaborate with computers in a way like how humans collaborate with each other. The project consisted of four research thrusts and resulted in the following outcomes on algorithm and system development under each thrust:</p>\n<p>1)&nbsp;&nbsp;&nbsp; Attending to Human Performances</p>\n<ol>\n<li>Algorithms (DLB, BeatNet, SingNet, BeatNet+) for real-time beat and downbeat tracking of music with and without percussive components (e.g., singing)</li>\n<li>Algorithms for automatic transcription of piano music into notes</li>\n<li>Algorithms for guitar tablature transcription</li>\n<li>Algorithms for detecting singing voice deepfakes</li>\n<li>An algorithm for automatic transcription of choral music</li>\n<li>An algorithm for audio-visual source association of chamber music performances</li>\n<li>An algorithm for audiovisual singing voice separation</li>\n<li>An algorithm for music source separation in an unsupervised manner</li>\n</ol>\n<p>2)&nbsp;&nbsp;&nbsp; Rendering Expressive Machine Performance</p>\n<ol>\n<li>An algorithm (MusicHiFi) for high-fidelity stereophonic music audio synthesis and bandwidth extension</li>\n</ol>\n<p>3)&nbsp;&nbsp;&nbsp; Modeling Music Language for Improvisation</p>\n<ol>\n<li>Algorithms (RL-Duet, BachDuet) for online human-computer duet counterpoint improvisation</li>\n<li>An algorithm (FolkDuet) for fusing Western counterpoint and Chinese folk melody styles</li>\n<li>An algorithm (CollageNet) for fusing arbitrary melodies and accompaniments into coherent songs</li>\n<li>An algorithm (Harana) for symbolic music harmonic analysis</li>\n<li>An algorithm (Measure by Measure) for modeling the music language in Western modern staff notation</li>\n</ol>\n<p>4)&nbsp;&nbsp;&nbsp; System Integration</p>\n<ol>\n<li>A web-based system (<a href=\"https://bachduet.com/\">https://bachduet.com/</a>) for real-time human-computer duet counterpoint improvisation</li>\n<li>A web-based framework (<a href=\"https://euterpeframework.org/\">https://euterpeframework.org/</a>) for developing and prototyping interactive music systems</li>\n<li>A web-based system (<a href=\"https://draw-n-listen.netlify.app/\">https://draw-n-listen.netlify.app/</a>) for music inpainting through drawing melodic and note density curves</li>\n<li>A system for rendering effects in real-time in augmented concerts using score following techniques</li>\n</ol>\n<p>We also created or curated the following datasets, which are all freely available on the web:</p>\n<p>1)&nbsp;&nbsp;&nbsp; URSing: the first audiovisual singing dataset with 65 songs of isolated singing and accompaniment tracks</p>\n<p>2)&nbsp;&nbsp;&nbsp; SingFake, WildSVDD, CtrSVDD: the first singing voice deepfake detection datasets containing hundreds of hours of recordings. WildSVDD (in-the-wild recordings) and CtrSVDD (lab-generated recordings) are part of the first singing voice deepfake detection challenge that we co-organize</p>\n<p>3)&nbsp;&nbsp;&nbsp; SynthTab: the first large-scale string-accurate guitar tablature transcription dataset containing 6,700 hours of recordings</p>\n<p>4)&nbsp;&nbsp;&nbsp; YouChorale: the first large-scale choral music dataset with aligned scores containing 22 hours of recordings</p>\n<p>On education and outreach, this project helped with the development of a 4-day mini-course on &ldquo;Music and Math&rdquo; for high school students and curricular materials for undergraduate and graduate classes (&ldquo;Audio Signal Processing&rdquo;, &ldquo;Computer Audition&rdquo;, &ldquo;The Art of Machine Learning&rdquo;) in the Audio and Music Engineering program at the University of Rochester. It also helped develop materials for four tutorials (&ldquo;Audiovisual Music Processing&rdquo;, &ldquo;Audio-Visual Scene Understanding&rdquo;, &ldquo;A Brief History of Interactive Music Systems&rdquo;, &ldquo;Computer Assisted Music-Making Systems: Taxonomy, Review and Live Coding&rdquo;) at international conferences, two review articles (&ldquo;Automatic Music Transcription&rdquo;, &ldquo;Audiovisual Music Processing&rdquo;) and several lectures at the 2023 Winter School on Speech and Audio Processing. This project involved 9 PhD students, 4 master&rsquo;s students, and 8 undergraduate students in research. Through this project, we developed collaborations with researchers from many different institutions and backgrounds.</p>\n<p>This research is intellectually transformative as it is of key interest in human-computer interaction, audio-visual and multi-modal machine learning. The research on computational music language models is fundamental for many tasks in music informatics including transcription, composition, and retrieval. The integration of analysis, performance and music language modeling towards a real-time collaborative system represents a new level of intelligent real-time computing.</p>\n<p>This project has broader Impacts beyond the above-mentioned educational opportunities because it demonstrates<strong> t</strong>he power and potential of the connection between music and technology to a broad audience. It could also enable many applications that can greatly benefit the society, including education (music education and STEM education through music), entertainment (music analysis and interaction), virtual and augmented reality (augmented concerts) and healthcare (music therapy).</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/20/2024<br>\nModified by: Zhiyao&nbsp;Duan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn a world where the interaction between humans and computers is becoming deeper and broader, developing intelligent systems that allow us to collaborate with computers is one of the main missions in the research of cyber-human systems, robotics, and artificial intelligence. Music is part of every culture on earth. The highly collaborative nature of music performance is an ideal substance for the research on human-computer collaboration.\n\n\nThis project aimed to develop a human-computer collaborative music making system, which allows humans to collaborate with computers in a way like how humans collaborate with each other. The project consisted of four research thrusts and resulted in the following outcomes on algorithm and system development under each thrust:\n\n\n1) Attending to Human Performances\n\nAlgorithms (DLB, BeatNet, SingNet, BeatNet+) for real-time beat and downbeat tracking of music with and without percussive components (e.g., singing)\nAlgorithms for automatic transcription of piano music into notes\nAlgorithms for guitar tablature transcription\nAlgorithms for detecting singing voice deepfakes\nAn algorithm for automatic transcription of choral music\nAn algorithm for audio-visual source association of chamber music performances\nAn algorithm for audiovisual singing voice separation\nAn algorithm for music source separation in an unsupervised manner\n\n\n\n2) Rendering Expressive Machine Performance\n\nAn algorithm (MusicHiFi) for high-fidelity stereophonic music audio synthesis and bandwidth extension\n\n\n\n3) Modeling Music Language for Improvisation\n\nAlgorithms (RL-Duet, BachDuet) for online human-computer duet counterpoint improvisation\nAn algorithm (FolkDuet) for fusing Western counterpoint and Chinese folk melody styles\nAn algorithm (CollageNet) for fusing arbitrary melodies and accompaniments into coherent songs\nAn algorithm (Harana) for symbolic music harmonic analysis\nAn algorithm (Measure by Measure) for modeling the music language in Western modern staff notation\n\n\n\n4) System Integration\n\nA web-based system (https://bachduet.com/) for real-time human-computer duet counterpoint improvisation\nA web-based framework (https://euterpeframework.org/) for developing and prototyping interactive music systems\nA web-based system (https://draw-n-listen.netlify.app/) for music inpainting through drawing melodic and note density curves\nA system for rendering effects in real-time in augmented concerts using score following techniques\n\n\n\nWe also created or curated the following datasets, which are all freely available on the web:\n\n\n1) URSing: the first audiovisual singing dataset with 65 songs of isolated singing and accompaniment tracks\n\n\n2) SingFake, WildSVDD, CtrSVDD: the first singing voice deepfake detection datasets containing hundreds of hours of recordings. WildSVDD (in-the-wild recordings) and CtrSVDD (lab-generated recordings) are part of the first singing voice deepfake detection challenge that we co-organize\n\n\n3) SynthTab: the first large-scale string-accurate guitar tablature transcription dataset containing 6,700 hours of recordings\n\n\n4) YouChorale: the first large-scale choral music dataset with aligned scores containing 22 hours of recordings\n\n\nOn education and outreach, this project helped with the development of a 4-day mini-course on Music and Math for high school students and curricular materials for undergraduate and graduate classes (Audio Signal Processing, Computer Audition, The Art of Machine Learning) in the Audio and Music Engineering program at the University of Rochester. It also helped develop materials for four tutorials (Audiovisual Music Processing, Audio-Visual Scene Understanding, A Brief History of Interactive Music Systems, Computer Assisted Music-Making Systems: Taxonomy, Review and Live Coding) at international conferences, two review articles (Automatic Music Transcription, Audiovisual Music Processing) and several lectures at the 2023 Winter School on Speech and Audio Processing. This project involved 9 PhD students, 4 masters students, and 8 undergraduate students in research. Through this project, we developed collaborations with researchers from many different institutions and backgrounds.\n\n\nThis research is intellectually transformative as it is of key interest in human-computer interaction, audio-visual and multi-modal machine learning. The research on computational music language models is fundamental for many tasks in music informatics including transcription, composition, and retrieval. The integration of analysis, performance and music language modeling towards a real-time collaborative system represents a new level of intelligent real-time computing.\n\n\nThis project has broader Impacts beyond the above-mentioned educational opportunities because it demonstrates the power and potential of the connection between music and technology to a broad audience. It could also enable many applications that can greatly benefit the society, including education (music education and STEM education through music), entertainment (music analysis and interaction), virtual and augmented reality (augmented concerts) and healthcare (music therapy).\n\n\n\t\t\t\t\tLast Modified: 08/20/2024\n\n\t\t\t\t\tSubmitted by: ZhiyaoDuan\n"
 }
}