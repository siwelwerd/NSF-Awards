{
 "awd_id": "1937435",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RTML: Large: Collaborative: Harmonizing Predictive Algorithms and Mixed Signal/Precision Circuits via Computation-Data Access Exchange and Adaptive Dataflows",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2019-09-10",
 "awd_max_amd_letter_date": "2019-09-10",
 "awd_abstract_narration": "Recent advances in machine learning are fueling a growing demand for intelligent Internet of Things (IoT), i.e., edge network applications. Many of them, such as autonomous vehicles, robots, and healthcare wearables, require real-time and in-situ learning to be perceived as truly intelligent. However, the limited computing and energy resources available at the edge device (e.g., mobile devices, sensors) stand at odds with the massive and growing cost of state-of-the-art machine learning training, posing a grand challenge for real-time machine learning (RTML) at the edge. This goal of this project is to foster a systematic breakthrough in achieving efficient online training of state-of-the-art machine learning algorithms in pervasive resource-constrained platforms and applications. An order of magnitude advance in RTML would enable numerous edge devices to proactively interpret and learn from new data, improve their own performance using what they have learned, and adapt to dynamic environments, all in real time. Success in this project will enable truly intelligent edge devices to penetrate all walks of life and thus generate significant impacts on societies and economies. This project will lead to new courses and open-education resources that can attract diverse groups of students and eventually deliver a platform for inclusion and innovation.  \r\n\r\nThe project addresses the RTML grand challenge using a three-pronged 'co-design' approach that seamlessly integrates algorithm, architecture, and circuit-level innovations. Specifically, at the algorithm level, an efficient training framework for RTML, for which trained models are also natively efficient for inference, will be established. Aggressive time and energy reductions can be achieved, at first by improving general training techniques, and then by focusing particularly on online learning and adaptation. At the architecture level, the project will first target reducing the high cost of data movement by trading it for lower-cost computation, and then generate optimal dataflows and hardware architectures to maximize the joint benefits of algorithms and hardware. At the circuit level, the project will leverage adaptive low-precision algorithms and architectures to design ultra-energy-efficient mixed-signal compute fabrics. Statistical computing techniques will be incorporated to demonstrate efficient, scalable, and robust machine learning chips. Finally, at the system level, an integration effort will be included to aid the realization of realistic system goals and to evaluate the innovations of the three core thrusts.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiran",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiran Chen",
   "pi_email_addr": "yiran.chen@duke.edu",
   "nsf_id": "000575362",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "701 W. Main St.",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277015013",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "082Z",
   "pgm_ref_txt": "RTML-Real Time Machine Learning"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We developed a co-design approach to address the challenges in real-time machine learning by seamlessly integrating algorithm, architecture, and circuit-level innovations:</p>\n<p>&nbsp;</p>\n<p>At the algorithm level, we establish an efficient training framework for RTML whose resulting trained models are also natively efficient for inference;</p>\n<p>&nbsp;</p>\n<p>At the architecture level, we first target reducing the high cost of data movement by trading it for lower-cost computation, and then generating optimal dataflows and hardware architectures to &nbsp;maximize the joint benefits of algorithms and hardware;</p>\n<p>&nbsp;</p>\n<p>At the circuit level, we leverage the created adaptive low-precision algorithms and architectures to design ultra-energy-efficient mixed-signal compute fabrics;</p>\n<p>&nbsp;</p>\n<p>At the system level, an integration effort is included to aid the realization of realistic system goals and to evaluate the above innovations.</p>\n<p>&nbsp;</p>\n<p>The resource is an important constraint when deploying Deep Neural Networks (DNNs) on mobile and edge devices. Existing Neural Architecture Search (NAS) methods commonly adopt the cell-based search approach, which limits the flexibility of network patterns in learned cell structures. Moreover, due to the topology-agnostic nature of existing works, including both cell-based and node-based approaches, the search process is time-consuming and the performance of found architecture may be sub-optimal. To address these problems, we proposed AutoShrink, a topology-aware NAS technique for searching efficient building blocks of neural architectures. AutoShrink is node-based and thus can learn flexible network patterns in cell structures within a topological search space. Directed Acyclic Graphs (DAGs) are used to abstract DNN architectures and progressively optimize the cell structure through edge shrinking. As the search space intrinsically reduces as the edges are progressively shrunk, AutoShrink explores a more flexible search space with even less search time. We evaluate AutoShrink on image classification and language tasks by crafting ShrinkCNN and ShrinkRNN models. ShrinkCNN is able to achieve more than half parameter reduction and save more than 1/3 computations with comparable accuracy of state-of-the-art (SOTA) models.</p>\n<p>&nbsp;</p>\n<p>Estimator-based NAS has been proposed recently to model the relationship between architectures and their performance to enable scalable and flexible search. However, existing estimator-based methods encode the architecture into a latent space without considering graph similarity. Ignoring graph similarity in node-based search space may induce a large inconsistency between similar graphs and their distance in the continuous encoding space, leading to inaccurate encoding representation and/or reduced representation capacity that can yield sub-optimal search results. To preserve graph correlation information in encoding, we proposed NASGEM, which stands for Neural Architecture Search via Graph Embedding Method. NASGEM is driven by a novel graph embedding method equipped with similarity measures to capture the graph topology information. By precisely estimating the graph distance and using an auxiliary Weisfeiler-Lehman kernel to guide the encoding, NASGEM can utilize additional structural information to get a more accurate graph representation to improve the search efficiency. GEMNet, a set of networks discovered by NASGEM, consistently outperforms networks crafted by existing search methods in classification tasks while having substantially fewer Multiply-Accumulates. We further transfer GEMNet for COCO object detection. In both one-stage and two-stage detectors, our GEMNet surpasses its manually-crafted and automatically-searched counterparts.</p>\n<p>&nbsp;</p>\n<p>The performance and efficiency of running modern Deep Neural Networks (DNNs) are heavily bounded by data movement. To mitigate the data movement bottlenecks, recent DNN inference accelerator designs widely adopt aggressive compression techniques and sparse-skipping mechanisms. These mechanisms avoid transferring or computing with zero-valued weights or activations to save time and energy. However, such sparse-skipping logic involves large input buffers and irregular data access patterns, thus precluding many energy-efficient data reuse opportunities and dataflows. We proposed Cascading Structured Pruning (CSP), a technique that preserves significantly more data reuse opportunities for higher energy efficiency while maintaining comparable performance relative to recent sparse architectures such as SparTen. CSP includes the following two components: At the algorithm level, CSP-A induces a predictable sparsity pattern that allows for low-overhead compression of weight data and sequential access to both activation and weight data. At the architecture level, CSP-H leverages CSP-A?s induced sparsity pattern with a novel dataflow to access unique activation data only once, thus removing the demand for large input buffers. Each CSP-H processing element (PE) employs a novel accumulation buffer design and a counter-based sparse-skipping mechanism to support the data flow with minimum controller overhead. We verify our approach on several representative models. Our simulated results show that CSP achieves on average 15X energy efficiency improvement over SparTen with comparable or superior speedup under most evaluations.</p>\n<p>&nbsp;</p>\n<p>This 3-year research project supported 5 graduate students and publications of 6 conference papers and 2 journal papers in total.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/11/2023<br>\n\t\t\t\t\tModified by: Yiran&nbsp;Chen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe developed a co-design approach to address the challenges in real-time machine learning by seamlessly integrating algorithm, architecture, and circuit-level innovations:\n\n \n\nAt the algorithm level, we establish an efficient training framework for RTML whose resulting trained models are also natively efficient for inference;\n\n \n\nAt the architecture level, we first target reducing the high cost of data movement by trading it for lower-cost computation, and then generating optimal dataflows and hardware architectures to  maximize the joint benefits of algorithms and hardware;\n\n \n\nAt the circuit level, we leverage the created adaptive low-precision algorithms and architectures to design ultra-energy-efficient mixed-signal compute fabrics;\n\n \n\nAt the system level, an integration effort is included to aid the realization of realistic system goals and to evaluate the above innovations.\n\n \n\nThe resource is an important constraint when deploying Deep Neural Networks (DNNs) on mobile and edge devices. Existing Neural Architecture Search (NAS) methods commonly adopt the cell-based search approach, which limits the flexibility of network patterns in learned cell structures. Moreover, due to the topology-agnostic nature of existing works, including both cell-based and node-based approaches, the search process is time-consuming and the performance of found architecture may be sub-optimal. To address these problems, we proposed AutoShrink, a topology-aware NAS technique for searching efficient building blocks of neural architectures. AutoShrink is node-based and thus can learn flexible network patterns in cell structures within a topological search space. Directed Acyclic Graphs (DAGs) are used to abstract DNN architectures and progressively optimize the cell structure through edge shrinking. As the search space intrinsically reduces as the edges are progressively shrunk, AutoShrink explores a more flexible search space with even less search time. We evaluate AutoShrink on image classification and language tasks by crafting ShrinkCNN and ShrinkRNN models. ShrinkCNN is able to achieve more than half parameter reduction and save more than 1/3 computations with comparable accuracy of state-of-the-art (SOTA) models.\n\n \n\nEstimator-based NAS has been proposed recently to model the relationship between architectures and their performance to enable scalable and flexible search. However, existing estimator-based methods encode the architecture into a latent space without considering graph similarity. Ignoring graph similarity in node-based search space may induce a large inconsistency between similar graphs and their distance in the continuous encoding space, leading to inaccurate encoding representation and/or reduced representation capacity that can yield sub-optimal search results. To preserve graph correlation information in encoding, we proposed NASGEM, which stands for Neural Architecture Search via Graph Embedding Method. NASGEM is driven by a novel graph embedding method equipped with similarity measures to capture the graph topology information. By precisely estimating the graph distance and using an auxiliary Weisfeiler-Lehman kernel to guide the encoding, NASGEM can utilize additional structural information to get a more accurate graph representation to improve the search efficiency. GEMNet, a set of networks discovered by NASGEM, consistently outperforms networks crafted by existing search methods in classification tasks while having substantially fewer Multiply-Accumulates. We further transfer GEMNet for COCO object detection. In both one-stage and two-stage detectors, our GEMNet surpasses its manually-crafted and automatically-searched counterparts.\n\n \n\nThe performance and efficiency of running modern Deep Neural Networks (DNNs) are heavily bounded by data movement. To mitigate the data movement bottlenecks, recent DNN inference accelerator designs widely adopt aggressive compression techniques and sparse-skipping mechanisms. These mechanisms avoid transferring or computing with zero-valued weights or activations to save time and energy. However, such sparse-skipping logic involves large input buffers and irregular data access patterns, thus precluding many energy-efficient data reuse opportunities and dataflows. We proposed Cascading Structured Pruning (CSP), a technique that preserves significantly more data reuse opportunities for higher energy efficiency while maintaining comparable performance relative to recent sparse architectures such as SparTen. CSP includes the following two components: At the algorithm level, CSP-A induces a predictable sparsity pattern that allows for low-overhead compression of weight data and sequential access to both activation and weight data. At the architecture level, CSP-H leverages CSP-A?s induced sparsity pattern with a novel dataflow to access unique activation data only once, thus removing the demand for large input buffers. Each CSP-H processing element (PE) employs a novel accumulation buffer design and a counter-based sparse-skipping mechanism to support the data flow with minimum controller overhead. We verify our approach on several representative models. Our simulated results show that CSP achieves on average 15X energy efficiency improvement over SparTen with comparable or superior speedup under most evaluations.\n\n \n\nThis 3-year research project supported 5 graduate students and publications of 6 conference papers and 2 journal papers in total.\n\n \n\n\t\t\t\t\tLast Modified: 02/11/2023\n\n\t\t\t\t\tSubmitted by: Yiran Chen"
 }
}