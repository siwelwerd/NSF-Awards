{
 "awd_id": "1849249",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: COLLAB: Planning Coordinated Event Observation for Structured Narratives",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-03-15",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 199999.0,
 "awd_amount": 209998.0,
 "awd_min_amd_letter_date": "2019-03-14",
 "awd_max_amd_letter_date": "2021-06-28",
 "awd_abstract_narration": "People easily recognize the dramatic moments that unfold in human events.  Dramatic turns of events are key to recognizing and communicating effective reports or stories about events.  Autonomous systems will work more effectively with humans in obtaining and conveying such narrative when they too can recognize what is dramatic (or tragic, or comical) about human events.  The challenge is to effectively convey such concepts to a computer in such a way that humans and autonomous systems can effectively work together in this. This research studies how to direct a team of robots to obtain video footage to produce clips that trace a dramatic story arc. It is an examination of how such systems might achieve goals that people consider to be abstract or high-level. Within this project, the programs that command teams of robots must predict likely events, direct the robots to be in position for obtaining the desired footage, and re-plan based on observed events. This challenge encompasses a rich and previously unstudied class of problems for robot systems.  It will constitute a unique demonstration of robots that are capable of achieving high-level goals as they process data in forms which combine both continuous and discrete views of the world in a new and unusual way.  More broadly, the research will advance how computers can fuse and summarize video streams. Both skills are needed for automatically generating synopses and in editing videos. Obvious places where this is useful include helping secure the nation (for surveillance), taming the deluge of online multimedia content (for summarization), and advancing applications in the creative industries (for editing). The research project will also use the ideas underlying these pieces in a new robotics course with students at three institutions going head-to-head in a series of competition-based class projects. This course (taught, among other places, at a Hispanic-Serving Institution) will contribute to the development of the STEM workforce of the future, helping increase American competitiveness.\r\n\r\nThe project advances current knowledge by formulating new theory and developing novel algorithms for autonomous and robot systems, with a focus on those systems with minimal or no human operator intervention. The research contributes novel data representations for robots that will inhabit rich environments such as those characterized by uncertain, unanticipated, and dynamically changing circumstances. One of the foundational ideas of the project is a means to specify sophisticated mission objectives via a recursive structure using prior work in compiler theory for computer languages. The project involves a strong connection between this theoretical work and demonstrated systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dylan",
   "pi_last_name": "Shell",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Dylan A Shell",
   "pi_email_addr": "dshell@cs.tamu.edu",
   "nsf_id": "000542643",
   "pi_start_date": "2019-03-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778454645",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  },
  {
   "pgm_ele_code": "054y00",
   "pgm_ele_name": "GVF - Global Venture Fund"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  },
  {
   "pgm_ref_code": "5935",
   "pgm_ref_txt": "FINLAND"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 199999.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 9999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project considered how one might permit robots to intelligently cooperate to observe and record time- and space-extended activities happening in the world. For instance, you might imagine a robot photographer attending a wedding, or a team of videographer robots in a safari park. Both instances have some high-level structure: for the wedding, you know there will speeches and dancing, whereas for the safari, activities of diurnal vs nocturnal animals differ in their activities. But these scenarios also involve uncertainty when we think about specifics. Will Aunt Gertrude pull a ridiculous dance move that makes her Internet famous? Or will a particularly famished&nbsp;crocodile&nbsp;remain hungry because the zebra successfully flee? We can't know beforehand, other than in some coarse statistical fashion.</p>\n<p>Primarily, our&nbsp;research contributed new methods to enable robots to reason about where they might capture events that fit some desirable story. Because robots do not have control over Aunt Getrude, they instead seek out positions&nbsp; where there is the best hope that something suitable will happen. Our work identified ways to express desirable sequences of events, and introduced new ways for teams of robots to coperate to optimize the capture process. In collaboration with research groups at the University of Houston and the University of&nbsp;South Carolina, we implemented and tested these methods on robots recording students running a race around a circuit.</p><br>\n<p>\n Last Modified: 12/21/2023<br>\nModified by: Dylan&nbsp;A&nbsp;Shell</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project considered how one might permit robots to intelligently cooperate to observe and record time- and space-extended activities happening in the world. For instance, you might imagine a robot photographer attending a wedding, or a team of videographer robots in a safari park. Both instances have some high-level structure: for the wedding, you know there will speeches and dancing, whereas for the safari, activities of diurnal vs nocturnal animals differ in their activities. But these scenarios also involve uncertainty when we think about specifics. Will Aunt Gertrude pull a ridiculous dance move that makes her Internet famous? Or will a particularly famishedcrocodileremain hungry because the zebra successfully flee? We can't know beforehand, other than in some coarse statistical fashion.\n\n\nPrimarily, ourresearch contributed new methods to enable robots to reason about where they might capture events that fit some desirable story. Because robots do not have control over Aunt Getrude, they instead seek out positions where there is the best hope that something suitable will happen. Our work identified ways to express desirable sequences of events, and introduced new ways for teams of robots to coperate to optimize the capture process. In collaboration with research groups at the University of Houston and the University ofSouth Carolina, we implemented and tested these methods on robots recording students running a race around a circuit.\t\t\t\t\tLast Modified: 12/21/2023\n\n\t\t\t\t\tSubmitted by: DylanAShell\n"
 }
}