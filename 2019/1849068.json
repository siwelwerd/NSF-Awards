{
 "awd_id": "1849068",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: INT: COLLAB: Do the Right Thing: Competing Ethical Frameworks Mediated by Moral Emotions in Human Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-02-15",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-02-12",
 "awd_max_amd_letter_date": "2019-02-12",
 "awd_abstract_narration": "Ethical decision-making is complex; and it is especially difficult to incorporate into autonomous systems. This project is focused on creating and testing an architecture for a robot that allows the system to be adaptable and ethical in its decision-making in complex situations. Since these systems may be called upon to adapt to evolving norms and cultural expectations, the development of an architecture that incorporates a range of philosophical theories and perspectives is crucially important. The robot's decisions will therefore be guided by multiple ethical frameworks and a simulated moral emotional state when they are tasked with making an ethical choice. Findings from the project could influence the design of countless types of robots including robot teachers, greeters, therapists, and companions. Thus, this research has the potential to impact large sections of society, including vulnerable populations such as individuals with disabilities that may interact with future robots. \r\n \r\nThe different underlying computational architectures will be expanded to reason in multiple ways according to different ethical frameworks; the selection of the framework and resulting action will depend upon the moral emotional state of the agent. Moral emotions can help to guide decision making when conflicting choices are available as recommended by competing or cooperating ethical frameworks.  The robot models the human subject's moral emotional state to choose an ethical action (from a set of competing actions derived from different ethical frameworks)  to select an action that best conforms to the human's expectations, existing biases, and the situational context itself. Thus, the researchers will develop an action selection mechanism that recognizes the moral emotional state of the overall situation, and use it as a means for selecting the appropriate ethical action to undertake in the face of conflicting choices. Specifically this research will investigate the appropriateness of the application of other-deception (deception intended to benefit the human being deceived by the robot)  dependent upon the subject?s emotional state (e.g., shame, embarrassment, guilt, or empathy) and the specific task context to determine whether to have the robot deceive the subject or not based on these conditions.The plan is to encode ethical frameworks, such as Kantianism, Utilitarianism, and W.D. Ross?s moral duties, that will serve as a basis for a robot?s decision-making.  Moral emotions will help guide the robot towards the selection of which ethical framework should take dominance for a particular situation and individuals involved.  The approach will be tested in two main scenarios dealing with practical ethical decisions.  First, a robot playing a game with a child and the role of other-deception for losing on purpose based on the perceived emotional state of the child.  Second, a robot interacting with older adults who are performing tasks such as pill sorting and whether the robot should seek to reduce frustration to facilitate training with a potential trade-off on safety.  A key goal is to have the robot reproduce an average human ethical choice and/or a choice that reflects the consensus of ethical experts where available.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Wagner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Wagner",
   "pi_email_addr": "alan.r.wagner@psu.edu",
   "nsf_id": "000602722",
   "pi_start_date": "2019-02-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>This project focused on challenges that arise when developing ethical and adaptable artificially intelligent systems.</strong></p>\n<p>Our research team investigated several ethical complexities that could arise when developing AI systems, particularly focusing on situations in which a robot might deceive children or give false encouragement to older adults if it might benefit them to do so. Our work focused on two hypothetical scenarios: the first involves whether a robot should deceive a child by intentionally making losing moves in a board game to make the child happy; and the second is a pill sorting task where a robot provides false encouragement to reduce an older adult&rsquo;s frustration while learning a pill sorting task. Our team developed a series of surveys to collect data about the scenarios, including ethics experts&rsquo; opinions as to how one respond if they were applying a formal ethical framework. We also surveyed American adults regarding how they would respond if they were placed in the scenarios. Finally, a separate set of respondents were asked how a robot should behave in the scenarios. Our results show that different facets of the scenarios, such as the described level of risk, seem to influence how the respondents determine whether deception is appropriate. When asked about the use of deception in low-risk (game-playing) versus high-risk (pill-sorting) situations, most people oppose the use of deception in the high-risk scenario whether the deceiver is robot or a person but are less opposed to deception in the low-risk scenario. The results, in general, indicate that people&rsquo;s opinions were the same regardless of whether the deceiver was another person or a robot. In summary, our project generated a large amount of data and insight that can inform the development of decision-making architectures embedded in robots that interact with humans.&nbsp;</p>\n<p><strong>In support of embedding ethics into a human-robot interaction system, we developed a case-based robotic architecture that can be guided by multiple ethical frameworks. </strong></p>\n<p>The architecture can determine whether it is appropriate to provide false encouragement in a pill-sorting scenario where a robot is guiding an older adult to perform a pill-sorting based on a formal ethical framework. Videos were produced that depict an older adult performing pill-sorting tasks under the guidance of a robot. We used these videos in survey studies to illustrate a hypothetical high-risk scenario in human-robot interaction.</p>\n<p><strong>During a later stage of the project, our team investigated whether the responses and reasons generated by large-language models (LLMs) align with the responses and reasons of survey participants when asked if a robot should deceive a person. </strong></p>\n<p>We queried and analyzed the responses of five popular LLMs across four hypothetical scenarios&mdash;two were high-risk and two were low-risk.&nbsp; The scenarios were similar to those developed during earlier stages of the project. &nbsp;Our findings reveal that the responses from the set of LLMs align with survey participants&rsquo; opinions that safety must be prioritized and thus deception should in general not be used in the high-risk scenario. On the other hand, our work also shows that the respondents&rsquo; opinions often diverge from the LLM responses in low-risk situations. These differences likely reflect challenges associated with AI development and the need to accurately capture individual human social nuances and moral perspectives.</p><br>\n<p>\n Last Modified: 03/01/2024<br>\nModified by: Alan&nbsp;Wagner</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1849068/1849068_10592254_1709307419534_Diagram2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1849068/1849068_10592254_1709307419534_Diagram2--rgov-800width.jpg\" title=\"Case-Based Architecture\"><img src=\"/por/images/Reports/POR/2024/1849068/1849068_10592254_1709307419534_Diagram2--rgov-66x44.jpg\" alt=\"Case-Based Architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Case-Based robot architecture with multiple underlying ethical frameworks implementation for pill sorting</div>\n<div class=\"imageCredit\">Ronald Arkin</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Alan&nbsp;Wagner\n<div class=\"imageTitle\">Case-Based Architecture</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849068/1849068_10592254_1709307283963_Diagram1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/1849068/1849068_10592254_1709307283963_Diagram1--rgov-800width.jpg\" title=\"Designing Ethical Decision-making Systems\"><img src=\"/por/images/Reports/POR/2024/1849068/1849068_10592254_1709307283963_Diagram1--rgov-66x44.jpg\" alt=\"Designing Ethical Decision-making Systems\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The results of this work serve as a basis for future investigations and design of ethical decision-making systems.</div>\n<div class=\"imageCredit\">Alan R. Wagner</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Alan&nbsp;Wagner\n<div class=\"imageTitle\">Designing Ethical Decision-making Systems</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project focused on challenges that arise when developing ethical and adaptable artificially intelligent systems.\n\n\nOur research team investigated several ethical complexities that could arise when developing AI systems, particularly focusing on situations in which a robot might deceive children or give false encouragement to older adults if it might benefit them to do so. Our work focused on two hypothetical scenarios: the first involves whether a robot should deceive a child by intentionally making losing moves in a board game to make the child happy; and the second is a pill sorting task where a robot provides false encouragement to reduce an older adults frustration while learning a pill sorting task. Our team developed a series of surveys to collect data about the scenarios, including ethics experts opinions as to how one respond if they were applying a formal ethical framework. We also surveyed American adults regarding how they would respond if they were placed in the scenarios. Finally, a separate set of respondents were asked how a robot should behave in the scenarios. Our results show that different facets of the scenarios, such as the described level of risk, seem to influence how the respondents determine whether deception is appropriate. When asked about the use of deception in low-risk (game-playing) versus high-risk (pill-sorting) situations, most people oppose the use of deception in the high-risk scenario whether the deceiver is robot or a person but are less opposed to deception in the low-risk scenario. The results, in general, indicate that peoples opinions were the same regardless of whether the deceiver was another person or a robot. In summary, our project generated a large amount of data and insight that can inform the development of decision-making architectures embedded in robots that interact with humans.\n\n\nIn support of embedding ethics into a human-robot interaction system, we developed a case-based robotic architecture that can be guided by multiple ethical frameworks. \n\n\nThe architecture can determine whether it is appropriate to provide false encouragement in a pill-sorting scenario where a robot is guiding an older adult to perform a pill-sorting based on a formal ethical framework. Videos were produced that depict an older adult performing pill-sorting tasks under the guidance of a robot. We used these videos in survey studies to illustrate a hypothetical high-risk scenario in human-robot interaction.\n\n\nDuring a later stage of the project, our team investigated whether the responses and reasons generated by large-language models (LLMs) align with the responses and reasons of survey participants when asked if a robot should deceive a person. \n\n\nWe queried and analyzed the responses of five popular LLMs across four hypothetical scenariostwo were high-risk and two were low-risk. The scenarios were similar to those developed during earlier stages of the project. Our findings reveal that the responses from the set of LLMs align with survey participants opinions that safety must be prioritized and thus deception should in general not be used in the high-risk scenario. On the other hand, our work also shows that the respondents opinions often diverge from the LLM responses in low-risk situations. These differences likely reflect challenges associated with AI development and the need to accurately capture individual human social nuances and moral perspectives.\t\t\t\t\tLast Modified: 03/01/2024\n\n\t\t\t\t\tSubmitted by: AlanWagner\n"
 }
}