{
 "awd_id": "1917994",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Exploring Social Learning in Collaborative Augmented Reality with Virtual Agents as Learning Companions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925126",
 "po_email": "abaylor@nsf.gov",
 "po_sign_block_name": "Amy Baylor",
 "awd_eff_date": "2019-07-15",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 749706.0,
 "awd_amount": 797242.0,
 "awd_min_amd_letter_date": "2019-07-10",
 "awd_max_amd_letter_date": "2022-06-22",
 "awd_abstract_narration": "In the context of medical student education, this Cyberlearning project will investigate how to design rich social learning experiences that integrate real and virtual features (objects or people) to enhance the learning process. To simulate a role-playing experience for patient communication, the project will incorporate  augmented reality, a 3D technology that enhances perception of the real world through a contextual overlay of virtual objects/information onto physical objects in real time. This project mirrors how medical students may work in a future telemedicine environment where intelligent virtual entities and human teams seamlessly interact for patient care. Results will inform the design of virtual agents/humans to support learning in a variety of educational domains. The research will provides powerful new tools for medical education including communication with patients which will foster lifelong, and just-in-time training and will contribute to advancing national health, prosperity and welfare.\r\n\r\nThe research employs a human-like, artificial intelligence-driven, high-fidelity, Emotive Virtual Patient that has life-like emotions and nonverbal expression with conversational and assessment capability using natural language processing. This is integrated within the Microsoft HoloLens to allow a remote participant to review a student's performance and provide feedback through text, audio, and video. Through iterative design-based research, the research will investigate: 1) how students learn by proxy through observing co-located and remote virtual and real collaborators; 2) whether students prefer to receive educational feedback from a co-located or remote peer, virtual peer, or a virtual professor; and, 3) the effects of collective agency when students can choose to learn socially from combinations of real or virtual learning companions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marjorie",
   "pi_last_name": "Zielke",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Marjorie A Zielke",
   "pi_email_addr": "maz031000@utdallas.edu",
   "nsf_id": "000534555",
   "pi_start_date": "2019-07-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Scotty",
   "pi_last_name": "Craig",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Scotty D Craig",
   "pi_email_addr": "scotty.craig@asu.edu",
   "nsf_id": "000345563",
   "pi_start_date": "2019-07-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Rege",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Robert Rege",
   "pi_email_addr": "Robert.Rege@UTSouthwestern.edu",
   "nsf_id": "000767837",
   "pi_start_date": "2019-07-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Wagner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Wagner",
   "pi_email_addr": "James.Wagner@UTSouthwestern.edu",
   "nsf_id": "000767889",
   "pi_start_date": "2019-07-10",
   "pi_end_date": "2021-08-10"
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Dallas",
  "inst_street_address": "800 WEST CAMPBELL RD.",
  "inst_street_address_2": "SP2.25",
  "inst_city_name": "RICHARDSON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9728832313",
  "inst_zip_code": "750803021",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "TX24",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT DALLAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJCVPNN1WFS5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Dallas",
  "perf_str_addr": "800 West Campbell Road",
  "perf_city_name": "Richardson",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "750803021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "TX24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802000",
   "pgm_ele_name": "Cyberlearn & Future Learn Tech"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8045",
   "pgm_ref_txt": "Cyberlearn & Future Learn Tech"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 749706.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 15838.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 15838.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 15860.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Social learning, simply defined as learning from others, is valuable as a modality that provides quick, informal education.&nbsp; Augmented reality (AR) offers learning advantages by providing a framework for human-machine teaming through virtual and real Pedagogical Agents as Learning Companions (PALs) [1] anchored in real environments.&nbsp; We present results from three collaborative AR experiments that explore social learning with PALs. &nbsp;These configurations are individual AR, co-located AR which involves two students in the same virtual patient (VP) simulation, and distributed AR which includes a remote live expert assessing and offering feedback to a student in an AR simulation.&nbsp; Our use case focuses on medical school students learning how to interview a patient with stroke symptoms.&nbsp; Despite noted challenges in quickly advancing technology, specifically the performance of the natural language processing (NLP), the research produced many instances of significant results in self-efficacy and conceptual and procedural learning.&nbsp;</p>\n<p>We call the general framework for this work A Metaverse AR Framework&nbsp;for Education and Training with Virtual and Real Humans (MARFA).&nbsp; The study use case is the Emotive Virtual Patient (EVP).&nbsp; The EVP prototype includes several interactive PALs - the conversational EVP, two virtual peers and a virtual professor.&nbsp; The EVP is a high-fidelity human-sized 3D hologram that responds to learners? questions during a medical interview. The medical interview is based on content from the Objective Structured Clinical Examinations (OSCEs) and covers topics like chief complaint, history of present illness, family history, and lifestyle factors. The virtual peers and virtual professor serve as learning companions and provide demonstrations and feedback to learners. Two novel interaction modalities have been developed as part of the MARFA framework: co-located AR and distributed AR. Co-located AR enables two students to share an AR learning experience; and the distributed AR modality enables a remote expert to observe a student?s experience and provide feedback. &nbsp;MARFA enables four levels for human-machine teaming: observation, interaction, assessment, and feedback.&nbsp; In each level, a learner can work with a variety of virtual and real PALs with different characteristics in the three different AR configurations.&nbsp; Figure 1 represents key components of the EVP prototype, and Figure 2 represents the four levels of social learning.</p>\n<p>Seventy-six first-and second-year medical students participated in three experiments which explored three configurations for social learning in collaborative AR: 1) individual learning while paired with a virtual colleague such as a peer or professor; 2) team learning while paired with a co-located human peer; and 3) individual learning through observation and interaction with a virtual PAL, and assessment and feedback from a human PAL expert through distributed AR. In all three experiments, we measured learners? self-efficacy, procedural and conceptual learning, perceptions of PALs, and preferences for social learning configurations. These experiments investigated existing social learning paradigms, such as the Social Cognitive Theory, and laid the groundwork for new social learning and human-machine teaming frameworks in collaborative AR.&nbsp;</p>\n<p>Experiment one, n=36, explored how different PALs (peers vs. expert) affect learning outcomes, self-efficacy, and perceptions of social learning with PALs. Students paired with the virtual professor experienced higher gains in self-efficacy and learning when compared to students paired with the virtual peer.&nbsp; Statistical tests revealed an overall significant increase in self-efficacy and procedural learning between pre-test and post-test measures. These findings echo findings from social learning literature and suggest more research is needed to better understand nuances of learning companion roles.</p>\n<p>Experiment two, n=30, explored two social learning factors: learning by proxy and learning by personal agency.&nbsp; Learning by proxy consisted of observing a fellow student interview the virtual patient and then providing feedback.&nbsp; Learning by personal agency consisted of interviewing the virtual patient and then receiving feedback.&nbsp; Learning by proxy received higher ratings than personal agency. &nbsp;This finding highlights the potential of collaborative AR for high-fidelity demonstrations and guided learning.</p>\n<p>Experiment three, n=10, iterated on experiment one and two and explored social learning when both a virtual and a human PAL is present. Students first observed a demonstration from a virtual professor, then interviewed the EVP, and then received feedback from a remote human expert.&nbsp; Statistical tests revealed a significant increase in procedural and conceptual knowledge, and an increase in self-efficacy.&nbsp;</p>\n<p>This work advances PALs research, in some cases supporting previous results, and in others bringing new ideas forward. As AR-devices and networks improve and become more ubiquitous; as more robust AR headsets become available, cost effective and ergonomic; and as advances in generative AI improve the NLP, assessment, and feedback modules the use of PALs will become even more flexible and effective -- supporting collaborative AR-based student social learning in a relatively stress-free, flexible environment.</p>\n<p>1.&nbsp;&nbsp;&nbsp;&nbsp; Kim, Y., Baylor, A. L., &amp; Pals Group. (2006). Pedagogical agents as learning companions: The role of agent competency and type of interaction.&nbsp;<em>Educational technology research and development</em>,&nbsp;<em>54</em>, 223-243.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2023<br>\n\t\t\t\t\tModified by: Marjorie&nbsp;Zielke</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1917994/1917994_10619946_1694121488301_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1917994/1917994_10619946_1694121488301_Figure1--rgov-800width.jpg\" title=\"Figure 1 - Components of the EVP Learning Module through the MARFA Framework\"><img src=\"/por/images/Reports/POR/2023/1917994/1917994_10619946_1694121488301_Figure1--rgov-66x44.jpg\" alt=\"Figure 1 - Components of the EVP Learning Module through the MARFA Framework\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The EVP learning module on the MARFA framework consists of the conversational EVP; virtual colleagues such as a female peer, a male peer, and a professor; and three distribution modalities --individual, co-located and distributed AR.</div>\n<div class=\"imageCredit\">The Center for Simulation and Synthetic Humans, UT Dallas</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marjorie&nbsp;Zielke</div>\n<div class=\"imageTitle\">Figure 1 - Components of the EVP Learning Module through the MARFA Framework</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1917994/1917994_10619946_1694121364542_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1917994/1917994_10619946_1694121364542_Figure2--rgov-800width.jpg\" title=\"Figure 2 - Four Social Learning Levels in MARFA\"><img src=\"/por/images/Reports/POR/2023/1917994/1917994_10619946_1694121364542_Figure2--rgov-66x44.jpg\" alt=\"Figure 2 - Four Social Learning Levels in MARFA\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">For each of the levels -- observation, interaction, assessment, and feedback, a learner can work with a variety of virtual and real PALs with different characteristics in three different AR configurations -- individual AR, co-located AR, and distributed AR.</div>\n<div class=\"imageCredit\">The Center for Simulation and Synthetic Humans, UT Dallas</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marjorie&nbsp;Zielke</div>\n<div class=\"imageTitle\">Figure 2 - Four Social Learning Levels in MARFA</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nSocial learning, simply defined as learning from others, is valuable as a modality that provides quick, informal education.  Augmented reality (AR) offers learning advantages by providing a framework for human-machine teaming through virtual and real Pedagogical Agents as Learning Companions (PALs) [1] anchored in real environments.  We present results from three collaborative AR experiments that explore social learning with PALs.  These configurations are individual AR, co-located AR which involves two students in the same virtual patient (VP) simulation, and distributed AR which includes a remote live expert assessing and offering feedback to a student in an AR simulation.  Our use case focuses on medical school students learning how to interview a patient with stroke symptoms.  Despite noted challenges in quickly advancing technology, specifically the performance of the natural language processing (NLP), the research produced many instances of significant results in self-efficacy and conceptual and procedural learning. \n\nWe call the general framework for this work A Metaverse AR Framework for Education and Training with Virtual and Real Humans (MARFA).  The study use case is the Emotive Virtual Patient (EVP).  The EVP prototype includes several interactive PALs - the conversational EVP, two virtual peers and a virtual professor.  The EVP is a high-fidelity human-sized 3D hologram that responds to learners? questions during a medical interview. The medical interview is based on content from the Objective Structured Clinical Examinations (OSCEs) and covers topics like chief complaint, history of present illness, family history, and lifestyle factors. The virtual peers and virtual professor serve as learning companions and provide demonstrations and feedback to learners. Two novel interaction modalities have been developed as part of the MARFA framework: co-located AR and distributed AR. Co-located AR enables two students to share an AR learning experience; and the distributed AR modality enables a remote expert to observe a student?s experience and provide feedback.  MARFA enables four levels for human-machine teaming: observation, interaction, assessment, and feedback.  In each level, a learner can work with a variety of virtual and real PALs with different characteristics in the three different AR configurations.  Figure 1 represents key components of the EVP prototype, and Figure 2 represents the four levels of social learning.\n\nSeventy-six first-and second-year medical students participated in three experiments which explored three configurations for social learning in collaborative AR: 1) individual learning while paired with a virtual colleague such as a peer or professor; 2) team learning while paired with a co-located human peer; and 3) individual learning through observation and interaction with a virtual PAL, and assessment and feedback from a human PAL expert through distributed AR. In all three experiments, we measured learners? self-efficacy, procedural and conceptual learning, perceptions of PALs, and preferences for social learning configurations. These experiments investigated existing social learning paradigms, such as the Social Cognitive Theory, and laid the groundwork for new social learning and human-machine teaming frameworks in collaborative AR. \n\nExperiment one, n=36, explored how different PALs (peers vs. expert) affect learning outcomes, self-efficacy, and perceptions of social learning with PALs. Students paired with the virtual professor experienced higher gains in self-efficacy and learning when compared to students paired with the virtual peer.  Statistical tests revealed an overall significant increase in self-efficacy and procedural learning between pre-test and post-test measures. These findings echo findings from social learning literature and suggest more research is needed to better understand nuances of learning companion roles.\n\nExperiment two, n=30, explored two social learning factors: learning by proxy and learning by personal agency.  Learning by proxy consisted of observing a fellow student interview the virtual patient and then providing feedback.  Learning by personal agency consisted of interviewing the virtual patient and then receiving feedback.  Learning by proxy received higher ratings than personal agency.  This finding highlights the potential of collaborative AR for high-fidelity demonstrations and guided learning.\n\nExperiment three, n=10, iterated on experiment one and two and explored social learning when both a virtual and a human PAL is present. Students first observed a demonstration from a virtual professor, then interviewed the EVP, and then received feedback from a remote human expert.  Statistical tests revealed a significant increase in procedural and conceptual knowledge, and an increase in self-efficacy. \n\nThis work advances PALs research, in some cases supporting previous results, and in others bringing new ideas forward. As AR-devices and networks improve and become more ubiquitous; as more robust AR headsets become available, cost effective and ergonomic; and as advances in generative AI improve the NLP, assessment, and feedback modules the use of PALs will become even more flexible and effective -- supporting collaborative AR-based student social learning in a relatively stress-free, flexible environment.\n\n1.     Kim, Y., Baylor, A. L., &amp; Pals Group. (2006). Pedagogical agents as learning companions: The role of agent competency and type of interaction. Educational technology research and development, 54, 223-243.\n\n \n\n\t\t\t\t\tLast Modified: 09/29/2023\n\n\t\t\t\t\tSubmitted by: Marjorie Zielke"
 }
}