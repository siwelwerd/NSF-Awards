{
 "awd_id": "1915786",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Bootstrap Methods in Modern Settings: Inference and Computation",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 220000.0,
 "awd_amount": 220000.0,
 "awd_min_amd_letter_date": "2019-06-19",
 "awd_max_amd_letter_date": "2021-07-01",
 "awd_abstract_narration": "Bootstrap methods play a central role in statistical methodology, and are among the most widely used tools for uncertainty quantification. Although bootstrap methods have an extensive literature, their scope of applicability is far from being fully understood in modern statistical problems. This is especially the case when data are described by high-dimensional models with many unknown parameters, or when then the amount of data exceeds computational constraints. Motivated by these challenges, the proposed research has two main objectives, which are to (1) analyze the theoretical validity of bootstrap methods in high-dimensional models, and (2) develop novel ways of using bootstrap methods to enhance large-scale computations. The graduate student will focus on the analysis of bootstrap methods for high-dimensional and large-scale data. \r\n\r\nWith regard to the first objective, the research will focus on overcoming certain limitations of the non-parametric bootstrap, particularly in the context of high-dimensional principal components analysis. This will be pursued through a generalization of the parametric bootstrap that is tailored to the statistics of interest. Examples of such statistics include those arising from the eigenvalues of sample covariance matrices. For the second objective, the research will explore bootstrap methods as a systematic way to estimate the errors of randomized algorithms. Given that bootstrap methods have been historically labeled as computationally intensive, this application is based on a relatively distinct perspective, since it seeks to use bootstrap methods to assist computation. In particular, bootstrap methods will be developed to obtain numerical error estimates that offer a practical alternative to worst-case error analysis.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Miles",
   "pi_last_name": "Lopes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Miles Lopes",
   "pi_email_addr": "melopes@ucdavis.edu",
   "nsf_id": "000703401",
   "pi_start_date": "2019-06-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "One Shields Ave",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 62299.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 89543.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 68158.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Bootstrap methods are a broad family of statistical techniques that are designed to measure the sampling variation of estimators and test statistics. Historically, bootstrap methods have found widespread success in solving traditional inference problems, but the full potential of these methods has yet to be realized in many contemporary settings. This project aimed to extend both the application and theory of bootstrap methods with regard to two essential aspects of modern statistics: high-dimensional data and randomized algorithms.</p>\n<p>Intellectual merit. With regard to inference tasks involving high-dimensional data, the project analyzed how well bootstrap methods can approximate the distributions of statistics that are computed from the eigenvalues of high-dimensional sample covariance matrices. Statistics of this type appear frequently in multivariate hypothesis testing problems, and are important in many applications ranging from finance to signal processing. Often, the literature analyzing these statistics assumes that the data are generated from an elliptical model. Within this context, the project led to the development of a new parametric bootstrap method that provably corrects for certain limitations of the non-parametric bootstrap method that relies on sampling with replacement. Alternatively, the project also showed that there are high-dimensional settings in which the non-parametric bootstrap can still successfully approximate the distributions of eigenvalue-based statistics, provided that the associated population covariance matrix has eigenvalues that decay at a suitable rate.</p>\n<p>Beyond the setting of high-dimensional data, the second main component of the project dealt with randomized algorithms, which have become extensively used to reduce the computational cost of processing very large datasets. Typically, the inexpensive cost of these algorithms is achieved by introducing a moderate amount of random error into their outputs. However, in practice, the user rarely knows how large these errors actually are. Towards a solution, the project developed and analyzed a number of bootstrap methods that can numerically estimate the errors of several fundamental types of randomized algorithms. Examples include randomized Newton methods in optimization, and randomized singular value decompositions in numerical linear algebra.</p>\n<p>Broader impacts. While completing the research described above, the PI contributed to the training of one undergraduate student, three PhD students, and one postdoc.&nbsp; Also, the PI actively communicated research results by giving many research presentations, and several of the papers produced in the course of the project were accompanied by open-source software repositories.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/26/2023<br>\n\t\t\t\t\tModified by: Miles&nbsp;Lopes</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBootstrap methods are a broad family of statistical techniques that are designed to measure the sampling variation of estimators and test statistics. Historically, bootstrap methods have found widespread success in solving traditional inference problems, but the full potential of these methods has yet to be realized in many contemporary settings. This project aimed to extend both the application and theory of bootstrap methods with regard to two essential aspects of modern statistics: high-dimensional data and randomized algorithms.\n\nIntellectual merit. With regard to inference tasks involving high-dimensional data, the project analyzed how well bootstrap methods can approximate the distributions of statistics that are computed from the eigenvalues of high-dimensional sample covariance matrices. Statistics of this type appear frequently in multivariate hypothesis testing problems, and are important in many applications ranging from finance to signal processing. Often, the literature analyzing these statistics assumes that the data are generated from an elliptical model. Within this context, the project led to the development of a new parametric bootstrap method that provably corrects for certain limitations of the non-parametric bootstrap method that relies on sampling with replacement. Alternatively, the project also showed that there are high-dimensional settings in which the non-parametric bootstrap can still successfully approximate the distributions of eigenvalue-based statistics, provided that the associated population covariance matrix has eigenvalues that decay at a suitable rate.\n\nBeyond the setting of high-dimensional data, the second main component of the project dealt with randomized algorithms, which have become extensively used to reduce the computational cost of processing very large datasets. Typically, the inexpensive cost of these algorithms is achieved by introducing a moderate amount of random error into their outputs. However, in practice, the user rarely knows how large these errors actually are. Towards a solution, the project developed and analyzed a number of bootstrap methods that can numerically estimate the errors of several fundamental types of randomized algorithms. Examples include randomized Newton methods in optimization, and randomized singular value decompositions in numerical linear algebra.\n\nBroader impacts. While completing the research described above, the PI contributed to the training of one undergraduate student, three PhD students, and one postdoc.  Also, the PI actively communicated research results by giving many research presentations, and several of the papers produced in the course of the project were accompanied by open-source software repositories.\n\n\t\t\t\t\tLast Modified: 10/26/2023\n\n\t\t\t\t\tSubmitted by: Miles Lopes"
 }
}