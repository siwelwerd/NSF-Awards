{
 "awd_id": "1925231",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Self-supervised Object Discovery, Detection and Visual Object Search",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 499990.0,
 "awd_amount": 499990.0,
 "awd_min_amd_letter_date": "2019-08-22",
 "awd_max_amd_letter_date": "2019-08-22",
 "awd_abstract_narration": "The ubiquitous deployment of service robots in homes and service environments rests on the ability to detect and recognize objects of interest and navigate towards them. In the past few years, largely enabled by machine-learning approaches, there has seen tremendous progress by the computer vision community. The standard datasets for training and evaluation, however, typically consist of static images curated from the internet and requiring extensive manual annotation. While this paradigm is effective for learning commonly encountered object categories, it does not generalize to possibly thousands of objects of interest in service robotics applications. The development of learning algorithms which do not require supervision through detailed human annotations is one of the central problems in computer vision and artificial intelligence. The open problems in this area are motivated by our understanding how humans and biological systems acquire new knowledge about visual content in the environments. This project will lead to a new class of algorithms for object discovery, object detection, 3-D environment modeling, and navigation. The research will support a cohort of diverse graduate and undergraduate students at George Mason University and will further advance the active vision benchmark dataset for evaluating the development and deployment of service robots.\r\n\r\nTechnical aims of the project focus on the development of methods for learning representations of objects which are specific to the context where the robot operates, can be learned in self-supervised manner without need for laborious annotations, and are reusable for multiple tasks. This research utilizes the camera motion as a form of self-supervision for learning the new multi-view object embeddings, followed by zero-shot or few-shot detection training of powerful object detector models with little or no labelling effort. The inherent limitations of object detection will be tackled in the robotic setting by semantic target driven navigation techniques, learned in a reinforcement learning framework on top of representations and architectures developed for object detection. These policies will constitute a basic set of visually guided navigation skills of the robotic agent and will be integrated with mapping and exploration strategies. The approaches will be motivated by the current challenges of embodied agents' perception in indoors scenes, but the solutions will be broadly applicable in settings which require the long-term on-going interactions of an agent with dynamically changing environments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jana",
   "pi_last_name": "Kosecka",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jana Kosecka",
   "pi_email_addr": "kosecka@gmu.edu",
   "nsf_id": "000207363",
   "pi_start_date": "2019-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Mason University",
  "inst_street_address": "4400 UNIVERSITY DR",
  "inst_street_address_2": "",
  "inst_city_name": "FAIRFAX",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7039932295",
  "inst_zip_code": "220304422",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "VA11",
  "org_lgl_bus_name": "GEORGE MASON UNIVERSITY",
  "org_prnt_uei_num": "H4NRWLFCDF43",
  "org_uei_num": "EADLFP7Z72E5"
 },
 "perf_inst": {
  "perf_inst_name": "George Mason University",
  "perf_str_addr": "4400 University Drive",
  "perf_city_name": "Fairfax",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "220304422",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "VA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499990.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-7c2824a2-7fff-1168-46de-8a0be17971d5\"> </span></p>\n<p dir=\"ltr\"><span>The focus of this project is to develop techniques for self-supervised techniques for semantic scene understanding in indoors scenes and integration of semantic and geometric representations with local and global navigation tasks. This entails components for learning semantic representations from video sequences, semantic mapping and localization techniques for indoor scenes and navigation and exploration policies that exploit the semantic representations and local maps. Our approach focused on data driven deep learning strategies and their combination with more traditional spatial mapping and planning approaches. Within the scope of this project we developed self-supervised techniques for learning semantic representation of indoors scenes, that have competitive performance, require significantly smaller fraction of labeled data and are more flexible and easier to adapt to new environments. The approach exploits video or multiple views of the environment, matches regions across the views and samples pixels from corresponding regions that serve as pairwise positive examples for learning effective representations. The approach can achieve competitive performance with only 5% of labeled data compared to supervised learning methods. We further demonstrated the effectiveness of semantic representations for design of efficient point-goal navigation and exploration policies for mobile agents. Instead of using computationally intensive Reinforcement Learning framework and end-to-end learning,&nbsp; we adopted frontier based approach to navigation and exploration.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We have shown how to use semantic segmentation to learn the rewards and success probabilities of views associated with frontiers in supervised manner and&nbsp; instantiated traditional model based MDP planning for point goal navigation and exploration tasks.&nbsp; </span><span>For exploration task, where the objective is to maximize the coverage, the learning module estimated the amount of space that will be revealed with exploring beyond&nbsp; frontiers and the time it will take to explore the area. For local visual servoing approach training the policy using representation derived from geometric correspondences as inputs yielded robust local navigation strategy.&nbsp;&nbsp;</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/19/2023<br>\nModified by: Jana&nbsp;Kosecka</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702956056273_example_traj_copy--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702956056273_example_traj_copy--rgov-800width.png\" title=\"Semantically informed exploration policy.\"><img src=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702956056273_example_traj_copy--rgov-66x44.png\" alt=\"Semantically informed exploration policy.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A comparison between the coverage of LFE-UNet explorer and the PONI-Greedy baseline on the test scenes. LFE-UNet outperforms the Greedy baseline by 17% on coverage after\n500 steps. On a small scene LFE-UNet finishes exploring the entire environment with 73 fewer steps than the Greedy baseline.</div>\n<div class=\"imageCredit\">Jana Kosecka</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jana&nbsp;Kosecka\n<div class=\"imageTitle\">Semantically informed exploration policy.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702955785069_semantic_segmentations_results--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702955785069_semantic_segmentations_results--rgov-800width.png\" title=\"Semantic Segmentations Results\"><img src=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702955785069_semantic_segmentations_results--rgov-66x44.png\" alt=\"Semantic Segmentations Results\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">RGB images (top row), ground truth (middle row) and predictions from our method (bottom row). Images  are from the\nReplica dataset and AVD dataset. Dark pixels in segmentation images do not have a valid class.</div>\n<div class=\"imageCredit\">Jana Kosecka</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jana&nbsp;Kosecka\n<div class=\"imageTitle\">Semantic Segmentations Results</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702955651555_semnatic_segmentaion_architecture--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702955651555_semnatic_segmentaion_architecture--rgov-800width.png\" title=\"Self-supervised Semantic Segmentation Approach.\"><img src=\"/por/images/Reports/POR/2023/1925231/1925231_10636774_1702955651555_semnatic_segmentaion_architecture--rgov-66x44.png\" alt=\"Self-supervised Semantic Segmentation Approach.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The segmentation model separately processes two views using the region correspondence. In exact matching only the\ncorrespondence points form positive pixels pairs. In region matching, pixels are samples from corresponding regions and are aligned using Barlow Twins loss.</div>\n<div class=\"imageCredit\">Jana Kosecka</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jana&nbsp;Kosecka\n<div class=\"imageTitle\">Self-supervised Semantic Segmentation Approach.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThe focus of this project is to develop techniques for self-supervised techniques for semantic scene understanding in indoors scenes and integration of semantic and geometric representations with local and global navigation tasks. This entails components for learning semantic representations from video sequences, semantic mapping and localization techniques for indoor scenes and navigation and exploration policies that exploit the semantic representations and local maps. Our approach focused on data driven deep learning strategies and their combination with more traditional spatial mapping and planning approaches. Within the scope of this project we developed self-supervised techniques for learning semantic representation of indoors scenes, that have competitive performance, require significantly smaller fraction of labeled data and are more flexible and easier to adapt to new environments. The approach exploits video or multiple views of the environment, matches regions across the views and samples pixels from corresponding regions that serve as pairwise positive examples for learning effective representations. The approach can achieve competitive performance with only 5% of labeled data compared to supervised learning methods. We further demonstrated the effectiveness of semantic representations for design of efficient point-goal navigation and exploration policies for mobile agents. Instead of using computationally intensive Reinforcement Learning framework and end-to-end learning, we adopted frontier based approach to navigation and exploration.\n\n\nWe have shown how to use semantic segmentation to learn the rewards and success probabilities of views associated with frontiers in supervised manner and instantiated traditional model based MDP planning for point goal navigation and exploration tasks. For exploration task, where the objective is to maximize the coverage, the learning module estimated the amount of space that will be revealed with exploring beyond frontiers and the time it will take to explore the area. For local visual servoing approach training the policy using representation derived from geometric correspondences as inputs yielded robust local navigation strategy.\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 12/19/2023\n\n\t\t\t\t\tSubmitted by: JanaKosecka\n"
 }
}