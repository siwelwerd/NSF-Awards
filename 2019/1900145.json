{
 "awd_id": "1900145",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Theory of Optimization Geometry and Algorithms for Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-07-26",
 "awd_max_amd_letter_date": "2019-07-26",
 "awd_abstract_narration": "Deep learning has attracted a significant amount of interest in recent years due to its widespread applicability in computer vision, artificial intelligence and natural language processing, alongside recent strides in autonomous driving. The theoretical underpinnings behind such success, however, remain elusive to a large extent, hindering its further adoption in other applications. This project aims to advance the theoretical foundations of training neural networks in terms of optimization landscape and algorithmic efficacy, which in turn should have a measurable impact on the practice of deep learning by providing guiding principles for network design, algorithm selection, hyperparameter tuning, and adversarial training. This project adopts an interdisciplinary approach fusing ideas from machine learning, optimization, statistical signal processing, high-dimensional statistics, nonparametric statistics, and information theory. This project will likewise develop courses and tutorials on theoretical foundations of large-scale machine learning and provide extensive training opportunities for students at all levels.\r\n\r\nThis project aims to develop a comprehensive theory to characterize the optimization landscape and geometry of loss functions and algorithmic regularizations of major neural network training problems, and explore how the network architecture---including depth, width, and activation functions---affect these properties, thus providing guidelines for the design of algorithms to train these networks more efficiently with theoretical performance guarantees. The project will explore the geometric properties and their impact on the optimization performance in training multi-layer neural networks, auto-encoders, generative adversarial networks, and adversarial training involving non-convex and saddle-point problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yingbin",
   "pi_last_name": "Liang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yingbin Liang",
   "pi_email_addr": "liang.889@osu.edu",
   "nsf_id": "000502099",
   "pi_start_date": "2019-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "The Ohio State University",
  "perf_str_addr": "606 Dreese Lab, 2015 Neil Avenue",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this research program is to develop theory of optimization geometry and its impact on the convergence performance of algorithms for deep learning, which is central in understanding the success of training neural networks. The proposed research consists of three thrusts: a) development of theory to understand the geometric property of optimization loss functions of deep learning and its impact on the convergence guarantee; b) exploration of statistical concentration properties of input data and its impact on the deep learning performance; and c) design of algorithms for saddle-point (i.e., min-max) problems in deep learning such as in generative adversarial networks (GANs) and adversarial training and characterization of the convergence guarantee for these algorithms. The following are the major research findings from the project.</p>\r\n<p>1. We investigated the convergence guarantee of training neural network models and transformers in various learning problems, including classification over one-layer neural networks, classification for overparameterized two-layer neural networks with the random pruning at initialization, classification over one-hidden-layer overparameterized ReLU networks in the neural tangent kernel (NTK) regime with large bias initialization, regression tasks in overparameterized meta-learning, and in-context learning with one attention layer. For all these problems, we characterized the training dynamics of gradient descent and established the corresponding convergence rate guarantee. Our results capture how the pruning, sparsity, attention mechanism, and meta-learning affect the generalization and benign overfitting of learning models.</p>\r\n<p>2. We investigated saddle-point problems (i.e., min-max optimization problems) in various settings to explore how the geometry of the loss functions affects the convergence rate, and further design faster algorithms for such problems. We first studied the saddle-point problem under KL geometry, and established the variable convergence of proximal-GDA to a critical point. We further proposed novel variance reduction algorithms for saddle-point problems, which enjoys less restrictive initialization requirement and an accuracy-independent (and much bigger) stepsize than the existing algorithm. We also studied the saddle-point problem over Markovian data and developed a novel model-free algorithms for finding near-optimal policy with improved sample efficiency than the existing algorithms of the same type.</p>\r\n<p>3. We investigated bilevel optimization problems, which is a more general framework of saddle-point problems and has arisen as a powerful tool for solving many deep learning problems such as meta-learning, hyperparameter optimization, neural architecture search, etc. We first provided a comprehensive convergence rate analysis for two popular bilevel algorithms and then further developed a more efficient Hessian-free bilevel algorithms for faster deep learning. We then investigated pessimistic bilevel optimization, and developed a novel algorithm to solve such a problem with convergence rate guarantee. We further applied our developed bilevel algorithms to various deep learning problems such as meta-learning and adversarial machine learning, and have shown that our algorithms outperform the baselines in experiments.&nbsp;</p>\r\n<p>4. We investigated various online learning and sequential decision-making problems, including online nonconvex optimization with limited oracle feedback, policy optimization problems in reinforcement learning over deterministic policies, zero-sum Markov game problems, and the general non-Markovian decision-making problem that includes partially observable MDPs (POMDPs) as special cases. We proposed novel sample- and computation-efficient algorithms and characterized the convergence guarantee for these algorithms. We also developed useful techniques for analyzing convergence processes under Markovian data.</p>\r\n<p>The results and findings of this project have been disseminated through numerous journal and conference publications as well as conference presentations in several research communities, including machine learning, optimization, signal processing, and information theory. The PI Liang gave seminar talks about the findings of this project at several universities. She also gave keynote talks in 2024 Conference on Parsimony and Learning (CPAL) and 2019 IEEE International Workshop on Machine Learning for Signal Processing (MLSP). The PI Liang also gave a tutorial lecture based on the findings of this project at 2024 North American School of Information Theory (NASIT).</p>\r\n<p>The students working on the project had opportunities to investigate a wide spectrum of projects such as deep learning, nonconvex optimization, reinforcement learning, and online learning. They also had opportunities to collaborate with other researchers in the field and to present the research results at various conferences.&nbsp;</p>\r\n<p>The project also conducted significant broadening participation in computing activities. PI Liang recruited the PhD student Daouda Sow from West Africa to conduct several projects including meta-learning, bilevel optimization, and robust adversarial training. Liang also served as the advisor for the high school student William Li, who pursued undergraduate study in the STEM field at Stanford University. Liang co-organized the 2022 Deep Learning Summer School at the Ohio State University, which featured tutorials and research presentations on deep learning and its applications. Liang has been co-organizing AI-EDGE Institute's Annual Summer Research Experiences for Undergraduates (REU) program for three years, which recruited undergraduate students nationally in U.S., particularly from regions with limited local AI capacity, and provided them with hands-on research opportunities on AI, machine learning and networking.&nbsp; &nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/03/2024<br>\nModified by: Yingbin&nbsp;Liang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this research program is to develop theory of optimization geometry and its impact on the convergence performance of algorithms for deep learning, which is central in understanding the success of training neural networks. The proposed research consists of three thrusts: a) development of theory to understand the geometric property of optimization loss functions of deep learning and its impact on the convergence guarantee; b) exploration of statistical concentration properties of input data and its impact on the deep learning performance; and c) design of algorithms for saddle-point (i.e., min-max) problems in deep learning such as in generative adversarial networks (GANs) and adversarial training and characterization of the convergence guarantee for these algorithms. The following are the major research findings from the project.\r\n\n\n1. We investigated the convergence guarantee of training neural network models and transformers in various learning problems, including classification over one-layer neural networks, classification for overparameterized two-layer neural networks with the random pruning at initialization, classification over one-hidden-layer overparameterized ReLU networks in the neural tangent kernel (NTK) regime with large bias initialization, regression tasks in overparameterized meta-learning, and in-context learning with one attention layer. For all these problems, we characterized the training dynamics of gradient descent and established the corresponding convergence rate guarantee. Our results capture how the pruning, sparsity, attention mechanism, and meta-learning affect the generalization and benign overfitting of learning models.\r\n\n\n2. We investigated saddle-point problems (i.e., min-max optimization problems) in various settings to explore how the geometry of the loss functions affects the convergence rate, and further design faster algorithms for such problems. We first studied the saddle-point problem under KL geometry, and established the variable convergence of proximal-GDA to a critical point. We further proposed novel variance reduction algorithms for saddle-point problems, which enjoys less restrictive initialization requirement and an accuracy-independent (and much bigger) stepsize than the existing algorithm. We also studied the saddle-point problem over Markovian data and developed a novel model-free algorithms for finding near-optimal policy with improved sample efficiency than the existing algorithms of the same type.\r\n\n\n3. We investigated bilevel optimization problems, which is a more general framework of saddle-point problems and has arisen as a powerful tool for solving many deep learning problems such as meta-learning, hyperparameter optimization, neural architecture search, etc. We first provided a comprehensive convergence rate analysis for two popular bilevel algorithms and then further developed a more efficient Hessian-free bilevel algorithms for faster deep learning. We then investigated pessimistic bilevel optimization, and developed a novel algorithm to solve such a problem with convergence rate guarantee. We further applied our developed bilevel algorithms to various deep learning problems such as meta-learning and adversarial machine learning, and have shown that our algorithms outperform the baselines in experiments.\r\n\n\n4. We investigated various online learning and sequential decision-making problems, including online nonconvex optimization with limited oracle feedback, policy optimization problems in reinforcement learning over deterministic policies, zero-sum Markov game problems, and the general non-Markovian decision-making problem that includes partially observable MDPs (POMDPs) as special cases. We proposed novel sample- and computation-efficient algorithms and characterized the convergence guarantee for these algorithms. We also developed useful techniques for analyzing convergence processes under Markovian data.\r\n\n\nThe results and findings of this project have been disseminated through numerous journal and conference publications as well as conference presentations in several research communities, including machine learning, optimization, signal processing, and information theory. The PI Liang gave seminar talks about the findings of this project at several universities. She also gave keynote talks in 2024 Conference on Parsimony and Learning (CPAL) and 2019 IEEE International Workshop on Machine Learning for Signal Processing (MLSP). The PI Liang also gave a tutorial lecture based on the findings of this project at 2024 North American School of Information Theory (NASIT).\r\n\n\nThe students working on the project had opportunities to investigate a wide spectrum of projects such as deep learning, nonconvex optimization, reinforcement learning, and online learning. They also had opportunities to collaborate with other researchers in the field and to present the research results at various conferences.\r\n\n\nThe project also conducted significant broadening participation in computing activities. PI Liang recruited the PhD student Daouda Sow from West Africa to conduct several projects including meta-learning, bilevel optimization, and robust adversarial training. Liang also served as the advisor for the high school student William Li, who pursued undergraduate study in the STEM field at Stanford University. Liang co-organized the 2022 Deep Learning Summer School at the Ohio State University, which featured tutorials and research presentations on deep learning and its applications. Liang has been co-organizing AI-EDGE Institute's Annual Summer Research Experiences for Undergraduates (REU) program for three years, which recruited undergraduate students nationally in U.S., particularly from regions with limited local AI capacity, and provided them with hands-on research opportunities on AI, machine learning and networking. \r\n\n\n\t\t\t\t\tLast Modified: 12/03/2024\n\n\t\t\t\t\tSubmitted by: YingbinLiang\n"
 }
}