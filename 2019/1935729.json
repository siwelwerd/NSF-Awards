{
 "awd_id": "1935729",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Information-Based Subdata Selection Inspired by Optimal Design of Experiments",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pena Edsel",
 "awd_eff_date": "2019-05-10",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 59994.0,
 "awd_amount": 59994.0,
 "awd_min_amd_letter_date": "2019-05-24",
 "awd_max_amd_letter_date": "2019-05-24",
 "awd_abstract_narration": "Extraordinary amounts of data are collected in many branches of science, in industry, and in government. The massive amounts of data provide incredible opportunities for making knowledge-based decisions and for advancing complicated research problems through data-driven discoveries. To capitalize on these opportunities, it is critical to develop methodology that facilitates the extraction of useful information from massive data in a computationally efficient way. Even the simplest analyses of the data can be computationally intensive or may no longer be feasible for big data. It is however often the case that valid conclusions can be drawn by considering only some of the data, referred to as subdata. This project develops optimal strategies for selecting subdata that retain, as much as possible, relevant information that was available in the massive data set. The methodology helps to identify the most informative data points, after which an analysis can proceed based on the selected subdata only. This facilitates data-driven decisions, scientific discoveries, and technological breakthroughs with computing resources that are readily available.\r\n \r\nExisting investigations for extracting information from big data with common computing power have focused on random subsampling-based approaches, which have as limitation that the amount of information extracted is only scalable to the subdata size, not the full data size. This project develops and expands the Information-Based Optimal Subdata Selection (IBOSS) method proposed by the PIs in the following directions: 1) It combines IBOSS with sparse variable selection methods in linear regression; 2) it develops subdata selection methods for generalized linear models; 3) it constructs computationally efficient algorithms for selecting the most informative subdata; and 4) it develops user-friendly software that supports the methodology.  The research is a significant addition to the field of big data science. It advances a new method for dealing with big data and has the potential to create novel research opportunities in statistical science and other quantitative fields. The results are valuable even when supercomputers are available, because cutting edge high performance computing facilities will always trail the exponential growth of data volume.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Stufken",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Stufken",
   "pi_email_addr": "jstufken@gmu.edu",
   "nsf_id": "000313720",
   "pi_start_date": "2019-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina Greensboro",
  "inst_street_address": "1000 SPRING GARDEN ST",
  "inst_street_address_2": "",
  "inst_city_name": "GREENSBORO",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "3363345878",
  "inst_zip_code": "274125068",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "NC06",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT GREENSBORO",
  "org_prnt_uei_num": "",
  "org_uei_num": "C13DF16LC3H4"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina Greensboro",
  "perf_str_addr": "1111 Spring Garden Street 2702 Moore Hum. Research Admin",
  "perf_city_name": "Greensboro",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "274125013",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NC06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 59994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>With massive amounts of data being collected in many areas of investigation, the size of datasets can become enormous. Traditional statistical methods for exploring or analyzing the data can become exorbitantly expensive in terms of computating time, if they can be used at all. This has led to the exploration of various ideas for reaching reliable conclusions or drawing valid inferences based on only a small portion (called a subsample or subdata) of an entire dataset (called the full data). One would like the size of the subdata to be small for reducing the computing time and computational complexity significantly, but also to be sufficiently large for obtaining reliable conclusions or inferences. Moreover, it is not only the size of the subdata that matters, but it is also important which observations from the full data are selected in the subdata. For subdata sets of the same size, whether selected stochastically or deterministically, some are more informative than others. Loosely, by selecting more informative subdata, one can select smaller subdata sets for reliable results, thereby reducing computational time to the largest extent.</span></p>\n<p><span>This project focused on developing and studying subdata selection methods inspired by ideas from optimal design of experiments. Results were obtained and reported under the assumption that the full data could be modeled through a linear model as well as under the assumption of lack of knowledge of an appropriate model for the full data. Subdata selection methods that are based on a specific model for the full data, such as a linear regression model, can give excellent results provided that the assumed model is indeed the correct model. But methods that perform well under such strong assumptions for the full data, may perform poorly when these assumptions are violated. Since it is not always easy to verify to what extent assumptions about the full data are violated, and since relationships between the variables in the full data can often be complicated, it is necessary to develop and study smart subdata selection methods for far weaker assumption about the full data. The project made significant progress on such methods. The project also made progress on combining subdata selection with variable selection in situations where the number of variables in the full data is large.</span></p>\n<p><span>The PI reported 11 publications in refereed professional outlets in statistics on work related to this project. He also gave 34 invited presentations at international professional conferences and at various universities, with the vast majority being closely related to this project. The PI also mentored and supervised contributions to the project by undergraduate and graduate students at multiple universities, primarily at Arizona State University (3 PhD students, one of whom is now employed as a researcher at a government lab) and at the University of North Carolina at Greensboro (4 undergraduate students). At the University of North Carolina at Greensboro, the PI also mentored a postdoctoral fellow for two and a half years while collaborating on various aspects of the project. She is now employed as a tenure track assistant professor at a research 1 university.</span></p>\n<p><span>In summary, the project succeeded in developing novel ideas for subdata selection methods that require fewer assumptions for the full data, consequently forming the foundation for methods that are more widely applicable. It leads to reliable results using subdata with dramatically shorter computing times. In the process, the project was also instrumental in research training for multiple students and a postdoctoral associate at two institutions.&nbsp;</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/04/2022<br>\n\t\t\t\t\tModified by: John&nbsp;Stufken</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith massive amounts of data being collected in many areas of investigation, the size of datasets can become enormous. Traditional statistical methods for exploring or analyzing the data can become exorbitantly expensive in terms of computating time, if they can be used at all. This has led to the exploration of various ideas for reaching reliable conclusions or drawing valid inferences based on only a small portion (called a subsample or subdata) of an entire dataset (called the full data). One would like the size of the subdata to be small for reducing the computing time and computational complexity significantly, but also to be sufficiently large for obtaining reliable conclusions or inferences. Moreover, it is not only the size of the subdata that matters, but it is also important which observations from the full data are selected in the subdata. For subdata sets of the same size, whether selected stochastically or deterministically, some are more informative than others. Loosely, by selecting more informative subdata, one can select smaller subdata sets for reliable results, thereby reducing computational time to the largest extent.\n\nThis project focused on developing and studying subdata selection methods inspired by ideas from optimal design of experiments. Results were obtained and reported under the assumption that the full data could be modeled through a linear model as well as under the assumption of lack of knowledge of an appropriate model for the full data. Subdata selection methods that are based on a specific model for the full data, such as a linear regression model, can give excellent results provided that the assumed model is indeed the correct model. But methods that perform well under such strong assumptions for the full data, may perform poorly when these assumptions are violated. Since it is not always easy to verify to what extent assumptions about the full data are violated, and since relationships between the variables in the full data can often be complicated, it is necessary to develop and study smart subdata selection methods for far weaker assumption about the full data. The project made significant progress on such methods. The project also made progress on combining subdata selection with variable selection in situations where the number of variables in the full data is large.\n\nThe PI reported 11 publications in refereed professional outlets in statistics on work related to this project. He also gave 34 invited presentations at international professional conferences and at various universities, with the vast majority being closely related to this project. The PI also mentored and supervised contributions to the project by undergraduate and graduate students at multiple universities, primarily at Arizona State University (3 PhD students, one of whom is now employed as a researcher at a government lab) and at the University of North Carolina at Greensboro (4 undergraduate students). At the University of North Carolina at Greensboro, the PI also mentored a postdoctoral fellow for two and a half years while collaborating on various aspects of the project. She is now employed as a tenure track assistant professor at a research 1 university.\n\nIn summary, the project succeeded in developing novel ideas for subdata selection methods that require fewer assumptions for the full data, consequently forming the foundation for methods that are more widely applicable. It leads to reliable results using subdata with dramatically shorter computing times. In the process, the project was also instrumental in research training for multiple students and a postdoctoral associate at two institutions. \n\n\t\t\t\t\tLast Modified: 12/04/2022\n\n\t\t\t\t\tSubmitted by: John Stufken"
 }
}