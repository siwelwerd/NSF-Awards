{
 "awd_id": "1934405",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Framework for Integrative Data Equity Systems",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 656000.0,
 "awd_amount": 656000.0,
 "awd_min_amd_letter_date": "2019-09-15",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Data Science continues to have a transformative impact on Science and Engineering, and on society at large, by enabling evidence-based decision making, reducing costs and errors, and improving objectivity. The techniques and technologies of data science also have enormous potential for harm if they reinforce inequity or leak private information.  As a result, sensitive datasets in the public and private sector are restricted from research use, slowing progress in those areas that have the most to gain: human services in the public sector.  Furthermore, the misuse of data science techniques and technologies will disproportionately harm underrepresented groups across race, gender, physical ability, sexual orientation, education, and more. These data equity issues are pervasive, and represent an existential risk for the use of data-driven methods in science and engineering. This project will establish a  Framework for Integrative Data Equity Systems (FIDES): an Institute for the study of systems that enable research on sensitive data while preventing misuse and misinterpretation. \r\n\r\nFIDES will enable interdisciplinary community convergence around data equity systems, with an initial study in critical domains such as mobility, housing, education, economic indicators, and government transparency, leading to the development of a novel data analytics infrastructure that supports responsibility in integrative data science.  Towards this goal, the project will address several technically challenging problems: (1) To be able to use data from multiple sources, risks related to privacy, bias, and the potential for misuse must be addressed. This project will develop principled methods for dataset processing to overcome these concerns.  (2) Individual datasets are difficult to integrate for use in advanced multi-layer network models.  This project considers methods to create pre-trained tensors over large collections of spatially and temporally coherent datasets, making them easier to incorporate while controlling for fairness and equity.  (3) Any dataset or model must be equipped with sufficient information to determine fitness for use, communicate limitations, and describe underlying assumptions.  This project will develop tools and techniques to produce \"nutritional labels\" for data and models, formalizing and standardizing ad hoc metadata approaches to provenance, specialized for equity issues. In addition to supporting methodological innovation in data science, the Institute will become a focal point for sharing expertise in data equity systems.  It will do so by establishing interfaces for interaction between data science and domain experts to promote expertise development and sharing of best practices, and by consistently supporting efforts on diversity and equity.\r\n\r\nThis project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bill",
   "pi_last_name": "Howe",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bill Howe",
   "pi_email_addr": "billhowe@uw.edu",
   "nsf_id": "000084191",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "1851 NE Grant Ln",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "099y00",
   "pgm_ele_name": "HDR-Harnessing the Data Revolu"
  },
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 305515.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 350485.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project established an area of research in data equity that is distinguished (though related) to responsibility in ML and AI.&nbsp; The distributional representation imbalances that can cause fairness issues in downstream ML systems are one source of inequity, but inequitable access to data for research, inequitable impact of use, inequitable effects of processing and cleaning on the distribution, and more all distinguish the field from ML-centric efforts.</p>\n<p>From an application perspective, we emphasize social science datasets and research analytics use cases, developing tools to reduce epistemic error from conclusions drawn through computational processes operating on imperfect (and inequitable).&nbsp; To this end, we characterize the distributions of ICPSR datasets, using the results to predict downstream equity issues in ML systems, afford discovery of datasets with more appropriate distributional properties, and combine datasets to provide a more representative sample.</p>\n<p>We synthesize spatio-temporal urban datasets to complete the sparse data record across cities and across policies to reduce biases from overfitting to the few available convenience samples.&nbsp; For example, taxi trip data has long been available in NYC due to a one-time policy decision that later was considered to incur too much privacy risk.&nbsp; As a result, a vast proportion of ride-hailing, traffic prediction, and mobility studies rely on this data, collectively overfitting to the dynamics of one (or a few) particular places.&nbsp; We aim to correct, extend, and synthesize data across domains to provide a more complete and representative source of information for data studies.&nbsp; In the primary figure, we provide an example of the model's capability to reconsittute missing data: the original data is at left, the mask used to simulate missing data is the next column, the masked data is in the third column, and the reconstituted data is at right.&nbsp; Qualitatively and quantiatively, we can complete the data record to make is more useful and accessible.&nbsp;</p>\n<p>We measure and intervene on data processing pipelines that change the distribution of the dataset as a silent side effect.&nbsp; For example, string-based algorithms for identifying duplicate names are less effective on the shorter strings common in romanized Asian names. These biases can accumulate in unpredictable ways during data processing; tools developed in this project can help monitor&nbsp; and counteract these distribution shifts.&nbsp;</p>\n<p>We consider the epistemological impacts of privacy interventions, where the distributions may change to affect the conclusions one can draw.&nbsp;&nbsp; Differential privacy --- injecting noise to hide identifying information --- provides strong guarantees about privacy but offers only empirical evidence about utility.&nbsp; We delivered a new evaluation methodology that measures utility in terms of published results that persist after noise is added, providing a more principled foundation for reasoning about utility and equity.&nbsp; In the second figure, we illustrate the overall workflow: we extract quantitiative claims from papers at left, generate synthetic data using various methods of differential privacy, then assess which claims hold on the synthetic data.&nbsp; We use this analysis to provide a more practical and actionable measure of utility. Our results show that some datasets are challenging for all methods, some claims appear non-reproducible as they do not hold under any method, and that modern methods appear viable for protecting social scientific data under reasonable and vetted assumptions about statistical methods, number of variables, amount of data, and correlation structures.&nbsp;&nbsp;</p>\n<p>In addition, the team is committed to dissemination of results to improve data literacy among experts and non-experts alike.&nbsp; Through talks, structured and unstructured learning modules, events, and more, the team has engaged the general public about these topics and how they affect our daily lives.</p>\n<p>In the context of generative AI, data equity issues become an even more important framing for reasoning about the proliferation of bias: models trained on closed, uncurated data exhibit emergent biases, which can then pollute other systems.&nbsp; In ongoing work, we emphasize controlling the distribution on input and output as a measure of model quality, using the tenets of data equity studied in this project to define this distribution.&nbsp;</p><br>\n<p>\n Last Modified: 04/18/2024<br>\nModified by: Bill&nbsp;Howe</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1934405/1934405_10642172_1713460209615_bikeshare_inpainting_example_short--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1934405/1934405_10642172_1713460209615_bikeshare_inpainting_example_short--rgov-800width.png\" title=\"Learning to fill in missing urban data\"><img src=\"/por/images/Reports/POR/2024/1934405/1934405_10642172_1713460209615_bikeshare_inpainting_example_short--rgov-66x44.png\" alt=\"Learning to fill in missing urban data\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Raster bikeshare data in NYC.  Our model uses partial convolutions to fill in missing patches based on historical data.  In this example, the mask simulates a large section of missing data (e.g., due to cloud cover obscuring satellites, or outages in on-the-ground telemetry.)</div>\n<div class=\"imageCredit\">Bill Howe</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bill&nbsp;Howe\n<div class=\"imageTitle\">Learning to fill in missing urban data</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1934405/1934405_10642172_1713460695120_workflow--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1934405/1934405_10642172_1713460695120_workflow--rgov-800width.png\" title=\"Epistemic parity workflow\"><img src=\"/por/images/Reports/POR/2024/1934405/1934405_10642172_1713460695120_workflow--rgov-66x44.png\" alt=\"Epistemic parity workflow\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Diagram of the process for assessing differential privacy metrics in terms of scientific conclusions that hold after injecting noise using various differential privacy methods.</div>\n<div class=\"imageCredit\">Bill Howe</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bill&nbsp;Howe\n<div class=\"imageTitle\">Epistemic parity workflow</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project established an area of research in data equity that is distinguished (though related) to responsibility in ML and AI. The distributional representation imbalances that can cause fairness issues in downstream ML systems are one source of inequity, but inequitable access to data for research, inequitable impact of use, inequitable effects of processing and cleaning on the distribution, and more all distinguish the field from ML-centric efforts.\n\n\nFrom an application perspective, we emphasize social science datasets and research analytics use cases, developing tools to reduce epistemic error from conclusions drawn through computational processes operating on imperfect (and inequitable). To this end, we characterize the distributions of ICPSR datasets, using the results to predict downstream equity issues in ML systems, afford discovery of datasets with more appropriate distributional properties, and combine datasets to provide a more representative sample.\n\n\nWe synthesize spatio-temporal urban datasets to complete the sparse data record across cities and across policies to reduce biases from overfitting to the few available convenience samples. For example, taxi trip data has long been available in NYC due to a one-time policy decision that later was considered to incur too much privacy risk. As a result, a vast proportion of ride-hailing, traffic prediction, and mobility studies rely on this data, collectively overfitting to the dynamics of one (or a few) particular places. We aim to correct, extend, and synthesize data across domains to provide a more complete and representative source of information for data studies. In the primary figure, we provide an example of the model's capability to reconsittute missing data: the original data is at left, the mask used to simulate missing data is the next column, the masked data is in the third column, and the reconstituted data is at right. Qualitatively and quantiatively, we can complete the data record to make is more useful and accessible.\n\n\nWe measure and intervene on data processing pipelines that change the distribution of the dataset as a silent side effect. For example, string-based algorithms for identifying duplicate names are less effective on the shorter strings common in romanized Asian names. These biases can accumulate in unpredictable ways during data processing; tools developed in this project can help monitor and counteract these distribution shifts.\n\n\nWe consider the epistemological impacts of privacy interventions, where the distributions may change to affect the conclusions one can draw. Differential privacy --- injecting noise to hide identifying information --- provides strong guarantees about privacy but offers only empirical evidence about utility. We delivered a new evaluation methodology that measures utility in terms of published results that persist after noise is added, providing a more principled foundation for reasoning about utility and equity. In the second figure, we illustrate the overall workflow: we extract quantitiative claims from papers at left, generate synthetic data using various methods of differential privacy, then assess which claims hold on the synthetic data. We use this analysis to provide a more practical and actionable measure of utility. Our results show that some datasets are challenging for all methods, some claims appear non-reproducible as they do not hold under any method, and that modern methods appear viable for protecting social scientific data under reasonable and vetted assumptions about statistical methods, number of variables, amount of data, and correlation structures.\n\n\nIn addition, the team is committed to dissemination of results to improve data literacy among experts and non-experts alike. Through talks, structured and unstructured learning modules, events, and more, the team has engaged the general public about these topics and how they affect our daily lives.\n\n\nIn the context of generative AI, data equity issues become an even more important framing for reasoning about the proliferation of bias: models trained on closed, uncurated data exhibit emergent biases, which can then pollute other systems. In ongoing work, we emphasize controlling the distribution on input and output as a measure of model quality, using the tenets of data equity studied in this project to define this distribution.\t\t\t\t\tLast Modified: 04/18/2024\n\n\t\t\t\t\tSubmitted by: BillHowe\n"
 }
}