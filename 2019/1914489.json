{
 "awd_id": "1914489",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: EAGER: Representation Learning of Connotation and Denotation Knowledge for Atomic Information Units",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 80000.0,
 "awd_amount": 104000.0,
 "awd_min_amd_letter_date": "2019-06-14",
 "awd_max_amd_letter_date": "2020-06-04",
 "awd_abstract_narration": "Many complex large-scale symbolic systems have been developed by human society, such as natural languages, logic, mathematics, which can encode very complicated information. While the major application and motivation for symbolic systems is communication among different entities either horizontally/spatially (e.g., a speaker gives a presentation in a meeting) and/or vertically/temporarily (e.g., reading a history book), information represented by these symbolic systems are ultimately created, revised, and processed/computed by human brain, a large volume of neural network processing information at the sub-symbolic level. What is the relationship and connection between symbolic processing and sub-symbolic processing? What is the internal structure and mechanism at the sub-symbolic level that supports symbol-level processing? Is there any deep computation mechanism for symbolic systems beyond shallow techniques (e.g., string match in Natural Language Processing)? All of these questions are fundamental to multiple research fields and scientific disciplines, and have attracted researchers and scientists of many generations ranging from the early study of denotation and connotation in philosophy to more recent investigation of semantic space construction. This project will focus on modeling and representation of denotation information for words in a natural language. With the fundamental focus on understanding of semantics at the sub-symbolic level, this project will provide valuable insight to natural languages and human intelligence in general, pave the way to build a large-scale testbed for fields such as computational linguistics, psychology, language acquisition, and bring broad interdisciplinary impact on many scientific fields. This project includes a carefully-crafted education component, which directly promotes undergraduate and graduate research and training, encourages minority and woman participation, and has a sustainable impact on Computer Science curricula and courseware beyond the scope of this project.\r\n\r\nThe overall goal of this project is to investigate how to represent a word with an internal structure (e.g., a neural network) beyond the existing approach of vector space to support more sophisticated symbolic processing techniques beyond shallow string matching. Specifically, there are three research objectives in this project. The first objective is to study the various options for representing internal structures of a word, which is closely related to the active research field of Neural Architecture Search. In the second objective, due to the large vocabulary size in a natural language and complex connotation information for modeling, a huge number of parameters in these neural architectures need to be learned and tuned, a bootstrapping approach will be developed to overcome the problem of data sparsity that challenges many deep learning models. With an unsupervised approach, the third objective of this project is to investigate a viable way for large-scale knowledge acquisition, which is generally recognized as a serious barrier for building real-world Artificial Intelligence systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ping",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ping Chen",
   "pi_email_addr": "ping.chen@umb.edu",
   "nsf_id": "000243183",
   "pi_start_date": "2019-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Boston",
  "inst_street_address": "100 WILLIAM T MORRISSEY BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "DORCHESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172875370",
  "inst_zip_code": "021253300",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "MA08",
  "org_lgl_bus_name": "UNIVERSITY OF MASS AT BOSTON",
  "org_prnt_uei_num": "CGCDJ24JJLZ1",
  "org_uei_num": "CGCDJ24JJLZ1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Boston",
  "perf_str_addr": "Department of Engineering, 100 M",
  "perf_city_name": "Boston,",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021253393",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "MA08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 80000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 24000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many complex large-scale symbolic systems have been developed by human society, such as natural languages, logic, mathematics, which can accurately encode very complicated information. The overall goal of this project is to model a language at multiple levels including the level of atomic information unit (e.g., a word in a natural language) and higher levels (e.g., a sentence) with more complex internal structure (e.g., a neural network).&nbsp;</p>\n<p>The first outcome of this project is a novel representation model at the sentence level. Learning sentence representations which capture rich semantic meanings has been crucial for many Natural Language Processing tasks. Pre-trained language models have achieved great success, but sentence embeddings extracted directly from these models do not perform well without fine tuning. We propose Contrastive Learning of Sentence Representations, a novel approach which applies contrastive learning to learn universal sentence representations on top of pre-trained language models. To evaluate the performance, we run experiments on a range of pre-trained language models including their variants on a series of Semantic Contextual Similarity tasks. Results show that our model gains significant performance improvements over existing language models.</p>\n<p>As the second outcome, this project contributes to the field of language modeling, understanding, and representation by focusing on integration and internal sub-symbolic representation of textual information at different levels (e.g., words, sentences) and heterogenous data types. In real-world applications often various types of data were acquired that are common. Learning from multiple data sources often leads to more valuable findings than any of the data sources can provide alone. However, simply merging features from disparate datasets usually will not produce a synergy effect. Hence, it becomes crucial to properly represent and manage the synergy, complementarity, and conflicts that arise in multi-source learning. In this work, we propose a multi-source learning approach called the Synergy LSTM model, which exploits complementarity and representation among multiple modalities. With the fundamental focus on understanding of internal representation and processing, this project will provide valuable insight to the field of language modeling and representation.To optimize the contribution and impact to society and benefit critical real-world problems, we applied our Synergy model to a challenging task of fall classification and fall risk factors identification, a critical problem for elder adults. Falls are classified based on short-text descriptions of the fall and physical characteristics of the person. Our results show high levels of performance in classifying falls, even when several details are not present in the description (e.g., activity is not explicitly stated). The results demonstrate the capability of using learned complementarities to successfully identify fall risk factors for an aging population.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/26/2022<br>\n\t\t\t\t\tModified by: Ping&nbsp;Chen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany complex large-scale symbolic systems have been developed by human society, such as natural languages, logic, mathematics, which can accurately encode very complicated information. The overall goal of this project is to model a language at multiple levels including the level of atomic information unit (e.g., a word in a natural language) and higher levels (e.g., a sentence) with more complex internal structure (e.g., a neural network). \n\nThe first outcome of this project is a novel representation model at the sentence level. Learning sentence representations which capture rich semantic meanings has been crucial for many Natural Language Processing tasks. Pre-trained language models have achieved great success, but sentence embeddings extracted directly from these models do not perform well without fine tuning. We propose Contrastive Learning of Sentence Representations, a novel approach which applies contrastive learning to learn universal sentence representations on top of pre-trained language models. To evaluate the performance, we run experiments on a range of pre-trained language models including their variants on a series of Semantic Contextual Similarity tasks. Results show that our model gains significant performance improvements over existing language models.\n\nAs the second outcome, this project contributes to the field of language modeling, understanding, and representation by focusing on integration and internal sub-symbolic representation of textual information at different levels (e.g., words, sentences) and heterogenous data types. In real-world applications often various types of data were acquired that are common. Learning from multiple data sources often leads to more valuable findings than any of the data sources can provide alone. However, simply merging features from disparate datasets usually will not produce a synergy effect. Hence, it becomes crucial to properly represent and manage the synergy, complementarity, and conflicts that arise in multi-source learning. In this work, we propose a multi-source learning approach called the Synergy LSTM model, which exploits complementarity and representation among multiple modalities. With the fundamental focus on understanding of internal representation and processing, this project will provide valuable insight to the field of language modeling and representation.To optimize the contribution and impact to society and benefit critical real-world problems, we applied our Synergy model to a challenging task of fall classification and fall risk factors identification, a critical problem for elder adults. Falls are classified based on short-text descriptions of the fall and physical characteristics of the person. Our results show high levels of performance in classifying falls, even when several details are not present in the description (e.g., activity is not explicitly stated). The results demonstrate the capability of using learned complementarities to successfully identify fall risk factors for an aging population.\n\n \n\n\t\t\t\t\tLast Modified: 07/26/2022\n\n\t\t\t\t\tSubmitted by: Ping Chen"
 }
}