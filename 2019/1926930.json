{
 "awd_id": "1926930",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: AI-DCL: Collaborative Research: Understanding and Overcoming Biases in STEM Education Using Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 24000.0,
 "awd_amount": 24000.0,
 "awd_min_amd_letter_date": "2019-08-19",
 "awd_max_amd_letter_date": "2019-08-19",
 "awd_abstract_narration": "Diversity is the cornerstone of innovation and essential for the progress of science. However, the number of female students in engineering, computing, and physical sciences in the United States remains strikingly low. The lack of diversity in science, technology, engineering, and mathematics (STEM) education is, to a significant extent, due to biases at different stages of schooling (e.g., different perceptions of math achievements by male and female students, lack of encouragement for female student enrollment in advance placement classes, stereotypes influencing college course selection). These biases appear as early as middle school: a critical period when student's educational experience can significantly influence their academic choices in high school and, ultimately, in deciding whether or not to enroll in STEM majors in college. In order to broaden the participation of women in STEM, it is critical to identify factors and practices in middle school learning environments that may attract (or repel) students into science. This award will use machine learning (ML) to develop new, automated, and data-driven methods for discovering and monitoring biases in STEM classrooms, focusing on middle school and early adolescence science and mathematics education.\r\n\r\nThe project combines methods from social psychology, machine learning, and information theory to create algorithmic tools that monitor middle school student, teacher, and school-level data for factors that impact students' engagement in STEM. These tools will (i) help identify pedagogical or socio-economic factors that have a disparate impact on the decisions made by female students, (ii) predict which students are most vulnerable to being discouraged from pursuing STEM fields, and (iii) inform effective interventions that help close the gender gap. Despite its potential, the use of ML in education is a double-edged sword: while ML algorithms may be able to flag discriminatory patterns, they can also propagate biases and have an unwarranted disparate impact if left unchecked.  Thus, in parallel, this project also aims to characterize the fairness challenges involved in deploying ML in education settings. The proposed approach will be validated on a dataset collected during a five year period from middle school students from across the United States.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Muriel",
   "pi_last_name": "Medard",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Muriel Medard",
   "pi_email_addr": "medard@mit.edu",
   "nsf_id": "000106371",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 24000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div class=\"eventName\">Machine learning algorithms are rapidly being adopted to aid pedagogical decision-making in applications ranging from grading to student placement. Are these algorithms fair?&nbsp;ML algorithms are prone to discrimination in domains where racial inequalities are already pervasive.&nbsp;Data-driven algorithms can inherit and exacerbate human biases in applications such as criminal justice, child welfare, and hiring, resulting in unfair decisions for historically underprivileged groups.&nbsp;In K-12 education---and STEM subjects in particular---racial disparities are widespread, with inequities existing in school funding, access to advanced placement classes, teacher perception, among many other areas.&nbsp;The persistent racial disparities that exist in K-12 schools create a high-stakes minefield for ML algorithms. &nbsp;Nevertheless, the use of ML in education-related applications continues to increase at an unrestrained pace, with little to no guidelines and best practices to ensure that deployed algorithms are fair to students from diverse backgrounds.</div>\n<div class=\"eventName\"></div>\n<div class=\"eventName\">We prove that, for predicting students' math performance, the standard machine learning practice of selecting a model that maximizes predictive accuracy can result in algorithms that give significantly more benefit of the doubt to White, Asian students and are more punitive to Black, Hispanic, Native American students. This disparity is masked by comparatively high predictive accuracy across both groups. Accuracy, the metric most often used to choose a ML model, can be deceptive when it comes to assessing whether a ML model is fair in predicting student performance. Accuracy measures the rate of misclassification, but not all errors committed by a ML model are equal. False-positives give the benefit of the doubt to students and provide more opportunities, whereas false-negatives undercut students' future potential.</div>\n<div class=\"eventName\"></div>\n<div class=\"eventName\">We suggest new interventions that help close this performance gap and do not require the use of a different algorithm for each student group. We propose a simple yet effective intervention that significantly reduces the observed gap in false-positive and false-negative rates between BHN and WA students.&nbsp;A standard practice is to use a training set that is representative of the actual student population. We show that by varying mixtures of BHN and WA students in the training set, the gap between error rates can be reduced with minimal impact on the model's overall accuracy. This result indicates that selecting the fairest demographic composition is not always straightforward. In fact, we show that the most counter-intuitive choice of using a homogeneous training set comprised only of one group can result in a model with the smallest disparity between the groups.&nbsp;Together, our results suggest new best practices for applying machine learning to education-related applications.</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/01/2022<br>\n\t\t\t\t\tModified by: Muriel&nbsp;Medard</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nMachine learning algorithms are rapidly being adopted to aid pedagogical decision-making in applications ranging from grading to student placement. Are these algorithms fair? ML algorithms are prone to discrimination in domains where racial inequalities are already pervasive. Data-driven algorithms can inherit and exacerbate human biases in applications such as criminal justice, child welfare, and hiring, resulting in unfair decisions for historically underprivileged groups. In K-12 education---and STEM subjects in particular---racial disparities are widespread, with inequities existing in school funding, access to advanced placement classes, teacher perception, among many other areas. The persistent racial disparities that exist in K-12 schools create a high-stakes minefield for ML algorithms.  Nevertheless, the use of ML in education-related applications continues to increase at an unrestrained pace, with little to no guidelines and best practices to ensure that deployed algorithms are fair to students from diverse backgrounds.\n\nWe prove that, for predicting students' math performance, the standard machine learning practice of selecting a model that maximizes predictive accuracy can result in algorithms that give significantly more benefit of the doubt to White, Asian students and are more punitive to Black, Hispanic, Native American students. This disparity is masked by comparatively high predictive accuracy across both groups. Accuracy, the metric most often used to choose a ML model, can be deceptive when it comes to assessing whether a ML model is fair in predicting student performance. Accuracy measures the rate of misclassification, but not all errors committed by a ML model are equal. False-positives give the benefit of the doubt to students and provide more opportunities, whereas false-negatives undercut students' future potential.\n\nWe suggest new interventions that help close this performance gap and do not require the use of a different algorithm for each student group. We propose a simple yet effective intervention that significantly reduces the observed gap in false-positive and false-negative rates between BHN and WA students. A standard practice is to use a training set that is representative of the actual student population. We show that by varying mixtures of BHN and WA students in the training set, the gap between error rates can be reduced with minimal impact on the model's overall accuracy. This result indicates that selecting the fairest demographic composition is not always straightforward. In fact, we show that the most counter-intuitive choice of using a homogeneous training set comprised only of one group can result in a model with the smallest disparity between the groups. Together, our results suggest new best practices for applying machine learning to education-related applications.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/01/2022\n\n\t\t\t\t\tSubmitted by: Muriel Medard"
 }
}