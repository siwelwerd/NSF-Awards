{
 "awd_id": "1849303",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: COLLAB: Planning Coordinated Event Observation for Structured Narratives",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-03-15",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2019-03-14",
 "awd_max_amd_letter_date": "2019-03-14",
 "awd_abstract_narration": "People easily recognize the dramatic moments that unfold in human events.  Dramatic turns of events are key to recognizing and communicating effective reports or stories about events.  Autonomous systems will work more effectively with humans in obtaining and conveying such narrative when they too can recognize what is dramatic (or tragic, or comical) about human events.  The challenge is to effectively convey such concepts to a computer in such a way that humans and autonomous systems can effectively work together in this. This research studies how to direct a team of robots to obtain video footage to produce clips that trace a dramatic story arc. It is an examination of how such systems might achieve goals that people consider to be abstract or high-level. Within this project, the programs that command teams of robots must predict likely events, direct the robots to be in position for obtaining the desired footage, and re-plan based on observed events. This challenge encompasses a rich and previously unstudied class of problems for robot systems.  It will constitute a unique demonstration of robots that are capable of achieving high-level goals as they process data in forms which combine both continuous and discrete views of the world in a new and unusual way.  More broadly, the research will advance how computers can fuse and summarize video streams. Both skills are needed for automatically generating synopses and in editing videos. Obvious places where this is useful include helping secure the nation (for surveillance), taming the deluge of online multimedia content (for summarization), and advancing applications in the creative industries (for editing). The research project will also use the ideas underlying these pieces in a new robotics course with students at three institutions going head-to-head in a series of competition-based class projects. This course (taught, among other places, at a Hispanic-Serving Institution) will contribute to the development of the STEM workforce of the future, helping increase American competitiveness.\r\n\r\nThe project advances current knowledge by formulating new theory and developing novel algorithms for autonomous and robot systems, with a focus on those systems with minimal or no human operator intervention. The research contributes novel data representations for robots that will inhabit rich environments such as those characterized by uncertain, unanticipated, and dynamically changing circumstances. One of the foundational ideas of the project is a means to specify sophisticated mission objectives via a recursive structure using prior work in compiler theory for computer languages. The project involves a strong connection between this theoretical work and demonstrated systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Becker",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron T Becker",
   "pi_email_addr": "atbecker@uh.edu",
   "nsf_id": "000668782",
   "pi_start_date": "2019-03-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Houston",
  "inst_street_address": "4300 MARTIN LUTHER KING BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "HOUSTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7137435773",
  "inst_zip_code": "772043067",
  "inst_country_name": "United States",
  "cong_dist_code": "18",
  "st_cong_dist_code": "TX18",
  "org_lgl_bus_name": "UNIVERSITY OF HOUSTON SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "QKWEF8XLMTT3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Houston",
  "perf_str_addr": "4800 Calhoun Boulevard",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "772042015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "TX18",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-46e197e3-7fff-fdbe-44d8-a20a0fc01030\"> </span></p>\n<p dir=\"ltr\"><span>People easily recognize the dramatic moments that unfold in human events. Dramatic turns of events are key to recognizing and communicating effective reports or stories about events. Autonomous systems will work more effectively with humans in obtaining and conveying such a narrative when they too can recognize what is dramatic (or tragic, or comical) about human events. The challenge is to effectively convey such concepts to a computer in such a way that humans and autonomous systems can effectively work together in this. This research studied how to direct a team of robots to obtain video footage to produce clips that trace a dramatic story arc. It is an examination of how such systems might achieve goals that people consider to be abstract or high-level. Within this project, the programs that command teams of robots predict likely events, direct the robots to be in position for obtaining the desired footage, and re-plan based on observed events. This challenge constituted a unique demonstration of robots that are capable of achieving high-level goals as they process data in forms which combine both continuous and discrete views of the world in a new and unusual way. Obvious places where this is useful include helping secure the nation (for surveillance), taming the deluge of online multimedia content (for summarization), and advancing applications in the creative industries (for editing). This research, at a Hispanic-Serving Institution, contributes to the development of the STEM workforce of the future, helping increase American competitiveness.</span></p>\n<p dir=\"ltr\"><span>The project advanced current knowledge by formulating new theory and developing novel algorithms for autonomous and robot systems, with a focus on those systems with minimal or no human operator intervention. The research contributed novel data representations for robots that will inhabit rich environments such as those characterized by uncertain, unanticipated, and dynamically changing circumstances. One of the foundational ideas of the project is a means to specify sophisticated mission objectives via a recursive structure using prior work in compiler theory for computer languages. The project involved a strong connection between this theoretical work and demonstrated systems.</span></p>\n<p dir=\"ltr\"><span>&nbsp;Research for this project was used in the dissertations of two Ph.D. students, the theses of two MS students, introduced 9 undergraduates to university research, and led to 12 peer-reviewed publications.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/05/2023<br>\n\t\t\t\t\tModified by: Aaron&nbsp;T&nbsp;Becker</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688383870841_TwoRobotsSunset(1)--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688383870841_TwoRobotsSunset(1)--rgov-800width.jpg\" title=\"Two Robots at Sunset\"><img src=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688383870841_TwoRobotsSunset(1)--rgov-66x44.jpg\" alt=\"Two Robots at Sunset\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">he two robots used to demonstrate our algorithms byvideoing human runners with requested shot types. See videoattachment for representative footage</div>\n<div class=\"imageCredit\">Rhema Ike</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aaron&nbsp;T&nbsp;Becker</div>\n<div class=\"imageTitle\">Two Robots at Sunset</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688383937973_RunningWithRobots(1)--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688383937973_RunningWithRobots(1)--rgov-800width.jpg\" title=\"Running with robots\"><img src=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688383937973_RunningWithRobots(1)--rgov-66x44.jpg\" alt=\"Running with robots\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Robots recording runners with different shot types.</div>\n<div class=\"imageCredit\">Rhema Ike</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aaron&nbsp;T&nbsp;Becker</div>\n<div class=\"imageTitle\">Running with robots</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688384058000_Robot_Diagram--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688384058000_Robot_Diagram--rgov-800width.jpg\" title=\"Robot Diagram\"><img src=\"/por/images/Reports/POR/2023/1849303/1849303_10596984_1688384058000_Robot_Diagram--rgov-66x44.jpg\" alt=\"Robot Diagram\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The fast ground robot from the left and right side.</div>\n<div class=\"imageCredit\">Rhema Ike</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Aaron&nbsp;T&nbsp;Becker</div>\n<div class=\"imageTitle\">Robot Diagram</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nPeople easily recognize the dramatic moments that unfold in human events. Dramatic turns of events are key to recognizing and communicating effective reports or stories about events. Autonomous systems will work more effectively with humans in obtaining and conveying such a narrative when they too can recognize what is dramatic (or tragic, or comical) about human events. The challenge is to effectively convey such concepts to a computer in such a way that humans and autonomous systems can effectively work together in this. This research studied how to direct a team of robots to obtain video footage to produce clips that trace a dramatic story arc. It is an examination of how such systems might achieve goals that people consider to be abstract or high-level. Within this project, the programs that command teams of robots predict likely events, direct the robots to be in position for obtaining the desired footage, and re-plan based on observed events. This challenge constituted a unique demonstration of robots that are capable of achieving high-level goals as they process data in forms which combine both continuous and discrete views of the world in a new and unusual way. Obvious places where this is useful include helping secure the nation (for surveillance), taming the deluge of online multimedia content (for summarization), and advancing applications in the creative industries (for editing). This research, at a Hispanic-Serving Institution, contributes to the development of the STEM workforce of the future, helping increase American competitiveness.\nThe project advanced current knowledge by formulating new theory and developing novel algorithms for autonomous and robot systems, with a focus on those systems with minimal or no human operator intervention. The research contributed novel data representations for robots that will inhabit rich environments such as those characterized by uncertain, unanticipated, and dynamically changing circumstances. One of the foundational ideas of the project is a means to specify sophisticated mission objectives via a recursive structure using prior work in compiler theory for computer languages. The project involved a strong connection between this theoretical work and demonstrated systems.\n Research for this project was used in the dissertations of two Ph.D. students, the theses of two MS students, introduced 9 undergraduates to university research, and led to 12 peer-reviewed publications.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 07/05/2023\n\n\t\t\t\t\tSubmitted by: Aaron T Becker"
 }
}