{
 "awd_id": "1850206",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CPS: Towards a Model-Based Reinforcement Learning Approach for Safe Operation of Distributed Energy Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2019-05-01",
 "awd_exp_date": "2022-04-30",
 "tot_intn_awd_amt": 172000.0,
 "awd_amount": 172000.0,
 "awd_min_amd_letter_date": "2019-03-01",
 "awd_max_amd_letter_date": "2019-03-01",
 "awd_abstract_narration": "With the increasing penetration of renewables on the electric grid and ready availability of real-time data about electricity usage, the electric power grid is becoming a large-scale complex Cyber-Physical System (CPS) to meet consumer demands for electricity through the day, every day.  Reinforcement learning (RL) algorithms offer these CPS systems an approach to seamlessly integrating distributed energy sources into the legacy electric grid more efficiently, effectively and affordably.  It also offers significant potential savings in capital investment cost and labor, and greater resiliency to disruptions in service. \r\n\r\nThis research project develops a framework for model-based online reinforcement learning to address several classes of problems. First, it models control of energy CPS as finite horizon RL problems. Second, instead of focusing on asymptotic convergence, this project focuses on optimal finite time performance. Third, while a simplistic learning algorithm might drive an energy CPS to an unsafe region of operations, thereby risking unwanted consequences, this project develops safe RL algorithms that optimize performance and respect safety constraints. Fourth, this project exploits the physical properties of the energy CPS to avoid the dimensionality problems, often associated with RL problems.  Lastly, the project develops sequential algorithms using a \"contextual bandits\" approach for learning consumer specific parameters and adaptively scheduling to account for consumer usage.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dileep",
   "pi_last_name": "Kalathil",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dileep Kalathil",
   "pi_email_addr": "dileep.kalathil@tamu.edu",
   "nsf_id": "000760025",
   "pi_start_date": "2019-03-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "215E Weisenbaker Engineering Bdg",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433128",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 172000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on developing reinforcement learning (RL) based algorithms for Cyber-Physical System (CPS), with a special focus on energy CPS. The research goal of the proposal was divided into four main tasks. Task 1 addressed data efficient reinforcement learning algorithms with optimal finite time performance guarantees. Task 2 focused on designing safe and robust reinforcement learning algorithms for the CPS. Task 3 focused on tractable reinforcement learning for systems with very complex dynamics. Task 4 addressed the problem of learning the behavior of consumers from their usage data.&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>Intellectual Merit: As part of Task 1, we developed multi-armed bandits algorithms which make use of the structure of the underlying problem in order to achieve a bounded regret as opposed to the standard logarithmic regret. As part of Task 2, we considered the problem safe reinforcement learning. We developed multiple RL algorithms which maintain the safety constraints during learning and deployment without compromising on the performance. As part of Task 2, we also considered the problem of robust reinforcement learning where the goal is to find a control policy that is robust against parameter mismatches between the training and testing environment. We developed an online learning algorithm and an offline learning algorithm, addressing different facets of the robust RL, and provided provable guarantees on their performance. &nbsp;As part of Task 3, we developed RL algorithms for specific applications in energy CPS. In particular, we developed an RL&nbsp; algorithm for robust relay protection control in power distribution systems. We also developed a fully decentralized deep RL&nbsp; algorithm for controlling the Photovoltaics (PVs) in power distribution grids for the optimal provision of real and reactive power. As part of Task 4, we developed a mean field model and RL algorithm for addressing the learning problems with a very large number of agents.</p>\n<p>&nbsp;</p>\n<p>Broader Impact: Graduate students were involved in all aspects of research from formulating problems to solving them and presenting their work at seminars and conferences. The PI has mentored one female masters student and two undergraduate students as a part of this project. The PI has also mentored an undergraduate student from an underrepresented group through the Louis Stokes Alliance for Minority Participation (LSAMP) program at TAMU. The PI gave a special tutorial on `Reinforcement Learning' to a wider audience that consists of faculty and students from many departments across the college of engineering. The papers and codebase produced as part of&nbsp; this&nbsp;project are publicly available through the PI's webpage.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/27/2022<br>\n\t\t\t\t\tModified by: Dileep&nbsp;Kalathil</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focused on developing reinforcement learning (RL) based algorithms for Cyber-Physical System (CPS), with a special focus on energy CPS. The research goal of the proposal was divided into four main tasks. Task 1 addressed data efficient reinforcement learning algorithms with optimal finite time performance guarantees. Task 2 focused on designing safe and robust reinforcement learning algorithms for the CPS. Task 3 focused on tractable reinforcement learning for systems with very complex dynamics. Task 4 addressed the problem of learning the behavior of consumers from their usage data.  \n\n \n\nIntellectual Merit: As part of Task 1, we developed multi-armed bandits algorithms which make use of the structure of the underlying problem in order to achieve a bounded regret as opposed to the standard logarithmic regret. As part of Task 2, we considered the problem safe reinforcement learning. We developed multiple RL algorithms which maintain the safety constraints during learning and deployment without compromising on the performance. As part of Task 2, we also considered the problem of robust reinforcement learning where the goal is to find a control policy that is robust against parameter mismatches between the training and testing environment. We developed an online learning algorithm and an offline learning algorithm, addressing different facets of the robust RL, and provided provable guarantees on their performance.  As part of Task 3, we developed RL algorithms for specific applications in energy CPS. In particular, we developed an RL  algorithm for robust relay protection control in power distribution systems. We also developed a fully decentralized deep RL  algorithm for controlling the Photovoltaics (PVs) in power distribution grids for the optimal provision of real and reactive power. As part of Task 4, we developed a mean field model and RL algorithm for addressing the learning problems with a very large number of agents.\n\n \n\nBroader Impact: Graduate students were involved in all aspects of research from formulating problems to solving them and presenting their work at seminars and conferences. The PI has mentored one female masters student and two undergraduate students as a part of this project. The PI has also mentored an undergraduate student from an underrepresented group through the Louis Stokes Alliance for Minority Participation (LSAMP) program at TAMU. The PI gave a special tutorial on `Reinforcement Learning' to a wider audience that consists of faculty and students from many departments across the college of engineering. The papers and codebase produced as part of  this project are publicly available through the PI's webpage. \n\n \n\n\t\t\t\t\tLast Modified: 11/27/2022\n\n\t\t\t\t\tSubmitted by: Dileep Kalathil"
 }
}