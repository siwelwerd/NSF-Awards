{
 "awd_id": "1912286",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRCNS Research Proposal: Understanding Cortical Networks Related to Speech Using Deep Learning on ECOG Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 832574.0,
 "awd_amount": 848574.0,
 "awd_min_amd_letter_date": "2019-08-31",
 "awd_max_amd_letter_date": "2020-04-09",
 "awd_abstract_narration": "Despite significant advances in neural science, the dynamics by which neural activity propagates across cortex while we think of a word and produce it remains poorly understood. This proposal will develop novel, data-driven approaches for understanding functions and interactions of various brain regions by leveraging rare neural recordings obtained with electrocorticography (ECoG) sensors while neurosurgical patients participate in tasks involving language perception, semantic access and word production.  This project will produce a set of validated novel computational tools for estimating neural representations and their dynamics as well as elucidate the cortical networks subserving perception, semantic access, and production of speech. Although these tools will be developed for ECoG data, the proposed frameworks are applicable to other neural data modalities including fMRI and EEG, and thus have broad applications in neuroscience. The ability to robustly translate between speech and its neural representations is vital to the development of speech prosthetics, which would allow patients with degenerative conditions (Amyotrophic Lateral Sclerosis) or neurological damage (locked-in syndrome) to drive a speech synthesizer via control from intact cortical structures. The network connectivity tools could shed light on the propagation dynamics of epileptic seizures as well as on how cortical communication, when impaired, gives rise to language aphasias and disconnection syndromes. Furthermore, the decoding and network connectivity tools could help develop novel language mapping approaches for brain surgery without the associated risks of electrical stimulation mapping.\r\n\r\nThe project consists of three core thrusts: developing neural decoders for language processing, developing directed connectivity models, and experimental validation. The neural decoders will be based on deep-learning architectures able to learn a transformation between neural signals and the speech heard by the patient, the speech produced by the patient, or the semantic concept represented by the stimulus word. The connectivity models will generalize and coalesce current approaches for estimating the task-dependent, time-varying directed connectivity between cortical regions. Lastly, these findings will be experimentally validated via clinical electrical stimulation data and cortico-cortico evoked potential (CCEP) stimulation experiments. Current modeling approaches of ECoG data have mostly focused on variants of linear models and on speech acoustics. This project will harness the potential of highly non-linear and deep networks for modeling neural responses to both speech acoustics and access to semantics. Additionally, tools for inferring direct connectivity and interactions among neural regions will provide a detailed characterization of the network dynamics, which is largely overlooked by most ECoG decoding studies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yao",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yao Wang",
   "pi_email_addr": "yw523@nyu.edu",
   "nsf_id": "000467592",
   "pi_start_date": "2019-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Adeen",
   "pi_last_name": "Flinker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adeen Flinker",
   "pi_email_addr": "Adeen.Flinker@nyumc.org",
   "nsf_id": "000770084",
   "pi_start_date": "2019-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "2 Metrotech Center",
  "perf_city_name": "Brooklyn",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "112013848",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "NY07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "732700",
   "pgm_ele_name": "CRCNS-Computation Neuroscience"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 832574.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Despite significant advances in neural science, the dynamics by which neural activity propagates across cortex while we think of a word and produce it remains poorly understood. This project aims to develop novel, data-driven approaches for understanding functions and interactions of various brain regions by leveraging rare neural recordings obtained with electrocorticography (ECoG) sensors while neurosurgical patients participate in tasks involving language perception, semantic access and word production. &nbsp;The project has led to multiple significant outcomes.</p>\n<p>First, we developed a successful neural speech decoding framework, which translates ECoG signals captured from the brain cortex to speech waveforms by leveraging a novel speech synthesizer and advanced deep learning architectures. Our framework generates natural-sounding speech resembling the actual participants, and is highly reproducible across a cohort of 48 participants. Our latest deep learning architecture leveraging transformer operations across the electrodes can accommodate neural data from a variety of sensors including ECoG and sEEG. Our approach can decode speech with high correlation with the actual speech spectrograms even when limited to only causal operations, which is necessary for real-time speech generation. Furthermore, we can successfully decode speech in participants with either left or right hemisphere coverage, which could lead to speech prostheses in patients with speech deficits resulting from left hemisphere damage.&nbsp; Our decoding approach provides a promising direction for developing future practical speech neuroprostheses. Eventual successful development and adoption of such devices will have significant impact in terms of improving the quality of life of people with speech disability. Similar principles can be applied for developing other brain computer interfaces.</p>\n<p>Second, we have advanced the understanding of neural speech production mechanisms.&nbsp; Speech production is a complex human function requiring continuous feedforward commands together with reafferent feedback processing. These processes are carried out by distinct frontal and temporal cortical networks, but the degree and timing of their recruitment and dynamics remain poorly understood. Through conducting contribution analysis on our trained causal (decoding the speech using only current and past neural signals) and anti-causal (decoding the speech using only current and future neural signals) models, respectively, we are able to disentangle feedforward and feedback cortical contribution to speech production. We found a mixed cortical architecture in which frontal and temporal networks each process both feedforward and feedback information in tandem. We elucidated the timing of feedforward and feedback-related processing by quantifying the derived receptive fields. Our approach provides evidence for a surprisingly mixed cortical architecture of speech circuitry. Our approach for analyzing the contribution (overall contribution and temporal receptive field) of different brain regions using learnt neural decoders demonstrates the potential of deep learning for understanding general cortical functions.</p>\n<p>Third, we have developed novel tools for deriving the dynamic directed connectivity between different brain regions and to identify prototypical temporal connectivity patterns. A major cluster, replicated across tasks and patients, shows directed connectivity from motor cortex onto temporal cortex prior to and at the onset of speech. This result provides the first clear marker, in humans, for a widely theorized corollary discharge which presumably communicated the upcoming speech plan to temporal cortex. Furthermore, the directed communication from this source predicts the degree of auditory cortex suppression during speech, a well-documented consequence of the motor-related discharge. These results are the first to reveal the human corollary discharge source, timing and trajectory, which has far-reaching implication for speech motor control as well as auditory psychotic symptoms in humans. The computational tools we developed are generally applicable for analyzing brain activity signals (not limited to ECoG signal) and will help to understand the causality between the cortical regions in different phases of various cognitive tasks.</p>\n<p>Finally, we have collected a relatively large dataset containing the neural signals captured while the participants undertook 5 different language tasks, from 48 participants with implanted ECoG grids. These data were used to develop the previously stated outcomes. We also collected clinical electric stimulation data for mapping critical regions for language processing across 25 of these participants. These data have been used to validate results from our contribution analysis of speech decoding. All the data will be made available to the scientific community once papers describing our studies are published.</p>\n<p>The initial version of our decoding framework using a differentiable speech synthesizer and the analysis that disentangles feedforward and feedback contributions has been published in the Proceedings of National Academy of Science (PNAS). We have already published the source code for the decoding framework and contribution analysis along with the paper publications. Manuscripts describing the refined versions of our decoding framework and directed connectivity analysis are currently under review. We will open source all the developed computational tools once the papers are published, to enable reproducible research and drive research across the speech science and prostheses communities.</p><br>\n<p>\n Last Modified: 02/01/2024<br>\nModified by: Adeen&nbsp;Flinker</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nDespite significant advances in neural science, the dynamics by which neural activity propagates across cortex while we think of a word and produce it remains poorly understood. This project aims to develop novel, data-driven approaches for understanding functions and interactions of various brain regions by leveraging rare neural recordings obtained with electrocorticography (ECoG) sensors while neurosurgical patients participate in tasks involving language perception, semantic access and word production. The project has led to multiple significant outcomes.\n\n\nFirst, we developed a successful neural speech decoding framework, which translates ECoG signals captured from the brain cortex to speech waveforms by leveraging a novel speech synthesizer and advanced deep learning architectures. Our framework generates natural-sounding speech resembling the actual participants, and is highly reproducible across a cohort of 48 participants. Our latest deep learning architecture leveraging transformer operations across the electrodes can accommodate neural data from a variety of sensors including ECoG and sEEG. Our approach can decode speech with high correlation with the actual speech spectrograms even when limited to only causal operations, which is necessary for real-time speech generation. Furthermore, we can successfully decode speech in participants with either left or right hemisphere coverage, which could lead to speech prostheses in patients with speech deficits resulting from left hemisphere damage. Our decoding approach provides a promising direction for developing future practical speech neuroprostheses. Eventual successful development and adoption of such devices will have significant impact in terms of improving the quality of life of people with speech disability. Similar principles can be applied for developing other brain computer interfaces.\n\n\nSecond, we have advanced the understanding of neural speech production mechanisms. Speech production is a complex human function requiring continuous feedforward commands together with reafferent feedback processing. These processes are carried out by distinct frontal and temporal cortical networks, but the degree and timing of their recruitment and dynamics remain poorly understood. Through conducting contribution analysis on our trained causal (decoding the speech using only current and past neural signals) and anti-causal (decoding the speech using only current and future neural signals) models, respectively, we are able to disentangle feedforward and feedback cortical contribution to speech production. We found a mixed cortical architecture in which frontal and temporal networks each process both feedforward and feedback information in tandem. We elucidated the timing of feedforward and feedback-related processing by quantifying the derived receptive fields. Our approach provides evidence for a surprisingly mixed cortical architecture of speech circuitry. Our approach for analyzing the contribution (overall contribution and temporal receptive field) of different brain regions using learnt neural decoders demonstrates the potential of deep learning for understanding general cortical functions.\n\n\nThird, we have developed novel tools for deriving the dynamic directed connectivity between different brain regions and to identify prototypical temporal connectivity patterns. A major cluster, replicated across tasks and patients, shows directed connectivity from motor cortex onto temporal cortex prior to and at the onset of speech. This result provides the first clear marker, in humans, for a widely theorized corollary discharge which presumably communicated the upcoming speech plan to temporal cortex. Furthermore, the directed communication from this source predicts the degree of auditory cortex suppression during speech, a well-documented consequence of the motor-related discharge. These results are the first to reveal the human corollary discharge source, timing and trajectory, which has far-reaching implication for speech motor control as well as auditory psychotic symptoms in humans. The computational tools we developed are generally applicable for analyzing brain activity signals (not limited to ECoG signal) and will help to understand the causality between the cortical regions in different phases of various cognitive tasks.\n\n\nFinally, we have collected a relatively large dataset containing the neural signals captured while the participants undertook 5 different language tasks, from 48 participants with implanted ECoG grids. These data were used to develop the previously stated outcomes. We also collected clinical electric stimulation data for mapping critical regions for language processing across 25 of these participants. These data have been used to validate results from our contribution analysis of speech decoding. All the data will be made available to the scientific community once papers describing our studies are published.\n\n\nThe initial version of our decoding framework using a differentiable speech synthesizer and the analysis that disentangles feedforward and feedback contributions has been published in the Proceedings of National Academy of Science (PNAS). We have already published the source code for the decoding framework and contribution analysis along with the paper publications. Manuscripts describing the refined versions of our decoding framework and directed connectivity analysis are currently under review. We will open source all the developed computational tools once the papers are published, to enable reproducible research and drive research across the speech science and prostheses communities.\t\t\t\t\tLast Modified: 02/01/2024\n\n\t\t\t\t\tSubmitted by: AdeenFlinker\n"
 }
}