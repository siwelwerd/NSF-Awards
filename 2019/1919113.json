{
 "awd_id": "1919113",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SPX: Collaborative Research: Cross-stack Memory Optimizations for Boosting I/O Performance of Deep Learning HPC Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Damian Dechev",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 952884.0,
 "awd_amount": 968884.0,
 "awd_min_amd_letter_date": "2019-08-26",
 "awd_max_amd_letter_date": "2023-05-19",
 "awd_abstract_narration": "New computing applications are emerging in smart networks, scientific explorations, business management, security, and healthcare. These applications depend on very large amounts of data. This data must be used in a fast and efficient manner. The use of large supercomputers to analyze such data is on the rise. The techniques they use are referred to as deep learning (DL) high-performance computing (HPC). Researchers are using DL HPC to make sense of this flood of data and obtain useful information. To do this they must redesign HPC systems. A key challenge is how to use resources such as data storage and computer memory at a huge scale. This project will build Metis, a high-performance data storage system that uses new, end-to-end, hardware-supported memory and storage design to meet the needs of DL HPC applications. The goal is to satisfy the challenge posed by increasing data management performance for next-generation supercomputers. The project will connect several different computing communities and increase interactions among them. The project includes educational and engagement activities which will greatly increase the community's understanding of HPC systems. These activities include broadening participation activities to attract and retain new students. Special emphasis will be given to students from underrepresented groups. The project will encourage student interest in design and research in large-scale computing systems design.\r\n\r\nThis project brings together researchers in micro-architecture, distributed computing systems, namely cloud and HPC systems, storage systems, and power/energy modeling to boost DL HPC data processing performance. The research will yield a fundamentally new software-hardware co-designed memory compression technique that transparently compresses DL application memories with negligible runtime performance overhead. Metis will leverage the novel compression substrate to enable a distributed, intelligent, operating-system-level data cache that effectively exploits the physical memory freed via program-memory compression. The developed techniques will open doors for innovative HPC and scientific applications in a broad range of disciplines, which have not been previously possible. Metis' focus on addressing the challenges of increasing performance in the Exascale era, along with engaging researchers from multiple areas, aligns it very well with the goals and objectives of the SPX program. Additionally, the research will also create new knowledge on design principles of memory compression, and yield insights to provide seamless integration of DL applications into the next-generation DL-aware supercomputer infrastructure.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ali",
   "pi_last_name": "Butt",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Ali R Butt",
   "pi_email_addr": "butta@cs.vt.edu",
   "nsf_id": "000288467",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kirk",
   "pi_last_name": "Cameron",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Kirk W Cameron",
   "pi_email_addr": "cameron@cs.vt.edu",
   "nsf_id": "000194453",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xun",
   "pi_last_name": "Jian",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xun Jian",
   "pi_email_addr": "xunj@vt.edu",
   "nsf_id": "000753889",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "2202 Kraft Drive",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240606356",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 952884.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div id=\"docs-internal-guid-69530ac4-7fff-2ed5-1562-89e512da1047\" style=\"margin-left: 0pt;\" dir=\"ltr\">\r\n<table style=\"border: none; border-collapse: collapse;\">\r\n<colgroup><col width=\"622\"></col></colgroup> \r\n<tbody>\r\n<tr style=\"height: 24.25pt;\">\r\n<td style=\"vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden; overflow-wrap: break-word;\">\r\n<p style=\"line-height: 1.656; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: #f3f3f3; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project developed, implemented, and evaluated METIS, a high-performance storage subsystem that integrates innovative, end-to-end memory and storage abstractions specifically designed to meet the demands of Deep Learning (DL) High Performance Computing (HPC) applications. METIS leverages new main memory compression architectures, repurposes freed physical memory to create a cooperative in-memory I/O cache, architects a high-performance NVMe burst buffer as a backend, and incorporates extensive power modeling to understand the impact of this I/O redesign.</span></p>\r\n<p style=\"line-height: 1.656; margin-top: 12pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: #f3f3f3; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">These innovations aim to meet the high storage and computational demands of critical large-scale applications such as artificial intelligence, climate modeling, genomics, and financial analytics, across sectors like healthcare, environmental science, and finance, where efficient data handling is paramount.</span></p>\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project encompasses various advancements in high-performance computing (HPC) and deep learning (DL) infrastructure, focusing on optimized storage, scheduling, and data management solutions. Hawk was developed as an OS-CPU co-design enabling direct application access to hardware-compressed memory, achieving significant performance improvements over traditional NVMe swapping. MARBLE, a job scheduler, enhanced DL training efficiency on the Summit supercomputer by dynamically allocating GPU resources, achieving up to a 48.3% performance increase compared to the Platform Load Sharing Facility (LSF) scheduler. In HPC container environments, an empirical analysis of Docker, Podman, Singularity, and Charliecloud revealed key optimizations for container I/O performance, particularly with the Lustre file system.</span></p>\r\n<br />\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">BRINDEXER addressed limitations in metadata indexing on large-scale HPC systems, introducing leveled partitioning and in-tree indexing to increase scalability and reduce reindexing latency, while DupHunter, a Docker registry architecture, achieved up to 6.9x space savings and 2.8x lower latency by eliminating duplicate image layers. SHADE, a caching system, improved deep learning training throughput by up to 27% and reduced minibatch load times by up to 2x by prioritizing critical data samples. In federated learning, FedAT minimized communication costs by up to 8.5x and improved prediction performance by up to 21.09% through asynchronous, tiered client management. SchedTune optimized GPU usage for DL training by accurately predicting resource requirements.</span></p>\r\n<br />\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Supporting developments included TMCC and Dylect, which improved address translation in hardware-compressed memory; a CXL simulation platform for FaaS workloads; and a static analysis framework for decomposing HPC applications for cloud and serverless platforms. These innovations collectively address scalability, performance, and cost challenges in modern HPC and DL environments, providing empirical results that validate each solution&rsquo;s impact.</span></p>\r\n<br />\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The team has made significant progress in prototyping OS modifications and hardware for HPC applications, with publications in high-impact venues like MICRO, FAST, HPDC, ATC, and ISCA. The MARBLE scheduler for multi-GPU HPC systems has demonstrated up to a 47% reduction in job completion time over existing solutions. BRINDEXER has improved metadata indexing and querying on a 4.8 TB Lustre store by 69% and 91%, respectively, and DupHunter has achieved up to a 6.9x storage reduction and 2.8x latency reduction for Docker registries. The integration of Apache OpenWhisk with Intel Optane DC Persistent Memory enhances performance and reliability for stateful FaaS applications, while the SHADE caching system dynamically optimizes caching for deep learning workloads, offering foundational insights for future I/O systems.</span></p>\r\n<br />\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">These approaches have been applied to critical applications across fields such as genomics, climate modeling, and financial analytics, where big data processing and deep learning are essential. A deep learning scheduler for multi-GPU HPC systems was developed, achieving efficient resource usage in advanced HPC environments equipped with multiple GPUs, high-speed networks, powerful CPUs, and abundant memory.</span></p>\r\n<br />\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our work has been published in several top-conferences and journals (USENIX ATC 2020, IEEE TPDS, IEEE CLOUD 2020, CCGrid 2020, USENIX FAST 2020, ACM HPDC 2020). The PIs also visited research labs, such as IBM Research, and Oak Ridge National Lab., and peer institutions to inform the community of the importance of research in the field of memory management and deep learning to support the vast range of disciplines working in emerging scientific and web-scale applications. In addition to these, the PIs also interact with freshmen (via the university/department recruitment events) to inform them about the findings and the impact of this research.</span></p>\r\n<br />\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The project&rsquo;s broader impact includes the mentorship of seven Ph.D. students (four of whom are women), three MS students, and six REU students (including four women). Additionally, outreach modules were created for use in classroom instruction and to engage stakeholders in national labs (such as ORNL) and industry (e.g., IBM Research). All software tools developed through the project have been released to the community to support further research.</span></p>\r\n</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n</div><br>\n<p>\n Last Modified: 12/04/2024<br>\nModified by: Ali&nbsp;R&nbsp;Butt</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\r\n\r\n \r\n\r\n\r\n\r\n\n\nThe project developed, implemented, and evaluated METIS, a high-performance storage subsystem that integrates innovative, end-to-end memory and storage abstractions specifically designed to meet the demands of Deep Learning (DL) High Performance Computing (HPC) applications. METIS leverages new main memory compression architectures, repurposes freed physical memory to create a cooperative in-memory I/O cache, architects a high-performance NVMe burst buffer as a backend, and incorporates extensive power modeling to understand the impact of this I/O redesign.\r\n\n\nThese innovations aim to meet the high storage and computational demands of critical large-scale applications such as artificial intelligence, climate modeling, genomics, and financial analytics, across sectors like healthcare, environmental science, and finance, where efficient data handling is paramount.\r\n\n\nThe project encompasses various advancements in high-performance computing (HPC) and deep learning (DL) infrastructure, focusing on optimized storage, scheduling, and data management solutions. Hawk was developed as an OS-CPU co-design enabling direct application access to hardware-compressed memory, achieving significant performance improvements over traditional NVMe swapping. MARBLE, a job scheduler, enhanced DL training efficiency on the Summit supercomputer by dynamically allocating GPU resources, achieving up to a 48.3% performance increase compared to the Platform Load Sharing Facility (LSF) scheduler. In HPC container environments, an empirical analysis of Docker, Podman, Singularity, and Charliecloud revealed key optimizations for container I/O performance, particularly with the Lustre file system.\r\n\n\r\n\n\nBRINDEXER addressed limitations in metadata indexing on large-scale HPC systems, introducing leveled partitioning and in-tree indexing to increase scalability and reduce reindexing latency, while DupHunter, a Docker registry architecture, achieved up to 6.9x space savings and 2.8x lower latency by eliminating duplicate image layers. SHADE, a caching system, improved deep learning training throughput by up to 27% and reduced minibatch load times by up to 2x by prioritizing critical data samples. In federated learning, FedAT minimized communication costs by up to 8.5x and improved prediction performance by up to 21.09% through asynchronous, tiered client management. SchedTune optimized GPU usage for DL training by accurately predicting resource requirements.\r\n\n\r\n\n\nSupporting developments included TMCC and Dylect, which improved address translation in hardware-compressed memory; a CXL simulation platform for FaaS workloads; and a static analysis framework for decomposing HPC applications for cloud and serverless platforms. These innovations collectively address scalability, performance, and cost challenges in modern HPC and DL environments, providing empirical results that validate each solutions impact.\r\n\n\r\n\n\nThe team has made significant progress in prototyping OS modifications and hardware for HPC applications, with publications in high-impact venues like MICRO, FAST, HPDC, ATC, and ISCA. The MARBLE scheduler for multi-GPU HPC systems has demonstrated up to a 47% reduction in job completion time over existing solutions. BRINDEXER has improved metadata indexing and querying on a 4.8 TB Lustre store by 69% and 91%, respectively, and DupHunter has achieved up to a 6.9x storage reduction and 2.8x latency reduction for Docker registries. The integration of Apache OpenWhisk with Intel Optane DC Persistent Memory enhances performance and reliability for stateful FaaS applications, while the SHADE caching system dynamically optimizes caching for deep learning workloads, offering foundational insights for future I/O systems.\r\n\n\r\n\n\nThese approaches have been applied to critical applications across fields such as genomics, climate modeling, and financial analytics, where big data processing and deep learning are essential. A deep learning scheduler for multi-GPU HPC systems was developed, achieving efficient resource usage in advanced HPC environments equipped with multiple GPUs, high-speed networks, powerful CPUs, and abundant memory.\r\n\n\r\n\n\nOur work has been published in several top-conferences and journals (USENIX ATC 2020, IEEE TPDS, IEEE CLOUD 2020, CCGrid 2020, USENIX FAST 2020, ACM HPDC 2020). The PIs also visited research labs, such as IBM Research, and Oak Ridge National Lab., and peer institutions to inform the community of the importance of research in the field of memory management and deep learning to support the vast range of disciplines working in emerging scientific and web-scale applications. In addition to these, the PIs also interact with freshmen (via the university/department recruitment events) to inform them about the findings and the impact of this research.\r\n\n\r\n\n\nThe projects broader impact includes the mentorship of seven Ph.D. students (four of whom are women), three MS students, and six REU students (including four women). Additionally, outreach modules were created for use in classroom instruction and to engage stakeholders in national labs (such as ORNL) and industry (e.g., IBM Research). All software tools developed through the project have been released to the community to support further research.\r\n\r\n\r\n\r\n\r\n\t\t\t\t\tLast Modified: 12/04/2024\n\n\t\t\t\t\tSubmitted by: AliRButt\n"
 }
}