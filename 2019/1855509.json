{
 "awd_id": "1855509",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Random Matrices, Statistical Applications, and Spin Glass Dynamics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924885",
 "po_email": "tbartosz@nsf.gov",
 "po_sign_block_name": "Tomek Bartoszynski",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 270000.0,
 "awd_amount": 270000.0,
 "awd_min_amd_letter_date": "2019-04-18",
 "awd_max_amd_letter_date": "2021-07-01",
 "awd_abstract_narration": "The main objective of this award is to investigate the foundation and application of random matrix statistics. In the age of large data, classical probability theory does not offer sufficient tools for current applications in large data. One of the most basic objects in data analysis is large random matrices representing data and noise. The statistics of eigenvalues and eigenvectors of these matrices essentially determine the real information contained in the data matrices. In this work, we aim to develop tools in understanding the eigenvalue and eigenvector statistics of the large class of random matrices. Besides data matrices, the research also investigates the spectral properties of the associated matrices of large random graphs. In order to enhance the exchange of ideas among different groups of researchers, we have been running seminars jointly with the Statistics and Computer Science departments at the Center of Mathematical Sciences and Applications at Harvard University. We also co-organize the Charles River Lectures in Probability (jointly with MIT and Microsoft Research), the Current Development in Mathematics conference (jointly with MIT), and the CMSA's annual conference on Big Data. These programs will bring researchers from probability theory, statistics, combinatorics, mathematical physics, and computer science to work together. We are actively formulating mathematical questions that are interesting to statisticians and computer scientists. It is clear that the analysis of large random matrices is of great interest to these scientists and we expect fruitful results from these collaborations. \r\n\r\nThis research aims to extend random matrix statistics to large classes of matrix models, including band matrices, adjacency matrices of sparse random graphs, and heavy-tailed random matrices. Our goal is to understand to what extent random matrix statistics, in particular the statistics of the Gaussian orthogonal ensemble, can be shown to hold independent of the matrix law. Our work is driven by the grand vision of E. Wigner, asserting that random matrix statistics are universal laws for systems of high complexity. More precisely, we choose the following five projects: 1. Delocalization and universality of band matrices. 2. Quantum unique ergodicity and data analysis. 3. Edge statistics of sparse random graphs. 4. Levy matrices and heavy-tailed random matrices. 5. Spin glass dynamics on hypercubes. The first project aims to extend the current random matrix theory to non mean-field models since most basic physics laws are short-ranged and thus far from the mean field type. Project 1 concerning non mean-field band matrices is in our view one of the most important questions regarding \r\nWigner's original vision. Projects 2 and 3 concern edge statistics of both eigenvalues and eigenvectors of the adjacency matrices of random sparse graphs. Due to a Gaussian to Tracy-Widom transition in the edge statistics for the Erdos-Renyi graphs, we expect this project to clarify the role of sparsity in random graphs. In addition, our understanding of eigenvector statistics will have applications in restricted minimum singular value problems and signal recovery algorithms. Project 4 aims to classify the phase diagrams of heavy-tailed random matrices according to their spectral statistics. The last project aims to develop methods to prove spectral gaps and logarithmic Sobolev inequality for the Glauber dynamics of spin glasses on hypercubes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Horng-Tzer",
   "pi_last_name": "Yau",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Horng-Tzer Yau",
   "pi_email_addr": "htyau@math.harvard.edu",
   "nsf_id": "000191330",
   "pi_start_date": "2019-04-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021385369",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126300",
   "pgm_ele_name": "PROBABILITY"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 90000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 90000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 90000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intellectual Merits:&nbsp; In the age of large data, classical probability theory does not offer sufficient tools for current applications in large data. One of the most basic objects in data analysis is large random matrices representing data and noise. The statistics of eigenvalues and eigenvectors of these matrices essentially determine the real information contained in the data matrices. In this proposal, we&nbsp; have&nbsp; developed tools in understanding the eigenvalue and eigenvector statistics of the large class of random matrices. Besides data matrices, our work also investigates the spectral properties of the associated matrices of large random graphs.&nbsp;The main results of this&nbsp; proposal can be divided into three directions.&nbsp;1. The renormalization of energy in high dimensional random band matrices to all orders in the band width. This is a major&nbsp; breakthrough and it implies the weak delocalization and quantum diffusion of&nbsp; high dimensional band matrices. More precisely,&nbsp;we consider Hermitian random band matrices&nbsp; on the d-dimensional lattice so that&nbsp; &nbsp;their&nbsp; variance matrices&nbsp; have a banded structure so that&nbsp; the matrix elements are negligible whenever the distances&nbsp; exceed the band width W.&nbsp;In dimensions bigger than eight, we prove that, as long as the band width is not too small,&nbsp; with high probability&nbsp; most&nbsp; bulk eigenvectors&nbsp; are delocalized and quantum diffusion holds in the sense of Green's function.2. Estimates on eigenvalues and eigenvectors statistics for random d-regular&nbsp;graphs for&nbsp; all degree bigger than or equal to three. More precisely, we prove that,&nbsp; with high probability,&nbsp; &nbsp; &nbsp;(i) The eigenvalues are close to the classical eigenvalue locations given by the Kesten-McKay distribution.&nbsp; (ii)&nbsp; All&nbsp; eigenvectors of&nbsp; random d-regular graphs are completely delocalized.3. Dynamics of deep neural networks and&nbsp; neural tangent hierarchy.<span> </span>The evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel <span> </span>in the infinite width limit.&nbsp;However, it was observed&nbsp;that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks.&nbsp;We&nbsp; study the dynamic of the neural tangent kernel&nbsp; for finite width deep fully-connected neural networks.&nbsp;&nbsp;We derive an&nbsp; infinite hierarchy of ordinary differential equations, the neural tangent hierarchy&nbsp;which captures the&nbsp; &nbsp;gradient descent&nbsp; dynamic of the deep neural network.&nbsp;Moreover, under certain conditions&nbsp;on the neural network width and the data set dimension,&nbsp; we prove the truncated hierarchy of NTH approximates the dynamic of the deep neural networks up to arbitrary precision.</p>\n<p>&nbsp;Broader Impacts:&nbsp; The PI has co-organized several workshops (e.g., the Charles River Lectures, Ahlfors Lectures, Annual Big Data Conference) jointly with&nbsp; the statistics,&nbsp; economics, and&nbsp; computer science&nbsp; departments of MIT,&nbsp; Harvard,&nbsp; and Microsoft Research.&nbsp; The PI&nbsp; has&nbsp; supervised&nbsp; several graduate students and post-doctors, and&nbsp; served as&nbsp; undergraduate thesis advisers for several students. The list of&nbsp; students who were partly supported by this grant and graduated with Ph.D. degrees&nbsp; includes&nbsp; A. Adhikari (Stanford Univ.), J. Huang (NYU), J. Marcinek (who took a job in financial industry), and P. Lopatto (Brown Univ.).&nbsp;&nbsp;The postdocs trained in this grant period include&nbsp; &nbsp;C. Brennecke and M. Lemm.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/03/2022<br>\n\t\t\t\t\tModified by: Horng-Tzer&nbsp;Yau</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIntellectual Merits:  In the age of large data, classical probability theory does not offer sufficient tools for current applications in large data. One of the most basic objects in data analysis is large random matrices representing data and noise. The statistics of eigenvalues and eigenvectors of these matrices essentially determine the real information contained in the data matrices. In this proposal, we  have  developed tools in understanding the eigenvalue and eigenvector statistics of the large class of random matrices. Besides data matrices, our work also investigates the spectral properties of the associated matrices of large random graphs. The main results of this  proposal can be divided into three directions. 1. The renormalization of energy in high dimensional random band matrices to all orders in the band width. This is a major  breakthrough and it implies the weak delocalization and quantum diffusion of  high dimensional band matrices. More precisely, we consider Hermitian random band matrices  on the d-dimensional lattice so that   their  variance matrices  have a banded structure so that  the matrix elements are negligible whenever the distances  exceed the band width W. In dimensions bigger than eight, we prove that, as long as the band width is not too small,  with high probability  most  bulk eigenvectors  are delocalized and quantum diffusion holds in the sense of Green's function.2. Estimates on eigenvalues and eigenvectors statistics for random d-regular graphs for  all degree bigger than or equal to three. More precisely, we prove that,  with high probability,     (i) The eigenvalues are close to the classical eigenvalue locations given by the Kesten-McKay distribution.  (ii)  All  eigenvectors of  random d-regular graphs are completely delocalized.3. Dynamics of deep neural networks and  neural tangent hierarchy. The evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel  in the infinite width limit. However, it was observed that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. We  study the dynamic of the neural tangent kernel  for finite width deep fully-connected neural networks.  We derive an  infinite hierarchy of ordinary differential equations, the neural tangent hierarchy which captures the   gradient descent  dynamic of the deep neural network. Moreover, under certain conditions on the neural network width and the data set dimension,  we prove the truncated hierarchy of NTH approximates the dynamic of the deep neural networks up to arbitrary precision.\n\n Broader Impacts:  The PI has co-organized several workshops (e.g., the Charles River Lectures, Ahlfors Lectures, Annual Big Data Conference) jointly with  the statistics,  economics, and  computer science  departments of MIT,  Harvard,  and Microsoft Research.  The PI  has  supervised  several graduate students and post-doctors, and  served as  undergraduate thesis advisers for several students. The list of  students who were partly supported by this grant and graduated with Ph.D. degrees  includes  A. Adhikari (Stanford Univ.), J. Huang (NYU), J. Marcinek (who took a job in financial industry), and P. Lopatto (Brown Univ.).  The postdocs trained in this grant period include   C. Brennecke and M. Lemm. \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/03/2022\n\n\t\t\t\t\tSubmitted by: Horng-Tzer Yau"
 }
}