{
 "awd_id": "1916271",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Integrative Analysis on Heterogeneous Datasets with High-Dimensional and Non-Standard Models",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 180000.0,
 "awd_amount": 180000.0,
 "awd_min_amd_letter_date": "2019-08-13",
 "awd_max_amd_letter_date": "2021-08-16",
 "awd_abstract_narration": "Advances in data collection technology in the past decade have enabled practitioners to collect larger and more comprehensive datasets about many natural and social phenomena. Although this trend has enabled practitioners to gain new insights, it also comes with caveats that, if not addressed, may lead to erroneous conclusions that lie at the core of the reproducibility crisis in some areas of science. The caveats include: (i) Modern datasets are growing in heterogeneity, not only as a consequence of the inherent diversity in the world, but also the trend of combining data from multiple sources to create more comprehensive datasets. Not properly accounting for this growing heterogeneity may lead practitioners to systematically biased conclusions. (ii) The size of modern datasets is a hindrance to drawing inferences from them. Fitting a standard model to a massive dataset can be computationally intractable. (iii) The comprehensive nature of modern datasets raises privacy and security concerns. This is exacerbated by integrative analysis that may uncover combinations of patterns in multiple sources that are individually innocuous, but jointly identifying.\r\n\r\nThe Principal Investigator aims to address the heterogeneity, size, and privacy/security concerns that arise in integrative analysis of heterogeneous datasets by designing communication avoiding methods. At a high level, the general approach is to trade local computation for communication: compute lossy summaries of each data source and perform integrative analysis on the summaries. This way, only the summaries are assembled, thereby reducing the communication costs and preserving the anonymity and security of the separate data sources. The specific aims of the PI's work include (i) effective computational strategies for distributed computing under heterogeneity in popular high-dimensional models with provable statistical guarantees, and (ii) new methodological and theoretical insights into data integration for \"non-differentiable\" statistical problems in which estimators are obtained by projecting either on the  boundaries of a convex cone or via the optimization of discontinuous criterion functions, and which arise increasingly in modern domains of research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuekai",
   "pi_last_name": "Sun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuekai Sun",
   "pi_email_addr": "yuekai@umich.edu",
   "nsf_id": "000758966",
   "pi_start_date": "2019-08-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Moulinath",
   "pi_last_name": "Banerjee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Moulinath Banerjee",
   "pi_email_addr": "moulib@umich.edu",
   "nsf_id": "000429273",
   "pi_start_date": "2019-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "1085 S. University Ave., 271 WH",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091107",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 58412.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 60045.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 61543.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>First, we show that even in a simple distributed regression problem, naive integrative analysis in high-dimensions may (in the worst case) lead to no gain in statistical efficiency from the size of the combined dataset. Intuitively, the issue is the global parameter that encodes the shared structure in the combined dataset may not preserve the low-dimensional structure in the local datasets. The main takeaway is in high-dimensional integrative analysis, the analyst must define the global parameter carefully. In the paper, we also leverage results from robust statistics to define global parameters in integrative regression that preserve the sparsity of the local regression coefficients.&nbsp;</p>\n<p><br />Second, we develop a new method for federated learning of neural networks that has since become a standard baseline in the area. The main problem we studied is how to combine the weights of trained neural networks. Due to redundancies in the parameterization of neural networks, different sets of weights may correspond to the same function. For example, swapping the order of neuron in a layer corresponds to permuting the rows/columns of weight matrices, but changing the order of neurons does not change the function itself. This means naively averaging the weights of neural networks usually gives non-sensical results. To overcome this problem with naive averaging, we leverage recent advances in optimal transport to _match_ the neurons in neural network before averaging. The resulting algorithm, FedMA, is now a standard baseline in federated learning. It's main benefit over more basic methods for fusing neural networks (e.g. federated averaging) is it allows practitioners to average neural networks whose weights are in different basins of the energy landscape.</p>\n<p>Finally, we developed a method for distributed estimation in non-standard problems. In standard parametric problems, simply averaging the fitted parameters works well because the estimation error is dominated by variance and averaging reduces the variance. In non-standard problems, this may no longer be the case. If the bias dominates (or at least contributes equally to) the estimation error, then averaging will not improve the convergence rate because averaging does not reduce bias. One way to overcome this issue is to tune the bias-variance trade-off in the local parameter estimation problems so that variance dominates. This restores the benefits of averaging. We worked out the details for isotonic regression problems in growing dimensions.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/03/2024<br>\nModified by: Yuekai&nbsp;Sun</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nFirst, we show that even in a simple distributed regression problem, naive integrative analysis in high-dimensions may (in the worst case) lead to no gain in statistical efficiency from the size of the combined dataset. Intuitively, the issue is the global parameter that encodes the shared structure in the combined dataset may not preserve the low-dimensional structure in the local datasets. The main takeaway is in high-dimensional integrative analysis, the analyst must define the global parameter carefully. In the paper, we also leverage results from robust statistics to define global parameters in integrative regression that preserve the sparsity of the local regression coefficients.\n\n\n\nSecond, we develop a new method for federated learning of neural networks that has since become a standard baseline in the area. The main problem we studied is how to combine the weights of trained neural networks. Due to redundancies in the parameterization of neural networks, different sets of weights may correspond to the same function. For example, swapping the order of neuron in a layer corresponds to permuting the rows/columns of weight matrices, but changing the order of neurons does not change the function itself. This means naively averaging the weights of neural networks usually gives non-sensical results. To overcome this problem with naive averaging, we leverage recent advances in optimal transport to _match_ the neurons in neural network before averaging. The resulting algorithm, FedMA, is now a standard baseline in federated learning. It's main benefit over more basic methods for fusing neural networks (e.g. federated averaging) is it allows practitioners to average neural networks whose weights are in different basins of the energy landscape.\n\n\nFinally, we developed a method for distributed estimation in non-standard problems. In standard parametric problems, simply averaging the fitted parameters works well because the estimation error is dominated by variance and averaging reduces the variance. In non-standard problems, this may no longer be the case. If the bias dominates (or at least contributes equally to) the estimation error, then averaging will not improve the convergence rate because averaging does not reduce bias. One way to overcome this issue is to tune the bias-variance trade-off in the local parameter estimation problems so that variance dominates. This restores the benefits of averaging. We worked out the details for isotonic regression problems in growing dimensions.\n\n\n\t\t\t\t\tLast Modified: 01/03/2024\n\n\t\t\t\t\tSubmitted by: YuekaiSun\n"
 }
}