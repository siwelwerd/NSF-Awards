{
 "awd_id": "1901199",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Theory of Optimization Geometry and Algorithms for Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2019-07-26",
 "awd_max_amd_letter_date": "2019-07-26",
 "awd_abstract_narration": "Deep learning has attracted a significant amount of interest in recent years due to its widespread applicability in computer vision, artificial intelligence and natural language processing, alongside recent strides in autonomous driving. The theoretical underpinnings behind such success, however, remain elusive to a large extent, hindering its further adoption in other applications. This project aims to advance the theoretical foundations of training neural networks in terms of optimization landscape and algorithmic efficacy, which in turn should have a measurable impact on the practice of deep learning by providing guiding principles for network design, algorithm selection, hyperparameter tuning, and adversarial training. This project adopts an interdisciplinary approach fusing ideas from machine learning, optimization, statistical signal processing, high-dimensional statistics, nonparametric statistics, and information theory. This project will likewise develop courses and tutorials on theoretical foundations of large-scale machine learning and provide extensive training opportunities for students at all levels.\r\n\r\nThis project aims to develop a comprehensive theory to characterize the optimization landscape and geometry of loss functions and algorithmic regularizations of major neural network training problems, and explore how the network architecture---including depth, width, and activation functions---affect these properties, thus providing guidelines for the design of algorithms to train these networks more efficiently with theoretical performance guarantees. The project will explore the geometric properties and their impact on the optimization performance in training multi-layer neural networks, auto-encoders, generative adversarial networks, and adversarial training involving non-convex and saddle-point problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuejie",
   "pi_last_name": "Chi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuejie Chi",
   "pi_email_addr": "yuejiechi@cmu.edu",
   "nsf_id": "000627307",
   "pi_start_date": "2019-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Deep learning has attracted a significant amount of research interest in recent years due to the success of applying deep neural networks in practical domains such as computer vision, artificial intelligence and natural language processing. However, the theoretical underpinnings behind such success remain mysterious to a large extent, hindering its adoption to safety-critical applications. This project&nbsp;developed rigorous theory of optimization geometry and elucidated its impact on the convergence performance of optimization algorithms for deep learning, which is central in understanding the success of training neural networks, and helps&nbsp;cross-fertilize and complement other aspects of deep learning theory. More precisely, this project developed a comprehensive set of theory to characterize the optimization landscape and geometry of loss functions and algorithmic regularizations of major neural network training problems and explore how different algorithmic and architectural choices affect these properties. The results of this project provide useful insights and guidelines for the design of algorithms to train these networks more efficiently with theoretical performance guarantees. This project also leads to the development of courses and tutorials on theoretical foundations of large-scale machine learning and provides extensive training opportunities for students at all levels, including but not limited to women and underrepresented students.</p>\n<div></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/24/2024<br>\nModified by: Yuejie&nbsp;Chi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nDeep learning has attracted a significant amount of research interest in recent years due to the success of applying deep neural networks in practical domains such as computer vision, artificial intelligence and natural language processing. However, the theoretical underpinnings behind such success remain mysterious to a large extent, hindering its adoption to safety-critical applications. This projectdeveloped rigorous theory of optimization geometry and elucidated its impact on the convergence performance of optimization algorithms for deep learning, which is central in understanding the success of training neural networks, and helpscross-fertilize and complement other aspects of deep learning theory. More precisely, this project developed a comprehensive set of theory to characterize the optimization landscape and geometry of loss functions and algorithmic regularizations of major neural network training problems and explore how different algorithmic and architectural choices affect these properties. The results of this project provide useful insights and guidelines for the design of algorithms to train these networks more efficiently with theoretical performance guarantees. This project also leads to the development of courses and tutorials on theoretical foundations of large-scale machine learning and provides extensive training opportunities for students at all levels, including but not limited to women and underrepresented students.\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 10/24/2024\n\n\t\t\t\t\tSubmitted by: YuejieChi\n"
 }
}