{
 "awd_id": "1850023",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Bayesian Models for Fairness, and Fairness for Bayesian Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 174869.0,
 "awd_amount": 174869.0,
 "awd_min_amd_letter_date": "2019-03-14",
 "awd_max_amd_letter_date": "2022-06-16",
 "awd_abstract_narration": "In our interconnected society, artificial intelligence (AI) and machine learning (ML) systems have become ubiquitous.  Every day, machine learning systems influence our purchasing decisions, our navigation through virtual and physical spaces, the friendships we make, and even the romantic relationships we form.  The decisions automated by these systems have increasingly important real-world consequences, from credit scoring, to college admissions, to the prediction of re-offending behavior in the criminal justice system, as is already being used for bail and sentencing decisions across the United States of America.  With the growing impact of artificial intelligence and machine learning technologies on our society, and their importance to the economic competitiveness and technological leadership of the United States, it is imperative that we ensure that these systems behave in a fair and trustworthy manner.  Recent studies have shown that data-driven AI and ML systems can in some cases exhibit unfair and unjust behavior, for example due to biases hidden in the input data, or because of flawed engineering decisions.  This project develops a suite of tools for modeling, measuring, and correcting unfair and discriminatory behavior in AI and ML systems.  The research focuses on simultaneously addressing algorithmic discrimination that may occur across several overlapping dimensions, including gender, race, national origin, sexual orientation, disability status, and socioeconomic class.  The novel AI techniques developed in this project address the two main technical challenges which specifically arise in this context: uncertainty in the measurement of fairness, and correlations in the data.\r\n\r\nWhen ensuring AI fairness regarding multiple protected dimensions such as gender and race, data sparsity rapidly becomes a challenge as the number of dimensions, or the number of values per dimension, increase.  This data sparsity directly results in uncertainty in the measurement of fairness.  The project will leverage Bayesian inference, a branch of statistics which specifically addresses uncertainty, to manage this issue.  Correlations between the protected (and other) attributes will be leveraged using probabilistic graphical models, a class of machine learning models which encode dependence relationships.   Using a novel Bayesian definition of fairness as a unifying framework, the project's contributions consist of three interdependent tracks.  The first track will focus on developing general modeling techniques for the statistically efficient measurement of fairness, using latent variable models to produce parsimonious representations, and hierarchical modeling to achieve data efficiency.  The second track develops adversarial optimization algorithms to train machine learning algorithms to respect fairness constraints when the data distribution is uncertain.  In the third track, the project will develop methods for ensuring fairness in Bayesian inference, which can be used to prevent the inferences from reflecting negative stereotypes. The methods will be validated with case studies on applications across a wide range of data regimes, including modeling census income data, criminal justice recidivism prediction, and social media analytics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Foulds",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Foulds",
   "pi_email_addr": "jfoulds@umbc.edu",
   "nsf_id": "000762372",
   "pi_start_date": "2019-03-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland Baltimore County",
  "inst_street_address": "1000 HILLTOP CIR",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4104553140",
  "inst_zip_code": "212500001",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND BALTIMORE COUNTY",
  "org_prnt_uei_num": "",
  "org_uei_num": "RNKYWXURFRL5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland Baltimore County",
  "perf_str_addr": "1000 Hilltop Circle",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212500002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 174869.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With the rise of machine learning (ML) and artificial intelligence (AI) systems and their impact on our daily lives, there are growing concerns that these algorithms can behave unfairly by inadvertently encoding or introducing discriminatory bias. The field of algorithmic fairness aims to develop machine learning methods which automatically ensure fair behavior. However, there are several challenges. First, the protected attributes, for which we desire to ensure fairness, are typically correlated with other attributes (including class labels as well as other protected attributes). This foils naive approaches to fairness, and complicates both the definition and the measurement of bias. Furthermore, multiple protected attributes, such as gender, race, nationality, and disability status, may need to be considered simultaneously, as motivated by intersectionality theory from the humanities and legal literature, and which may be mandated by law (e.g. Title VII of the Civil Rights Act of 1964). Existing fairness methods nevertheless frequently assume a single protected attribute, which is often further assumed to be binary.<br /><br />While it is conceptually straightforward to extend fairness methods to multiple protected attributes with multiple values, data sparsity rapidly becomes an issue as the number of dimensions or values increases, leading to uncertainty in the measurement of fairness. This project developed methods for measuring and correcting bias that account for and manage uncertainty. Using a novel statistical definition of fairness as a unifying framework, the project's contributions include statistically efficient algorithms for measuring fairness in high-dimensional settings, fairness-preserving Bayesian inference algorithms, and fair learning algorithms which take uncertainty into account.&nbsp;&nbsp;<br /><br />We employed the principles of Bayesian statistical modeling, a branch of statistics that emphasizes the quantification of uncertainty, throughout this work to manage uncertainty in the high-dimensional fairness setting. The research advanced the state of the art in the important intersectional fairness scenario using probabilistic modeling techniques. The methods obtain theoretical guarantees regarding intersectionality and privacy, by synthesizing disparate lines of work from the areas of fairness in machine learning, Bayesian statistics, and differential privacy.<br /><br />In the first line of work, we developed Bayesian methods for measuring the fairness in algorithms and in the societal processes that underly data on human beings. We employed various machine learning models including probabilistic classifiers and deep neural networks, and we developed new models specifically for this task. Second, we tackled the challenge of scaling up learning algorithms while handling the uncertainty in estimating the fairness metrics when considering multiple protected attributes simultaneously. We leveraged online learning methodologies to improve the stability of the learning process in this setting. The third line of work was to ensure that inference in Bayesian models is fair. We developed a fair version of neural-network based Bayesian inference techniques to solve this task. We evaluated our methods in a variety of problem domains, including career recommendation based on social media data, and in the criminal justice domain, as well as on standard benchmark datasets.<br /><br />This project will help artificial intelligence practitioners to detect bias in AI and ML systems, and to thereby ensure that the methods behave in a fair manner, with important implications for society in areas including credit, employment, and criminal justice. The methods will help to preserve trust in AI systems, which is critical for continued growth in this area, a keystone for the economic competitiveness of the United States of America.<br /><br />Overall, the project has produced fourteen peer-reviewed publications, including ten conference or workshop proceeding papers and four journal articles, with further papers expected to follow. The project helped to train a Ph.D. student who defended his dissertation primarily based on this work. Numerous additional graduate students were trained in fair AI research, as well as two undergraduate students and two high-school students. The principal investigator (PI) has given twenty-two oral presentations on the work to date, in addition to presentations for conference papers which were given by either the students or the PI.</p><br>\n<p>\n Last Modified: 11/04/2023<br>\nModified by: James&nbsp;Foulds</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWith the rise of machine learning (ML) and artificial intelligence (AI) systems and their impact on our daily lives, there are growing concerns that these algorithms can behave unfairly by inadvertently encoding or introducing discriminatory bias. The field of algorithmic fairness aims to develop machine learning methods which automatically ensure fair behavior. However, there are several challenges. First, the protected attributes, for which we desire to ensure fairness, are typically correlated with other attributes (including class labels as well as other protected attributes). This foils naive approaches to fairness, and complicates both the definition and the measurement of bias. Furthermore, multiple protected attributes, such as gender, race, nationality, and disability status, may need to be considered simultaneously, as motivated by intersectionality theory from the humanities and legal literature, and which may be mandated by law (e.g. Title VII of the Civil Rights Act of 1964). Existing fairness methods nevertheless frequently assume a single protected attribute, which is often further assumed to be binary.\n\nWhile it is conceptually straightforward to extend fairness methods to multiple protected attributes with multiple values, data sparsity rapidly becomes an issue as the number of dimensions or values increases, leading to uncertainty in the measurement of fairness. This project developed methods for measuring and correcting bias that account for and manage uncertainty. Using a novel statistical definition of fairness as a unifying framework, the project's contributions include statistically efficient algorithms for measuring fairness in high-dimensional settings, fairness-preserving Bayesian inference algorithms, and fair learning algorithms which take uncertainty into account.\n\nWe employed the principles of Bayesian statistical modeling, a branch of statistics that emphasizes the quantification of uncertainty, throughout this work to manage uncertainty in the high-dimensional fairness setting. The research advanced the state of the art in the important intersectional fairness scenario using probabilistic modeling techniques. The methods obtain theoretical guarantees regarding intersectionality and privacy, by synthesizing disparate lines of work from the areas of fairness in machine learning, Bayesian statistics, and differential privacy.\n\nIn the first line of work, we developed Bayesian methods for measuring the fairness in algorithms and in the societal processes that underly data on human beings. We employed various machine learning models including probabilistic classifiers and deep neural networks, and we developed new models specifically for this task. Second, we tackled the challenge of scaling up learning algorithms while handling the uncertainty in estimating the fairness metrics when considering multiple protected attributes simultaneously. We leveraged online learning methodologies to improve the stability of the learning process in this setting. The third line of work was to ensure that inference in Bayesian models is fair. We developed a fair version of neural-network based Bayesian inference techniques to solve this task. We evaluated our methods in a variety of problem domains, including career recommendation based on social media data, and in the criminal justice domain, as well as on standard benchmark datasets.\n\nThis project will help artificial intelligence practitioners to detect bias in AI and ML systems, and to thereby ensure that the methods behave in a fair manner, with important implications for society in areas including credit, employment, and criminal justice. The methods will help to preserve trust in AI systems, which is critical for continued growth in this area, a keystone for the economic competitiveness of the United States of America.\n\nOverall, the project has produced fourteen peer-reviewed publications, including ten conference or workshop proceeding papers and four journal articles, with further papers expected to follow. The project helped to train a Ph.D. student who defended his dissertation primarily based on this work. Numerous additional graduate students were trained in fair AI research, as well as two undergraduate students and two high-school students. The principal investigator (PI) has given twenty-two oral presentations on the work to date, in addition to presentations for conference papers which were given by either the students or the PI.\t\t\t\t\tLast Modified: 11/04/2023\n\n\t\t\t\t\tSubmitted by: JamesFoulds\n"
 }
}