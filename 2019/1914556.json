{
 "awd_id": "1914556",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Computer-Intensive Methods for Nonparametric Analysis of Dependent Data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2019-07-29",
 "awd_max_amd_letter_date": "2019-07-29",
 "awd_abstract_narration": "Ever since the fundamental recognition of the potential role of the computer in modern statistics, the bootstrap and other computer-intensive statistical methods have been developed extensively for inference with independent data. Such methods are even more important in the context of dependent data where the distribution theory for estimators and test statistics may be difficult or impractical to obtain. Furthermore, the recent information explosion has resulted in datasets of unprecedented size that call for flexible, nonparametric, and, by necessity, computer-intensive methods of data analysis. Time series analysis in particular is vital in many diverse scientific disciplines, e.g., in economics, engineering, acoustics, geostatistics, biostatistics, medicine, ecology, forestry, seismology, and meteorology. As a consequence of the  proposal's development of efficient and robust methods for the statistical analysis of dependent data, more accurate and reliable inferences may be drawn from datasets of practical import resulting into appreciable benefits to society.  Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics  (e.g. fMRI data), and bioinformatics (e.g. genetics and microarray data).\r\n\r\n\r\nThe project focuses on the development of methods of inference for the analysis of dependent data that do not rely on unrealistic or unverifiable model assumptions. In particular, the principal investigator and his collaborators will work on: (a) Subsampling and resampling for Big Data, including bootstrap for multivariatetime series of large dimension; (b) New model fitting and resampling for ARMA (p,q) models with both p,q large; (c) New smoothing estimators of time-varying covariance matrices for locally stationary multivariate time series; (d) Resampling for time series with an (almost) periodic component; (e) Model-free Bootstrap for stationary and non-stationary data; (f) Estimating the degree of smoothness and support of the common density of stationary data; (g) Improved nonparametric estimation of a hazard rate function; (h) A bootstrap test for the null hypothesis of `overdifferencing'; (i) Markov-type resampling and Linear Process Bootstrap for stationary random fields; and (j) Different aspects of resampling of functional and high-dimensional time series.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dimitris",
   "pi_last_name": "Politis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dimitris Politis",
   "pi_email_addr": "dpolitis@ucsd.edu",
   "nsf_id": "000178487",
   "pi_start_date": "2019-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive #0112",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930112",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">Starting at the latter part of the 20th century, statisticians have been gradually moving away from parametric models that often rely on restrictive and/or unreliable assumptions, and going towards nonparametric models that are more flexible. Bootstrap methods---also known as `resampling'--have been instrumental in that respect since they provide practitioners with a general way to conduct statistical inference in the nonparametric context.</p>\n<p class=\"Default\">The computer-intensive methodology of Model-Free prediction was further developed under this project, and is pushing the nonparametric envelope a bit further. For example, in the important setting of nonparametric regression, the Model-Free paradigm shows that an additive model is not needed in order to conduct statistical inference, i.e., estimation, prediction, confidence intervals, etc. Moreover, the need of preliminary transformations and data preprocessing---a mainstay of statistical practice for over a hundred years---is rendered superfluous under the Model-Free paradigm; the PI's 2015 monograph contains the seeds of these ideas that have been further developed by the PI in papers published in the Journal of the American Statistical Association, the Annals of Statistics, Bernoulli, etc.</p>\n<p class=\"Default\">Another important class of problems involves time series data, i.e., observations obtained over time. Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics and bioinformatics (e.g. fMRI data), etc. In this context, the PI and other researchers have been devising different resampling algorithms with the purpose of capturing the dependence/correlation that this type of data invariably exhibit. One such new development is the linear process bootstrap proposed by the PI and collaborators in 2010, and further developed since. An older method is the autoregressive sieve bootstrap proposed 25 years ago. For a long time, both of these methods were thought to be valid only for linear time series. Nevertheless, many time series of interest are well-known to be nonlinear; a prime example is financial returns data. In the last few years, the PI and co-authors were able to extend the applicability of the autoregressive sieve bootstrap and the linear process bootstrap---as well as the newly proposed moving average sieve bootstrap---to nonlinear time series.</p>\n<p>In order to better disseminate the problems and methods of modern time series analysis, the PI co-authored the textbook <em>Time Series: a First Course with Bootstrap Starter</em> in 2020. The book is dual-level: the first half is geared towards an upper-level undergraduate course, while the 2<sup>nd</sup> half is graduate-level, exposing the reader to some state-of-the-art methods. In particular, this is the first Time Series textbook to address resampling and subsampling for time series. All R code and data from the book and the PI's papers are available via the PI's website at UCSD.</p>\n<p class=\"Default\">Notably, high-dimensional inference has been at the center of attention of late. For example, the success of the Lasso in the era of high-dimensional regression can be attributed to its conducting an implicit model selection, i.e., zeroing out regression coefficients that are not significant. By contrast, classical ridge regression can not reveal a potential sparsity of parameters, and may also introduce a large bias under the high-dimensional setting. Nevertheless, recent work on the Lasso involves debiasing and thresholding---the latter in order to further enhance the model selection. As a consequence, ridge regression is worth another look since ---after debiasing and thresholding--- it may offer some advantages over the Lasso, e.g., it can be easily computed using a closed-form expression while it has comparable practical performance to (thresholded) Lasso. The PI with his students defined a debiased and thresholded ridge regression method, and proved its asymptotic consistency. Furthermore, a wild bootstrap algorithm was devised to construct confidence regions and perform hypothesis testing. The problem of prediction of future data was also addressed via a novel, hybrid bootstrap algorithm tailored to yield prediction intervals.</p>\n<p class=\"Default\">Moreover, the PI and collaborators have developed resampling methods that are applicable in other useful, albeit complicated, settings involving correlated data. One such setting has to do with time series that exhibit seasonal variation, e.g., annual variation due to the seasons, or daily variation; for example, consider a time series of a city's electricity demand measured monthly (seasonal variation) or hourly (daily variation). Another interesting setting has to do with time series that are locally ---as opposed to globally--- stationary; this applies to phenomena whose structure may be slowly evolving over time, e.g. climate data over the course of hundreds of years. The PI and co-authors have recently proposed resampling methods that are applicable to data that are exhibit nonstationarities, either because of seasonality or due to a slowly drifting probabilistic structure.&nbsp;&nbsp;</p>\n<p class=\"Default\">Finally, the project allowed for the training and involvement of graduate and undergraduate students in this research through regular coursework, independent study, and projects; several students were involved in parts of this project during its duration. The PI has a strong record of mentoring students including students coming from under-represented minorities.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/07/2023<br>\n\t\t\t\t\tModified by: Dimitris&nbsp;Politis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Starting at the latter part of the 20th century, statisticians have been gradually moving away from parametric models that often rely on restrictive and/or unreliable assumptions, and going towards nonparametric models that are more flexible. Bootstrap methods---also known as `resampling'--have been instrumental in that respect since they provide practitioners with a general way to conduct statistical inference in the nonparametric context.\nThe computer-intensive methodology of Model-Free prediction was further developed under this project, and is pushing the nonparametric envelope a bit further. For example, in the important setting of nonparametric regression, the Model-Free paradigm shows that an additive model is not needed in order to conduct statistical inference, i.e., estimation, prediction, confidence intervals, etc. Moreover, the need of preliminary transformations and data preprocessing---a mainstay of statistical practice for over a hundred years---is rendered superfluous under the Model-Free paradigm; the PI's 2015 monograph contains the seeds of these ideas that have been further developed by the PI in papers published in the Journal of the American Statistical Association, the Annals of Statistics, Bernoulli, etc.\nAnother important class of problems involves time series data, i.e., observations obtained over time. Examples include data from meteorology/atmospheric science (e.g. climate data), economics (e.g. stock market returns), biostatistics and bioinformatics (e.g. fMRI data), etc. In this context, the PI and other researchers have been devising different resampling algorithms with the purpose of capturing the dependence/correlation that this type of data invariably exhibit. One such new development is the linear process bootstrap proposed by the PI and collaborators in 2010, and further developed since. An older method is the autoregressive sieve bootstrap proposed 25 years ago. For a long time, both of these methods were thought to be valid only for linear time series. Nevertheless, many time series of interest are well-known to be nonlinear; a prime example is financial returns data. In the last few years, the PI and co-authors were able to extend the applicability of the autoregressive sieve bootstrap and the linear process bootstrap---as well as the newly proposed moving average sieve bootstrap---to nonlinear time series.\n\nIn order to better disseminate the problems and methods of modern time series analysis, the PI co-authored the textbook Time Series: a First Course with Bootstrap Starter in 2020. The book is dual-level: the first half is geared towards an upper-level undergraduate course, while the 2nd half is graduate-level, exposing the reader to some state-of-the-art methods. In particular, this is the first Time Series textbook to address resampling and subsampling for time series. All R code and data from the book and the PI's papers are available via the PI's website at UCSD.\nNotably, high-dimensional inference has been at the center of attention of late. For example, the success of the Lasso in the era of high-dimensional regression can be attributed to its conducting an implicit model selection, i.e., zeroing out regression coefficients that are not significant. By contrast, classical ridge regression can not reveal a potential sparsity of parameters, and may also introduce a large bias under the high-dimensional setting. Nevertheless, recent work on the Lasso involves debiasing and thresholding---the latter in order to further enhance the model selection. As a consequence, ridge regression is worth another look since ---after debiasing and thresholding--- it may offer some advantages over the Lasso, e.g., it can be easily computed using a closed-form expression while it has comparable practical performance to (thresholded) Lasso. The PI with his students defined a debiased and thresholded ridge regression method, and proved its asymptotic consistency. Furthermore, a wild bootstrap algorithm was devised to construct confidence regions and perform hypothesis testing. The problem of prediction of future data was also addressed via a novel, hybrid bootstrap algorithm tailored to yield prediction intervals.\nMoreover, the PI and collaborators have developed resampling methods that are applicable in other useful, albeit complicated, settings involving correlated data. One such setting has to do with time series that exhibit seasonal variation, e.g., annual variation due to the seasons, or daily variation; for example, consider a time series of a city's electricity demand measured monthly (seasonal variation) or hourly (daily variation). Another interesting setting has to do with time series that are locally ---as opposed to globally--- stationary; this applies to phenomena whose structure may be slowly evolving over time, e.g. climate data over the course of hundreds of years. The PI and co-authors have recently proposed resampling methods that are applicable to data that are exhibit nonstationarities, either because of seasonality or due to a slowly drifting probabilistic structure.  \nFinally, the project allowed for the training and involvement of graduate and undergraduate students in this research through regular coursework, independent study, and projects; several students were involved in parts of this project during its duration. The PI has a strong record of mentoring students including students coming from under-represented minorities.\n\n \n\n\t\t\t\t\tLast Modified: 09/07/2023\n\n\t\t\t\t\tSubmitted by: Dimitris Politis"
 }
}