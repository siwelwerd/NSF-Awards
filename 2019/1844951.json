{
 "awd_id": "1844951",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Coding Theory for Robust Large-Scale Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-05-01",
 "awd_exp_date": "2024-04-30",
 "tot_intn_awd_amt": 508271.0,
 "awd_amount": 508271.0,
 "awd_min_amd_letter_date": "2018-12-11",
 "awd_max_amd_letter_date": "2021-08-19",
 "awd_abstract_narration": "Coding theory has played a critical role in modern information technology by supporting robustness of information against a backdrop of multifaceted uncertainty. Following recent successes in machine learning, robustness has emerged as a desired principle, but now in the context of large-scale computation.  Challenges related to robustness are prevalent when deploying machine learning solutions in real applications and non-curated settings, which are often non-ideal environments. This project aims to address these challenges by developing novel solutions based on coding theory for computation. These solutions offer provable robustness guarantees, can outperform more traditional solutions in practice, and extend to machine learning systems the gains that have transformed communication and storage systems. Existing and new collaborations of the investigator will facilitate industry cooperation and increase the transition to practice for the frameworks and algorithms generated from this project. The research will be strongly coupled with educational developments guided by recent advances in education science, alongside an outreach program within the Wisconsin Institute for Discovery. \r\n\r\nThis project aims to develop novel coding-theoretic solutions and fundamental trade-offs for robust large-scale machine learning. The research program is centered around three thrusts. The first thrust focuses on robustness during distributed optimization in the presence of delays and straggler nodes, where the speed of convergence is affected by nodes in the system that are significantly slower than average. The second thrust focuses on robustness during distributed optimization in the presence of Byzantine nodes and worst-case failures. Recent studies proposed robust aggregation rules to filter out the effect of worst-case or adversarial failures. This project develops coding-theoretic solutions that can be orders of magnitude faster, and give rise to unexplored trade-offs between computation and Byzantine tolerance. The third thrust focuses on adversarial perturbations during prediction that can force state-of-the-art models to consistently mis-classify events/data. The coding-theoretic approach of this project pursues provable defense mechanisms against adversarial attacks through ensembles of models with inherent redundancy and through data augmentation. The proposed theoretical and algorithmic solutions are afforded by an interdisciplinary mix of tools from information and coding theory, distributed optimization, and machine learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dimitrios",
   "pi_last_name": "Papailiopoulos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dimitrios Papailiopoulos",
   "pi_email_addr": "dimitris@ece.wisc.edu",
   "nsf_id": "000738713",
   "pi_start_date": "2018-12-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1415 Engineering Drive",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061691",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0123",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 86947.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 203560.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 217764.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Project Outcomes Report for the General Public</strong></p>\n<p><strong>Title:</strong>&nbsp;CAREER: Coding Theory for Robust Large-Scale Machine Learning</p>\n<p>Over the duration of this project, we investigated the application of coding theory to enhance the robustness and efficiency of large-scale machine learning systems. Traditionally used to protect information in communication systems against errors and noise, coding theory principles were extended to address challenges in distributed machine learning, such as delays from slow compute nodes (stragglers), communication bottlenecks, and security vulnerabilities.</p>\n<p><strong>Intellectual Merit</strong></p>\n<ol>\n<li>\n<p><strong>Mitigating Stragglers with Coded Computation</strong>: We developed methods that introduce algorithmic redundancy using coding theory to mitigate the impact of slow or failed compute nodes in distributed systems. Our \"Approximate Gradient Coding\" techniques employ sparse random graphs to ensure fast and approximately accurate computations, significantly improving the efficiency of distributed machine learning.</p>\n</li>\n<li>\n<p><strong>Byzantine-Resilient Training</strong>: We introduced frameworks like DRACO (Distributed Reliable Algorithms for Consensus and Optimization) that utilize coding theory to defend against Byzantine failures&mdash;malicious or faulty compute nodes that can corrupt the training process. These methods provide strong robustness guarantees without incurring significant computational overhead, outperforming traditional median-based approaches.</p>\n</li>\n<li>\n<p><strong>Communication-Efficient Learning</strong>: To address communication bottlenecks in distributed training, we proposed techniques such as Pufferfish, which integrates gradient compression into the training process through low-rank, pre-factorized neural networks. This approach reduces communication costs without sacrificing model accuracy or adding computational overhead, enabling faster training and more scalable systems.</p>\n</li>\n<li>\n<p><strong>Understanding Data Augmentation and Robustness</strong>: We investigated whether data augmentation techniques lead to increased robustness by enforcing a positive margin in classification tasks. Our findings offer theoretical insights into how data augmentation can improve model generalization and robustness, informing best practices in training machine learning models.</p>\n</li>\n<li>\n<p><strong>Robustness and Security in Federated Learning</strong>: We explored vulnerabilities in federated learning systems, demonstrating that adversaries can introduce backdoor attacks affecting model performance on specific tasks without detection. This work underscores the need for improved security measures in federated learning and provides guidance on defending against such attacks.</p>\n</li>\n</ol>\n<p><strong>Broader Impacts</strong></p>\n<ul>\n<li>\n<p><strong>Advancing Interdisciplinary Research</strong>: Our work bridged the gap between coding theory and machine learning, fostering new research directions and collaborations. We co-organized workshops and conferences, such as the Dagstuhl Workshop on Coding Theory for Inference, Learning, and Optimization, and the ICML Workshop on Coding for Machine Learning, helping to build a community around this interdisciplinary field.</p>\n</li>\n<li>\n<p><strong>Educational Contributions</strong>: Findings from the project were integrated into undergraduate and graduate courses, enhancing education in data science and machine learning. We mentored several Ph.D. students who have advanced in their careers, including positions in academia and industry.</p>\n</li>\n<li>\n<p><strong>Industry Applications</strong>: Techniques developed during this project have been adopted by leading technology companies, including Sony, IBM, and Microsoft, improving the efficiency and robustness of their machine learning systems. Collaborations with industry partners facilitated the testing and application of our methods in real-world settings.</p>\n</li>\n<li>\n<p><strong>Recognition and Awards</strong>: The project received several prestigious awards, including the IEEE Joint Communications Society/Information Theory Society Best Paper Award (2020) and the IEEE Education Society Mac Van Valkenburg Early Career Teaching Award (2021), acknowledging both the research innovations and contributions to education..</p>\n</li>\n</ul>\n<p><strong>Conclusion</strong></p>\n<p>This project, grounded on experimental and theoretical evidence, demonstrated that coding theory offers powerful tools for addressing critical challenges in large-scale machine learning. By applying coding principles to distributed computation, we developed methods that improve robustness, efficiency, and security in machine learning systems. The interdisciplinary nature of this work advanced theoretical understanding and led to practical applications benefiting academia and industry. Our contributions lay the groundwork for continued exploration in this promising area, with the potential to significantly impact the future of machine learning and distributed computing.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/18/2024<br>\nModified by: Dimitrios&nbsp;Papailiopoulos</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nProject Outcomes Report for the General Public\n\n\nTitle:CAREER: Coding Theory for Robust Large-Scale Machine Learning\n\n\nOver the duration of this project, we investigated the application of coding theory to enhance the robustness and efficiency of large-scale machine learning systems. Traditionally used to protect information in communication systems against errors and noise, coding theory principles were extended to address challenges in distributed machine learning, such as delays from slow compute nodes (stragglers), communication bottlenecks, and security vulnerabilities.\n\n\nIntellectual Merit\n\n\n\n\nMitigating Stragglers with Coded Computation: We developed methods that introduce algorithmic redundancy using coding theory to mitigate the impact of slow or failed compute nodes in distributed systems. Our \"Approximate Gradient Coding\" techniques employ sparse random graphs to ensure fast and approximately accurate computations, significantly improving the efficiency of distributed machine learning.\n\n\n\n\nByzantine-Resilient Training: We introduced frameworks like DRACO (Distributed Reliable Algorithms for Consensus and Optimization) that utilize coding theory to defend against Byzantine failuresmalicious or faulty compute nodes that can corrupt the training process. These methods provide strong robustness guarantees without incurring significant computational overhead, outperforming traditional median-based approaches.\n\n\n\n\nCommunication-Efficient Learning: To address communication bottlenecks in distributed training, we proposed techniques such as Pufferfish, which integrates gradient compression into the training process through low-rank, pre-factorized neural networks. This approach reduces communication costs without sacrificing model accuracy or adding computational overhead, enabling faster training and more scalable systems.\n\n\n\n\nUnderstanding Data Augmentation and Robustness: We investigated whether data augmentation techniques lead to increased robustness by enforcing a positive margin in classification tasks. Our findings offer theoretical insights into how data augmentation can improve model generalization and robustness, informing best practices in training machine learning models.\n\n\n\n\nRobustness and Security in Federated Learning: We explored vulnerabilities in federated learning systems, demonstrating that adversaries can introduce backdoor attacks affecting model performance on specific tasks without detection. This work underscores the need for improved security measures in federated learning and provides guidance on defending against such attacks.\n\n\n\n\nBroader Impacts\n\n\n\n\nAdvancing Interdisciplinary Research: Our work bridged the gap between coding theory and machine learning, fostering new research directions and collaborations. We co-organized workshops and conferences, such as the Dagstuhl Workshop on Coding Theory for Inference, Learning, and Optimization, and the ICML Workshop on Coding for Machine Learning, helping to build a community around this interdisciplinary field.\n\n\n\n\nEducational Contributions: Findings from the project were integrated into undergraduate and graduate courses, enhancing education in data science and machine learning. We mentored several Ph.D. students who have advanced in their careers, including positions in academia and industry.\n\n\n\n\nIndustry Applications: Techniques developed during this project have been adopted by leading technology companies, including Sony, IBM, and Microsoft, improving the efficiency and robustness of their machine learning systems. Collaborations with industry partners facilitated the testing and application of our methods in real-world settings.\n\n\n\n\nRecognition and Awards: The project received several prestigious awards, including the IEEE Joint Communications Society/Information Theory Society Best Paper Award (2020) and the IEEE Education Society Mac Van Valkenburg Early Career Teaching Award (2021), acknowledging both the research innovations and contributions to education..\n\n\n\n\nConclusion\n\n\nThis project, grounded on experimental and theoretical evidence, demonstrated that coding theory offers powerful tools for addressing critical challenges in large-scale machine learning. By applying coding principles to distributed computation, we developed methods that improve robustness, efficiency, and security in machine learning systems. The interdisciplinary nature of this work advanced theoretical understanding and led to practical applications benefiting academia and industry. Our contributions lay the groundwork for continued exploration in this promising area, with the potential to significantly impact the future of machine learning and distributed computing.\n\n\n\t\t\t\t\tLast Modified: 10/18/2024\n\n\t\t\t\t\tSubmitted by: DimitriosPapailiopoulos\n"
 }
}