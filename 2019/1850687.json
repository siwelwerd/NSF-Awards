{
 "awd_id": "1850687",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Computational modeling of speech perception and imagery in the human brain",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jonathan Fritz",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 698910.0,
 "awd_amount": 698910.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "Human communication relies on spoken language, a critical ability that is disrupted in millions of patients with neuromuscular disorders, traumatic brain injury, stroke, or communication disorders. This project develops computational predictive models of how the human brain encodes speech, using direct brain recordings from neurosurgical patients to validate these models. The outcome of this project is immediately applicable to understanding communication disorders and to the development of neural prosthetic systems that aim to dramatically improve life for large patient populations with disordered speech function.\r\n\r\nDefining the basic neural mechanisms supporting speech comprehension is a fundamental challenge for clinical insights into communication disorders such as aphasia or dyslexia. Better understanding of the neural basis of speech may also enable development of neural prosthetic devices for disabling neurological disorders affecting communication ability (e.g., ALS or stroke). Humans fluidly understand speech despite large variations in speakers and environmental conditions, such as speaker identity, rate, and background noise, but the underlying neural representations that support this invariant speech recognition ability are unknown. We will establish and test computational models of the invariant neural representations used by the human auditory system to achieve reliable speech comprehension. An important application of this knowledge is the development of robust neural decoding algorithms that can be used in neural prosthetic systems designed to restore conversational speech to individuals with disabling language disorders. To achieve these goals, this project will investigate intracranial responses to speech measured with high density electrode arrays implanted in the auditory cortex of patients undergoing neurosurgical procedures. The first aim will investigate neural encoding of invariant speech representations believed to comprise key organizational units of spoken language, including phonetic and syllable structure. The second aim develops a novel machine learning framework to build decoding models directly from neural data recorded during intended silent (imagined) speech. This research program will provide new quantitative tools to understand neural mechanisms of speech comprehension and imagery in the human brain with a goal to advance application of these findings toward development of neural interfaces to restore natural speech.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Pasley",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brian Pasley",
   "pi_email_addr": "bpasley@berkeley.edu",
   "nsf_id": "000754084",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "Helen Wills Neuroscience Institute - Knight Lab",
  "perf_str_addr": "132 Barker Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947203370",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "139700",
   "pgm_ele_name": "Cross-Directorate  Activities"
  },
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 698910.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project examined higher-order neural representations of speech and music in human cortex using electrocorticography (ECoG) recordings during various speech and music tasks.&nbsp; We used neural encoding models to assess how well different candidate representations predicted measured neural recordings.&nbsp; We compared the predictive accuracy of represenations based on the auditory spectrogram, individual phonemes, consonant categories, and syllable types (e.g., consonant-vowel,&nbsp; consonant-vowel-consonant, vowel-consonant). We obtained preliminary results indicating the existence of a syllable-like representation in human superior temporal gyrus that encoded these complex consonant-vowel syllabalic sequences.&nbsp; This representation provided the highest prediction accuracy among the tested represenations.&nbsp;&nbsp;</p>\n<p class=\"p1\">Another outcome of this project was the development of an auditory localizer for human ECoG recordings.&nbsp; We designed a set of auditory stimuli and curated TIMIT speech corpus sentences based on verbal, syntactic and speaker statistics, to produce a 5-min auditory localizer task. This was recorded in 20+ patients with multiple dimensions and contrasts embedded in the stimulus sequence (e.g., pure vs complex tones, tonotopy, predictable vs random sequence, verb vs noun&hellip;) to enable the exploration of many auditory neuroscience questions from a very short, easy to record, passive-listening task.</p>\n<p>In addition, we investigated the neural representation of music perception.&nbsp; We used encoding and decoding models linking ECoG activity from two thousand electrodes across 29 patients and the acoustics of a rock song, to derive insights on the laterality of music perception, parcellation of the STG, representation of different musical features and methodological factors impacting decoding accuracy.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2023<br>\n\t\t\t\t\tModified by: Brian&nbsp;Pasley</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project examined higher-order neural representations of speech and music in human cortex using electrocorticography (ECoG) recordings during various speech and music tasks.  We used neural encoding models to assess how well different candidate representations predicted measured neural recordings.  We compared the predictive accuracy of represenations based on the auditory spectrogram, individual phonemes, consonant categories, and syllable types (e.g., consonant-vowel,  consonant-vowel-consonant, vowel-consonant). We obtained preliminary results indicating the existence of a syllable-like representation in human superior temporal gyrus that encoded these complex consonant-vowel syllabalic sequences.  This representation provided the highest prediction accuracy among the tested represenations.  \nAnother outcome of this project was the development of an auditory localizer for human ECoG recordings.  We designed a set of auditory stimuli and curated TIMIT speech corpus sentences based on verbal, syntactic and speaker statistics, to produce a 5-min auditory localizer task. This was recorded in 20+ patients with multiple dimensions and contrasts embedded in the stimulus sequence (e.g., pure vs complex tones, tonotopy, predictable vs random sequence, verb vs noun&hellip;) to enable the exploration of many auditory neuroscience questions from a very short, easy to record, passive-listening task.\n\nIn addition, we investigated the neural representation of music perception.  We used encoding and decoding models linking ECoG activity from two thousand electrodes across 29 patients and the acoustics of a rock song, to derive insights on the laterality of music perception, parcellation of the STG, representation of different musical features and methodological factors impacting decoding accuracy.\n\n \n\n\t\t\t\t\tLast Modified: 01/03/2023\n\n\t\t\t\t\tSubmitted by: Brian Pasley"
 }
}