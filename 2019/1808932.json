{
 "awd_id": "1808932",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Deep neural networks for multi-channel speaker localization and speech separation",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Anthony Kuh",
 "awd_eff_date": "2018-12-01",
 "awd_exp_date": "2022-11-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2018-12-04",
 "awd_max_amd_letter_date": "2018-12-04",
 "awd_abstract_narration": "In recent years, there is a dramatic increase in the deployment of the voice-based interface for human-machine communication. Such devices typically have multiple microphones (or channels), and as they are used in homes, cars, and so on, a major technical challenge is how to reliably localize a target speaker and recognize his/her speech in everyday environments with multiple sound sources and room reverberation. The performance of traditional approaches to localization and separation degrades significantly in the presence of interfering sounds and room reverberation. This project investigates multi-channel speaker localization and speech separation from a deep learning perspective. The innovative approach in this project is to train deep neural networks to perform single-channel speech separation in order to identify the time-frequency regions dominated by the target speaker. Such regions across microphone pairs provide the basis for robust speaker localization and separation. \r\n\r\nBuilding on this novel perspective, the proposed research seeks to achieve robust speaker localization and speech separation. For robust speaker localization, time-frequency (T-F) masks will be generated by deep neural networks (DNN) from single-channel noisy speech signals. Across each pair of microphones, an integrated mask will be calculated from the two corresponding single-channel masks and then used to weight a generalized cross-correlation function, from which the direction of the target speaker will be estimated. An alternative method for localization will be based on mask-weighted steered responses. For robust speech separation, masking-based beamforming will be initially performed, where T-F masking and accurate speaker localization are expected to enhance beamforming results substantially. To overcome the limitation of spatial filtering in multi-source reverberant conditions, spectral (monaural) and spatial information will be integrated as DNN input features in order to separate only the target signal with speech characteristics and originating from a specific direction. The proposed approach will be evaluated using automatic speech recognition rate, as well as localization and separation accuracy, on multi-channel noisy and reverberant datasets recorded in real-world environments. This will ensure a broader impact not only in advancing speech processing technology but also in facilitating the design of next-generation hearing aids in the long run.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "DeLiang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "DeLiang Wang",
   "pi_email_addr": "dwang@cse.ohio-state.edu",
   "nsf_id": "000486642",
   "pi_start_date": "2018-12-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "2015 Neil Ave.",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101277",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The deployment of the voice-based interface for human-machine communication has become commonplace in recent years. Such devices typically have multiple microphones (or channels), and a major technical challenge is how to reliably separate and localize a target speaker and recognize his/her speech in everyday environments with multiple sound sources and room reverberation. This project aimed to investigate multi-channel speech separation, speaker localization, and speech recognition, from a deep learning perspective. During the course of the project, several key advances were made, including new approaches to multi-channel speech dereverberation, speech enhancement, speaker separation, and automatic speech recognition (ASR).</p>\n<p>One key outcome is the development of a complex spectral mapping approach to multi-channel speech dereverberation. In this approach, the real and imaginary (RI) parts of multiple microphones are concatenated as the input features of a deep neural network (DNN) that is trained to predict the RI parts of the direct-path signal(s) captured at a reference microphone. After single-channel enhancement, a beamformer is formed to cancel the direct-path signal, and then the RI parts of the cancelled signal are used as additional features to perform speech dereverberation. Trained on only simulated room impulse responses, this approach achieves excellent speech dereverberation and ASR performance on the recorded test set of the REVERB challenge.</p>\n<p>Another important outcome is a new approach to multi-channel speech enhancement and robust ASR. The proposed system contains two DNNs. The first one performs complex spectral mapping for speech enhancement on each channel. The estimated complex spectra of individual channels are used to compute a beamformer whose RI parts are combined with those of the mixture to train the second DNN for multi-channel complex spectral mapping. The proposed system significantly advances state-of-the-art enhancement and recognition results on the single-, two- and six-microphone tasks of CHiME-4, which is a standard open corpus for assessing robust ASR performance in single-channel, two-channel and six-channel conditions.&nbsp;&nbsp;</p>\n<p>The next outcome is a state-of-the-art algorithm for multi-channel speaker separation and ASR, where multi-channel complex spectral mapping was employed for multi-talker speaker separation in reverberant and noisy conditions. Initial speaker separation results are utilized to compute target and nontarget spatial covariance matrices for beamforming, and post-filtering is performed on the RI parts of the beamforming results and microphone signals. Although the DNN is trained only on simulated room impulse responses with a fixed array geometry, it generalizes well to a real array with the same geometry. Systematic evaluations and comparisons demonstrate that the proposed DNN model obtains the best speaker separation and ASR results in both single-channel and seven-channel test conditions of the recorded LibriCSS dataset.</p>\n<p>Common among the above outcomes is multi-channel complex spectral mapping for spatial filtering. Compared with beamforming, the most widely-used spatial filtering approach, the trained DNN in multi-channel complex spectral mapping becomes a nonlinear, time-varying spectrospatial filter. Comprehensive evaluations demonstrate that neural spectrospatial filtering yields separation results comparable to or better than beamforming for different array geometries and speech separation tasks. This neural spectrospatial filter, which is conceptually simple and computationally efficient, provides a strong alternative to traditional and mask-based beamforming.</p>\n<p>Permutation ambiguity is a crucial issue for deep learning based talker-independent speaker separation. Another substantial outcome is location-based training (LBT), a new training technique to achieve talker independency in multi-channel speaker separation. Two new training criteria: azimuth-based and distance-based training, were proposed to resolve permutation ambiguity using speaker azimuths and distances relative to a microphone array. Evaluation results show that LBT significantly outperforms prevailing permutation-invariant training on two-talker and three-talker mixtures with different array geometries and in various acoustic conditions.</p>\n<p>The primary impact of this project is advancing the state of the art in multi-channel speech enhancement, speaker separation, and ASR. With advances in speech separation in real world environments, speech recognition and speaker localization technologies become available in a wider range of environmental settings. The outcomes of this project are expected to also impact speech and hearing technologies, including hearing prosthesis for listeners with impaired binaural hearing. Furthermore, the project provided training opportunities for four doctoral students over the course of the project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/20/2023<br>\n\t\t\t\t\tModified by: Deliang&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe deployment of the voice-based interface for human-machine communication has become commonplace in recent years. Such devices typically have multiple microphones (or channels), and a major technical challenge is how to reliably separate and localize a target speaker and recognize his/her speech in everyday environments with multiple sound sources and room reverberation. This project aimed to investigate multi-channel speech separation, speaker localization, and speech recognition, from a deep learning perspective. During the course of the project, several key advances were made, including new approaches to multi-channel speech dereverberation, speech enhancement, speaker separation, and automatic speech recognition (ASR).\n\nOne key outcome is the development of a complex spectral mapping approach to multi-channel speech dereverberation. In this approach, the real and imaginary (RI) parts of multiple microphones are concatenated as the input features of a deep neural network (DNN) that is trained to predict the RI parts of the direct-path signal(s) captured at a reference microphone. After single-channel enhancement, a beamformer is formed to cancel the direct-path signal, and then the RI parts of the cancelled signal are used as additional features to perform speech dereverberation. Trained on only simulated room impulse responses, this approach achieves excellent speech dereverberation and ASR performance on the recorded test set of the REVERB challenge.\n\nAnother important outcome is a new approach to multi-channel speech enhancement and robust ASR. The proposed system contains two DNNs. The first one performs complex spectral mapping for speech enhancement on each channel. The estimated complex spectra of individual channels are used to compute a beamformer whose RI parts are combined with those of the mixture to train the second DNN for multi-channel complex spectral mapping. The proposed system significantly advances state-of-the-art enhancement and recognition results on the single-, two- and six-microphone tasks of CHiME-4, which is a standard open corpus for assessing robust ASR performance in single-channel, two-channel and six-channel conditions.  \n\nThe next outcome is a state-of-the-art algorithm for multi-channel speaker separation and ASR, where multi-channel complex spectral mapping was employed for multi-talker speaker separation in reverberant and noisy conditions. Initial speaker separation results are utilized to compute target and nontarget spatial covariance matrices for beamforming, and post-filtering is performed on the RI parts of the beamforming results and microphone signals. Although the DNN is trained only on simulated room impulse responses with a fixed array geometry, it generalizes well to a real array with the same geometry. Systematic evaluations and comparisons demonstrate that the proposed DNN model obtains the best speaker separation and ASR results in both single-channel and seven-channel test conditions of the recorded LibriCSS dataset.\n\nCommon among the above outcomes is multi-channel complex spectral mapping for spatial filtering. Compared with beamforming, the most widely-used spatial filtering approach, the trained DNN in multi-channel complex spectral mapping becomes a nonlinear, time-varying spectrospatial filter. Comprehensive evaluations demonstrate that neural spectrospatial filtering yields separation results comparable to or better than beamforming for different array geometries and speech separation tasks. This neural spectrospatial filter, which is conceptually simple and computationally efficient, provides a strong alternative to traditional and mask-based beamforming.\n\nPermutation ambiguity is a crucial issue for deep learning based talker-independent speaker separation. Another substantial outcome is location-based training (LBT), a new training technique to achieve talker independency in multi-channel speaker separation. Two new training criteria: azimuth-based and distance-based training, were proposed to resolve permutation ambiguity using speaker azimuths and distances relative to a microphone array. Evaluation results show that LBT significantly outperforms prevailing permutation-invariant training on two-talker and three-talker mixtures with different array geometries and in various acoustic conditions.\n\nThe primary impact of this project is advancing the state of the art in multi-channel speech enhancement, speaker separation, and ASR. With advances in speech separation in real world environments, speech recognition and speaker localization technologies become available in a wider range of environmental settings. The outcomes of this project are expected to also impact speech and hearing technologies, including hearing prosthesis for listeners with impaired binaural hearing. Furthermore, the project provided training opportunities for four doctoral students over the course of the project.\n\n\t\t\t\t\tLast Modified: 02/20/2023\n\n\t\t\t\t\tSubmitted by: Deliang Wang"
 }
}