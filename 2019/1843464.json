{
 "awd_id": "1843464",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "STTR Phase I:  Spatial Artificial Intelligence System for the Visually Impaired (NavigAid)",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032924392",
 "po_email": "amonk@nsf.gov",
 "po_sign_block_name": "Alastair Monk",
 "awd_eff_date": "2019-02-01",
 "awd_exp_date": "2020-01-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2019-01-31",
 "awd_max_amd_letter_date": "2019-10-25",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Technology Transfer Phase I project are its two intended contributions: it will advance computer vision for navigation and it will create unprecedented opportunity and independence for individuals with visual impairment. 253 million people with blindness or visual impairment around the world experience difficulty to independently navigate in new spaces. The lack of independence directly leads to disadvantages in getting education and joining the workforce. This project, NavigAid,  is aimed at creating a breakthrough spatial intelligence aid offered as a mobile application which will solve spatial tasks from locating objects to identifying paths of navigation, and to generating rich descriptions of the surroundings. Solving this problem will unlock an estimated direct economic benefit of $26B  annually for people with visual impairment around the world, by decreasing the need for human assistance. The main technological innovation of this project, Ally Networks, is a novel neural network architecture that can learn more robust representations than existing models are able to. The robust representations prevent the networks from making mistakes and make them more operable and useful in situations that need high reliability. Ally Networks thus represent a potential breakthrough to the field of computer-vision navigation.\r\n\r\n\r\nThis Small Business Technology Transfer Phase I project will introduce a novel spatial intelligence system, NavigAid, to assist individuals with visual impairment in crucial navigation tasks. NavigAid will generate contextually relevant, task-oriented spatial information from smartphone cameras. With NavigAid, users will be able to navigate independently in unfamiliar, complex environments, thus achieving unprecedented mobility. NavigAid will advance assistive technologies by providing unprecedented services, such as locating objects and generating functionally relevant natural language descriptions of complex environments. The core innovation in this project is Ally Networks, a novel neural network architecture that learns robust spatial semantics rather than 2-dimensional representations. This unique multimodal learning strategy is a high-risk endeavour, with broad impact if successful. Large-scale multimodal learning is difficult, and our technique?s success will revolutionize the state of the art: neural network vision will transform from systems that fail in inscrutable ways, to systems that never fail under circumstances in which human vision would not also fail. Key objectives of this project are to 1) develop and validate Ally Networks on benchmarks, 2) develop spatial problem solvers that address the most pressing needs of users with visual impairment, and 3) develop a test suite to evaluate the spatial problem solvers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Cagri",
   "pi_last_name": "Zaman",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Cagri H Zaman",
   "pi_email_addr": "cagri@mediate.tech",
   "nsf_id": "000782918",
   "pi_start_date": "2019-01-31",
   "pi_end_date": "2019-07-03"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Emre",
   "pi_last_name": "Sarbak",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Emre Sarbak",
   "pi_email_addr": "emre.sarbak@gmail.com",
   "nsf_id": "000780215",
   "pi_start_date": "2019-07-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Patrick",
   "pi_last_name": "Winston",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Patrick H Winston",
   "pi_email_addr": "phw@mit.edu",
   "nsf_id": "000308018",
   "pi_start_date": "2019-01-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "VIRTUAL COLLABORATION RESEARCH INC.",
  "inst_street_address": "16A IVALOO ST",
  "inst_street_address_2": "",
  "inst_city_name": "SOMERVILLE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "3144458028",
  "inst_zip_code": "021433608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "VIRTUAL COLLABORATION RESEARCH INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "KJXAVAZD21F4"
 },
 "perf_inst": {
  "perf_inst_name": "Virtual Collaboration Research Inc.",
  "perf_str_addr": "16A Ivaloo St",
  "perf_city_name": "Somerville",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021433608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "150500",
   "pgm_ele_name": "STTR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1505",
   "pgm_ref_txt": "STTR PHASE I"
  },
  {
   "pgm_ref_code": "8038",
   "pgm_ref_txt": "Biotechnology"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2fffebbc-7fff-e43e-efb4-cb3a3b38cd18\">\n<p dir=\"ltr\"><span>Virtual Collaboration Research Inc.&rsquo;s goal is to empower the visually disabled with the promising abilities of the latest artificial intelligence and augmented reality, leveraging the untapped potential of mobile computing. Our vision is to make the 2020s the most pivotal decade in the history of visual impairment. The focus of this project is to develop an AI-based spatial understanding system called NavigAid.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>NavigAid provides task-driven solutions to problems, such as finding objects, identifying paths of ingress and egress, and understanding the salient features in an environment. Ally Networks, our core innovation that enables NavigAid, is a novel neural network architecture that is capable of extracting semantically and functionally relevant spatial features from images and producing human-like understanding of physical environments.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>In this STTR Phase I project, we validated our novel neural network architecture, Ally Networks, and determined its feasibility for implementing it as a spatial intelligence aid in mobile devices. Specifically, we demonstrated that Ally Networks can bootstrap multimodal information and learn robust spatial features that can be used in various tasks including depth estimation and semantic segmentation. We developed a dual channel depth estimator, which produced a 40% reduction in error in comparison to the benchmarks. Similarly, multimodal ally networks that are trained on a combination of RGB, depth, and surface normal data produced state-of-the art results in semantic segmentation tasks on the benchmark datasets. In addition, we evaluated two different object localization models and determined that Single Shot Detectors (SSD) are more suitable for NavigAid. Our experiments with our mobile test suite, NavigAid MVP, achieved object localization with a precision error rate lower than our target. When released to the public, NavigAid MVP received very high interest and usage proving the public value of our innovation.</span></p>\n<br />\n<p dir=\"ltr\"><span>The social and economic impact of the research is significant. There are more than 285 million visually impaired people, of which 39 million are blind. With the aging population, this population will continue to grow. Our project has immediate and long-term benefits to this community by taking a significant step towards independence with indoor and outdoor navigation.</span></p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/31/2020<br>\n\t\t\t\t\tModified by: Emre&nbsp;Sarbak</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nVirtual Collaboration Research Inc.\u2019s goal is to empower the visually disabled with the promising abilities of the latest artificial intelligence and augmented reality, leveraging the untapped potential of mobile computing. Our vision is to make the 2020s the most pivotal decade in the history of visual impairment. The focus of this project is to develop an AI-based spatial understanding system called NavigAid. \n\n\nNavigAid provides task-driven solutions to problems, such as finding objects, identifying paths of ingress and egress, and understanding the salient features in an environment. Ally Networks, our core innovation that enables NavigAid, is a novel neural network architecture that is capable of extracting semantically and functionally relevant spatial features from images and producing human-like understanding of physical environments. \n\n\nIn this STTR Phase I project, we validated our novel neural network architecture, Ally Networks, and determined its feasibility for implementing it as a spatial intelligence aid in mobile devices. Specifically, we demonstrated that Ally Networks can bootstrap multimodal information and learn robust spatial features that can be used in various tasks including depth estimation and semantic segmentation. We developed a dual channel depth estimator, which produced a 40% reduction in error in comparison to the benchmarks. Similarly, multimodal ally networks that are trained on a combination of RGB, depth, and surface normal data produced state-of-the art results in semantic segmentation tasks on the benchmark datasets. In addition, we evaluated two different object localization models and determined that Single Shot Detectors (SSD) are more suitable for NavigAid. Our experiments with our mobile test suite, NavigAid MVP, achieved object localization with a precision error rate lower than our target. When released to the public, NavigAid MVP received very high interest and usage proving the public value of our innovation.\n\n\nThe social and economic impact of the research is significant. There are more than 285 million visually impaired people, of which 39 million are blind. With the aging population, this population will continue to grow. Our project has immediate and long-term benefits to this community by taking a significant step towards independence with indoor and outdoor navigation.\n\n\n\t\t\t\t\tLast Modified: 03/31/2020\n\n\t\t\t\t\tSubmitted by: Emre Sarbak"
 }
}