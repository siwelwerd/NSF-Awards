{
 "awd_id": "1910306",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Explaining Unsupervised Learning: Combinatorial Optimization Formulations, Methods and Applications",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927978",
 "po_email": "racharya@nsf.gov",
 "po_sign_block_name": "Raj Acharya",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 265000.0,
 "awd_amount": 281000.0,
 "awd_min_amd_letter_date": "2019-09-03",
 "awd_max_amd_letter_date": "2021-12-13",
 "awd_abstract_narration": "Clustering is a common machine learning and data mining technique which takes a collection of instances/records/things and divides them into groups. It is used in a large variety of domains including social networks (to find communities), biology (to create taxonomies) and neuroscience (to find regions of interest in the brain). There are many clustering algorithms already in existence, but these algorithms do not always explain the clustering. This award addresses the problem of describing the clustering algorithm results to a variety of stakeholders including data scientists, domain scientists and the general public. Explaining the results of these algorithms will allow them to be better understood by stakeholders and allow their use in challenging and sensitive domains where transparency is required. Explanations are given in an initiative form using easy to understand auxiliary information such as tags. This project will consist of three inter-twined tasks. The first will develop easy to understand mechanisms to explain a clustering, whilst the second will allow a human to interact with the explanation by asking queries about it. Finally the third task will attach measure of stability, trust and correctness to the explanations generated from task one.\r\n\r\nThe area of unsupervised learning is immensely popular due to the lack of need for labeled data and there exist many algorithms that can work on a variety of data types: images, graphs, documents, spatial and temporal data. Many domains have a preferred/well-accepted clustering algorithm. However, most algorithms provide just a grouping of the instances/objects into clusters with limited description. The work on describing and/or explaining a solution has gained popularity in the supervised learning context but is under-studied in the unsupervised context.  This award explores these novel explanation problems through discrete combinatorial optimization formulations. Such formulations help in developing explanations requiring interpretable (hence discrete) results, best possible explanations (not any plausible explanation) and in enforcing complex constraints to make explanations match human expectations. This research will leverage much work  in theoretical computer science and use tools from declarative paradigms such as ILP solvers and constraint programming languages. Such tools allow for easy modifications of formulations, a desirable trait as different domains may need different variations in explanation. The usefulness of the techniques will be  demonstrated through their applications to several domains including social networks and genomic data and evaluated by two domain experts in the area.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ian",
   "pi_last_name": "Davidson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ian Davidson",
   "pi_email_addr": "davidson@cs.ucdavis.edu",
   "nsf_id": "000292619",
   "pi_start_date": "2019-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 179091.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 85909.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Overview: </strong>With the tremendous growth in the number of applications in which data mining and machine learning algorithms are used, it has become crucial to add explanations to the decisions made by these algorithms. This area of research is referred to as Explainable AI (XAI). When this project started, much of the work on XAI was on explaining outcomes of supervised learning methods where learning is achieved using training data (e.g., methods using neural networks). This project explored techniques for developing explanations for unsupervised learning tasks such as clustering and outlier detection, which had not received enough attention in the literature. Explanation methods for unsupervised learning are important since a large number of algorithms for clustering and outlier detection are used in practice. For example, in the context of clustering, it is important to explain why an algorithm included two data points in the same cluster or why it put them in different clusters. For example, if the clusters are constructed from a dataset involving people, organizations may use different actions (e.g., approve or deny a loan) on different clusters. In the context of outlier detection, it is important to explain why a data point was identified as an outlier; this is because the technique is used in applications where the identified outliers may be further investigated or policed.</p>\r\n<p><strong>Intellectual Merit: </strong>This research developed rigorous formulations of problems associated with the generation of explanations for unsupervised learning tasks. These problems belong to the category of combinatorial optimization problems which can be studied using techniques from algorithms and complexity theory. The work explored several forms of explanation.</p>\r\n<p>One form used auxiliary information (called tags) associated with each data point to develop explanations. This form exploited the connection with the well known minimum set cover problem in theoretical computer science to identify a subset of tags as the explanation for each cluster. The results of this work led to algorithmic and mathematical programming based techniques for constructing optimal or provably near-optimal explanations. This form of explanation has been used for clusters obtained from many different datasets.</p>\r\n<p>A second form of explanation used small subsets of data points themselves (called exemplars) to as explanations of clusters. The corresponding optimization problem combined the tasks of clustering and generating exemplars. Since each of these problems is known to be computationally intractable, approximation algorithms with provable performance guarantees were developed. This exemplar-based explanation was shown to be useful for datasets from domains which are difficult to understand by humans (e.g., data obtained by embedding images and text).</p>\r\n<p>A third form of explanation is useful in the context of distinguishing between outliers and normal data points. This form of explanation is called contrastive explanation and it also relies on the use of tags (auxiliary information). By focusing on the relationship between tags and data points, a rigorous formulation of the contrastive explanation problem was developed using the well known Knapsack problem from combinatorial optimization as the basis. This enabled the development of efficient algorithms for some versions of the problem and complexity results for other versions. The resulting algorithms were applied to multiple modalities of data (i.e., images, text and databases).</p>\r\n<p>Other topics related to explanation were also explored during the period of this grant. One such topic the stability of explanations, that is, whether the generated explanation changes significantly when minor changes are made to the auxiliary information (i.e., tags) or the clusters. Another topic was the use of another combinatorial problem (namely, the hitting set problem) to generate a more detailed explanation for each cluster. Results produced from the work have been published in well known conferences and journals in machine learning. Several additional papers are under preparation.</p>\r\n<p><strong>Broader Impact: </strong>The grant supported recently graduated students Dr Michael Livanos and Dr Ge Shi and undergraduates Ms Avigail Shekar Gafni and Mr Nicolas Kennedy. All students supported received valuable training in several areas of computer science, including algorithm design, application and implementation, combinatorial optimization, mathematical programming and software design/testing. The PI (Davidson) used the basic research in this grant as a basis for the applied NIH grant R01 MH076989 (Pathophysiological Biomarkers of Treatment Response in Early Psychosis,&rdquo;&nbsp;) for which he is a co-PI.</p><br>\n<p>\n Last Modified: 02/22/2025<br>\nModified by: Ian&nbsp;Davidson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nOverview: With the tremendous growth in the number of applications in which data mining and machine learning algorithms are used, it has become crucial to add explanations to the decisions made by these algorithms. This area of research is referred to as Explainable AI (XAI). When this project started, much of the work on XAI was on explaining outcomes of supervised learning methods where learning is achieved using training data (e.g., methods using neural networks). This project explored techniques for developing explanations for unsupervised learning tasks such as clustering and outlier detection, which had not received enough attention in the literature. Explanation methods for unsupervised learning are important since a large number of algorithms for clustering and outlier detection are used in practice. For example, in the context of clustering, it is important to explain why an algorithm included two data points in the same cluster or why it put them in different clusters. For example, if the clusters are constructed from a dataset involving people, organizations may use different actions (e.g., approve or deny a loan) on different clusters. In the context of outlier detection, it is important to explain why a data point was identified as an outlier; this is because the technique is used in applications where the identified outliers may be further investigated or policed.\r\n\n\nIntellectual Merit: This research developed rigorous formulations of problems associated with the generation of explanations for unsupervised learning tasks. These problems belong to the category of combinatorial optimization problems which can be studied using techniques from algorithms and complexity theory. The work explored several forms of explanation.\r\n\n\nOne form used auxiliary information (called tags) associated with each data point to develop explanations. This form exploited the connection with the well known minimum set cover problem in theoretical computer science to identify a subset of tags as the explanation for each cluster. The results of this work led to algorithmic and mathematical programming based techniques for constructing optimal or provably near-optimal explanations. This form of explanation has been used for clusters obtained from many different datasets.\r\n\n\nA second form of explanation used small subsets of data points themselves (called exemplars) to as explanations of clusters. The corresponding optimization problem combined the tasks of clustering and generating exemplars. Since each of these problems is known to be computationally intractable, approximation algorithms with provable performance guarantees were developed. This exemplar-based explanation was shown to be useful for datasets from domains which are difficult to understand by humans (e.g., data obtained by embedding images and text).\r\n\n\nA third form of explanation is useful in the context of distinguishing between outliers and normal data points. This form of explanation is called contrastive explanation and it also relies on the use of tags (auxiliary information). By focusing on the relationship between tags and data points, a rigorous formulation of the contrastive explanation problem was developed using the well known Knapsack problem from combinatorial optimization as the basis. This enabled the development of efficient algorithms for some versions of the problem and complexity results for other versions. The resulting algorithms were applied to multiple modalities of data (i.e., images, text and databases).\r\n\n\nOther topics related to explanation were also explored during the period of this grant. One such topic the stability of explanations, that is, whether the generated explanation changes significantly when minor changes are made to the auxiliary information (i.e., tags) or the clusters. Another topic was the use of another combinatorial problem (namely, the hitting set problem) to generate a more detailed explanation for each cluster. Results produced from the work have been published in well known conferences and journals in machine learning. Several additional papers are under preparation.\r\n\n\nBroader Impact: The grant supported recently graduated students Dr Michael Livanos and Dr Ge Shi and undergraduates Ms Avigail Shekar Gafni and Mr Nicolas Kennedy. All students supported received valuable training in several areas of computer science, including algorithm design, application and implementation, combinatorial optimization, mathematical programming and software design/testing. The PI (Davidson) used the basic research in this grant as a basis for the applied NIH grant R01 MH076989 (Pathophysiological Biomarkers of Treatment Response in Early Psychosis,) for which he is a co-PI.\t\t\t\t\tLast Modified: 02/22/2025\n\n\t\t\t\t\tSubmitted by: IanDavidson\n"
 }
}