{
 "awd_id": "1941178",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: World Modeling for Natural Language Understanding",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 226063.0,
 "awd_amount": 226063.0,
 "awd_min_amd_letter_date": "2019-09-04",
 "awd_max_amd_letter_date": "2019-09-04",
 "awd_abstract_narration": "A key goal of artificial intelligence (AI) is to build systems that can read and understand language as humans do. This capability underlies a broad range of technologies, including question answering, machine translation, and dialogue systems. While progress has been made, AI systems currently lack the robustness and flexibility of human language understanding---typical systems leverage shallow pattern-matching strategies to perform tasks, and as a result are only effective at the specific tasks they are built for, and fail easily even within those settings. This project addresses these issues by improving the ability of systems to construct rich representations of the \"world\" described in text: Who are the entities involved, and what are their attributes and relationships? What events are taking place, who is participating in those events, and why are they occurring? The design of the systems' notion of a world uses concepts like these that have been identified by cognitive scientists and psychologists as fundamental in human language understanding. The expected benefit of this work is the development of AI systems that can use language flexibly and robustly because, like humans, these systems will perform tasks based on the core information conveyed in language, rather than superficial pattern-matching. In addition to improving systems, this project will have the benefit of building bridges between the AI community and cognitive scientists, psychologists, and linguists---the project's modeling framework provides a pathway through which insights from cognitive science can be translated to model implementation, which can be utilized both for improvement of AI systems and for testing of cognitive hypotheses. \r\n\r\nThis exploratory EAGER project improves the capacity of systems to automatically construct the world underlying the text being analyzed, and designs targeted probing tasks to enable fine-grained assessment of the extent to which systems have captured this information. The modeling framework uses memory-augmented neural networks, leveraging the external memory components to represent worlds. Rather than explicit annotation, the project implements cognitively-inspired design of both world components themselves and inductive bias for encouraging particular components to capture what is intended. Learning is carried out via self-supervised objectives and auxiliary supervision on large datasets of narratives. System evaluation consists of both standard reading comprehension question answering tasks and the development of novel probing tasks. The use of controlled probing tasks draws critically from methodological approaches used in cognitive neuroscience and psycholinguistics, applying these scientific methods for interpretation of artificial systems. These probing tasks allow for targeted analysis of individual world components and provide guidance for model improvement. The methodology of the project iterates between model design and targeted testing via probing tasks, using the results of the latter to guide the former.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Gimpel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin Gimpel",
   "pi_email_addr": "kgimpel@ttic.edu",
   "nsf_id": "000629415",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "6045 S. Kenwood Ave.",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372803",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 226063.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-73ba2839-7fff-a7f2-55c5-f7980c39b9cd\"> </span></p>\n<p dir=\"ltr\"><span>A key goal of artificial intelligence (AI) is to build systems that can read and understand language as humans do. Achieving this goal would improve many applications, including question answering, machine translation, and dialogue systems. However, current AI systems lack the robustness and flexibility of human language understanding. The goal of this project is to address these limitations by modeling the \"world\" being described in the text being analyzed. That is, who are the people involved, and what are their attributes and relationships? What events are taking place, who is participating in those events, and why are they occurring?&nbsp;</span></p>\n<p dir=\"ltr\"><span>Our primary efforts in representing world information focused on recognizing mentions of entities in text, identifying when multiple entity mentions correspond to the same underlying entity, and tracking the actions or movement of entities described in text. We developed several models and methods that improve the ability of AI systems to accomplish these goals.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We developed a model called PeTra (\"People Tracking\"), a neural network with an external memory like that found in a computer. The external memory contains a number of memory slots, and PeTra is designed to use its memory slots to track entities while reading text. PeTra attains high accuracy while also offering greater interpretability than prior memory models, as evidenced by a qualitative human evaluation.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We modified PeTra to track entities in very long documents, such as entire books. To do so, we restricted the number of entities being tracked at a time to a fixed number, which keeps computation limited as document length increases. Our experiments demonstrated that (a) our model is highly accurate compared to slower models, and (b) our model learns an effective strategy to decide when to remember and forget entities based on characteristics of the text and the entities. We also developed ways to adapt off-the-shelf language models for long text sequences. Most models process 1,000 to 2,000 words at a time, but a typical novel has 10,000 to 50,000 words. We brought the key idea from one neural network architecture into another, enabling off-the-shelf models to process sequences of any length. These improvements enable our models to be run on novels, which feature rich worlds with complex inter-character relationships and many events related by causal and temporal links. We have made available our code and trained models for both research and commercial use.</span></p>\n<p dir=\"ltr\"><span>In another research thread, we developed a framework for studying the capabilities of language models for entity and state tracking. We considered the domain of chess. Here, entities are chess pieces and states are board positions. The observation sequence is a sequence of moves, and a chess simulator can provide board positions for all pieces after each move in the game. We trained transformer language models on move sequences and measured their ability to correctly predict a legal next move, finding that they could do so with high accuracy. We proposed this testbed as a benchmark for future work on the development and analysis of transformer language models, and used the testbed to explore the capabilities of both standard and modified transformers.</span></p>\n<p dir=\"ltr\"><span>The project's results offer ways to improve AI technologies for long documents. Such documents, including books, long-form journalism, long-form interview transcripts, and medical note histories, have been difficult for AI. It is rare to see deployed AI technology that can summarize, analyze, interpret, or help a reader better understand such documents. The results of this project can serve as components in larger AI systems that seek to understand long documents and provide insights to users.</span></p>\n<p dir=\"ltr\"><span>The results also have the potential to impact digital humanities. Since the project's models can be run with high accuracy on entire novels, researchers can use them to study questions pertaining to character trajectories and character relationships with automated analysis of many novels. In addition, the results of the project can bear on cognitive science. Since our models use a fixed-size memory, they are more cognitively plausible than prior work, which considered an unbounded number of entity mentions as candidates while performing resolution. Future work in cognitive science can compare these two frameworks to human behavior.</span></p>\n<p dir=\"ltr\"><span>The project provided opportunities for research and mentoring in engineering at the student and junior faculty levels. Two student researchers led the execution of the work described, facilitating their development in the process of conceptualizing and implementing this work. The code and data we have made available can be used for educational purposes. In particular, the code for our entity tracking model has been made available, along with a CoLab notebook which can be easily used in the classroom to demonstrate a state-of-the-art model on any input text.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/11/2022<br>\n\t\t\t\t\tModified by: Kevin&nbsp;Gimpel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nA key goal of artificial intelligence (AI) is to build systems that can read and understand language as humans do. Achieving this goal would improve many applications, including question answering, machine translation, and dialogue systems. However, current AI systems lack the robustness and flexibility of human language understanding. The goal of this project is to address these limitations by modeling the \"world\" being described in the text being analyzed. That is, who are the people involved, and what are their attributes and relationships? What events are taking place, who is participating in those events, and why are they occurring? \nOur primary efforts in representing world information focused on recognizing mentions of entities in text, identifying when multiple entity mentions correspond to the same underlying entity, and tracking the actions or movement of entities described in text. We developed several models and methods that improve the ability of AI systems to accomplish these goals. \nWe developed a model called PeTra (\"People Tracking\"), a neural network with an external memory like that found in a computer. The external memory contains a number of memory slots, and PeTra is designed to use its memory slots to track entities while reading text. PeTra attains high accuracy while also offering greater interpretability than prior memory models, as evidenced by a qualitative human evaluation. \nWe modified PeTra to track entities in very long documents, such as entire books. To do so, we restricted the number of entities being tracked at a time to a fixed number, which keeps computation limited as document length increases. Our experiments demonstrated that (a) our model is highly accurate compared to slower models, and (b) our model learns an effective strategy to decide when to remember and forget entities based on characteristics of the text and the entities. We also developed ways to adapt off-the-shelf language models for long text sequences. Most models process 1,000 to 2,000 words at a time, but a typical novel has 10,000 to 50,000 words. We brought the key idea from one neural network architecture into another, enabling off-the-shelf models to process sequences of any length. These improvements enable our models to be run on novels, which feature rich worlds with complex inter-character relationships and many events related by causal and temporal links. We have made available our code and trained models for both research and commercial use.\nIn another research thread, we developed a framework for studying the capabilities of language models for entity and state tracking. We considered the domain of chess. Here, entities are chess pieces and states are board positions. The observation sequence is a sequence of moves, and a chess simulator can provide board positions for all pieces after each move in the game. We trained transformer language models on move sequences and measured their ability to correctly predict a legal next move, finding that they could do so with high accuracy. We proposed this testbed as a benchmark for future work on the development and analysis of transformer language models, and used the testbed to explore the capabilities of both standard and modified transformers.\nThe project's results offer ways to improve AI technologies for long documents. Such documents, including books, long-form journalism, long-form interview transcripts, and medical note histories, have been difficult for AI. It is rare to see deployed AI technology that can summarize, analyze, interpret, or help a reader better understand such documents. The results of this project can serve as components in larger AI systems that seek to understand long documents and provide insights to users.\nThe results also have the potential to impact digital humanities. Since the project's models can be run with high accuracy on entire novels, researchers can use them to study questions pertaining to character trajectories and character relationships with automated analysis of many novels. In addition, the results of the project can bear on cognitive science. Since our models use a fixed-size memory, they are more cognitively plausible than prior work, which considered an unbounded number of entity mentions as candidates while performing resolution. Future work in cognitive science can compare these two frameworks to human behavior.\nThe project provided opportunities for research and mentoring in engineering at the student and junior faculty levels. Two student researchers led the execution of the work described, facilitating their development in the process of conceptualizing and implementing this work. The code and data we have made available can be used for educational purposes. In particular, the code for our entity tracking model has been made available, along with a CoLab notebook which can be easily used in the classroom to demonstrate a state-of-the-art model on any input text.\n\n\t\t\t\t\tLast Modified: 11/11/2022\n\n\t\t\t\t\tSubmitted by: Kevin Gimpel"
 }
}