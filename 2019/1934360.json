{
 "awd_id": "1934360",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Advancing Science with Accelerated Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927382",
 "po_email": "vlukin@nsf.gov",
 "po_sign_block_name": "Vyacheslav (Slava) Lukin",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 600001.0,
 "awd_amount": 600001.0,
 "awd_min_amd_letter_date": "2019-09-15",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "In the next generation of big science experiments, the demands for computing resources are expected to outstrip the capabilities of existing computing infrastructure. In light of this, a radical rethinking of the cyberinfrastructure is needed to contend with these developments. With the onset of deep learning, parallelized processing architectures have emerged as a solution. Combined with deep learning algorithms, parallelized processing architectures, in particular, Field Programmable Gate Arrays (FPGAs) have been shown to give large speedups in computing when compared with conventional CPUs. This project aims to bring machine learning based accelerated computing with FPGAs into the scientific community by targeting two big-data physics experiments: the Large Hadron Collider (LHC) and the Laser Interferometer Gravitational-wave  Observatory (LIGO). This project will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. The PIs and their collaborators will build upon their recent work to design and exploit state-of-the-art neural network models for real-time data analytics, reducing overall computing latency. This new computing paradigm aims to significantly increase the processing capability at the LHC and LIGO, leading to an increased scientific output of these devices and,  potentially, foundational discoveries. The students to be mentored and trained in this research will interact closely with industry partners, creating new career opportunities, and strengthening synergies between academia and industry. In addition to sharing algorithms with the community through open source repositories, the team will continue to educate the community regarding credit and citation of scientific software.\r\n\r\nIn this project, the PIs will build upon their recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets using Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms. The team will develop machine learning based acceleration tools focusing on FPGAs to be used within LIGO and the LHC experiments. The team's immediate goal is to take benchmark examples of LHC high level trigger processing and LIGO gravitational wave processing and construct demonstrators in each scenario. For this benchmark, they aim to design and implement an FPGA based accelerator that can perform low latency gravitational wave identification and LHC event reconstruction.  Additionally, the PIs aim to add the capability of graph based neural network accelerators for FPGAs. The open source tools to be developed as part of these activities will be readily shared with LIGO, LHC, and LSST. The project will create an advisory group, including members of large and small projects,  members of the neutrino physics, multi-messenger astronomy community, industry partners, computer scientists, and computational biologists. This project aims to bring together representatives of the different communities that will benefit from and can contribute to this work. The PIs will organize deep learning workshops and boot camps to train students and researchers on how to use and contribute to the framework, creating a wide network of contributors and developers across key science missions.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.\r\n\r\nThis project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shih-Chieh",
   "pi_last_name": "Hsu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shih-Chieh Hsu",
   "pi_email_addr": "schsu@uw.edu",
   "nsf_id": "000624379",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Hauck",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Scott A Hauck",
   "pi_email_addr": "hauck@uw.edu",
   "nsf_id": "000189513",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981051016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "099y00",
   "pgm_ele_name": "HDR-Harnessing the Data Revolu"
  },
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300707.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 299294.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9d9ed849-7fff-5daa-cdfd-2e5b33366cf0\">\n<p dir=\"ltr\"><span>Real-time algorithms are critical in the Large Hadron Collider (LHC) experiments at CERN and gravitational wave experiments, including the Laser Interferometer Gravitational-Wave Observatory (LIGO). The challenge for gravitational waves is to quickly identify events and alert the rest of the community in a process known as multi-messenger astronomy. </span><span>Multi-messenger events have proven to be some of the most elucidating astrophysical events ever observed</span><span>. At the LHC, the data rates are so enormous that we can only save one collision in 40,000 to reconstruct further. As a result, we rely on a real-time system that can only perform a cursory analysis to determine whether the event is interesting. This analysis leaves critical events behind, and we know that we can improve our knowledge of particle interactions if we were to do a better job. Our goal in this project is to strengthen these pipelines with better and faster algorithms utilizing deep learning. Faster algorithms lead to lower latency gravitational wave alerts and more LHC collisions analyzed.</span></p>\n<p dir=\"ltr\"><span>Deep learning algorithms have taken on rapid adoption across many scientific domains due to their incredible ability to provide algorithmic solutions to very complicated tasks. A significant reason for their effectiveness originates from the rise of heterogeneous computation, which takes advantage of the parallelism of certain types of processors, particularly GPUs and FPGAs. In this project, we developed an end-to-end infrastructure to enable a deep learning computational pipeline for gravitational wave data and high energy physics within their real-time systems.&nbsp; Our pipeline can take deep learning algorithms and optimally deploy them into the data acquisition of these large data experiments with minimal effort. Moreover, with deep learning, we created new algorithms for these pipelines that will significantly increase their future abilities allowing for the potential of more scientific discovery.&nbsp;</span></p>\n<br /><br />\n<p dir=\"ltr\"><span>With the LHC, we have focused on the deployment of algorithms at three levels of the computational pipeline. The first tier of the pipeline utilizes Field Programmable Gate Arrays (FPGA) and requires algorithms that run in less than a microsecond. Here, we have developed the HLS4ML toolkit, which enables the rapid deployment of deep learning algorithms on FPGA processors. With HLS4ML, we have created deep learning algorithms to perform tau lepton and b-quark identification to be used in the future running of the CMS detector, and we have shown that deploying these algorithms will lead to substantial physics performance improvements, including enhanced sensitivity to the Higgs self-coupling. Furthermore, through HLS4ML and our examples, the community is quickly adopting our toolkit and strategy, and another roughly 20 algorithms are under development for the LHC low latency real-time system.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>In the latter two tiers of the LHC, we instead integrated inference-as-a-service technology into the CPU-based reconstruction workflow of the CMS detector. Our setup can quickly deploy ML and non-ML algorithms on FPGAs or GPUs, enabling optimized computational load balancing to ensure all resources are fully efficient. Our FPGA-as-a-service toolkit is unique, and we have shown it can be particularly effective with specific algorithms. With our setup, we demonstrated how integrating this technology led to significant speed-ups by transferring existing deep learning algorithms to our system. Additionally,&nbsp; we created a new deep learning algorithm that can go from 1000s of raw hits to physics objects in one pass. This work is heralding a new era of LHC computation where single-particle algorithms are replaced by event-level multi-particle deep learning algorithms that work at low latency through heterogeneous computing. </span><span>&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>Our work is leading the way toward the integration of real-time AI&nbsp; for scientific experiments, e.g. Gravitational waves. The interplay between LHC and gravitational waves has been critical. Communication between the domains and computer scientists has enabled the optimal use of heterogeneous computing. We have shared our experiences and algorithmic approaches, including developing a deep-learning anomaly pipeline in both experiments.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>Through this effort, we have developed new concepts in AI deployment on processors and created the infrastructure needed for fast adoption across scientific domains. </span><span>This grant has trained five undergraduate students, five master students, three Ph.D students, supported a postdoc, organized 3 international workshops, developed online tutorials, and helped cultivate a community to deploy low latency, high-throughput, fast machine learning.&nbsp;</span></p>\n</span></p><br>\n<p>\n Last Modified: 01/13/2024<br>\nModified by: Shih-Chieh&nbsp;Hsu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nReal-time algorithms are critical in the Large Hadron Collider (LHC) experiments at CERN and gravitational wave experiments, including the Laser Interferometer Gravitational-Wave Observatory (LIGO). The challenge for gravitational waves is to quickly identify events and alert the rest of the community in a process known as multi-messenger astronomy. Multi-messenger events have proven to be some of the most elucidating astrophysical events ever observed. At the LHC, the data rates are so enormous that we can only save one collision in 40,000 to reconstruct further. As a result, we rely on a real-time system that can only perform a cursory analysis to determine whether the event is interesting. This analysis leaves critical events behind, and we know that we can improve our knowledge of particle interactions if we were to do a better job. Our goal in this project is to strengthen these pipelines with better and faster algorithms utilizing deep learning. Faster algorithms lead to lower latency gravitational wave alerts and more LHC collisions analyzed.\n\n\nDeep learning algorithms have taken on rapid adoption across many scientific domains due to their incredible ability to provide algorithmic solutions to very complicated tasks. A significant reason for their effectiveness originates from the rise of heterogeneous computation, which takes advantage of the parallelism of certain types of processors, particularly GPUs and FPGAs. In this project, we developed an end-to-end infrastructure to enable a deep learning computational pipeline for gravitational wave data and high energy physics within their real-time systems. Our pipeline can take deep learning algorithms and optimally deploy them into the data acquisition of these large data experiments with minimal effort. Moreover, with deep learning, we created new algorithms for these pipelines that will significantly increase their future abilities allowing for the potential of more scientific discovery.\n\n\n\n\n\nWith the LHC, we have focused on the deployment of algorithms at three levels of the computational pipeline. The first tier of the pipeline utilizes Field Programmable Gate Arrays (FPGA) and requires algorithms that run in less than a microsecond. Here, we have developed the HLS4ML toolkit, which enables the rapid deployment of deep learning algorithms on FPGA processors. With HLS4ML, we have created deep learning algorithms to perform tau lepton and b-quark identification to be used in the future running of the CMS detector, and we have shown that deploying these algorithms will lead to substantial physics performance improvements, including enhanced sensitivity to the Higgs self-coupling. Furthermore, through HLS4ML and our examples, the community is quickly adopting our toolkit and strategy, and another roughly 20 algorithms are under development for the LHC low latency real-time system.\n\n\n\n\nIn the latter two tiers of the LHC, we instead integrated inference-as-a-service technology into the CPU-based reconstruction workflow of the CMS detector. Our setup can quickly deploy ML and non-ML algorithms on FPGAs or GPUs, enabling optimized computational load balancing to ensure all resources are fully efficient. Our FPGA-as-a-service toolkit is unique, and we have shown it can be particularly effective with specific algorithms. With our setup, we demonstrated how integrating this technology led to significant speed-ups by transferring existing deep learning algorithms to our system. Additionally, we created a new deep learning algorithm that can go from 1000s of raw hits to physics objects in one pass. This work is heralding a new era of LHC computation where single-particle algorithms are replaced by event-level multi-particle deep learning algorithms that work at low latency through heterogeneous computing. \n\n\n\n\nOur work is leading the way toward the integration of real-time AI for scientific experiments, e.g. Gravitational waves. The interplay between LHC and gravitational waves has been critical. Communication between the domains and computer scientists has enabled the optimal use of heterogeneous computing. We have shared our experiences and algorithmic approaches, including developing a deep-learning anomaly pipeline in both experiments.\n\n\n\n\nThrough this effort, we have developed new concepts in AI deployment on processors and created the infrastructure needed for fast adoption across scientific domains. This grant has trained five undergraduate students, five master students, three Ph.D students, supported a postdoc, organized 3 international workshops, developed online tutorials, and helped cultivate a community to deploy low latency, high-throughput, fast machine learning.\n\t\t\t\t\tLast Modified: 01/13/2024\n\n\t\t\t\t\tSubmitted by: Shih-ChiehHsu\n"
 }
}