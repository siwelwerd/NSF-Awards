{
 "awd_id": "1928481",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FW-HTF-RM: Measuring learning gains in man-machine assemblage when augmenting radiology work with artificial intelligence",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032927557",
 "po_email": "amedinab@nsf.gov",
 "po_sign_block_name": "Alexandra Medina-Borja",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 826795.0,
 "awd_amount": 826795.0,
 "awd_min_amd_letter_date": "2019-08-01",
 "awd_max_amd_letter_date": "2019-08-01",
 "awd_abstract_narration": "The work setting of the future presents an opportunity for human-technology partnerships, where a harmonious connection between human-technology produces unprecedented productivity gains. A conundrum at this human-technology frontier remains - will humans be augmented by technology or will technology be augmented by humans? This project overcomes the conundrum of human and machine as separate entities and instead, treats them as an assemblage. As groundwork for the harmonious human-technology connection, this assemblage needs to learn to fit synergistically. This learning is called assemblage learning and it will be important for Artificial Intelligence (AI) applications in health care, where diagnostic and treatment decisions augmented by AI will have a direct and significant impact on patient care and outcomes. This project will also identify ways in which learning can be shared between assemblages, such that collective swarms of connected assemblages can be created. The project will create a new learning model that integrates and measures concepts from individuals learning to swarm learn. The project will help demonstrate a symbiotic learning assemblage, such that envisioned productivity gains from AI can be achieved without loss of human jobs. Even though the focus is on visual cognitive tasks in radiology, lessons from this project may be applicable to other domains where human intelligence will be augmented by machine intelligence.\r\n\r\nRecent studies of human versus machine competitions have demonstrated that assemblages that combine human-technology partnerships are stronger than individual humans or machines. By building on these, this project will integrate state-of-the-art algorithms into the radiology workflow. The project will answer the following research questions: Q1: How to develop assemblages, such that human-technology partnerships produce a \"good fit\" for visually based cognition-oriented tasks in radiology? Q2: What level of training should pre-exist in the individual human (radiologist) and independent machine learning model for human-technology partnerships to thrive? Q3: Which aspects and to what extent does an assemblage learning approach lead to reduced errors, improved accuracy, faster turn-around times, reduced fatigue, improved self-efficacy, and resilience? A rigorous counterbalanced trial will be performed to assess individual radiologists interpreting images with and without the assemblage. Data on clinician engagement from EHR systems will be captured and analyzed, along with pre-test and post-test surveys and interviews. Deep and wide analysis of the quantitative and qualitative data from the trial will answer questions related to learning gains, task performance, emotional as well as behavioral aspects of learning in an assemblage. The project employs perspectives from Science & Technology Studies, Computer Science, Psychology, and Learning Sciences, to create and study assemblages that can produce gains in routine radiology work.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Saptarshi",
   "pi_last_name": "Purkayastha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Saptarshi Purkayastha",
   "pi_email_addr": "saptpurk@iu.edu",
   "nsf_id": "000719307",
   "pi_start_date": "2019-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Joshua",
   "pi_last_name": "Danish",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Joshua A Danish",
   "pi_email_addr": "jdanish@indiana.edu",
   "nsf_id": "000528547",
   "pi_start_date": "2019-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Elizabeth",
   "pi_last_name": "Krupinski",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Elizabeth A Krupinski",
   "pi_email_addr": "ekrupin@emory.edu",
   "nsf_id": "000799304",
   "pi_start_date": "2019-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Judy",
   "pi_last_name": "Gichoya",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Judy W Gichoya",
   "pi_email_addr": "Judywawira@gmail.com",
   "nsf_id": "000799371",
   "pi_start_date": "2019-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "545 w. Michigan st",
  "perf_city_name": "indianapolis",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "462022915",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IN07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "103Y00",
   "pgm_ele_name": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 826795.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This NSF-funded project investigated how radiologists and artificial intelligence (AI) systems can work together in a symbiotic relationship to improve diagnosis of medical images. The researchers integrated multiple state-of-the-art AI image analysis models into an open-source radiology workflow system called LibreHealth Radiology Information System (RIS). This allows radiologists to get real-time insights from the AI models as they are interpreting medical images.</p>\n<p>The researchers then conducted rigorous studies where radiologists read sets of 60-120 medical images both with and without the AI assistance at Emory University and Indiana University. Quantitative performance metrics and qualitative feedback through surveys and interviews were collected. The studies found that the radiologists performed better on diagnostic accuracy metrics with the AI assistant. The radiologists also subjectively reported increased confidence, reduced fatigue, and greater efficiency when working alongside the AI.</p>\n<p>Through corrections and feedback the radiologists provided, the AI models were also able to improve their performance. This mutually beneficial relationship between humans and AI was termed \"assemblage learning\" by the researchers. It demonstrates how AI does not have to replace human roles, but can augment human capabilities and cognition. The radiologist-AI assemblages outperformed either alone. This symbiotic paradigm has significant impacts for human-AI collaboration in many fields beyond radiology.</p>\n<p>To facilitate further research, the project developed and released several open-source software tools. LibreHealth RIS allows integration of custom AI services into clinical workflows. The AI Model Service provides a standardized API for connecting different AI algorithms. These tools lower barriers for exploring human-AI interaction in real clinical environments.</p>\n<p>The researchers also published multiple papers investigating bias in AI systems. They found that while high-performing models can encode racial, gender, and other biases, human-in-the-loop oversight during model development and application can combat this. Human interaction can guide AI systems to become fairer and more trustworthy.</p>\n<p>Related works have looked at human-AI collaboration in domains like manufacturing, finance, and self-driving vehicles. However, this project uniquely combined real clinical systems, practicing radiologists, and rigorous mixed-methods studies. The findings contribute conceptual knowledge on designing AI agents that productively collaborate with human experts, not just replace them. This \"AI augmentation\" paradigm maintains human oversight, transparency, and trust.</p>\n<p>With over 12 papers published, the project made critical contributions at the intersection of human-computer interaction, health informatics, and machine learning. The open-source platforms developed will enable future research to continue investigating safe and effective integration of AI in clinical practice. Overall, the project advanced knowledge and tools to design AI as an ally rather than adversary to human professionals.</p><br>\n<p>\n Last Modified: 01/29/2024<br>\nModified by: Saptarshi&nbsp;Purkayastha</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis NSF-funded project investigated how radiologists and artificial intelligence (AI) systems can work together in a symbiotic relationship to improve diagnosis of medical images. The researchers integrated multiple state-of-the-art AI image analysis models into an open-source radiology workflow system called LibreHealth Radiology Information System (RIS). This allows radiologists to get real-time insights from the AI models as they are interpreting medical images.\n\n\nThe researchers then conducted rigorous studies where radiologists read sets of 60-120 medical images both with and without the AI assistance at Emory University and Indiana University. Quantitative performance metrics and qualitative feedback through surveys and interviews were collected. The studies found that the radiologists performed better on diagnostic accuracy metrics with the AI assistant. The radiologists also subjectively reported increased confidence, reduced fatigue, and greater efficiency when working alongside the AI.\n\n\nThrough corrections and feedback the radiologists provided, the AI models were also able to improve their performance. This mutually beneficial relationship between humans and AI was termed \"assemblage learning\" by the researchers. It demonstrates how AI does not have to replace human roles, but can augment human capabilities and cognition. The radiologist-AI assemblages outperformed either alone. This symbiotic paradigm has significant impacts for human-AI collaboration in many fields beyond radiology.\n\n\nTo facilitate further research, the project developed and released several open-source software tools. LibreHealth RIS allows integration of custom AI services into clinical workflows. The AI Model Service provides a standardized API for connecting different AI algorithms. These tools lower barriers for exploring human-AI interaction in real clinical environments.\n\n\nThe researchers also published multiple papers investigating bias in AI systems. They found that while high-performing models can encode racial, gender, and other biases, human-in-the-loop oversight during model development and application can combat this. Human interaction can guide AI systems to become fairer and more trustworthy.\n\n\nRelated works have looked at human-AI collaboration in domains like manufacturing, finance, and self-driving vehicles. However, this project uniquely combined real clinical systems, practicing radiologists, and rigorous mixed-methods studies. The findings contribute conceptual knowledge on designing AI agents that productively collaborate with human experts, not just replace them. This \"AI augmentation\" paradigm maintains human oversight, transparency, and trust.\n\n\nWith over 12 papers published, the project made critical contributions at the intersection of human-computer interaction, health informatics, and machine learning. The open-source platforms developed will enable future research to continue investigating safe and effective integration of AI in clinical practice. Overall, the project advanced knowledge and tools to design AI as an ally rather than adversary to human professionals.\t\t\t\t\tLast Modified: 01/29/2024\n\n\t\t\t\t\tSubmitted by: SaptarshiPurkayastha\n"
 }
}