{
 "awd_id": "1926925",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: AI-DCL: Collaborative Research: Understanding and Overcoming Biases in STEM Education using Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 251730.0,
 "awd_amount": 251730.0,
 "awd_min_amd_letter_date": "2019-08-19",
 "awd_max_amd_letter_date": "2019-08-19",
 "awd_abstract_narration": "Diversity is the cornerstone of innovation and essential for the progress of science. However, the number of female students in engineering, computing, and physical sciences in the United States remains strikingly low. The lack of diversity in science, technology, engineering, and mathematics (STEM) education is, to a significant extent, due to biases at different stages of schooling (e.g., different perceptions of math achievements by male and female students, lack of encouragement for female student enrollment in advance placement classes, stereotypes influencing college course selection). These biases appear as early as middle school: a critical period when student's educational experience can significantly influence their academic choices in high school and, ultimately, in deciding whether or not to enroll in STEM majors in college. In order to broaden the participation of women in STEM, it is critical to identify factors and practices in middle school learning environments that may attract (or repel) students into science. This award will use machine learning (ML) to develop new, automated, and data-driven methods for discovering and monitoring biases in STEM classrooms, focusing on middle school and early adolescence science and mathematics education.\r\n\r\nThe project combines methods from social psychology, machine learning, and information theory to create algorithmic tools that monitor middle school student, teacher, and school-level data for factors that impact students' engagement in STEM. These tools will (i) help identify pedagogical or socio-economic factors that have a disparate impact on the decisions made by female students, (ii) predict which students are most vulnerable to being discouraged from pursuing STEM fields, and (iii) inform effective interventions that help close the gender gap. Despite its potential, the use of ML in education is a double-edged sword: while ML algorithms may be able to flag discriminatory patterns, they can also propagate biases and have an unwarranted disparate impact if left unchecked.  Thus, in parallel, this project also aims to characterize the fairness challenges involved in deploying ML in education settings. The proposed approach will be validated on a dataset collected during a five year period from middle school students from across the United States.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Flavio",
   "pi_last_name": "Calmon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Flavio Calmon",
   "pi_email_addr": "flavio@seas.harvard.edu",
   "nsf_id": "000733796",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University SEAS",
  "perf_str_addr": "33 Oxford Street, 147",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "40XX",
   "app_name": "NSF TRUST FUND",
   "app_symb_id": "048960",
   "fund_code": "4007XXXXDB",
   "fund_name": "NSF TRUST FUND",
   "fund_symb_id": "048960"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 251730.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Algorithms are rapidly being adopted to aid pedagogical decision-making in applications ranging from grading to student placement. This project examined the potential bias and discrimination risks in using machine learning algorithms to support decisions that impact students and their educational opportunities. The project involved an interdisciplinary collaboration between researchers with expertise in fair machine learning, information theory, and social psychology. The project's outcomes include tools and guidelines that help ensure that the use of machine learning algorithms in education is fair and unbiased.</p>\n<p><br />The project investigated how machine learning models trained on student achievement data can perpetuate and -- at worst -- amplify racial discrimination and bias patterns that exist in the American education system. The study analyzed two datasets from middle and high school students in the United States. It found that standard machine learning models used to predict math performance are consistently more pessimistic in predicting academic achievements for Black, Hispanic, and Native American students. High prediction accuracy numbers -- the standard figure-of-merit used to evaluate machine learning performance -- can hide these disparities. The research also examined how the demographic composition of datasets used to train machine learning models impact their performance. A key finding is that changing the demographic composition of training data can produce a trade-off between false-positive and false-negative predictions between student groups.</p>\n<p><br />An important project outcome was a new method for mitigating discriminatory bias in machine learning models used in multi-class classification tasks. This method, called FairProjection, consists of a parallelizable algorithm for reducing bias in classification and has provable sample complexity and convergence guarantees. Comparisons with state-of-the-art fairness interventions in machine learning demonstrated that FairProjection maintains competitive performance across several accuracy and fairness metrics and has favorable runtime on large datasets from the education domain. The FairProjection algorithm is publicly available through open-source code.</p>\n<p><br />The project also studied the discrimination risks of training machine learning models using data with missing values. Missing entries are common in data collected from students, parents, and teachers in education-related applications (e.g., questionnaires with absent responses). In the machine learning pipeline, missing values are often imputed (i.e., filled in) before the model is trained. However, patterns of missing values in a dataset can depend on group attributes like sex and race. The research examined the discrimination risks in machine learning models that are trained on imputed datasets. As a result of this study, a new algorithm was developed for creating decision tree-based models that can handle missing values without requiring data imputation. This algorithm trains a decision tree with missing values incorporated as attributes while optimizing a fairness-regularized objective function. Benchmarks on education datasets demonstrated that an integrated approach of simultaneous imputation and classification is more effective than applying existing fairness intervention methods to imputed datasets.</p>\n<p><br />The research established that racial inequities emerge when machine learning algorithms are used to predict students' future educational achievements. By bringing these issues to light, the project's findings can contribute to the goal of preventing machine learning algorithms from perpetuating biases that discourage students from disadvantaged backgrounds from pursuing careers in fields such as science, technology, engineering, and mathematics. The project also offered hands-on training opportunities for undergraduate, graduate, and post-doctoral researchers in machine learning, artificial intelligence for education, and information theory.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2022<br>\n\t\t\t\t\tModified by: Flavio&nbsp;Calmon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAlgorithms are rapidly being adopted to aid pedagogical decision-making in applications ranging from grading to student placement. This project examined the potential bias and discrimination risks in using machine learning algorithms to support decisions that impact students and their educational opportunities. The project involved an interdisciplinary collaboration between researchers with expertise in fair machine learning, information theory, and social psychology. The project's outcomes include tools and guidelines that help ensure that the use of machine learning algorithms in education is fair and unbiased.\n\n\nThe project investigated how machine learning models trained on student achievement data can perpetuate and -- at worst -- amplify racial discrimination and bias patterns that exist in the American education system. The study analyzed two datasets from middle and high school students in the United States. It found that standard machine learning models used to predict math performance are consistently more pessimistic in predicting academic achievements for Black, Hispanic, and Native American students. High prediction accuracy numbers -- the standard figure-of-merit used to evaluate machine learning performance -- can hide these disparities. The research also examined how the demographic composition of datasets used to train machine learning models impact their performance. A key finding is that changing the demographic composition of training data can produce a trade-off between false-positive and false-negative predictions between student groups.\n\n\nAn important project outcome was a new method for mitigating discriminatory bias in machine learning models used in multi-class classification tasks. This method, called FairProjection, consists of a parallelizable algorithm for reducing bias in classification and has provable sample complexity and convergence guarantees. Comparisons with state-of-the-art fairness interventions in machine learning demonstrated that FairProjection maintains competitive performance across several accuracy and fairness metrics and has favorable runtime on large datasets from the education domain. The FairProjection algorithm is publicly available through open-source code.\n\n\nThe project also studied the discrimination risks of training machine learning models using data with missing values. Missing entries are common in data collected from students, parents, and teachers in education-related applications (e.g., questionnaires with absent responses). In the machine learning pipeline, missing values are often imputed (i.e., filled in) before the model is trained. However, patterns of missing values in a dataset can depend on group attributes like sex and race. The research examined the discrimination risks in machine learning models that are trained on imputed datasets. As a result of this study, a new algorithm was developed for creating decision tree-based models that can handle missing values without requiring data imputation. This algorithm trains a decision tree with missing values incorporated as attributes while optimizing a fairness-regularized objective function. Benchmarks on education datasets demonstrated that an integrated approach of simultaneous imputation and classification is more effective than applying existing fairness intervention methods to imputed datasets.\n\n\nThe research established that racial inequities emerge when machine learning algorithms are used to predict students' future educational achievements. By bringing these issues to light, the project's findings can contribute to the goal of preventing machine learning algorithms from perpetuating biases that discourage students from disadvantaged backgrounds from pursuing careers in fields such as science, technology, engineering, and mathematics. The project also offered hands-on training opportunities for undergraduate, graduate, and post-doctoral researchers in machine learning, artificial intelligence for education, and information theory.\n\n\t\t\t\t\tLast Modified: 12/30/2022\n\n\t\t\t\t\tSubmitted by: Flavio Calmon"
 }
}