{
 "awd_id": "1918252",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Quantifying systematicity, iconicity, and arbitrariness in the American Sign Language Lexicon",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927920",
 "po_email": "jvaldesk@nsf.gov",
 "po_sign_block_name": "Jorge Valdes Kroff",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 317838.0,
 "awd_amount": 317838.0,
 "awd_min_amd_letter_date": "2019-08-07",
 "awd_max_amd_letter_date": "2019-08-07",
 "awd_abstract_narration": "This collaborative project will study how signs convey meaning in American Sign Language (ASL) by analyzing the semantic organization of the ASL lexicon. Understanding the structure and organization of the human lexicon is critical to both linguistic and psycholinguistic theories of language. However, current theories are predominantly built upon evidence from spoken languages, and may underrepresent characteristics that are particularly common to sign languages. For example, a core assumption regarding the organization of the lexicon is that there is a sharp separation between semantic structure and phonological (form) structure--the way words are pronounced is generally thought to be unrelated to what they mean. However, mounting evidence suggests that iconicity, words or signs that resemble their meaning, is pervasive in both signed and spoken languages. Examples of iconicity in English are words like \"ping\" and \"sizzle\" that sound like what they mean; examples of iconicity in ASL are signs like DRINK and HAMMER which look like what they mean. While semantic and phonological structure might not be fully independent from each other in ASL, we know relatively little about how they relate to one another and whether or how iconicity may shape the lexicon. \r\n \r\nThis project represents the first comprehensive quantitative analysis of the semantic organization of the ASL lexicon. The project will collect valuable information about the semantic similarity of ASL signs and the size of semantic neighborhoods, which will be key to uncovering how knowledge about sign meaning is stored and organized, as well as how this structure is acquired. Specifically, this project aims to 1) conduct a lexicon-wide evaluation of the semantic associations between signs, 2) characterize iconic and non-iconic systematic relationships between form and meaning using visualization techniques inspired by network science, and 3) implement a novel approach to quantify iconicity in a subset of the lexicon in an effort to understand which semantic features participate in iconic mappings and how iconicity might shape semantic processing. The data collected under this project will be integrated into a large interactive lexical database of the semantic, phonological, and iconic structure that is publicly available (ASL-LEX: http://asl-lex.org/). These materials constitute essential tools that will allow scientists and educators to create well-controlled stimuli for use in research and the classroom. Finally, it is important to recognize that deaf people often have difficulty pursuing research careers because of communication roadblocks that hamper interaction with hearing scientists. The researchers on this project have \"deaf-friendly labs\" (e.g., project staff are fluent in ASL) and provide training that facilitates the entrance of deaf students into scientific and academic fields. Thus, a parallel aim of the project is to increase the representation of deaf people in science by including deaf researchers on the project and by providing an accessible environment for deaf students to gain training and research experience.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Naomi",
   "pi_last_name": "Caselli",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Naomi Caselli",
   "pi_email_addr": "nkc@bu.edu",
   "nsf_id": "000715234",
   "pi_start_date": "2019-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 317838.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"text-align: center;\"><strong>Intellectual Merit</strong></p>\n<p>Most of what we know about how language is structured, used, and learned is based on studies of spoken language. That means our understanding of language may overlook traits that are unique to sign languages. A central idea in linguistic theory is that words are fundamentally arbitrary&mdash;there is no connection between what a word means and how it is pronounced. The research conducted during this project shows this central idea is false. Instead, we find widespread, systematic alignment between how signs are articulated and what they mean, and this alignment is often there for a reason (e.g., many signs for foods are produced near the mouth). We documented this pattern by identifying how signs are related to one another with respect to meaning (e.g., CAT and DOG) and comparing this to information about how signs are related to one another with respect to form (e.g., ONION and APPLE are rhyming signs in ASL). We also took a detailed look at the semantic features of signs to see whether these were similar to spoken languages and to see whether semantic features were iconically motivated. This project provided the first comprehensive quantitative analysis of the semantic organization of the ASL lexicon, which is key to uncovering how knowledge about sign meaning is stored and organized, as well as how this structure is acquired. Overall, the project informs theories of lexical representation and processing by discovering how language modality does and does not impact patterns of lexical structure.&nbsp; &nbsp;</p>\n<p style=\"text-align: center;\"><strong>Broader Impact</strong></p>\n<p>In addition to answering the research questions outlined above, this project resulted in a series of tools for other researchers.</p>\n<p><strong>The SEM-LEX Benchmark.</strong> Almost all communication technology is designed around people who use a spoken or written language, and excludes people who primarily use a sign language. Sign language technology lags in part because of a lack of high quality datasets. As part of this study, we collected the largest dataset of ASL signs to date, and published the data along with a model of sign recognition as the SEM-LEX benchmark. These data are much higher in quality than the currently available datasets, and just with data quality alone led to state-of-the-art accuracy rates in sign recognition.&nbsp;</p>\n<p><strong>SignLab.</strong> We built a software system called SignLab, which enabled us to build this largescale sign language dataset. SignLab allows researchers to rapidly collect, segment, and label sign language video data. We have launched a beta release, and are working toward a full release this year.</p>\n<p><strong>Sign-LEX.</strong> Teams working on sign languages around the world including are building replications of the ASL-LEX lexical database for their respective sign languages. We have made Sign-LEX, the interactive visualization that we used to share ASL-LEX, publicly available and are supporting these other teams in using it to share their dataset. The Israeli team published ISL-LEX last year, and teams in Kenya, Germany, Japan, China, Nicaragua, and Spain are actively working on replications now.</p>\n<p><strong>Semantic Associate Database. </strong>The semantic structure of a language can be estimated from word associations. To create a semantic associate database, deaf ASL signers saw each of the 2,723 signs in the ASL-LEX database and were filmed producing the first three signs that came to mind. Up to 15 participants responded to each cue sign, yielding up to 45 associations per sign. The complete ASL dataset includes 113,883 semantic associations.</p>\n<p><strong>Training Opportunities. </strong>Finally, this project has provided substantial training opportunities for dozens of emerging scientists, about half of whom are deaf. Deafness has a substantial impact on the ability of students to gain access to research careers because of communication roadblocks that hamper interaction with hearing scientists. During this NSF project, the researchers provided a unique signing environment, training by both deaf and hearing researchers, and mentoring for deaf students who gained research skills and experience that have helped them succeed in entering a Ph.D. program, to thrive in a Ph.D. program, or to gain employment in a STEM field.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/28/2023<br>\nModified by: Naomi&nbsp;Caselli</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIntellectual Merit\n\n\nMost of what we know about how language is structured, used, and learned is based on studies of spoken language. That means our understanding of language may overlook traits that are unique to sign languages. A central idea in linguistic theory is that words are fundamentally arbitrarythere is no connection between what a word means and how it is pronounced. The research conducted during this project shows this central idea is false. Instead, we find widespread, systematic alignment between how signs are articulated and what they mean, and this alignment is often there for a reason (e.g., many signs for foods are produced near the mouth). We documented this pattern by identifying how signs are related to one another with respect to meaning (e.g., CAT and DOG) and comparing this to information about how signs are related to one another with respect to form (e.g., ONION and APPLE are rhyming signs in ASL). We also took a detailed look at the semantic features of signs to see whether these were similar to spoken languages and to see whether semantic features were iconically motivated. This project provided the first comprehensive quantitative analysis of the semantic organization of the ASL lexicon, which is key to uncovering how knowledge about sign meaning is stored and organized, as well as how this structure is acquired. Overall, the project informs theories of lexical representation and processing by discovering how language modality does and does not impact patterns of lexical structure. \n\n\nBroader Impact\n\n\nIn addition to answering the research questions outlined above, this project resulted in a series of tools for other researchers.\n\n\nThe SEM-LEX Benchmark. Almost all communication technology is designed around people who use a spoken or written language, and excludes people who primarily use a sign language. Sign language technology lags in part because of a lack of high quality datasets. As part of this study, we collected the largest dataset of ASL signs to date, and published the data along with a model of sign recognition as the SEM-LEX benchmark. These data are much higher in quality than the currently available datasets, and just with data quality alone led to state-of-the-art accuracy rates in sign recognition.\n\n\nSignLab. We built a software system called SignLab, which enabled us to build this largescale sign language dataset. SignLab allows researchers to rapidly collect, segment, and label sign language video data. We have launched a beta release, and are working toward a full release this year.\n\n\nSign-LEX. Teams working on sign languages around the world including are building replications of the ASL-LEX lexical database for their respective sign languages. We have made Sign-LEX, the interactive visualization that we used to share ASL-LEX, publicly available and are supporting these other teams in using it to share their dataset. The Israeli team published ISL-LEX last year, and teams in Kenya, Germany, Japan, China, Nicaragua, and Spain are actively working on replications now.\n\n\nSemantic Associate Database. The semantic structure of a language can be estimated from word associations. To create a semantic associate database, deaf ASL signers saw each of the 2,723 signs in the ASL-LEX database and were filmed producing the first three signs that came to mind. Up to 15 participants responded to each cue sign, yielding up to 45 associations per sign. The complete ASL dataset includes 113,883 semantic associations.\n\n\nTraining Opportunities. Finally, this project has provided substantial training opportunities for dozens of emerging scientists, about half of whom are deaf. Deafness has a substantial impact on the ability of students to gain access to research careers because of communication roadblocks that hamper interaction with hearing scientists. During this NSF project, the researchers provided a unique signing environment, training by both deaf and hearing researchers, and mentoring for deaf students who gained research skills and experience that have helped them succeed in entering a Ph.D. program, to thrive in a Ph.D. program, or to gain employment in a STEM field.\n\n\n\t\t\t\t\tLast Modified: 12/28/2023\n\n\t\t\t\t\tSubmitted by: NaomiCaselli\n"
 }
}