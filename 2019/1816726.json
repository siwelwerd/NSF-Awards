{
 "awd_id": "1816726",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Compounding Dividends on Voice Banking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-03-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 104052.0,
 "awd_amount": 104052.0,
 "awd_min_amd_letter_date": "2019-02-27",
 "awd_max_amd_letter_date": "2021-11-29",
 "awd_abstract_narration": "Text to speech (TTS) synthesis has become a successful and ubiquitous technology. The area of application for TTS technology that motivates this research is its use for Augmentative and Alternative Communication (AAC). According to the American Speech-Language and Hearing Association (ASHA), more than two million people in the United States have severe communication disorders that impair their ability to talk. AAC devices that use TTS to create spoken output are used by many of these people to support communication. Historically, AAC users have had access to a relatively small family of generic TTS voices that are neither unique to them nor typically age- or dialect-appropriate. However, advances in TTS technology make it possible to create personalized synthetic voices that capture the unique vocal identity of AAC device users if they are able to record enough speech. This allows patients with neurodegenerative diseases such as ALS to \"bank\" their voice - that is, to record examples of their speech that can later be used to create a personal TTS voice - before the disease progresses to a point that they can no longer speak. Unfortunately, one major barrier to voice banking, especially for patients who may already be experiencing some difficulty speaking, is the amount of speech needed to create a natural sounding TTS voice that fully captures the vocal identity of the voice banker. To reduce this barrier, this research will combine a type of speech synthesis called parallel formant synthesis that was developed several decades ago, with deep learning computational techniques that allow a computer to learn how to control the parameters of the parallel formant synthesizer to reproduce the speech of a target speaker given examples of the target speaker's speech. A parallel formant synthesizer will be implemented and trained to model speech recorded by voice bankers, and its output will be compared with that of other synthesizers that have been trained with the same speech data. Objective measures of similarity between synthetic and natural utterances, and subjective measures of voice quality and similarity using human listeners, will be used. This will be the first step toward building a parallel formant synthesis-based voice conversion system capable of creating TTS voices from a small number of natural speech samples, and also better able to model the expressive nature of natural speech.\r\n\r\nDespite advances in TTS technology, there are multiple challenges to the application of this technology for voice banking. Specifically: (a) the amount of speech required (several hours) to create the most natural sounding TTS voices using unit selection or hybrid DNN/unit selection is prohibitive for most voice bankers; (b) existing voice conversion techniques that do not require large amounts of parallel speech from the target talker generally produce speech sounding less natural and less like the target speaker when compared to concatenative synthesis; and (c) both concatenative and statistical parametric techniques produce speech that is only as expressive as the data within the speech corpus from which they have been constructed or trained. Parallel formant synthesis, because it is based explicitly on the perceptually most salient features of natural speech and lends itself to independently modeling laryngeal, suprasegmental, and segmental features should be better able to address all three of these challenges. As proof of concept, a parallel formant synthesis (PFS) vocoder with DNN-based parameter estimation will be implemented. The vocoder will be implemented within the Merlin DNN synthesis framework so that speech output of the PFS system can be directly compared to output generated by the World and MagPhase vocoders. Training will be based on corpora drawn from the same set of 1600 utterances recorded by multiple individuals who have contributed their recordings to the ModelTalker project. The selected target talkers will be balanced for gender and span a wide range of English dialects, but use of speakers with noticeable levels of dysarthria will be avoided. Objective comparisons will be based on Mel-Cepstral Difference (MCD) between synthetic and natural sentence tokens that were not used in training the synthesizers. Subjective measures (Mean Opinion Scores) will be obtained from human listeners via Amazon Mechanical Turk.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "H. Timothy",
   "pi_last_name": "Bunnell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "H. Timothy Bunnell",
   "pi_email_addr": "tbunnell@nemours.org",
   "nsf_id": "000584520",
   "pi_start_date": "2019-02-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Alfred I du Pont Hospital for Children",
  "inst_street_address": "1600 ROCKLAND RD",
  "inst_street_address_2": "",
  "inst_city_name": "WILMINGTON",
  "inst_state_code": "DE",
  "inst_state_name": "Delaware",
  "inst_phone_num": "3026516832",
  "inst_zip_code": "198033607",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DE00",
  "org_lgl_bus_name": "THE NEMOURS FOUNDATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "ETGMYV7CKAJ7"
 },
 "perf_inst": {
  "perf_inst_name": "Alfred I du Pont Hospital for Children",
  "perf_str_addr": "1600 Rockland Rd",
  "perf_city_name": "Wilmington",
  "perf_st_code": "DE",
  "perf_st_name": "Delaware",
  "perf_zip_code": "198033607",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DE00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 104052.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\"><span>The aim of this project was to develop a new artificial speech synthesis technology that combines the classical &ldquo;source-filter&rdquo; model of speech production with modern machine-learning (e.g. &ldquo;artificial intelligence&rdquo; or &ldquo;AI&rdquo;) methodology. The &ldquo;source-filter&rdquo; model posits that speech can be modeled as an excitation &ldquo;source&rdquo; signal that is then &ldquo;filtered&rdquo; by the vocal tract. Theoretically, the &ldquo;source&rdquo; and &ldquo;filter&rdquo; can be mathematically modeled separately, as distinct sets of parameters (i.e. measurements), and then combined to produce speech that is not only intelligible, but recognizable as the person whose speech was modeled. The particular approach is called formant synthesis, as the vocal tract filter is modeled as a set of &ldquo;formants&rdquo;, the measurable acoustic resonances of the vocal tract that are shaped by the vowels and consonants being spoken as well as by the individual&rsquo;s unique vocal tract anatomy.</span></p>\n<p class=\"Default\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Over the course of the project, we have developed machine-learning methods to estimate both source and vocal-tract parameters from a set of recordings of an individual speaker. We&rsquo;ve also developed a formant-synthesis program (called pfsyn) that can take the estimated source and vocal-tract parameters and combine them into recognizable novel speech &ndash; although the quality of the resulting output does not yet match those of existing speech-synthesis systems. Our investigations suggest that the output problems rest primarily in inaccuracies in the estimation of the source parameters, and/or in the algorithms that combine the source parameters with the vocal-tract parameters.</span></p>\n<p class=\"Default\"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The methodology developed to estimate the vocal-tract parameters deserves special note, as it is based on &ldquo;deep neural network&rdquo; (DNN) technology, a form of machine learning algorithms, and directly estimates the frequency, amplitude, and bandwidth of speech formants. Experiments indicate that our method of formant measurement (called FormantNet) is generally more accurate than existing analytical methods commonly used by speech acoustics scientists. Recently, other DNN methods of measuring formants have been published in the literature, but crucially rely on manually-measured formant parameters as training data, which are costly and time-consuming to acquire. Our novel methodology does not require such training data, which means that the DNN model can be trained more quickly and on more data, making the entire process of formant measurement faster, cheaper, and more accurate. Since formant measurement is central to the broader study of human speech as a whole, we believe our formant-measurement technology will have an impact on many disciplines outside of speech synthesis, including linguistic studies of acoustic phonetics, human speech perception, and speech recognition technology.</span></p>\n<p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In addition, we have incorporated pfsyn and FormantNet into an existing text-to-speech (TTS) system used as the foundation of &ldquo;ModelTalker&rdquo;. ModelTalker is a system for &ldquo;voice banking&rdquo;: a process in which an individual can record (&ldquo;bank&rdquo;) their voice systematically in such a way that the recordings can be used to construct a synthetic voice that sounds recognizably and uniquely like the individual who was recorded. So far, the ModelTalker synthetic voices constructed using pfsyn do not sound as natural as ModelTalker voices produced using other techniques, but we aim to continue to improving the technology in future projects. Thus, this project is the first step toward developing an alternative method of applying TTS technology to the production of personalized synthetic voices in assistive technology for children and adults with neurological communication disorders.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/29/2023<br>\n\t\t\t\t\tModified by: H. Timothy&nbsp;Bunnell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The aim of this project was to develop a new artificial speech synthesis technology that combines the classical \"source-filter\" model of speech production with modern machine-learning (e.g. \"artificial intelligence\" or \"AI\") methodology. The \"source-filter\" model posits that speech can be modeled as an excitation \"source\" signal that is then \"filtered\" by the vocal tract. Theoretically, the \"source\" and \"filter\" can be mathematically modeled separately, as distinct sets of parameters (i.e. measurements), and then combined to produce speech that is not only intelligible, but recognizable as the person whose speech was modeled. The particular approach is called formant synthesis, as the vocal tract filter is modeled as a set of \"formants\", the measurable acoustic resonances of the vocal tract that are shaped by the vowels and consonants being spoken as well as by the individual\u2019s unique vocal tract anatomy.\n            Over the course of the project, we have developed machine-learning methods to estimate both source and vocal-tract parameters from a set of recordings of an individual speaker. We\u2019ve also developed a formant-synthesis program (called pfsyn) that can take the estimated source and vocal-tract parameters and combine them into recognizable novel speech &ndash; although the quality of the resulting output does not yet match those of existing speech-synthesis systems. Our investigations suggest that the output problems rest primarily in inaccuracies in the estimation of the source parameters, and/or in the algorithms that combine the source parameters with the vocal-tract parameters.\n            The methodology developed to estimate the vocal-tract parameters deserves special note, as it is based on \"deep neural network\" (DNN) technology, a form of machine learning algorithms, and directly estimates the frequency, amplitude, and bandwidth of speech formants. Experiments indicate that our method of formant measurement (called FormantNet) is generally more accurate than existing analytical methods commonly used by speech acoustics scientists. Recently, other DNN methods of measuring formants have been published in the literature, but crucially rely on manually-measured formant parameters as training data, which are costly and time-consuming to acquire. Our novel methodology does not require such training data, which means that the DNN model can be trained more quickly and on more data, making the entire process of formant measurement faster, cheaper, and more accurate. Since formant measurement is central to the broader study of human speech as a whole, we believe our formant-measurement technology will have an impact on many disciplines outside of speech synthesis, including linguistic studies of acoustic phonetics, human speech perception, and speech recognition technology.\n\n            In addition, we have incorporated pfsyn and FormantNet into an existing text-to-speech (TTS) system used as the foundation of \"ModelTalker\". ModelTalker is a system for \"voice banking\": a process in which an individual can record (\"bank\") their voice systematically in such a way that the recordings can be used to construct a synthetic voice that sounds recognizably and uniquely like the individual who was recorded. So far, the ModelTalker synthetic voices constructed using pfsyn do not sound as natural as ModelTalker voices produced using other techniques, but we aim to continue to improving the technology in future projects. Thus, this project is the first step toward developing an alternative method of applying TTS technology to the production of personalized synthetic voices in assistive technology for children and adults with neurological communication disorders.\n\n \n\n\t\t\t\t\tLast Modified: 04/29/2023\n\n\t\t\t\t\tSubmitted by: H. Timothy Bunnell"
 }
}