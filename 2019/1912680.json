{
 "awd_id": "1912680",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FoMR: DeepFetch: Compact Deep Learning based Prefetcher on Configurable Hardware",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Danella Zhao",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 199999.0,
 "awd_amount": 199999.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2019-08-15",
 "awd_abstract_narration": "Fast computer processors, tensor processing units, hardware accelerators, and heterogeneous architectures have enabled large-scale speed-ups in computational power, but memory speeds have not kept pace at the same time. Memory performance therefore has become the bottleneck in many applications that rely on heavy memory access. Several emerging memory technologies such 3D-Stacked Dynamic Random Access Memory (3D-DRAM) and non-volatile memory attempt to address memory bottleneck issues from a hardware perspective, but with a tradeoff among bandwidth, power, latency, and cost. Rather than redesigning existing algorithms to suit specific memory technology, this project will develop a Machine Learning-based approach that automatically learns access patterns which may be used to optimally prefetch data. Specifically, highly compact Long short-term memory (LSTM) models will be used as the centerpiece of the prefetcher for predicting memory accesses. Through novel model compression techniques, hierarchical memory modeling and dedicated hardware, this project will overcome barriers of fully exploiting machine learning and emerging hardware to improve prefetching. Successful completion of this project will lead to improved memory performance for applications, including signal processing, computer vision, and language processing.\r\n\r\nA practical LSTM based prefetcher implementation on hardware requires dealing with certain challenges that will be addressed in this endeavor: (i) training a small model (to enable fast inference) with large traces that is highly accurate in predicting memory accesses for multiple applications; (ii) model compression to ensure real-time inference; (iii) retraining the model online on-demand to learn application specific models, which would require fast learning with small amount of data; (iv) making prefetching decisions in real-time based on the prediction and uncertainty of the model ''what'', ''when'', and ''where'' to prefetch, which also requires careful modeling of the target memory hierarchy; (vi) based on the predictions, deciding in real-time if reordering data (dynamic data layout) can improve the latency, making future prefetches more effective; (vii) mapping the framework of predictions and decision making on limited available configurable hardware in - ensuring low latency training and high-throughput prefetching utilizing small area/power.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Viktor",
   "pi_last_name": "Prasanna",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Viktor K Prasanna",
   "pi_email_addr": "prasanna@usc.edu",
   "nsf_id": "000209825",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ajitesh",
   "pi_last_name": "Srivastava",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ajitesh Srivastava",
   "pi_email_addr": "ajiteshs@usc.edu",
   "nsf_id": "000791265",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S. Flower St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "021Z",
   "pgm_ref_txt": "Industry Partnerships"
  },
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "8585",
   "pgm_ref_txt": "NSF/Intel Partnership Projects"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 199999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Disclaimer</strong></p>\n<p>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual Merit </strong></p>\n<p>The project advanced the state of the art in Machine Learning for memory access prediction and data prefetching by accomplishing the following:</p>\n<p>We proposed a novel application clustering algorithm that captures features from long sequences. We developed a compact meta-LSTM model that quickly adapts to a cluster of applications. We developed a Transformer-based predictor that uses an encoder-decoder network to predict future access sequence. We also extracted software hints indicating the graph processing phases and thread IDs and integrate the hints into a memory access prediction model, which improved the prediction performance.</p>\n<p>We integrated memory access prediction models with existing computer architectures to build ML-based data prefetchers. We developed a recurrent neural network augmented offset prefetcher that uses LSTM as an assistant to an existing prefetching system. We also developed a parallelizable attention-based variable-degree prefetcher using fine-grained address segmentation as input which addressed the challenges of class explosion, tokenization, labeling, and latency in developing ML-based prefetchers.</p>\n<p>We developed an ensemble prefetching framework using reinforcement learning, which enables multiple prefetchers to complement each other on hybrid applications. We developed an ensemble controller using a multi-layer perceptron, which takes prefetch suggestions from all the prefetchers as input, selects the best suggestion dynamically, and learns online toward achieving a higher prefetch performance. For hardware implementation, we also developed a tabular variant of the ensemble controller.</p>\n<p>&nbsp;</p>\n<p><strong>Broader Impact</strong></p>\n<p>The broader impact of this work is in efficient use of emerging memory technologies and tightly coupled microarchitectures to realize high IPC systems. Memory performance is a fundamental challenge in achieving high performance for emerging workloads.</p>\n<p>The grant demonstrated high memory access prediction performance using machine learning methods. The accurate access prediction provides insights into the design of software and hardware prefetchers and the acceleration of memory-intensive applications. The Transformer-based model won the third place at the MLArchSys prefetching competition (co-located with ISCA 2020), leading to a broader interaction with experts in the community and understanding the impact of attention mechanism for memory access prediction.</p>\n<p>The results improved the practicality of the hardware implementation for ML-based prefetchers. Fine-grained address segmentation is a general preprocessing method for ML-based prefetching and can be easily transferred to other models.</p>\n<p>The proposed RL-based ensemble prefetching framework is general, versatile, and can be updated online. It is compatible with various input prefetchers, including traditional rule-based prefetchers and advanced ML-based prefetchers. The framework can be leveraged by the community to boost the performance of existing memory systems and to design advanced prefetchers.</p>\n<p>The optimizations for ML-based prefetchers will advance the use of machine learning for improving computer system performance.</p>\n<p>The proposed research will also produce materials appropriate for inclusion in graduate and undergraduate courses. The project will identify and engage underrepresented and underserved students and expose them to STEM through various programs in USC.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/29/2023<br>\n\t\t\t\t\tModified by: Viktor&nbsp;K&nbsp;Prasanna</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDisclaimer\n\nThis Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.\n\n \n\nIntellectual Merit \n\nThe project advanced the state of the art in Machine Learning for memory access prediction and data prefetching by accomplishing the following:\n\nWe proposed a novel application clustering algorithm that captures features from long sequences. We developed a compact meta-LSTM model that quickly adapts to a cluster of applications. We developed a Transformer-based predictor that uses an encoder-decoder network to predict future access sequence. We also extracted software hints indicating the graph processing phases and thread IDs and integrate the hints into a memory access prediction model, which improved the prediction performance.\n\nWe integrated memory access prediction models with existing computer architectures to build ML-based data prefetchers. We developed a recurrent neural network augmented offset prefetcher that uses LSTM as an assistant to an existing prefetching system. We also developed a parallelizable attention-based variable-degree prefetcher using fine-grained address segmentation as input which addressed the challenges of class explosion, tokenization, labeling, and latency in developing ML-based prefetchers.\n\nWe developed an ensemble prefetching framework using reinforcement learning, which enables multiple prefetchers to complement each other on hybrid applications. We developed an ensemble controller using a multi-layer perceptron, which takes prefetch suggestions from all the prefetchers as input, selects the best suggestion dynamically, and learns online toward achieving a higher prefetch performance. For hardware implementation, we also developed a tabular variant of the ensemble controller.\n\n \n\nBroader Impact\n\nThe broader impact of this work is in efficient use of emerging memory technologies and tightly coupled microarchitectures to realize high IPC systems. Memory performance is a fundamental challenge in achieving high performance for emerging workloads.\n\nThe grant demonstrated high memory access prediction performance using machine learning methods. The accurate access prediction provides insights into the design of software and hardware prefetchers and the acceleration of memory-intensive applications. The Transformer-based model won the third place at the MLArchSys prefetching competition (co-located with ISCA 2020), leading to a broader interaction with experts in the community and understanding the impact of attention mechanism for memory access prediction.\n\nThe results improved the practicality of the hardware implementation for ML-based prefetchers. Fine-grained address segmentation is a general preprocessing method for ML-based prefetching and can be easily transferred to other models.\n\nThe proposed RL-based ensemble prefetching framework is general, versatile, and can be updated online. It is compatible with various input prefetchers, including traditional rule-based prefetchers and advanced ML-based prefetchers. The framework can be leveraged by the community to boost the performance of existing memory systems and to design advanced prefetchers.\n\nThe optimizations for ML-based prefetchers will advance the use of machine learning for improving computer system performance.\n\nThe proposed research will also produce materials appropriate for inclusion in graduate and undergraduate courses. The project will identify and engage underrepresented and underserved students and expose them to STEM through various programs in USC.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/29/2023\n\n\t\t\t\t\tSubmitted by: Viktor K Prasanna"
 }
}