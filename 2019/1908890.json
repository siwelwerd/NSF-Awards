{
 "awd_id": "1908890",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Structured Non-Smooth Optimization: Theory and Methods",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pedro Embid",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 280410.0,
 "awd_amount": 280410.0,
 "awd_min_amd_letter_date": "2019-08-06",
 "awd_max_amd_letter_date": "2019-08-06",
 "awd_abstract_narration": "The investigator develops theory and algorithms to solve optimization problems arising in present-day business, science, and engineering applications, often specifically related to data analysis problems where the aim is to better inform decisions about alternative choices.  The project's topics arise in model selection techniques (used in applications such as the identification of social networks and the selection of covariates that best inform health outcomes), in imaging applications, and in the dynamics, tracking, and control of waste streams and of vehicles.  Commonly in applications the objective function (the function to be optimized) may be nonsmooth, nonconvex, or high-dimensional; each of these features presents major challenges for classical optimization methods.  A key aspect of the project is the use of nonsmooth techniques that allow for the introduction or discovery of special properties in the desired solution, such as sparsity, stability, and robustness.  The work falls into four main areas.  The first concerns study of the BFGS method and matrix secant methods for nonsmooth convex optimization (why BFGS works as well as it does is unclear), and the design of algorithms that automatically discover a so-called UV-decomposition of the objective function.  Here the U is the smooth part and the V is the nonsmooth part.  Such decompositions are essential for rapid solution identification as they allow one to follow a smooth valley (U) with steep sides (V) toward the optimal solution.  The second examines the rate at which a solution is obtained as well as its accuracy, depending on functional and parameter inputs.  The third examines the determination of optimal stability and control of linear dynamical systems occurring, for example, in orbital dynamics or drug metabolism.  The goal here is to help recognize optimal solutions.  Indeed, in many cases methods for identifying optimality remain unknown.  The final task concerns the development of novel smoothing methodologies for optimization problems over matrix spaces, such as those occurring in social network discovery.  These problems are typically very high-dimensional, and nonsmoothness of the objective function is the key to discovering hidden structures within the data.  Here the goal is to develop smooth approximations whose solution can be rapidly computed and whose proximity to the true solution is precisely controlled.  Graduate students participate in the research.\r\n\r\nMore technically, the areas of study are (i) BFGS and matrix secant methods for nonsmooth convex optimization, (ii) local and global convergence theory for convex-composite optimization, (iii) variational analysis of spectral functions for non-symmetric matrices, and (iv) the generalized matrix fractional function and smoothing on matrix spaces.  The study of BFGS methods focuses on the relationship between the iterates and smooth, convex, uniform approximations.  The study of convex composite problems analyses the behavior of algorithms using Robinson's method of generalized equation.  The study of nonsymmetric matrices focuses on extending variational techniques to the difficult nonderogatory case.  The final area considers the embedding of a wide range of matrix optimization problems in a smooth setting by the use of infimal projection with the generalized matrix fractional function introduced by the investigator and collaborators.  Graduate students participate in the research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Burke",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "James V Burke",
   "pi_email_addr": "jvburke@uw.edu",
   "nsf_id": "000311455",
   "pi_start_date": "2019-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "University of Washington, Box 35",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981954350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 280410.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Mathematical optimization has experienced explosive growth of over the past 30 years due to a number of factors including the growth of the internet, advances in computational power and computing architectures, the availability of very large data sets, as well as advances in science, engineering, communication, and business. This has contributed to the emergence of new modalities for data acquisition, management, knowledge discovery, and system design and modeling across the sciences, engineering, business, and medicine.&nbsp; At the core of much of this activity are emerging applications in artificial intelligence (AI) techniques across all disciplines, e.g. pattern recognition, classification, system identification, language learning models, and autonomous decision making systems.This progress leans heavily on optimization and variational tools for model development, numerical solution, sensitivity analysis, robustness, and uncertainty quantification particularly due to the presence of nonsmoothness.</p>\r\n<p>This research focused on the discovery and exploitation of the underlying mathematical structure present in many modern large-scale optimization problems including those occurring in AI. In particular, the research exploited regularized convex-composite structures to develop new mathematical understanding of these models addressing questions of model feasibility and solution sparsity as well as solution methodologies employing penalty parameter updating, iteratively reweighed least squares, gradient sampling, smoothing, and algorithmic globalization strategies. In addition, the research developed a new smoothing technology for covariate selection in linear mixed effects models that achieves dramatic improvements in both the accuracy in variable selection and speed of convergence. Overall, this research resolves a number of long-standing open questions as well as providing a spring board to new solution methodologies for a range of emerging and important problem classes.</p>\r\n<p>Fundamental contributions have been made in 8 general areas of study. A brief description of each is given below.</p>\r\n<p>(1) Sequential quadratic programming (SQP) for potentially infeasible nonlinear programs (NLP):&nbsp; A new dynamic penalty parameter updating technique is developed that cokputes a new search direction within a single quadratic subprogram. This search direction yields progress toward both feasibility and optimality. Under reasonable assumptions, the strategy does not modify the penalty parameter unnecessarily while providing convergence to geralized stationary points as well as early detection of infeasibility.</p>\r\n<p>(2) Convex convex-composite optimization:&nbsp;&nbsp;Conditions characterizing the convexity of convex-composite functions is established and a full variational&nbsp; calculus is established in finite dimensions. These results are applied to multiple examples of such functions arrising in optimization and linear algebra.</p>\r\n<p>(3) Sparsity in Phase Retrieval:&nbsp;A sparse recovery theory for real sparse phase retrieval that, in spirit, parallels the Candes-Tao theory for compressed sensing (l1-regression) is developed.&nbsp;The new theory introduces new concepts for phase retrieval that parallel similar ideas in compressed sensing. These results yield completely new results when using the so called ``natural metric'' for&nbsp;phase retrieval and have no precedence in the literature on sparse phase retrieval.</p>\r\n<p>(4) Iteratively Re-Weighted Least-Squares (IRLS):&nbsp;The first family of examples are constructed where the Daubechies-Devore-Fornasier-Gunturk IRLS algorithm fails to find a k-sparse solution for k = K where $K$ is the null-space property. In addition, a modification to Daubechies-Devore-Fornasier-Gunturk IRLS algorithm is given that provably converges to the unique k-sparse solutionfor k less than or equal to K while preserving the local linear rate.</p>\r\n<p>(5) Non-Lipschitz Gradient Sampling (GS):&nbsp;The convergence theory for the GS algorithm is extended to a class of non-Lipschitz function. These results expand our knowledge of what is possible in the non-Lipschitz setting where the subdifferentialsmay be either empty or unbounded.</p>\r\n<p>(6) Stability of Symmetric Block Tridiagonal System Solvers: The numerical stability of a range of numerical solvers for symmetric tridiagonal systems&nbsp;quantified and compared.&nbsp; The results show that many of the standard solvers possess the same stability properties in the sense that the eigenvalue bounds of the original system are maintained by all iterates. The results are applied to Kalman applications.</p>\r\n<p>(7) Globalization for Convex-Composite Optimization:&nbsp;New globalization strategies (i.e. strategies for arbitrary starting points) for non finite-valued convex-composite optimization problems are developed and their convergence properties are established.&nbsp;</p>\r\n<p>(8) Covariate Selection for Linear Mixed Effects (LME):&nbsp;New smoothing techniques for this problem are constructed that decouple the smooth and nonsmooth components of the objective. This allows partially optimization over the smooth portion to obtain a smooth optimal-value function that both captures global variational features of the objective as well regularizing the smooth portion through the use of second-order information. A variation of the proximal gradient descent algorithm is applied to this new objective. The resulting algorithm achieves dramatic improvements in both the accuracy in variable selection as well as speed of convergence.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/17/2025<br>\nModified by: James&nbsp;V&nbsp;Burke</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nMathematical optimization has experienced explosive growth of over the past 30 years due to a number of factors including the growth of the internet, advances in computational power and computing architectures, the availability of very large data sets, as well as advances in science, engineering, communication, and business. This has contributed to the emergence of new modalities for data acquisition, management, knowledge discovery, and system design and modeling across the sciences, engineering, business, and medicine. At the core of much of this activity are emerging applications in artificial intelligence (AI) techniques across all disciplines, e.g. pattern recognition, classification, system identification, language learning models, and autonomous decision making systems.This progress leans heavily on optimization and variational tools for model development, numerical solution, sensitivity analysis, robustness, and uncertainty quantification particularly due to the presence of nonsmoothness.\r\n\n\nThis research focused on the discovery and exploitation of the underlying mathematical structure present in many modern large-scale optimization problems including those occurring in AI. In particular, the research exploited regularized convex-composite structures to develop new mathematical understanding of these models addressing questions of model feasibility and solution sparsity as well as solution methodologies employing penalty parameter updating, iteratively reweighed least squares, gradient sampling, smoothing, and algorithmic globalization strategies. In addition, the research developed a new smoothing technology for covariate selection in linear mixed effects models that achieves dramatic improvements in both the accuracy in variable selection and speed of convergence. Overall, this research resolves a number of long-standing open questions as well as providing a spring board to new solution methodologies for a range of emerging and important problem classes.\r\n\n\nFundamental contributions have been made in 8 general areas of study. A brief description of each is given below.\r\n\n\n(1) Sequential quadratic programming (SQP) for potentially infeasible nonlinear programs (NLP): A new dynamic penalty parameter updating technique is developed that cokputes a new search direction within a single quadratic subprogram. This search direction yields progress toward both feasibility and optimality. Under reasonable assumptions, the strategy does not modify the penalty parameter unnecessarily while providing convergence to geralized stationary points as well as early detection of infeasibility.\r\n\n\n(2) Convex convex-composite optimization:Conditions characterizing the convexity of convex-composite functions is established and a full variational calculus is established in finite dimensions. These results are applied to multiple examples of such functions arrising in optimization and linear algebra.\r\n\n\n(3) Sparsity in Phase Retrieval:A sparse recovery theory for real sparse phase retrieval that, in spirit, parallels the Candes-Tao theory for compressed sensing (l1-regression) is developed.The new theory introduces new concepts for phase retrieval that parallel similar ideas in compressed sensing. These results yield completely new results when using the so called ``natural metric'' forphase retrieval and have no precedence in the literature on sparse phase retrieval.\r\n\n\n(4) Iteratively Re-Weighted Least-Squares (IRLS):The first family of examples are constructed where the Daubechies-Devore-Fornasier-Gunturk IRLS algorithm fails to find a k-sparse solution for k = K where $K$ is the null-space property. In addition, a modification to Daubechies-Devore-Fornasier-Gunturk IRLS algorithm is given that provably converges to the unique k-sparse solutionfor k less than or equal to K while preserving the local linear rate.\r\n\n\n(5) Non-Lipschitz Gradient Sampling (GS):The convergence theory for the GS algorithm is extended to a class of non-Lipschitz function. These results expand our knowledge of what is possible in the non-Lipschitz setting where the subdifferentialsmay be either empty or unbounded.\r\n\n\n(6) Stability of Symmetric Block Tridiagonal System Solvers: The numerical stability of a range of numerical solvers for symmetric tridiagonal systemsquantified and compared. The results show that many of the standard solvers possess the same stability properties in the sense that the eigenvalue bounds of the original system are maintained by all iterates. The results are applied to Kalman applications.\r\n\n\n(7) Globalization for Convex-Composite Optimization:New globalization strategies (i.e. strategies for arbitrary starting points) for non finite-valued convex-composite optimization problems are developed and their convergence properties are established.\r\n\n\n(8) Covariate Selection for Linear Mixed Effects (LME):New smoothing techniques for this problem are constructed that decouple the smooth and nonsmooth components of the objective. This allows partially optimization over the smooth portion to obtain a smooth optimal-value function that both captures global variational features of the objective as well regularizing the smooth portion through the use of second-order information. A variation of the proximal gradient descent algorithm is applied to this new objective. The resulting algorithm achieves dramatic improvements in both the accuracy in variable selection as well as speed of convergence.\r\n\n\n\t\t\t\t\tLast Modified: 01/17/2025\n\n\t\t\t\t\tSubmitted by: JamesVBurke\n"
 }
}