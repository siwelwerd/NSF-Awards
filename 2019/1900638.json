{
 "awd_id": "1900638",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CNS Core: Large: Autonomy and Privacy with Open Federated Virtual Assistants",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jason Hallstrom",
 "awd_eff_date": "2019-04-01",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 3000000.0,
 "awd_amount": 3000000.0,
 "awd_min_amd_letter_date": "2019-03-07",
 "awd_max_amd_letter_date": "2022-06-07",
 "awd_abstract_narration": "Virtual assistants, and more generally linguistic user interfaces, will become the norm for mobile and ubiquitous computing.  This research aims to create the best open virtual assistant designed to respect privacy.  Instead of just simple commands, virtual assistants will be able to perform complex tasks connecting different Internet-of-Things devices and web services.  Also, users may decide who, what, when, and how their data are to be shared.  By making the technology open-source, this research helps create a competitive industry that offers a great variety of innovative products, instead of closed platform monopolies. \r\n\r\nThis project unifies all the internet services and \"Internet of Things\" (IoT) devices into an interoperable web, with an open, crowdsourced, universal encyclopedia of public application interfaces called Thingpedia.  Resources in Thingpedia can be connected together using ThingTalk, a high-level virtual assistant language.  Another key contribution will be the Linguistic User Interface Network (LUInet) that can understand how to operate the world's digital interfaces in natural language.  LUInet uses deep learning to translate natural language into ThingTalk.  Privacy with fine-grain access control is provided through open-source federated virtual assistants. Transparent third-party sharing is supported by keeping human-understandable contracts and data transactions with a scalable blockchain technology.  \r\n\r\nThis research contributes to the creation of a decentralized computing ecosystem that protects user privacy and promotes open competition. Natural-language programming expands the utility of computing to ordinary people, reducing the programming bottleneck. All the technologies developed in this project will be made available as open source, supporting further research and development by academia and industry. Thingpedia and the ThingTalk dataset will be an important contribution to natural language processing. The large-scale research program for college and high-school students, with a focus on diverse students, broadens participation and teaches technology, research, and the importance of privacy.  All the information related to this project, papers, data, code, and results, are available at http://oval.cs.stanford.edu until at least 2026.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Monica",
   "pi_last_name": "Lam",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Monica S Lam",
   "pi_email_addr": "lam@cs.stanford.edu",
   "nsf_id": "000214922",
   "pi_start_date": "2019-03-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Landay",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "James A Landay",
   "pi_email_addr": "landay@stanford.edu",
   "nsf_id": "000458875",
   "pi_start_date": "2019-03-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Manning",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher D Manning",
   "pi_email_addr": "manning@cs.stanford.edu",
   "nsf_id": "000491177",
   "pi_start_date": "2019-03-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Mazi\u00e8res",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Mazi\u00e8res",
   "pi_email_addr": "dm-list-sup-nsf14@scs.stanford.edu",
   "nsf_id": "000100121",
   "pi_start_date": "2019-03-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Bernstein",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Michael S Bernstein",
   "pi_email_addr": "msb@cs.stanford.edu",
   "nsf_id": "000649517",
   "pi_start_date": "2019-03-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943055008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 627077.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 762741.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 793340.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 816842.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-19bb6c06-7fff-2785-bac8-a863fa47c184\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project advances and democratizes deep-learning based conversational virtual assistant technology.&nbsp; We have created new virtual assistant development methodologies and open-source tools that help developers (without being AI experts) create effective conversational agents for their chosen domains for different languages. We have validated our research with powerful privacy-preserving assistants and social chatbots.&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><strong><span style=\"font-size: 11pt; font-family: &quot;Times New Roman&quot;, serif; color: #000000; background-color: transparent; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Technical Contributions.</span></strong></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Despite recent dramatic advances in large language models (LLMs), they tend to hallucinate and are hence untrustworthy as virtual assistants. We show that we can eliminate hallucination from LLMs by grounding them with external corpora, which can comprise databases, knowledge bases, free-text documents, APIs on the internet, and theorem provers.&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We introduce a new formal, executable notation that supports inter-operation of these different types of resources. Unlike existing support for APIs in virtual assistants, our notation captures the full signature of the API calls, which includes the results, and not just the input parameters. This is necessary to support compositionality, the key to automating high-level functions.&nbsp;&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We reduce the prohibitively high cost of data collection for training semantic parsers, which translate natural language into code, with a data synthesizer that covers all constructs and properties in schemas.&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To ground LLMs in open-text documents, we create a multi-stage LLM system with few-shot examples that inject factuality into the natural-sounding responses from LLMs.&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We distill large language models for virtual assistants into models small enough to run on a personal device. This not only improves the speed of the chatbot, but also privacy, affordability, and accessibility to many more developers.&nbsp;&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To promote equity across people speaking low-resource languages, we develop methodology and tools to internationalize and localize task-oriented agents to new languages.&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our contributions to multi-modal assistants and robotic process automation include: We show that a virtual assistant microphone can be used to automatically determine the user position and gaze direction. We propose a novel way to combine touch and verbal commands so users can easily perform cross-application tasks on mobile devices. We allow non-programmers to automate sophisticated web routines by demonstrating them along with natural verbal instructions to incorporate programming concepts such as function composition, conditional, and iterative evaluation. We show how to create a universal call agent that follows natural language instructions to automatically interact with users and operate the web.</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To support privacy, we have created a federated virtual assistant architecture so users can use natural language to share with each other privately, with fine-grain access control, with the help of a satisfiability-modulo-theory prover.&nbsp;</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><strong><span style=\"font-size: 11pt; font-family: &quot;Times New Roman&quot;, serif; color: #000000; background-color: transparent; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Information Dissemination.</span></strong></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We have published extensively and given numerous keynotes and interviews on our research (https:/oval.cs.stanford.edu). We evangelized open-source assistants through organizing two workshops at Stanford. The first invitation-only workshop, held in 2019, was attended by practitioners and researchers from 30 different organizations. The second, a hybrid online/offline workshop, had over 400 registrants, from 60 educational institutions, 15 nonprofit/government organizations, and 200 companies in many different sectors. All our software is available at </span><a style=\"text-decoration: none;\" href=\"https://github.com/stanford-oval\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #1155cc; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: underline; text-decoration-skip-ink: none; vertical-align: baseline; white-space: pre-wrap;\">https://github.com/stanford-oval</span></a><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We have developed a new course, Conversational Virtual Assistant with Deep Learning, which offers students an opportunity to participate in state-of-the-art virtual assistant research. It was attended by a class consisting of 40% female and 10% black/hispanic students last year.</span></p>\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 10pt;\" dir=\"ltr\"><strong><span style=\"font-size: 11pt; font-family: &quot;Times New Roman&quot;, serif; color: #000000; background-color: transparent; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Impact.</span></strong></p>\n<ol style=\"margin-top: 0; margin-bottom: 0; padding-inline-start: 48px;\">\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We have created the first open-source, conversational virtual assistant capable of the top ten most popular skills, including voice control of 15 different kinds of IoT devices. It can run privately on users&rsquo; own devices, and has been awarded Popular Science's &ldquo;Best of What's New Award in Security&rdquo; in 2019. It was also released as an open-source privacy-preserving voice interface for Home Assistant, a popular, open-source, crowdsourced IoT platform.</span></li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our Chirpy Cardinal Socialbot won second place in Alexa Prize Socialbot Grand Competitions,&nbsp; 2020 and 2021. It is currently featured as part of AI: More than Human, an art exhibition about the relationship between humans and AI which has been seen by over 260,000 visitors in Barcelona, Guangzhou, and Liverpool.&nbsp;<br /><br /></span></li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In collaboration with the School of Medicine, we have created a coach for autistic individuals that improves their social skills through drills on conversations.&nbsp;<br /><br /></span></li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In collaboration with an international team, we created a new multilingual few-shot benchmark, X-RiSAWOZ, that covers languages spoken by 3.5B people (China, English, French, Hindi, Hindi-English, Korean) using a tool we created to facilitate data curation. &nbsp; Our few-shot agents achieve between 61-85% accuracy on dialogue state tracking across all the different languages.<br />&nbsp;</span> </li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We have developed the first non-hallucinating LLM-based open-domain chatbot that is informational and conversational, and grounded on Wikipedia and Wikidata.&nbsp; This unlocks the power of LLMs as a knowledge acquisition tool.&nbsp;<br /></span> </li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our open-source Genie virtual assistant generation framework is uniquely designed to eliminate hallucination, handle large data schemas along with free-text retrieval, and support multilinguality.<br /><br /></span></li>\n<li style=\"list-style-type: decimal; font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Times New Roman',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our multimodal virtual assistant techniques have been commercialized by at least two startups.&nbsp;</span> </li>\n</ol><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/07/2023<br>\n\t\t\t\t\tModified by: Monica&nbsp;S&nbsp;Lam</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project advances and democratizes deep-learning based conversational virtual assistant technology.  We have created new virtual assistant development methodologies and open-source tools that help developers (without being AI experts) create effective conversational agents for their chosen domains for different languages. We have validated our research with powerful privacy-preserving assistants and social chatbots. \nTechnical Contributions.\nDespite recent dramatic advances in large language models (LLMs), they tend to hallucinate and are hence untrustworthy as virtual assistants. We show that we can eliminate hallucination from LLMs by grounding them with external corpora, which can comprise databases, knowledge bases, free-text documents, APIs on the internet, and theorem provers. \nWe introduce a new formal, executable notation that supports inter-operation of these different types of resources. Unlike existing support for APIs in virtual assistants, our notation captures the full signature of the API calls, which includes the results, and not just the input parameters. This is necessary to support compositionality, the key to automating high-level functions.  \nWe reduce the prohibitively high cost of data collection for training semantic parsers, which translate natural language into code, with a data synthesizer that covers all constructs and properties in schemas. \nTo ground LLMs in open-text documents, we create a multi-stage LLM system with few-shot examples that inject factuality into the natural-sounding responses from LLMs. \nWe distill large language models for virtual assistants into models small enough to run on a personal device. This not only improves the speed of the chatbot, but also privacy, affordability, and accessibility to many more developers.  \nTo promote equity across people speaking low-resource languages, we develop methodology and tools to internationalize and localize task-oriented agents to new languages. \nOur contributions to multi-modal assistants and robotic process automation include: We show that a virtual assistant microphone can be used to automatically determine the user position and gaze direction. We propose a novel way to combine touch and verbal commands so users can easily perform cross-application tasks on mobile devices. We allow non-programmers to automate sophisticated web routines by demonstrating them along with natural verbal instructions to incorporate programming concepts such as function composition, conditional, and iterative evaluation. We show how to create a universal call agent that follows natural language instructions to automatically interact with users and operate the web.\nTo support privacy, we have created a federated virtual assistant architecture so users can use natural language to share with each other privately, with fine-grain access control, with the help of a satisfiability-modulo-theory prover. \nInformation Dissemination.\nWe have published extensively and given numerous keynotes and interviews on our research (https:/oval.cs.stanford.edu). We evangelized open-source assistants through organizing two workshops at Stanford. The first invitation-only workshop, held in 2019, was attended by practitioners and researchers from 30 different organizations. The second, a hybrid online/offline workshop, had over 400 registrants, from 60 educational institutions, 15 nonprofit/government organizations, and 200 companies in many different sectors. All our software is available at https://github.com/stanford-oval.\nWe have developed a new course, Conversational Virtual Assistant with Deep Learning, which offers students an opportunity to participate in state-of-the-art virtual assistant research. It was attended by a class consisting of 40% female and 10% black/hispanic students last year.\nImpact.\n\nWe have created the first open-source, conversational virtual assistant capable of the top ten most popular skills, including voice control of 15 different kinds of IoT devices. It can run privately on users\u2019 own devices, and has been awarded Popular Science's \"Best of What's New Award in Security\" in 2019. It was also released as an open-source privacy-preserving voice interface for Home Assistant, a popular, open-source, crowdsourced IoT platform.\nOur Chirpy Cardinal Socialbot won second place in Alexa Prize Socialbot Grand Competitions,  2020 and 2021. It is currently featured as part of AI: More than Human, an art exhibition about the relationship between humans and AI which has been seen by over 260,000 visitors in Barcelona, Guangzhou, and Liverpool. \n\n\nIn collaboration with the School of Medicine, we have created a coach for autistic individuals that improves their social skills through drills on conversations. \n\n\nIn collaboration with an international team, we created a new multilingual few-shot benchmark, X-RiSAWOZ, that covers languages spoken by 3.5B people (China, English, French, Hindi, Hindi-English, Korean) using a tool we created to facilitate data curation.   Our few-shot agents achieve between 61-85% accuracy on dialogue state tracking across all the different languages.\n  \nWe have developed the first non-hallucinating LLM-based open-domain chatbot that is informational and conversational, and grounded on Wikipedia and Wikidata.  This unlocks the power of LLMs as a knowledge acquisition tool. \n \nOur open-source Genie virtual assistant generation framework is uniquely designed to eliminate hallucination, handle large data schemas along with free-text retrieval, and support multilinguality.\n\n\nOur multimodal virtual assistant techniques have been commercialized by at least two startups.  \n\n\n\t\t\t\t\tLast Modified: 08/07/2023\n\n\t\t\t\t\tSubmitted by: Monica S Lam"
 }
}