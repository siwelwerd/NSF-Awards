{
 "awd_id": "1934303",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps: Control for Visual Scene Perception",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922160",
 "po_email": "rshuman@nsf.gov",
 "po_sign_block_name": "Ruth Shuman",
 "awd_eff_date": "2019-06-15",
 "awd_exp_date": "2020-11-30",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2019-06-18",
 "awd_max_amd_letter_date": "2019-06-18",
 "awd_abstract_narration": "The broader impact/commercial potential of the proposed I-Corps project consists of significantly increased safety and control for real-world applications of computer vision and mobile robotics. The first application in the optimal monitoring and control of smart environments will impact the safety and productivity of many industries, such as video surveillance, access control, and smart buildings/cities. Computer-vision software applications will impact industries such as autonomous automobiles by predicting actions of nearby pedestrians, animals, or other vehicles. Automated video processing and recognition software will potentially reduce the impact of catastrophes such as terrorist attacks or mass shootings via intent prediction and anomaly detection available through the proposed video analytics software. Additionally, in a different application the proposed solution may be used to optimize a building's energy efficiency and reduce building operating costs.\r\n\r\nThis I-Corps project further develops a platform for autonomous visual scene perception and feedback control. Autonomous real-time perception and predictive control will dramatically increase the safety and efficiency across several potential applications by directing a human operator's attention to a situation or automatically applying changes to the sensor field of view or controllable environmental conditions such as lighting or temperature. The unique deep learning Bayesian optimization framework does not require prior knowledge of the scene in which it is deployed and instead learns the scene representation over time. The project ties together machine learning, estimation, and control systems. Proof of concept testing has been successfully completed in real-time vehicles equipped with cameras and using deep learning in the loop for autonomous control of the sensor field of view.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Silvia",
   "pi_last_name": "Ferrari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Silvia Ferrari",
   "pi_email_addr": "ferrari@cornell.edu",
   "nsf_id": "000484512",
   "pi_start_date": "2019-06-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148530001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The proposed technology hinges on recent and continuing advances in computer vision applied to digital images and videos by optimizing a learned scene understanding through field-of-view (FOV) planning and control methodologies over long time horizons. Recently-developed deep learning and artificial intelligence algorithms have shown unprecedented performance on image and video understanding tasks when compared to traditional approaches that use hand-crafted features. Hand-crafted features are intuitive image characteristics that are interpretable by humans, such as edges and corners. In contrast, Deep convolutional neural networks (CNNs) take as input an image composed of pixel values and learns a rich feature representation for a particular task. Learned features from digital image and video input have been shown to perform exceptionally well at high-level tasks such as image classification, object detection, person re-identification, and action classification. The proposed autonomous system is capable of the aforementioned perception tasks and is extended to learn a high-level understanding of the scene where the system is deployed. This scene understanding can vary significantly depending on the context. For example, the system may discover locations in the scene where pedestrians often stop walking, such as a crosswalk, or the system may learn what actions take place in a particular scene and how these actions are distributed over the space. For example, the system could be used to learn where people often stop and read advertisements in a shopping mall or alternatively where (and when) an athlete will shoot as opposed to pass the ball.</p>\n<p><strong>Intellectual Merit:</strong></p>\n<p>Human beings are experts at scene perception and understanding, and can control their own or a cameras field-of-view (FOV) to maintain the in-flow of desired information over long time horizons. An example of this is the camera operator for recording professional sporting events. The human camera operator at a football game, for example, has learned a highly-accurate understanding of the scene, i.e., the game of football. This understanding allows the camera operator to make sub-second predictions and maintain the football within the camera FOV for long time-horizons. Training an artificial agent capable of reproducing this behavior that humans are capable of could potentially solve a myriad of problems in several industries. A major challenge in developing a system that is capable of maintaining this level of scene understanding is that it is not practically possible to have a human operator as alert as the sports camera operator in more general scenes. If a human operator is available for more general scenes, such as airports, casinos, or urban environments for example, the operator is responsible for understanding the scene state using simultaneous inputs from many cameras. This approach inevitably leads to missed features in the video that would otherwise give rise to valuable information about the scene. Therefore, a system capable of automatically detecting important scene features and alerting a single user about important signals would overcome this problem. The proposed system directly addresses this challenge.</p>\n<p><strong>Broader Impacts:</strong></p>\n<p>The proposed system has significant potential for commercial applications which has been highlighted in the previous paragraphs. The process towards commercializing this system would start with customer discovery and thoroughly investigating each of the potential applications that have been realized at the current time. In this iterative process, new potential applications&nbsp; and the system itself were identified, including security applications, human-robot interactive systems, and video monitoring and surveillance in which the system learns the scene?s normal flow and action distribution in order to identify anomalous behavior. Security-sensitive domains that will be impacted by this technology include airports, military bases, large office buildings, or casinos, for example. Throughout this project, the commercial potential for all of these applications was determined and the particular needs of the stakeholders was assessed. Through future commercialization steps it will be possible to bring these technological applications to bear through start-ups and industry collaborations, therefore impacting society at large hopefully in the near future.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/03/2021<br>\n\t\t\t\t\tModified by: Silvia&nbsp;Ferrari</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1934303/1934303_10611914_1620069495174_CameraFollowingSnapshot--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1934303/1934303_10611914_1620069495174_CameraFollowingSnapshot--rgov-800width.jpg\" title=\"Mobile video surveillance example\"><img src=\"/por/images/Reports/POR/2021/1934303/1934303_10611914_1620069495174_CameraFollowingSnapshot--rgov-66x44.jpg\" alt=\"Mobile video surveillance example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Autonomous mobile surveillance of a subway environment populated with pedestrians is developed with a video-controlled ground robot detecting and following important pedestrians.</div>\n<div class=\"imageCredit\">Jake Gemerek</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Silvia&nbsp;Ferrari</div>\n<div class=\"imageTitle\">Mobile video surveillance example</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe proposed technology hinges on recent and continuing advances in computer vision applied to digital images and videos by optimizing a learned scene understanding through field-of-view (FOV) planning and control methodologies over long time horizons. Recently-developed deep learning and artificial intelligence algorithms have shown unprecedented performance on image and video understanding tasks when compared to traditional approaches that use hand-crafted features. Hand-crafted features are intuitive image characteristics that are interpretable by humans, such as edges and corners. In contrast, Deep convolutional neural networks (CNNs) take as input an image composed of pixel values and learns a rich feature representation for a particular task. Learned features from digital image and video input have been shown to perform exceptionally well at high-level tasks such as image classification, object detection, person re-identification, and action classification. The proposed autonomous system is capable of the aforementioned perception tasks and is extended to learn a high-level understanding of the scene where the system is deployed. This scene understanding can vary significantly depending on the context. For example, the system may discover locations in the scene where pedestrians often stop walking, such as a crosswalk, or the system may learn what actions take place in a particular scene and how these actions are distributed over the space. For example, the system could be used to learn where people often stop and read advertisements in a shopping mall or alternatively where (and when) an athlete will shoot as opposed to pass the ball.\n\nIntellectual Merit:\n\nHuman beings are experts at scene perception and understanding, and can control their own or a cameras field-of-view (FOV) to maintain the in-flow of desired information over long time horizons. An example of this is the camera operator for recording professional sporting events. The human camera operator at a football game, for example, has learned a highly-accurate understanding of the scene, i.e., the game of football. This understanding allows the camera operator to make sub-second predictions and maintain the football within the camera FOV for long time-horizons. Training an artificial agent capable of reproducing this behavior that humans are capable of could potentially solve a myriad of problems in several industries. A major challenge in developing a system that is capable of maintaining this level of scene understanding is that it is not practically possible to have a human operator as alert as the sports camera operator in more general scenes. If a human operator is available for more general scenes, such as airports, casinos, or urban environments for example, the operator is responsible for understanding the scene state using simultaneous inputs from many cameras. This approach inevitably leads to missed features in the video that would otherwise give rise to valuable information about the scene. Therefore, a system capable of automatically detecting important scene features and alerting a single user about important signals would overcome this problem. The proposed system directly addresses this challenge.\n\nBroader Impacts:\n\nThe proposed system has significant potential for commercial applications which has been highlighted in the previous paragraphs. The process towards commercializing this system would start with customer discovery and thoroughly investigating each of the potential applications that have been realized at the current time. In this iterative process, new potential applications  and the system itself were identified, including security applications, human-robot interactive systems, and video monitoring and surveillance in which the system learns the scene?s normal flow and action distribution in order to identify anomalous behavior. Security-sensitive domains that will be impacted by this technology include airports, military bases, large office buildings, or casinos, for example. Throughout this project, the commercial potential for all of these applications was determined and the particular needs of the stakeholders was assessed. Through future commercialization steps it will be possible to bring these technological applications to bear through start-ups and industry collaborations, therefore impacting society at large hopefully in the near future.\n\n\t\t\t\t\tLast Modified: 05/03/2021\n\n\t\t\t\t\tSubmitted by: Silvia Ferrari"
 }
}