{
 "awd_id": "1924802",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Spatial Patterns of Behavior in Human-Robot Interaction Under Environmental Spatial Constraints",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 499059.0,
 "awd_amount": 531059.0,
 "awd_min_amd_letter_date": "2019-08-30",
 "awd_max_amd_letter_date": "2022-04-04",
 "awd_abstract_narration": "This project promotes the progress of science and robotics by advancing autonomous reasoning about spatial patterns of group behavior during human-robot conversations. Spatial patterns emerge during conversations as a result of every participant's need to communicate while simultaneously perceiving everyone else's response. For example, circular conversational groups often emerge in open spaces. In physically constrained spaces, though, people's position might be influenced by nearby elements, such as walls and other people, leading to variations in acceptable group structure. To enable robots to cope with this variability, the project provides the empirical knowledge and methods needed to incorporate spatial constraints into the way robots reason about human (and robot) spatial formations. The project outcomes have implications across socially-relevant application domains in which user acceptance of co-robots can have a positive impact, including mobile service applications, education, and healthcare. Research activities will offer training opportunities to broaden participation in computing, serve to mentor and train future roboticists, and engage the public in the science of robotics.\r\n\r\nBuilding on foundational work in Human-Robot Interaction (HRI), this project addresses three main questions to advance perception and decision-making for co-robots in group settings: (1) how do spatial constraints influence conversational group formations in HRI; (2) how can robots detect these formations under spatial constraints; and (3) how can they autonomously generate appropriate spatial behavior to sustain conversations in constrained environments. To this end, this research will first focus on a formative study to better understand the effect of spatial constraints on group formations in HRI. This effort will result in a new public dataset of group-robot interactions that can be used to benchmark group detection approaches. The data will contribute to lowering barriers of entry to studying group human-robot interaction. Then, new methods for detecting spatial group formations in HRI will be developed by combining model-based and data-driven learning methods. Special consideration will be given to identifying groups in constrained environments. Finally, the project will investigate mechanisms to enable robots to take part in group formations under varying environmental spatial constraints. This last effort will help co-robots communicate with users and sustain group conversations by physically adapting to the environment. Together the outcomes of the project will help robots cope with the inherent complexity of multi-party interactions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marynel",
   "pi_last_name": "Vazquez",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marynel Vazquez",
   "pi_email_addr": "marynel.vazquez@yale.edu",
   "nsf_id": "000776516",
   "pi_start_date": "2019-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "AK Watson Hall",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499059.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-79b0e81a-7fff-ab68-bf04-ad133a3e97b0\"> </span></p>\r\n<p><span id=\"docs-internal-guid-e99e3f34-7fff-cb01-4faa-304c60b11110\"> </span></p>\r\n<p dir=\"ltr\"><span>When people engage in social interactions with other humans and robots, they stand and use space relative to other interactants such that they can observe them but also communicate effectively. The result are spatial patterns of behavior that naturally emerge during these interactions. People tend to position themselves at a distance from other social interactions occurring in the same space and thereby keep their current interaction separate from other nearby activities.</span><span><br /></span><span><br /></span><span>This project investigated spatial behavior in Human-Robot Interaction (HRI), particularly spatial patterns of behavior that emerge during conversations. Examples include circular spatial formations, side-by-side formations, etc. The research focused on four areas: 1) understanding how people perceive spatial patterns of behavior (e.g., as a function of the robot's physical embodiment); 2) building real-time computer simulations that could be used to facilitate research related to spatial behavior in HRI; 3) computationally modeling spatial patterns of behavior to identify conversational interactions and to generate suitable robot behavior subject to physical spatial constraints (such as those imposed by walls or furniture in an indoor environment); and 4) using human spatial behavior as an implicit feedback signal for how well a robot navigates with and around people in human environments.</span></p>\r\n<p dir=\"ltr\"><span>This project resulted in practical lessons for designing and experimentally validating robotic systems that reason about spatial behavior in HRI. First, our findings suggested that the way a robot looks can influence human perceptions of conversational groups. While it is common to assume that spatial patterns of behavior would be similar in human-human and human-robot conversations, factors like whether the body orientation of a robot is easily discernible can affect human perceptions of these spatial patterns in HRI. Second, we found that during human-robot interactions, a person's spatial behavior could provide information about how they perceive the navigation behavior of the robot they interact with. Thus, observed human spatial behavior can serve as an implicit feedback signal for improving a robots' behavior in the future. Third, we found that using simulations of human-robot interactions in experimental evaluations can result in nuanced differences with real-world experimental data -- which is the gold-standard in HRI. Despite this, simulations were key to accelerate the pace of our research, facilitate reproducibility, and benchmark methods. This made simulations a valuable tool for testing research ideas quickly and systematically measuring progress, especially when building computational models of spatial behavior in HRI.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>From a computational modeling perspective, our work was driven by two ideas: using graph neural networks to model spatial behavior; and combining these data-driven methods with model-based methods in HRI. Graph neural networks proved to be more effective for predicting social group interactions in comparison to prior methods across two benchmark datasets. We also demonstrated their use during live, real-world interactions (e.g., where a robot explained to users how it detects who is interacting with whom in their vicinity). This group perception capability is important for robots to act in socially appropriate ways, such as by avoiding interrupting ongoing conversations, or being aware and acknowledging all of its interactants in multi-party social settings.</span></p>\r\n<p dir=\"ltr\"><span>We successfully combined model-based approaches for reasoning about group formations in HRI with graph neural networks. Model-based methods consist of succinct mathematical expressions that describe spatial formations typically seen during conversations. While they are easy to interpret, their lack of expressivity can limit how well they capture spatial behavior. Data-driven methods, like graph neural networks, instead leverage data to learn to reason about spatial behavior. Because data-driven methods are more expressive, they seem like a natural choice for spatial reasoning in HRI. However, as shown by our work, data-driven methods need high-quality data to learn from, which is hard to obtain from real-world human-robot interactions. Thus, in this project, we leveraged the model-based approaches to generate data from which the data-driven methods could learn. We demonstrated the value of this approach in a pose generation problem, where a robot had to predict where it should position itself to join a conversational group interaction in an indoor environment.</span></p>\r\n<p dir=\"ltr\"><span>This project contributed findings and best practices for studying spatial behavior in HRI through a number of workshops, sharing of research findings with industry, and open software and data. The Social Environment for Autonomous Navigation, which this project helped develop, is now used by a variety of research groups in academia and industry.</span></p>\r\n<p dir=\"ltr\"><span>Our project has been instrumental in training a new generation of computer scientists and roboticists, including five doctoral students, and more than 14 undergraduate students.</span></p>\r\n<div><span><br /></span></div>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\">&nbsp;</p><br>\n<p>\n Last Modified: 01/30/2025<br>\nModified by: Marynel&nbsp;Vazquez</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738181560494_Screen_Shot_2025_01_29_at_2.56.24_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738181560494_Screen_Shot_2025_01_29_at_2.56.24_PM--rgov-800width.png\" title=\"Group detection\"><img src=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738181560494_Screen_Shot_2025_01_29_at_2.56.24_PM--rgov-66x44.png\" alt=\"Group detection\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Demonstration of DANTE algorithm for group detection</div>\n<div class=\"imageCredit\">Marynel Vazquez</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marynel&nbsp;Vazquez\n<div class=\"imageTitle\">Group detection</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738182298288_Screen_Shot_2025_01_29_at_3.23.46_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738182298288_Screen_Shot_2025_01_29_at_3.23.46_PM--rgov-800width.png\" title=\"Pose generation problem\"><img src=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738182298288_Screen_Shot_2025_01_29_at_3.23.46_PM--rgov-66x44.png\" alt=\"Pose generation problem\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In this project, we studied methods to solve the pose generation problem: Given the poses of interactants and a map of the environment, predict a pose for a robot in the group.</div>\n<div class=\"imageCredit\">Marynel Vazquez</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marynel&nbsp;Vazquez\n<div class=\"imageTitle\">Pose generation problem</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738182168530_Screen_Shot_2025_01_29_at_3.21.14_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738182168530_Screen_Shot_2025_01_29_at_3.21.14_PM--rgov-800width.png\" title=\"Simulated groups\"><img src=\"/por/images/Reports/POR/2025/1924802/1924802_10639394_1738182168530_Screen_Shot_2025_01_29_at_3.21.14_PM--rgov-66x44.png\" alt=\"Simulated groups\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We studied perceptions of group formations with varied robots. Images (a) show a group interaction with 5 members. Images (b) show a group interaction with 2 members.</div>\n<div class=\"imageCredit\">Marynel Vazquez</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marynel&nbsp;Vazquez\n<div class=\"imageTitle\">Simulated groups</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\n \r\n\n\nWhen people engage in social interactions with other humans and robots, they stand and use space relative to other interactants such that they can observe them but also communicate effectively. The result are spatial patterns of behavior that naturally emerge during these interactions. People tend to position themselves at a distance from other social interactions occurring in the same space and thereby keep their current interaction separate from other nearby activities.\n\nThis project investigated spatial behavior in Human-Robot Interaction (HRI), particularly spatial patterns of behavior that emerge during conversations. Examples include circular spatial formations, side-by-side formations, etc. The research focused on four areas: 1) understanding how people perceive spatial patterns of behavior (e.g., as a function of the robot's physical embodiment); 2) building real-time computer simulations that could be used to facilitate research related to spatial behavior in HRI; 3) computationally modeling spatial patterns of behavior to identify conversational interactions and to generate suitable robot behavior subject to physical spatial constraints (such as those imposed by walls or furniture in an indoor environment); and 4) using human spatial behavior as an implicit feedback signal for how well a robot navigates with and around people in human environments.\r\n\n\nThis project resulted in practical lessons for designing and experimentally validating robotic systems that reason about spatial behavior in HRI. First, our findings suggested that the way a robot looks can influence human perceptions of conversational groups. While it is common to assume that spatial patterns of behavior would be similar in human-human and human-robot conversations, factors like whether the body orientation of a robot is easily discernible can affect human perceptions of these spatial patterns in HRI. Second, we found that during human-robot interactions, a person's spatial behavior could provide information about how they perceive the navigation behavior of the robot they interact with. Thus, observed human spatial behavior can serve as an implicit feedback signal for improving a robots' behavior in the future. Third, we found that using simulations of human-robot interactions in experimental evaluations can result in nuanced differences with real-world experimental data -- which is the gold-standard in HRI. Despite this, simulations were key to accelerate the pace of our research, facilitate reproducibility, and benchmark methods. This made simulations a valuable tool for testing research ideas quickly and systematically measuring progress, especially when building computational models of spatial behavior in HRI.\r\n\n\nFrom a computational modeling perspective, our work was driven by two ideas: using graph neural networks to model spatial behavior; and combining these data-driven methods with model-based methods in HRI. Graph neural networks proved to be more effective for predicting social group interactions in comparison to prior methods across two benchmark datasets. We also demonstrated their use during live, real-world interactions (e.g., where a robot explained to users how it detects who is interacting with whom in their vicinity). This group perception capability is important for robots to act in socially appropriate ways, such as by avoiding interrupting ongoing conversations, or being aware and acknowledging all of its interactants in multi-party social settings.\r\n\n\nWe successfully combined model-based approaches for reasoning about group formations in HRI with graph neural networks. Model-based methods consist of succinct mathematical expressions that describe spatial formations typically seen during conversations. While they are easy to interpret, their lack of expressivity can limit how well they capture spatial behavior. Data-driven methods, like graph neural networks, instead leverage data to learn to reason about spatial behavior. Because data-driven methods are more expressive, they seem like a natural choice for spatial reasoning in HRI. However, as shown by our work, data-driven methods need high-quality data to learn from, which is hard to obtain from real-world human-robot interactions. Thus, in this project, we leveraged the model-based approaches to generate data from which the data-driven methods could learn. We demonstrated the value of this approach in a pose generation problem, where a robot had to predict where it should position itself to join a conversational group interaction in an indoor environment.\r\n\n\nThis project contributed findings and best practices for studying spatial behavior in HRI through a number of workshops, sharing of research findings with industry, and open software and data. The Social Environment for Autonomous Navigation, which this project helped develop, is now used by a variety of research groups in academia and industry.\r\n\n\nOur project has been instrumental in training a new generation of computer scientists and roboticists, including five doctoral students, and more than 14 undergraduate students.\r\n\n\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 01/30/2025\n\n\t\t\t\t\tSubmitted by: MarynelVazquez\n"
 }
}