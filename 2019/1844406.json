{
 "awd_id": "1844406",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Adaptive Physical Interfaces",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2025-12-31",
 "tot_intn_awd_amt": 525223.0,
 "awd_amount": 605223.0,
 "awd_min_amd_letter_date": "2019-02-07",
 "awd_max_amd_letter_date": "2024-03-18",
 "awd_abstract_narration": "Adaptive User Interfaces, i.e. interfaces that change their layout and functionality based on working context, capabilities, skills, and motivations of users can help to complete tasks faster and widen access for populations with diverse abilities. For instance, an adaptive website re-layouts its content when viewed on a phone rather than a laptop screen to preserve fast access to information, an adaptive menu re-orders its functions to display the ones that are frequently used in the most prominent location, and an adaptive webpage stylizer replaces color schemes that are hard to see for users with visual impairments with those that are easy to process. So far, adaptive user interfaces have mainly been explored for on-screen user interfaces, such as webpages and other digital applications running on desktop computers, laptops, and mobile phones. However, with computing moving into our environment and the increased availability of \"smart\" everyday objects with integrated sensing, it now becomes feasible to ask how the approach deployed for digital adaptive user interfaces can be applied to physical user interfaces, i.e. the physical objects we use in our everyday lives. For instance, consider the example of a child learning how to ride a bike with the help of training wheels: sensors integrated in the wheels can measure how well the child is balancing and determine the current learning progress, then motors can lift or lower the training wheels to increase or decrease the difficulty level according to the child's progress. The research team will study a range of application use cases in the areas of learning (e.g., adaptive writing aids, adaptive toys for child development), health and rehabilitation (e.g., adaptive walkers, canes, casts), and accessibility (e.g., adaptive tools for meal preparation, personal care, and recreation).   \r\n\r\nThe investigator will follow the research plan that was initially deployed for influential early work on digital adaptive user interfaces. In the first phase, application areas and use cases will be identified for physically adaptive tools. After clustering the application areas, the team will build exemplary prototypes for physically adaptive tools. After finishing the first round of prototypes, usability testing and evaluation will be conducted. For the user studies, the team will compare user performance between adaptive physical tools vs. their non-adaptive counterparts. Each study task will be designed to match the context of the tool (e.g., measuring learning gain, productivity increase, health improvement), but the main study design will be coherent across all tools. Using the insights gained from this investigation, the team will build a framework of design principles for building adaptive physical tools and propose a set of metrics for measuring their successful application. The framework will cover aspects, such as the adaptation time during interaction (i.e., the time for the tool to change its physical state), the adaptation range (i.e., the range actuators have to cover when changing the physical tool), and the adaptation integration (i.e., adding sensors and actuators in a way that does not interfere with the user interaction). Based on the framework of design principles, the team will develop a design tool that facilitates the development of adaptive physical tools. With the framework, design tool, and formalized study design in hand, the team will investigate specific application areas, such as adaptive learning, health and rehabilitation, and accessibility, in depth.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Mueller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Mueller",
   "pi_email_addr": "stefanie.mueller@MIT.EDU",
   "nsf_id": "000736812",
   "pi_start_date": "2019-02-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 114817.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 101819.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 120930.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 235657.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": null
}