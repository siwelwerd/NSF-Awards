{
 "awd_id": "1907381",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: Small: Ultra-Efficient Neural Network and LSTM Architectures",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2019-07-09",
 "awd_max_amd_letter_date": "2019-07-09",
 "awd_abstract_narration": "Neural networks (NNs) have begun to have widespread impact on various important applications, such as image recognition, speech recognition, and machine translation.  The spurt of interest in machine learning and artificial intelligence in this decade can be traced back to the increase in accuracy that NNs have enabled.  Yet, how to come up with the best NN architecture has remained an open problem.  Hence, it is attracting a lot of attention from the academia and industry. This work will address this problem.\r\n\r\nNN synthesis has largely been limited to big-data applications and the NN models are typically expected to run in the cloud.  However, there is recent interest from the industry to have edge-level (e.g., in smartphone or smartwatch) NN models.  The current edge-level NNs sacrifice accuracy (by 4-5%) for energy and latency efficiency.  NNs are also often not competitive with other models for medium-data and small-data applications.  Finally, sequence-to-sequence modeling (e.g., for language translation) also needs to be made much more accurate, fast, and compact enough for edge devices.  All these problems will be tackled in this work through new NN synthesis techniques and tools.\r\n\r\nThis research has the potential to enable transformative advances in overcoming the deficiencies of current NN synthesis methodologies. Due to the explosion in machine learning applications, this research has the promise to provide a significant boost to U.S. companies and economy. Thus, it will involve significant industrial engagements. Several underrepresented (minority/female) will be involved in the research. The research outcomes will be included in two undergraduate courses on Machine Learning and Embedded Computing. Broad dissemination to the academic and industrial communities will be achieved through published papers, posters, and seminars. Additionally, various tools and models will be distributed online.\r\n\r\nThe list of publications/students and tools/data with appropriate documentation will be made available at https://www.princeton.edu/~jha/. Free use of data and artifacts will be permitted for research and educational purposes. The data will be available online for at least five years following the completion of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Niraj",
   "pi_last_name": "Jha",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Niraj K Jha",
   "pi_email_addr": "jha@princeton.edu",
   "nsf_id": "000123477",
   "pi_start_date": "2019-07-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue, 2nd floor",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Neural networks (NNs) have become the workhorse of the Artificial Intelligence (AI) industry.&nbsp; A decade ago, the trend was to design large NNs trained on big data.&nbsp; However, soon it was realized that edge-friendly NNs could have much broader applications.&nbsp; These NNs have to be highly energy-, memory- and latency-efficient since many edge devices are battery-operated. Such devices include smartphones and smartwatches.&nbsp; This project was focused on the design and applications of such NNs.</p>\n<p>Many strong pillars are needed to support edge-friendly NNs.&nbsp; The main one being a tool that synthesizes energy/memory/latency-efficient NNs from given datasets.&nbsp; We have developed a tool, called SCANN, that results in NNs that are, on an average, two orders of magnitude smaller than traditional fully-connected NNs, yet at the same time boost the NN accuracy.&nbsp; SCANN introduced a grow-and-prune synthesis paradigm modeled after the human brain.&nbsp; Starting from a seed architecture, akin to a baby brain, it adds neurons and connections to the NN to significantly improve its accuracy, ending in an NN that is akin to a toddler brain.&nbsp; Finally, SCANN specializes this NN into an adult-brain-like architecture that performs very accurately on the given application, but is orders of magnitude smaller.&nbsp; We extended SCANN to a tool called CURIOUS to fully exploit the grow-and-prune synthesis paradigm.</p>\n<p>Edge applications typically do not have large training datasets.&nbsp; However, NNs are very data-hungry.&nbsp; Thus, another important pillar is a synthetic data generator that can sample unlimited synthetic data instances from the same probability distribution as the real dataset.&nbsp; We have developed one such tool, called TUTOR.&nbsp; It provides a significant boost to accuracy of NNs.&nbsp; In medical applications, it also ensures privacy when real data cannot be shared.&nbsp; Instead, synthetic data can be shared without sacrificing on the accuracy of the models trained with such data.</p>\n<p>A third pillar is finding out which data instances are incorrectly labeled.&nbsp; In supervised machine learning, it is extremely important that the labels be correct.&nbsp; Otherwise, prediction accuracy suffers.&nbsp; We developed a tool, called CTRL, that automatically determines which labels are wrong.&nbsp; Removing the data instances with wrong labels boosts the accuracy of NNs significantly.&nbsp;&nbsp; CTRL is applicable to both tabular and image-based datasets. Thus, it is widely applicable across diverse Internet-of-Things (IoT) applications. &nbsp;It is an important preprocessing step before NN training commences.</p>\n<p>Real-world datasets often have missing values. Since the dataset sizes are often small, it is not simply possible to delete all data instances that have any missing values.&nbsp; Thus, imputing these values becomes important.&nbsp; We have developed a tool, called DINI, that imputes missing values and obtains state-of-the-art results in terms of NN accuracy.</p>\n<p>Edge-friendly NNs make new smart healthcare applications possible.&nbsp; We have developed a framework, called DOCTOR, that produces a multi-headed NN.&nbsp; The body of the NN can be trained with wearable sensor data that can be obtained from sensors found on the smartwatch or smartphone and the different heads can detect different diseases.&nbsp; We have shown how DOCTOR can detect Covid-19, Type I/II diabetes, and mental health disorders (e.g., depression, bipolar disorder, and schizophrenia) through the multiple heads.&nbsp; This multi-headed NN is highly energy/memory/latency-efficient and can reside on a smartwatch.</p>\n<p>Convolutional neural networks (CNNs) are often used in image-based applications. These are often very large models and thus their inference latency is often not compatible with latency-sensitive applications.&nbsp; We have developed a dynamic inference method that skips many layers of the CNN depending on the image it encounters at inference time, without giving up on prediction accuracy.&nbsp;</p>\n<p>Four PhD students were trained in this exciting area of research.&nbsp; Their work resulted in several journal articles.&nbsp; The results were disseminated to the wider public through various invited talks. The students also did technology transfer through their summer internships at various companies.&nbsp; The frameworks were used in various course projects.&nbsp; Several tools/frameworks developed in this project are also being commercialized by a startup company.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/15/2023<br>\n\t\t\t\t\tModified by: Niraj&nbsp;K&nbsp;Jha</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNeural networks (NNs) have become the workhorse of the Artificial Intelligence (AI) industry.  A decade ago, the trend was to design large NNs trained on big data.  However, soon it was realized that edge-friendly NNs could have much broader applications.  These NNs have to be highly energy-, memory- and latency-efficient since many edge devices are battery-operated. Such devices include smartphones and smartwatches.  This project was focused on the design and applications of such NNs.\n\nMany strong pillars are needed to support edge-friendly NNs.  The main one being a tool that synthesizes energy/memory/latency-efficient NNs from given datasets.  We have developed a tool, called SCANN, that results in NNs that are, on an average, two orders of magnitude smaller than traditional fully-connected NNs, yet at the same time boost the NN accuracy.  SCANN introduced a grow-and-prune synthesis paradigm modeled after the human brain.  Starting from a seed architecture, akin to a baby brain, it adds neurons and connections to the NN to significantly improve its accuracy, ending in an NN that is akin to a toddler brain.  Finally, SCANN specializes this NN into an adult-brain-like architecture that performs very accurately on the given application, but is orders of magnitude smaller.  We extended SCANN to a tool called CURIOUS to fully exploit the grow-and-prune synthesis paradigm.\n\nEdge applications typically do not have large training datasets.  However, NNs are very data-hungry.  Thus, another important pillar is a synthetic data generator that can sample unlimited synthetic data instances from the same probability distribution as the real dataset.  We have developed one such tool, called TUTOR.  It provides a significant boost to accuracy of NNs.  In medical applications, it also ensures privacy when real data cannot be shared.  Instead, synthetic data can be shared without sacrificing on the accuracy of the models trained with such data.\n\nA third pillar is finding out which data instances are incorrectly labeled.  In supervised machine learning, it is extremely important that the labels be correct.  Otherwise, prediction accuracy suffers.  We developed a tool, called CTRL, that automatically determines which labels are wrong.  Removing the data instances with wrong labels boosts the accuracy of NNs significantly.   CTRL is applicable to both tabular and image-based datasets. Thus, it is widely applicable across diverse Internet-of-Things (IoT) applications.  It is an important preprocessing step before NN training commences.\n\nReal-world datasets often have missing values. Since the dataset sizes are often small, it is not simply possible to delete all data instances that have any missing values.  Thus, imputing these values becomes important.  We have developed a tool, called DINI, that imputes missing values and obtains state-of-the-art results in terms of NN accuracy.\n\nEdge-friendly NNs make new smart healthcare applications possible.  We have developed a framework, called DOCTOR, that produces a multi-headed NN.  The body of the NN can be trained with wearable sensor data that can be obtained from sensors found on the smartwatch or smartphone and the different heads can detect different diseases.  We have shown how DOCTOR can detect Covid-19, Type I/II diabetes, and mental health disorders (e.g., depression, bipolar disorder, and schizophrenia) through the multiple heads.  This multi-headed NN is highly energy/memory/latency-efficient and can reside on a smartwatch.\n\nConvolutional neural networks (CNNs) are often used in image-based applications. These are often very large models and thus their inference latency is often not compatible with latency-sensitive applications.  We have developed a dynamic inference method that skips many layers of the CNN depending on the image it encounters at inference time, without giving up on prediction accuracy. \n\nFour PhD students were trained in this exciting area of research.  Their work resulted in several journal articles.  The results were disseminated to the wider public through various invited talks. The students also did technology transfer through their summer internships at various companies.  The frameworks were used in various course projects.  Several tools/frameworks developed in this project are also being commercialized by a startup company.\n\n\t\t\t\t\tLast Modified: 10/15/2023\n\n\t\t\t\t\tSubmitted by: Niraj K Jha"
 }
}