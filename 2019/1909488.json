{
 "awd_id": "1909488",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: End to End Security-Oriented Optimization of Image Acquisition Pipelines",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 497800.0,
 "awd_amount": 505800.0,
 "awd_min_amd_letter_date": "2019-06-10",
 "awd_max_amd_letter_date": "2022-05-24",
 "awd_abstract_narration": "Ensuring integrity of digital images is one of the most challenging problems of our times. Increasing capabilities of digital media editing software has spawned a torrent of shocking manipulation examples, including the infamous DeepFakes which has been deemed a looming challenge for privacy, democracy, and national security. Many solutions proposed to-date have fallen short and can be rendered ineffective with very simple post-processing. In particular, strong compression applied by social networks and photo sharing services render existing authentication protocols unreliable. Moreover, increasing adoption of deep learning and computational photography in imaging processors in digital cameras creates new challenges even in native photo authentication. This project will use modern machine learning techniques to pursue security-oriented design of image acquisition and distribution workflows to ensure that image integrity and provenance can be assured, thereby addressing an emerging problem in this social-media driven world. \r\n\r\nThis project seeks to tackle: 1) optimization of the imaging pipeline to facilitate reliable forensic analysis in the most challenging conditions; 2) design of training protocols that generalize to various authentication problems; 3) optimization of the entire acquisition and distribution workflow, including both the imaging processor and lossy compression. Preliminary experiments indicate that by exploiting feedback from post-distribution forensic analysis, the imaging pipeline can be modified to facilitate content authentication. A neural imaging pipeline can learn to introduce imperceptible artifacts, akin to digital watermarks, which significantly increase manipulation detection accuracy, from 45% to over 90%. Further gains will result from the explorations on this project along the thrusts noted above.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nasir",
   "pi_last_name": "Memon",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Nasir D Memon",
   "pi_email_addr": "Nm1214@nyu.edu",
   "nsf_id": "000403287",
   "pi_start_date": "2019-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 497800.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Ensuring the integrity of digital images, video, and audio is one of today's most vital challenges. Photographs and videos, commonly used for documenting important events, require efficient and reliable authentication mechanisms. The increasing capabilities of digital media generation and editing techniques, alongside freely available tools, have led to shocking manipulation examples, such as the infamous DeepFakes, which pose significant risks to privacy, democracy, and national security. This societal concern has spurred a flurry of research in both academia and industry.</p>\n<p>Despite extensive research on deepfake detection, the broader community has not focused on proactive mechanisms to prevent deepfakes. One preventive approach is to intervene in the digital media creation process and embed signals that can help detect manipulation, where the absence of signals would indicate synthetic media created using generative AI.</p>\n<p><strong>Computational Sensor Fingerprints</strong>: Emerging intelligent vision sensors can embed a fingerprint in media right at the time of capture. This fingerprint can indicate the time and place of capture and any subsequent processing that might erase it, signifying significant manipulations.</p>\n<p>We explored computational sensor fingerprints, designing an end-to-end imaging system to embed and detect media authenticity. Our modeling toolbox includes imaging sensor simulation and extensive comparisons of computational and intrinsic sensor fingerprints. We optimized our approach to account for lossy compression, significantly improving detection performance and the detection vs. image distortion trade-off.</p>\n<p>We conducted a comprehensive security assessment and developed a threat modeling framework to define various attack vectors, focusing on fingerprint spoofing. We considered both white-box and black-box attacks, tracking metrics reflecting raw attack success rates, fingerprint similarity, and their utility in attacks.</p>\n<p>We evaluated two generic spoofing countermeasures. First, adversarial training models attackers during training and modifies objectives using defense penalties targeting resistance to adversarial inputs. Depending on the threat model, an attacker might control either the input image alone or both the image and the fingerprint. Our defenses targeted both scenarios, yielding promising results.</p>\n<p><strong>Real-Time Deepfakes</strong>: The rise of AI-enabled Real-Time Deepfakes (RTDFs) has enabled live video interactions where an imposter's face is replaced with their victim&rsquo;s. Existing deepfake detection techniques are asynchronous and unsuitable for RTDFs. We proposed a challenge-response approach for real-time authenticity verification, focusing on talking-head video interactions. Our taxonomy of challenges targets RTDF generation limitations, evaluated with a unique dataset of eight challenges, which visibly degrade the quality of state-of-the-art deepfake generators. Human and automated scoring confirmed our approach's effectiveness, achieving 88.6% and 80.1% accuracy, respectively. These findings highlight the potential of challenge-response systems for real-time deepfake detection in practical scenarios.</p>\n<p><strong>Real-Time Audio Deepfakes</strong>: Scammers increasingly use AI voice-cloning, especially Audio Real-Time Deepfakes (ARTDFs), to make interactive and believable phone calls. Existing speaker verification methods fall short against ARTDFs. We explored a challenge-response approach for detecting deepfake audio calls, developing a diverse taxonomy of audio challenges and evaluating state-of-the-art speech synthesizers against 21 challenge instances. We created a novel challenge dataset from 100 smartphone and desktop users, resulting in 18,600 original and 1.6 million deepfake samples. Our evaluations scored 86% and 80% accuracy in deepfake detection, respectively, with a set of 12 challenges. Combining human and machine evaluations, we designed a tagging system that improved human accuracy by 8.4%, underscoring the benefits of pre-screening calls with AI.</p>\n<p>In summary, this project has made significant advances in protecting digital media integrity, contributing to two novel approaches that had received limited attention. Both approaches proactively target fake media creation limitations. The first approach uses AI during media creation or capture to facilitate detection, while the second introduces CAPTCHA-like challenges in real-time audio and video interactions. These challenges exploit media manipulation limitations, introducing artifacts that humans can see but machines struggle to handle. We anticipate increased research in these directions, leading to better defenses against fake media.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/18/2024<br>\nModified by: Nasir&nbsp;D&nbsp;Memon</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nEnsuring the integrity of digital images, video, and audio is one of today's most vital challenges. Photographs and videos, commonly used for documenting important events, require efficient and reliable authentication mechanisms. The increasing capabilities of digital media generation and editing techniques, alongside freely available tools, have led to shocking manipulation examples, such as the infamous DeepFakes, which pose significant risks to privacy, democracy, and national security. This societal concern has spurred a flurry of research in both academia and industry.\n\n\nDespite extensive research on deepfake detection, the broader community has not focused on proactive mechanisms to prevent deepfakes. One preventive approach is to intervene in the digital media creation process and embed signals that can help detect manipulation, where the absence of signals would indicate synthetic media created using generative AI.\n\n\nComputational Sensor Fingerprints: Emerging intelligent vision sensors can embed a fingerprint in media right at the time of capture. This fingerprint can indicate the time and place of capture and any subsequent processing that might erase it, signifying significant manipulations.\n\n\nWe explored computational sensor fingerprints, designing an end-to-end imaging system to embed and detect media authenticity. Our modeling toolbox includes imaging sensor simulation and extensive comparisons of computational and intrinsic sensor fingerprints. We optimized our approach to account for lossy compression, significantly improving detection performance and the detection vs. image distortion trade-off.\n\n\nWe conducted a comprehensive security assessment and developed a threat modeling framework to define various attack vectors, focusing on fingerprint spoofing. We considered both white-box and black-box attacks, tracking metrics reflecting raw attack success rates, fingerprint similarity, and their utility in attacks.\n\n\nWe evaluated two generic spoofing countermeasures. First, adversarial training models attackers during training and modifies objectives using defense penalties targeting resistance to adversarial inputs. Depending on the threat model, an attacker might control either the input image alone or both the image and the fingerprint. Our defenses targeted both scenarios, yielding promising results.\n\n\nReal-Time Deepfakes: The rise of AI-enabled Real-Time Deepfakes (RTDFs) has enabled live video interactions where an imposter's face is replaced with their victims. Existing deepfake detection techniques are asynchronous and unsuitable for RTDFs. We proposed a challenge-response approach for real-time authenticity verification, focusing on talking-head video interactions. Our taxonomy of challenges targets RTDF generation limitations, evaluated with a unique dataset of eight challenges, which visibly degrade the quality of state-of-the-art deepfake generators. Human and automated scoring confirmed our approach's effectiveness, achieving 88.6% and 80.1% accuracy, respectively. These findings highlight the potential of challenge-response systems for real-time deepfake detection in practical scenarios.\n\n\nReal-Time Audio Deepfakes: Scammers increasingly use AI voice-cloning, especially Audio Real-Time Deepfakes (ARTDFs), to make interactive and believable phone calls. Existing speaker verification methods fall short against ARTDFs. We explored a challenge-response approach for detecting deepfake audio calls, developing a diverse taxonomy of audio challenges and evaluating state-of-the-art speech synthesizers against 21 challenge instances. We created a novel challenge dataset from 100 smartphone and desktop users, resulting in 18,600 original and 1.6 million deepfake samples. Our evaluations scored 86% and 80% accuracy in deepfake detection, respectively, with a set of 12 challenges. Combining human and machine evaluations, we designed a tagging system that improved human accuracy by 8.4%, underscoring the benefits of pre-screening calls with AI.\n\n\nIn summary, this project has made significant advances in protecting digital media integrity, contributing to two novel approaches that had received limited attention. Both approaches proactively target fake media creation limitations. The first approach uses AI during media creation or capture to facilitate detection, while the second introduces CAPTCHA-like challenges in real-time audio and video interactions. These challenges exploit media manipulation limitations, introducing artifacts that humans can see but machines struggle to handle. We anticipate increased research in these directions, leading to better defenses against fake media.\n\n\n\t\t\t\t\tLast Modified: 06/18/2024\n\n\t\t\t\t\tSubmitted by: NasirDMemon\n"
 }
}