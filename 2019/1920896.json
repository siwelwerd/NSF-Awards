{
 "awd_id": "1920896",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "RII Track-2 FEC: The Visual Experience Database: A Large-Scale Point-of-View Video Database for Vision Research",
 "cfda_num": "47.083",
 "org_code": "01060100",
 "po_phone": "7032927088",
 "po_email": "jcolom@nsf.gov",
 "po_sign_block_name": "Jose Colom",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 3974003.0,
 "awd_amount": 3974003.0,
 "awd_min_amd_letter_date": "2019-08-08",
 "awd_max_amd_letter_date": "2023-01-17",
 "awd_abstract_narration": "Current artificial intelligence (AI) systems that recognize visual content require millions of training examples to achieve good performance. However, the databases used to train such systems often take photos and videos from the internet, and thus do not represent the content that humans see on a daily basis. This introduces substantial biases into the AI systems that can have serious implications for AI-based applications such as self-driving cars. This project, a collaboration between Bates College, the University of Nevada, Reno, and North Dakota State University, Fargo will create the Visual Experience Database (VED), a database of over 240 hours of video shot from the perspective of a diverse set of observers engaged in common, everyday activities such as shopping, eating, or walking. Along with these videos, we will track each observer's head and eye position in order to understand how people look at the world, and how this changes with environment, age, and task. Our goal is to make this database open and accessible to all. Having the computer skills to use the database is key to accessibility, so we will be releasing a suite of software tools for using the database, as well as implement a summer workshop in basic computer programming skills to grow a workforce that is prepared for a variety of scientific occupations. By making the database open to the public, we will enable scientists, historians, and even artists to benefit from this rich resource. \r\n\r\nProgress in both human visual neuroscience and computer vision are limited by the availability of representative visual data. However, currently available image and movie databases are not representative of typical first-person visual experience. This project, a collaboration between Bates College, the University of Nevada, Reno, and North Dakota State University, Fargo, will create the Visual Experience Database (VED), a database of over 240 hours of first-person video complete with eye- and head-tracking. We will record from people of diverse ages (5-70 years) across three geographically distinct sites as they engage in common, everyday activities such as shopping, eating, or walking. With these data, we will be able to assess how observers sample their visual environments, and how gaze patterns change with environment, age, and task. Further, these data can be used as training data for next-generation computer vision systems. In order to develop a workforce with the skills necessary to work with big data, we will teach a Big Data Skills Summer Workshop to provide undergraduate and graduate students with the basic skills of computer programming and computational literacy to make contributions to this project and to prepare them for a variety of STEM occupations. The VED will be of broad use across several academic communities (cognitive science, neuroscience, computer vision, and possibly digital humanities and art). By creating a database that represents common, human experiences, we bypass the many biases of extant datasets, which will increase the efficacy of computer vision algorithms. By making these data fully open, we will enable advances in these fields to be accessible to all.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "O/D",
 "org_dir_long_name": "Office Of The Director",
 "div_abbr": "OIA",
 "org_div_long_name": "OIA-Office of Integrative Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michelle",
   "pi_last_name": "Greene",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Michelle R Greene",
   "pi_email_addr": "mgreene@barnard.edu",
   "nsf_id": "000742245",
   "pi_start_date": "2019-08-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Balas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin Balas",
   "pi_email_addr": "benjamin.balas@ndsu.edu",
   "nsf_id": "000597131",
   "pi_start_date": "2019-08-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "MacNeilage",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Paul MacNeilage",
   "pi_email_addr": "pmacneilage@unr.edu",
   "nsf_id": "000736991",
   "pi_start_date": "2019-08-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Lescroart",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Mark D Lescroart",
   "pi_email_addr": "mlescroart@unr.edu",
   "nsf_id": "000795229",
   "pi_start_date": "2019-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Bates College",
  "inst_street_address": "2 ANDREWS ROAD",
  "inst_street_address_2": "",
  "inst_city_name": "LEWISTON",
  "inst_state_code": "ME",
  "inst_state_name": "Maine",
  "inst_phone_num": "2077868375",
  "inst_zip_code": "042406030",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "ME02",
  "org_lgl_bus_name": "PRESIDENT AND TRUSTEES OF BATES COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "D77HU977E973"
 },
 "perf_inst": {
  "perf_inst_name": "Bates College",
  "perf_str_addr": "2 Andrews Road",
  "perf_city_name": "Lewiston",
  "perf_st_code": "ME",
  "perf_st_name": "Maine",
  "perf_zip_code": "042406030",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "ME02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "194Y00",
   "pgm_ele_name": "EPSCoR RII: Focused EPSCoR Col"
  },
  {
   "pgm_ele_code": "721700",
   "pgm_ele_name": "EPSCoR Research Infrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 2004695.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 970934.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 998374.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-46228096-7fff-1839-b1fc-c0868bc0e8c4\" style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We have created and released the Visual Experience Dataset (VED), a compilation of over 200 hours of egocentric video combined with gaze- and head-tracking data that offers an unprecedented view of the visual world as experienced by humans. The dataset was recorded by 44 observers ranging from 6 to 41 years old. </span><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: #ffffff; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Videos were captured with head-mounted cameras, in the community instead of in the laboratory, while participants engaged in common, everyday activities such as hiking, grocery shopping, or visiting a museum.</span><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> This footage is unique in that it includes tracking of each observer's head and eye positions, providing invaluable information on how people observe their environments and how these patterns vary with changes in the environment, age, and tasks.&nbsp;</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">These data can help us answer basic questions about active human vision. For example, while it is generally assumed that the human visual system tunes itself to environmental regularities, we have lacked egocentrically recorded activities that are representative of natural visual experiences. Further, these data will provide critical insights into how we deploy our attention via head and eye movements. Finally, these data may help reduce some of the biases in computer vision systems that are trained on internet-sourced photos and videos. This may have serious implications, especially for applications like self-driving cars.</span></p>\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We have centered openness and transparency in the VED. The VED is accessible through established open science platforms and is intended to be a living dataset with plans for expansion and community contributions. Moreover, the project emphasizes the democratization of data and skill development. We have trained over 40 undergraduate students in data collection, data management, and computer programming to create this dataset (66% of students were women or gender minorities, and 39% were students of color). We have deepened the training opportunities by training over 100 students in essential computer programming and computational literacy skills in the Big Data Summer School. This effort not only prepared students to help us meet the project&rsquo;s objectives but also prepares participants for a range of STEM careers.</span></p><br>\n<p>\n Last Modified: 11/29/2023<br>\nModified by: Michelle&nbsp;R&nbsp;Greene</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1920896/1920896_10631779_1701297296330_InterestingFrames--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1920896/1920896_10631779_1701297296330_InterestingFrames--rgov-800width.jpg\" title=\"VED Examples\"><img src=\"/por/images/Reports/POR/2023/1920896/1920896_10631779_1701297296330_InterestingFrames--rgov-66x44.jpg\" alt=\"VED Examples\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Representative frames within the VED. Top row: public activities; Second row: cooking; Third row: playing music; Bottom row: outdoor activities.</div>\n<div class=\"imageCredit\">Michelle Greene</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Michelle&nbsp;R&nbsp;Greene\n<div class=\"imageTitle\">VED Examples</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nWe have created and released the Visual Experience Dataset (VED), a compilation of over 200 hours of egocentric video combined with gaze- and head-tracking data that offers an unprecedented view of the visual world as experienced by humans. The dataset was recorded by 44 observers ranging from 6 to 41 years old. Videos were captured with head-mounted cameras, in the community instead of in the laboratory, while participants engaged in common, everyday activities such as hiking, grocery shopping, or visiting a museum. This footage is unique in that it includes tracking of each observer's head and eye positions, providing invaluable information on how people observe their environments and how these patterns vary with changes in the environment, age, and tasks.\n\n\nThese data can help us answer basic questions about active human vision. For example, while it is generally assumed that the human visual system tunes itself to environmental regularities, we have lacked egocentrically recorded activities that are representative of natural visual experiences. Further, these data will provide critical insights into how we deploy our attention via head and eye movements. Finally, these data may help reduce some of the biases in computer vision systems that are trained on internet-sourced photos and videos. This may have serious implications, especially for applications like self-driving cars.\n\n\nWe have centered openness and transparency in the VED. The VED is accessible through established open science platforms and is intended to be a living dataset with plans for expansion and community contributions. Moreover, the project emphasizes the democratization of data and skill development. We have trained over 40 undergraduate students in data collection, data management, and computer programming to create this dataset (66% of students were women or gender minorities, and 39% were students of color). We have deepened the training opportunities by training over 100 students in essential computer programming and computational literacy skills in the Big Data Summer School. This effort not only prepared students to help us meet the projects objectives but also prepares participants for a range of STEM careers.\t\t\t\t\tLast Modified: 11/29/2023\n\n\t\t\t\t\tSubmitted by: MichelleRGreene\n"
 }
}