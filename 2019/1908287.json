{
 "awd_id": "1908287",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF-BSF: RI: Small: Learning to plan safely",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 419877.0,
 "awd_amount": 419877.0,
 "awd_min_amd_letter_date": "2019-07-26",
 "awd_max_amd_letter_date": "2019-07-26",
 "awd_abstract_narration": "Robots and autonomous vehicles, in order to achieve the goals they are given, create plans that specify what actions to take. A major impediment to this kind of automated planning is that it requires a description of the environment in which the robot or autonomous vehicle operates, of sufficient fidelity to accurately predict the outcome of those actions. Such accurate descriptions of an environment are difficult to write by hand, and so machine learning techniques have been proposed to automatically construct such descriptions from observations of the environment. These methods to-date have generally not provided any guarantees for the accuracy of the learned description. This leaves open the possibility that the actions of the robot or autonomous vehicle could have unintended consequences. In particular, its actions could violate portions of the specified objectives that were intended to ensure its operation is safe. This project will develop methods for learning descriptions of environments that enable the automated planning methods to guarantee that the resulting plan meets the specified objectives. \r\n\r\nThe project investigates how a planning agent can improve its model of the world by using examples of plan executions to learn the effects of actions and when those actions should be taken.  The goal is for the agent to either guarantee that its operation is safe or to detect that the requested operation is impossible to guarantee. The research team will develop algorithms that use the provided observations to build a partial, approximate model of the environment and a meta-model that enables control of the safety and effectiveness of plans produced using the models. The team will then develop automated planning algorithms that use these learned models to produce plans with the desired guarantees. The project combines model-based and data-driven methods to generate \"safe\" plans.  When safe plans cannot be guaranteed, the project aims to quantify the probability that a generated plan is \"approximately safe\" using concepts from probably approximately correct (PAC) learning theory. The project will establish when it is possible to guarantee that plans will succeed, and will furthermore determine the effect of tolerating a small probability of failure in formulating plans for a broader range of circumstances. This research is expected to increase range of domains in which automated planning can be applied, including more situations in which world models are difficult to obtain and safety is a requirement.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brendan",
   "pi_last_name": "Juba",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Brendan A Juba",
   "pi_email_addr": "bjuba@wustl.edu",
   "nsf_id": "000672054",
   "pi_start_date": "2019-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "",
  "perf_city_name": "St. Louis",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "014Z",
   "pgm_ref_txt": "NSF and US-Israel Binational Science Fou"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 419877.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The primary focus of this project was to investigate the learning of domain models for high-level task planning using examples of previously-executed plans in the domain. These could be demonstrations of competent performance by a human domain expert, for example. We sought theoretical guarantees that executions in the model would faithfully capture the actual states encountered in the real-world execution of the plan. We refer to this as \"safe\" learning because the process does not require uninformed exploration by the learning algorithm, and because additional safety constraints may then also be enforced during planning with the model. We furthermore sought theoretical guarantees that, when formulating plans with the model, it would be possible to (nearly) match the capabilities demonstrated in the examples, that is, reach the same goal states as in the examples. We were able to show in many cases that such (approximately) \"complete\" safe learning is possible with a relatively small number of examples, and moreover using algorithms that feature running times bounded by a polynomial in the size of the descriptions of domain models.</p>\n<p>&nbsp;</p>\n<p>Previously, work on learning models of high-level task planning did not have theoretical guarantees, and our algorithms are the first to guarantee successful learning of the kinds of domains that are used as challenge problems in planning competitions (IPC) with a number of examples that scales polynomially with the description size of the domains. Moreover we found empirically that these existing methods indeed produce models that incorrectly predict what will happen when plans are executed in the actual environment. The methods we propose actually also were found to run faster and require fewer examples in practice than these existing methods, on the families of problems used in planning competitions.</p>\n<p>&nbsp;</p>\n<p>Actually, we were able to prove that some of the methods we developed are essentially optimal in terms of the number of examples required to guarantee a certain bound on the number of states we might not be able to reach in the model, while simultaneously guaranteeing safety. We identified certain parameters that must be bounded in order for the number of examples to scale polynomially with the number of attributes characterizing states in the domain: in particular, if actions in the domain produce some effects only under certain conditions, then the number of attributes that those conditions refer to must be bounded by a (small) constant, as the exponent of the polynomial bound on the number of examples required scales with that bound. Similarly, if we are learning models of multiple agents acting simultaneously, then the number of agents that simultaneously act on objects must also be bounded by a small constant. We were also able to characterize when learning is possible for various families of syntactic rules describing the effects of the actions and the necessary preconditions for the actions to be taken.These characterizations relate these two components of our domain model learning task to previously studied models of machine learning tasks, learning from positive examples and interpolation.</p>\n<p>&nbsp;</p>\n<p>Our project found methods that could learn domains that feature relational structure, i.e., are formulated in terms of properties that hold (or do not hold) between collections of objects in the domain; domains in which the actions have conditional effects; domains that have numerical (real-valued) state attributes; domains in which the effects are stochastic, and either independent across attributes or in which the actions have a limited number of distinct possible sets of outcomes; and domains in which some of the state attributes may not be observed in the examples. In each case, we identified some reasonable assumptions that allow efficient learning, assumptions that hold for the planning competition domains and other existing benchmark problem settings.</p>\n<p>&nbsp;</p>\n<p>We have publicly released code for our experiments that include implementations of the algorithms we developed: see&nbsp; https://github.com/hsle/sam-learning  and&nbsp;https://github.com/argaman-aloni/sam_learning We note that in  contrast to Reinforcement Learning methods (especially deep Reinforcement Learning), our methods -- given human  demonstrations -- have a vastly smaller sample complexity, yet feature  good theoretical guarantees that include guarantees of safe operation  and feasible running times. We believe that this positions our methods  as an attractive alternative to the widely attempted applications of  deep RL.</p>\n<p>&nbsp;</p>\n<p>Finally, the project supported several graduate students' doctoral research in Machine Learning and Artificial Intelligence. Furthermore, the basic methods and their analysis are particularly simple and suitable for classroom instruction. We plan to present them in the current offering of a graduate-level course on theoretical Artificial Intelligence and Machine Learning.</p><br>\n<p>\n Last Modified: 01/31/2024<br>\nModified by: Brendan&nbsp;A&nbsp;Juba</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe primary focus of this project was to investigate the learning of domain models for high-level task planning using examples of previously-executed plans in the domain. These could be demonstrations of competent performance by a human domain expert, for example. We sought theoretical guarantees that executions in the model would faithfully capture the actual states encountered in the real-world execution of the plan. We refer to this as \"safe\" learning because the process does not require uninformed exploration by the learning algorithm, and because additional safety constraints may then also be enforced during planning with the model. We furthermore sought theoretical guarantees that, when formulating plans with the model, it would be possible to (nearly) match the capabilities demonstrated in the examples, that is, reach the same goal states as in the examples. We were able to show in many cases that such (approximately) \"complete\" safe learning is possible with a relatively small number of examples, and moreover using algorithms that feature running times bounded by a polynomial in the size of the descriptions of domain models.\n\n\n\n\n\nPreviously, work on learning models of high-level task planning did not have theoretical guarantees, and our algorithms are the first to guarantee successful learning of the kinds of domains that are used as challenge problems in planning competitions (IPC) with a number of examples that scales polynomially with the description size of the domains. Moreover we found empirically that these existing methods indeed produce models that incorrectly predict what will happen when plans are executed in the actual environment. The methods we propose actually also were found to run faster and require fewer examples in practice than these existing methods, on the families of problems used in planning competitions.\n\n\n\n\n\nActually, we were able to prove that some of the methods we developed are essentially optimal in terms of the number of examples required to guarantee a certain bound on the number of states we might not be able to reach in the model, while simultaneously guaranteeing safety. We identified certain parameters that must be bounded in order for the number of examples to scale polynomially with the number of attributes characterizing states in the domain: in particular, if actions in the domain produce some effects only under certain conditions, then the number of attributes that those conditions refer to must be bounded by a (small) constant, as the exponent of the polynomial bound on the number of examples required scales with that bound. Similarly, if we are learning models of multiple agents acting simultaneously, then the number of agents that simultaneously act on objects must also be bounded by a small constant. We were also able to characterize when learning is possible for various families of syntactic rules describing the effects of the actions and the necessary preconditions for the actions to be taken.These characterizations relate these two components of our domain model learning task to previously studied models of machine learning tasks, learning from positive examples and interpolation.\n\n\n\n\n\nOur project found methods that could learn domains that feature relational structure, i.e., are formulated in terms of properties that hold (or do not hold) between collections of objects in the domain; domains in which the actions have conditional effects; domains that have numerical (real-valued) state attributes; domains in which the effects are stochastic, and either independent across attributes or in which the actions have a limited number of distinct possible sets of outcomes; and domains in which some of the state attributes may not be observed in the examples. In each case, we identified some reasonable assumptions that allow efficient learning, assumptions that hold for the planning competition domains and other existing benchmark problem settings.\n\n\n\n\n\nWe have publicly released code for our experiments that include implementations of the algorithms we developed: see https://github.com/hsle/sam-learning  andhttps://github.com/argaman-aloni/sam_learning We note that in  contrast to Reinforcement Learning methods (especially deep Reinforcement Learning), our methods -- given human  demonstrations -- have a vastly smaller sample complexity, yet feature  good theoretical guarantees that include guarantees of safe operation  and feasible running times. We believe that this positions our methods  as an attractive alternative to the widely attempted applications of  deep RL.\n\n\n\n\n\nFinally, the project supported several graduate students' doctoral research in Machine Learning and Artificial Intelligence. Furthermore, the basic methods and their analysis are particularly simple and suitable for classroom instruction. We plan to present them in the current offering of a graduate-level course on theoretical Artificial Intelligence and Machine Learning.\t\t\t\t\tLast Modified: 01/31/2024\n\n\t\t\t\t\tSubmitted by: BrendanAJuba\n"
 }
}