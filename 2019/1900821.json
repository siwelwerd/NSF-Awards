{
 "awd_id": "1900821",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: To Sense or Not to Sense: Energy Efficient Adaptive Sensing for Autonomous Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 1199962.0,
 "awd_amount": 1199962.0,
 "awd_min_amd_letter_date": "2019-09-09",
 "awd_max_amd_letter_date": "2019-09-09",
 "awd_abstract_narration": "Sensing and computation have been crucial to the significant progress in semi- and fully-autonomous vehicles and robots. Proliferation of many types of sensors (LIDARs, cameras, RADARs, etc.) and the advent of compute-heavy and data-hungry deep-learning approaches have increased the performance of autonomous systems by leaps and bounds. But the wide variety of sensors differ in terms of their performance, cost, and operational difficulty. Thus, specific sets of sensors are chosen for a particular task on a particular robot. This horses-for-courses approach often results in one-off systems that are incapable of adapting to many tasks or robots. Thus, to ensure safety and reliability, multi-tasking systems like autonomous vehicles have resorted to over-engineering, with upwards of 15 sensors and multiple GPUs/CPUs in any car. And, to make matters worse, many of the sensed data is eventually discarded as unwanted background. Thus, while the energy footprint of sensing and computations is increasing at an alarming rate, the flexibility or adaptability of these systems is still lacking. Much of this state of affairs can be attributed to the fact that sensors and algorithms face vastly different hardware and software challenges and are hence designed, developed, and manufactured in separate academic units or industries.  This project takes a different approach: adaptively sense mostly (if not only) quantities which help solve the task accurately and within the allotted time. In other words, this project advocates folding adaptive and flexible sensing within a learning framework for autonomous systems. This is achieved by co-design and co-execution of sensing and algorithms to maximize accuracy and flexibility while minimizing expended energy and cost. The approach is motivated by how humans decide what, where, when, and how to sense and apply that to a robot learning framework. Research and education are closely integrated in a diverse and inclusive environment.\r\n\r\nThe project consists of three fundamental research thrusts.  Thrust 1: Development of highly novel and fully adaptive design and physical realization of 3D optical sensors. This thrust includes a fundamental mathematical framework that determines the optimal set of emitted and measured rays to achieve a particular task at hand. This is the mathematical foundation for developing a new class of sensors that detect and characterize obstacles---a time critical task of any autonomous system---with maximum energy efficiency, minimal latency (i.e., near-instantly) and with virtually no separate computation. Thrust 2: Novel decision-making framework that efficiently controls the adaptive sensors for the task at hand. This includes determining where and when to sense and adapting behavior policies accordingly. Thrust 3: Support the robot learning framework by learning and interacting with humans.  The project will demonstrate the generality of adaptive sensing using three disparate autonomous systems that have broad societal impact: a) autonomous vehicles, b) assistive robots, and c) robots in manufacturing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Srinivasa",
   "pi_last_name": "Narasimhan",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Srinivasa G Narasimhan",
   "pi_email_addr": "srinivas@cs.cmu.edu",
   "nsf_id": "000149438",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Henny",
   "pi_last_name": "Admoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Henny Admoni",
   "pi_email_addr": "hadmoni@andrew.cmu.edu",
   "nsf_id": "000754294",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Held",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Held",
   "pi_email_addr": "dheld@andrew.cmu.edu",
   "nsf_id": "000762987",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "O'Toole",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew O'Toole",
   "pi_email_addr": "motoole2@andrew.cmu.edu",
   "nsf_id": "000788069",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1199962.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Sensing and computation have been crucial to the significant progress in semi- and fully-autonomous vehicles and robots. Proliferation of many types of sensors (LIDARs, cameras, RADARs, etc.) and the advent of compute-heavy and data-hungry deep-learning approaches have increased the performance of autonomous systems by leaps and bounds. But the wide variety of sensors differ in terms of their performance, cost and operational difficulty. Thus, specific sets of sensors are chosen for a particular task on a particular robot. This horses-for-courses approach often results in one-off systems that are incapable of adapting to many tasks or robots. At the same time, the energy footprint of sensing and computations is increasing at an alarming rate but the flexibility or adaptability of these systems is still lacking.</p>\n<p>This project takes a different approach: adaptively sense mostly (if not only) quantities which help solve the task accurately and within the allotted time. In other words, this project advocates folding adaptive and flexible sensing within a learning framework for autonomous systems. This is achieved by co-design and co-execution of sensing and algorithms to maximize accuracy and flexibility while minimizing expended energy and cost. The approach is motivated by how humans decide what, where, when, and how to sense and apply that to a robot learning framework.</p>\n<p>The major technical outcomes of the project include:</p>\n<ul>\n<li>Development of a class of novel&nbsp;and fully adaptive sensors, known as programmable light curtains, capable of efficiently sensing environments.</li>\n<li>Development of a decision-making framework for adapting light curtains for autonomous vehicles, assistive robots, and robots in manufacturing.&nbsp;&nbsp;</li>\n<li>Creating a technical framework for the sensor to learn and interact with humans.</li>\n<li>Developed an approach to guage the role of peripheral eye gaze dynamics on human attention during driving.</li>\n<li>Developed DReyeVR, an open-source VR based driving simulator platform designed with behavioural and interaction research priorities in mind.</li>\n</ul>\n<p>The major demonstrations with broader impacts include:</p>\n<ul>\n<li><em>Computationally efficient safety sensing for assistive technologies such as a wheelchair that navigates fully autonomously to help people with disabilities be more mobile.</em> In collaboration with Traffic 21 institute at Carnegie Mellon, which is funded by Department of Transportation, a wheelchair was retro-fitted with the programmable light curtain and camera that allowed the wheelchair to detect and avoid obstacles in order to autonomously reach a destination.</li>\n<li><em>Cost-effective safety sensing for better collaboration between humans and robots in a manufacturing or factory setting.</em> In collaboration with the Manufacturing Futures Institute at Carnegie Mellon University, funded by the RKMellon Foundation, the programmable light curtains were installed around robots to enable safe operations with humans collaborating for various tasks. Since the curtains are highly flexible, they can potentially be adapted to facilitate safe operation in any factory configuration and any task. This significantly reduces the cost of ensuring safety in manufacturing without sacrificing safety.</li>\n</ul>\n<p>As part of the grant, the research and education was conducted in an inclusive environment training many students and post-doctoral researchers. Multiple publications resulting from the project won Best Paper or Honorable Mention awards. A start up was created to focus on commercializing the sensing technologies developed under this grant.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/15/2023<br>\n\t\t\t\t\tModified by: Srinivasa&nbsp;G&nbsp;Narasimhan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404399365_Picture2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404399365_Picture2--rgov-800width.jpg\" title=\"Autonomous wheel-chair\"><img src=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404399365_Picture2--rgov-66x44.jpg\" alt=\"Autonomous wheel-chair\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A wheel-chair retro-fitted with the programmable light curtain sensor developed in this project used for safe navigation.</div>\n<div class=\"imageCredit\">Carnegie Mellon University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Srinivasa&nbsp;G&nbsp;Narasimhan</div>\n<div class=\"imageTitle\">Autonomous wheel-chair</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404643917_Picture3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404643917_Picture3--rgov-800width.jpg\" title=\"Assistive robotics for Safe Feeding\"><img src=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404643917_Picture3--rgov-66x44.jpg\" alt=\"Assistive robotics for Safe Feeding\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Holographic curtain around a person (mannequin in this photo) can allow for safe assistance from robots. The picture shows a robot feeding.</div>\n<div class=\"imageCredit\">Carnegie Mellon University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Srinivasa&nbsp;G&nbsp;Narasimhan</div>\n<div class=\"imageTitle\">Assistive robotics for Safe Feeding</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404977052_Picture4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404977052_Picture4--rgov-800width.jpg\" title=\"Navigation in smoky mines\"><img src=\"/por/images/Reports/POR/2023/1900821/1900821_10640819_1676404977052_Picture4--rgov-66x44.jpg\" alt=\"Navigation in smoky mines\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The programmable light curtains developed in this project can be used as an effective 3D sensor in smoky or dusty conditions making them suitable for mines with poor visibility.</div>\n<div class=\"imageCredit\">Carnegie Mellon University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Srinivasa&nbsp;G&nbsp;Narasimhan</div>\n<div class=\"imageTitle\">Navigation in smoky mines</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nSensing and computation have been crucial to the significant progress in semi- and fully-autonomous vehicles and robots. Proliferation of many types of sensors (LIDARs, cameras, RADARs, etc.) and the advent of compute-heavy and data-hungry deep-learning approaches have increased the performance of autonomous systems by leaps and bounds. But the wide variety of sensors differ in terms of their performance, cost and operational difficulty. Thus, specific sets of sensors are chosen for a particular task on a particular robot. This horses-for-courses approach often results in one-off systems that are incapable of adapting to many tasks or robots. At the same time, the energy footprint of sensing and computations is increasing at an alarming rate but the flexibility or adaptability of these systems is still lacking.\n\nThis project takes a different approach: adaptively sense mostly (if not only) quantities which help solve the task accurately and within the allotted time. In other words, this project advocates folding adaptive and flexible sensing within a learning framework for autonomous systems. This is achieved by co-design and co-execution of sensing and algorithms to maximize accuracy and flexibility while minimizing expended energy and cost. The approach is motivated by how humans decide what, where, when, and how to sense and apply that to a robot learning framework.\n\nThe major technical outcomes of the project include:\n\nDevelopment of a class of novel and fully adaptive sensors, known as programmable light curtains, capable of efficiently sensing environments.\nDevelopment of a decision-making framework for adapting light curtains for autonomous vehicles, assistive robots, and robots in manufacturing.  \nCreating a technical framework for the sensor to learn and interact with humans.\nDeveloped an approach to guage the role of peripheral eye gaze dynamics on human attention during driving.\nDeveloped DReyeVR, an open-source VR based driving simulator platform designed with behavioural and interaction research priorities in mind.\n\n\nThe major demonstrations with broader impacts include:\n\nComputationally efficient safety sensing for assistive technologies such as a wheelchair that navigates fully autonomously to help people with disabilities be more mobile. In collaboration with Traffic 21 institute at Carnegie Mellon, which is funded by Department of Transportation, a wheelchair was retro-fitted with the programmable light curtain and camera that allowed the wheelchair to detect and avoid obstacles in order to autonomously reach a destination.\nCost-effective safety sensing for better collaboration between humans and robots in a manufacturing or factory setting. In collaboration with the Manufacturing Futures Institute at Carnegie Mellon University, funded by the RKMellon Foundation, the programmable light curtains were installed around robots to enable safe operations with humans collaborating for various tasks. Since the curtains are highly flexible, they can potentially be adapted to facilitate safe operation in any factory configuration and any task. This significantly reduces the cost of ensuring safety in manufacturing without sacrificing safety.\n\n\nAs part of the grant, the research and education was conducted in an inclusive environment training many students and post-doctoral researchers. Multiple publications resulting from the project won Best Paper or Honorable Mention awards. A start up was created to focus on commercializing the sensing technologies developed under this grant.\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/15/2023\n\n\t\t\t\t\tSubmitted by: Srinivasa G Narasimhan"
 }
}