{
 "awd_id": "1941481",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: An Exaflop-hour simulation in AWS to advance Multi-messenger Astrophysics with IceCube",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 295000.0,
 "awd_amount": 295000.0,
 "awd_min_amd_letter_date": "2019-09-07",
 "awd_max_amd_letter_date": "2019-09-07",
 "awd_abstract_narration": "Exascale High Throughput Computing (HTC) demonstrates burst capability to advance multi-messenger astrophysics (MMA) with the IceCube detector. At peak, the equivalent of 1.2 Exaflops of 32 bit floating-point compute power is used. This is equivalent to approximately 3 times the scale of the #1 in the Top500 Supercomputer listing as of June 2019. In one hour, roughly 125 terabytes of input data is used to produce 250 terabytes of simulated data that is stored at the University of Wisconsin, Madison to be used to advance IceCube science. This data amounts to about 5% of the annual simulation data produced by the IceCube collaboration in 2018.\r\n\r\nThis demonstration tests and evaluates the ability of HTC-focused applications to effectively utilize availability bursts of Exascale-class resources to produce scientifically valuable output and explores the first 32-bit floating point Exaflop science application.  The application concerns IceCube simulations of photon propagation through the ice at its South Pole detector. IceCube is the pre-eminent neutrino experiment for the detection of cosmic neutrinos, and thus an essential part of the MMA program listed among the NSF's 10 Big Ideas.\r\n\r\nThe simulation capacity of the IceCube collaboration is significantly enhanced by efficiently harnessing the power of  short-notice Exascale processing capacity at leadership class High Performance Computing  systems and commercial clouds. Investigating these capabilities is important to facilitate time-critical follow-up studies in MMA, as well as increasing the overall annual capacity in aggregate by exploiting opportunities for short bursts.\r\n\r\nThe demonstration is powered primarily by Amazon Web Services (AWS) and takes place in the Fall of 2019 during the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC19) in Denver, Colorado. It is a collaboration between the IceCube Maintenance & Operations program and a diverse set of Cyberinfrastructure projects, including the Pacific Research Platform, the Open Science Grid, and HTCondor. By further collaborating with Internet2 and AWS, the experimental project also explores more generally, large high-bandwidth data flows in and out of AWS. The outcomes of this project will thus have broad applicability across a wide range of domains sciences, and scales, ranging from small colleges to national and international scale facilities.\r\n\r\nThis project is supported by the Office of Advanced Cyberinfrastructure in the Directorate for Computer & Information Science & Engineering and the Division of Physics in the Directorate of Mathematical and Physical Sciences.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Wuerthwein",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Wuerthwein",
   "pi_email_addr": "fkw@ucsd.edu",
   "nsf_id": "000144338",
   "pi_start_date": "2019-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "069Z",
   "pgm_ref_txt": "Windows on the Universe (WoU)"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 295000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored the scalability of cloud resources worldwide for use in science. We used simulations for the IceCube observatory at the south pole as our science application. We learned that the global cloud infrastructure across Amazon, Microsoft, and Google is a globallly distributed network of compute resources not that dissimilar than some of the globally distributed resources used for science. It thus can be used reasonably easily using the same tools.</p>\n<p>We found that there was a global capacity of roughly 50,000 NVIDIA GPUs available for sale worldwide during the two hour period we tried on a saturday morning pacific time zone in November 2019.</p>\n<p>In addition to absolute scale, we also explored data handling in and out of the cloud. Moving data into the cloud is free, whereas bringing data back from the cloud incurs charges. These charges can be substantial, in fact can be easily more expensive than the compute costs. Internet2 offers a cloud peering service for the US based cloud regions of all three providers that can substantially decrease these costs but not eliminate them. There is no corresponding service to reduce networking costs for international cloud regions. Applications that require significant data to come back from the cloud are thus restricted to US cloud regions for financial reasons. In addition, configuring the routing to reduce networking costs is non-trivial and different in all three cloud providers.&nbsp;</p>\n<p>All of our experiences were published in a set of conference papers to disseminate what we learned to the wider community.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/26/2022<br>\n\t\t\t\t\tModified by: Frank&nbsp;Wuerthwein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project explored the scalability of cloud resources worldwide for use in science. We used simulations for the IceCube observatory at the south pole as our science application. We learned that the global cloud infrastructure across Amazon, Microsoft, and Google is a globallly distributed network of compute resources not that dissimilar than some of the globally distributed resources used for science. It thus can be used reasonably easily using the same tools.\n\nWe found that there was a global capacity of roughly 50,000 NVIDIA GPUs available for sale worldwide during the two hour period we tried on a saturday morning pacific time zone in November 2019.\n\nIn addition to absolute scale, we also explored data handling in and out of the cloud. Moving data into the cloud is free, whereas bringing data back from the cloud incurs charges. These charges can be substantial, in fact can be easily more expensive than the compute costs. Internet2 offers a cloud peering service for the US based cloud regions of all three providers that can substantially decrease these costs but not eliminate them. There is no corresponding service to reduce networking costs for international cloud regions. Applications that require significant data to come back from the cloud are thus restricted to US cloud regions for financial reasons. In addition, configuring the routing to reduce networking costs is non-trivial and different in all three cloud providers. \n\nAll of our experiences were published in a set of conference papers to disseminate what we learned to the wider community.\n\n\t\t\t\t\tLast Modified: 02/26/2022\n\n\t\t\t\t\tSubmitted by: Frank Wuerthwein"
 }
}