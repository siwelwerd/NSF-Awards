{
 "awd_id": "1910319",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Automatic Creation of New Speech Sound Inventories",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 259765.0,
 "awd_amount": 259765.0,
 "awd_min_amd_letter_date": "2019-07-01",
 "awd_max_amd_letter_date": "2019-07-01",
 "awd_abstract_narration": "Speech technology is supposed to be available for everyone, but in reality, it is not.  There are 7000 languages spoken in the world, but speech technology (speech-to-text recognition and text-to-speech synthesis) only works in a few hundred of them. This project will solve that problem, by automatically figuring out the set of phonemes for each new language, that is, the set of speech sounds that define differences between words (for example, \"peek\" versus \"peck:\" long-E and short-E are distinct phonemes in English).  Phonemes are the link between speaking and writing.  A neural net that converts speech into text using some kind of phoneme inventory, and then back again, can be said to have used the correct phoneme inventory if its resynthesized speech always has the same meaning as the speech it started with.  This approach can even be tested in languages that don't have any standard written form, because the text doesn't have to be real text: it could be chat alphabet (the kind of pseudo-Roman-alphabet that speakers of Arabic and Hindi sometimes use on twitter), or it could even be a picture (showing, in an image, what the user was describing).  This research will make it possible for people to talk to their artificial intelligence systems (smart speakers, smart phones, smart cars, etc.) using their native languages.  This research will advance science by providing big-data tools that scientists can use to study languages that do not have a (standard) writing system.\r\n\r\nEnd-to-end neural network methods can be used to develop speech-to-text-to-speech (S2T2S) and other spoken language processing applications with little additional software infrastructure, and little background knowledge. In fact, toolkits provide recipes so that a researcher with no prior speech experience can train an end-to-end neural system after only a few hours of data preparation. End-to-end systems are only practical, however, for languages with thousands of hours of transcribed data. For under-resourced languages (languages with very little transcribed speech) cross-language adaptation is necessary; for unwritten languages (those lacking any standard and well-known orthographic convention), it is necessary to define a spoken language task that doesn't require writing before one can even attempt cross-language adaptation. Preliminary evidence suggests that both types of cross-language adaptation are performed more accurately if the system has available, or creates, a phoneme inventory for the under-resourced language, and leverages the phoneme inventory to facilitate adaptation. The aim of this project is to automatically infer the acoustic phoneme inventory for under-resourced and unwritten languages in order to maximize the speech technology quality of an end-to-end neural system adapted into that language. The research team has demonstrated that it is possible to visualize sub-categorical distinctions between sounds as a neural net adapts to a new phoneme category; proposed experiments 1 and 2 leverage visualizations of this type, along with other methods of phoneme inventory validation, to improve cross-language adaptation. Experiments 3 and 4 go one step further, by adapting to languages without orthography; for a speech technology system to be trained and used in a language without orthography, it must first learn a useful phoneme inventory. Innovations in this project that occur nowhere else include: (1) the use of articulatory feature transcription as a multi-task training criterion for an end-to-end neural system that seeks to learn the phoneme set of a new language, (2) the use of visualization error rate as a training criterion in multi-task learning -- this training criterion is based on a method recently developed to visualize the adaptation of phoneme categories in a neural network, (3) the application of cross-language adaptation to improve the error rates of image2speech applications in a language without orthography, (4) the use of non-standard orthography (chat alphabet) to transcribe speech in an unwritten language, and (5) the use of non-native transcription (mismatched crowdsourcing) to jump-start the speech2chat training task.  The methods proposed here will facilitate the scientific study of language, for example, by helping phoneticians to document the phoneme inventories of undocumented languages, thereby expediting the study of currently undocumented endangered languages before they disappear. Conversely, in minority languages with active but shrinking native speaker populations, planned methods will help develop end-to-end neural training methods with which the native speakers can easily develop new speech applications. All planned software will be packaged as recipes for the speech recognition virtual kitchen, permitting high school students and undergraduates with no speech expertise to develop systems for their own languages, and encouraging their interest in speech.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Hasegawa-Johnson",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Mark A Hasegawa-Johnson",
   "pi_email_addr": "jhasegaw@illinois.edu",
   "nsf_id": "000431210",
   "pi_start_date": "2019-07-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "Board of Trustees of the University of Illinois",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 259765.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In her memoir \"All the Fishes Come Home to Roost,\" Rachel Manija Brown describes trying to learn Hindi.&nbsp; Her teacher started with the first two syllables in the Hindi alphabet, which, to her, sounded identical: \"ka\" and \"ka.\"&nbsp; Unable to distinguish ka from ka, she never progressed to learn any of the rest of the language.&nbsp; Imagine you are an AI (Artificial Intelligence) tasked with learning a new language.&nbsp; Suppose, furthermore, that your human teacher doesn't want to write anything down -- perhaps she's too busy, or perhaps the language doesn't have any standard written form.&nbsp; The NSF grant \"RI: Small: Automatic Creation of New Phone Inventories\" asked the question: How can an AI learn a new language if its teachers don't write anything down?&nbsp; We developed five methods.<br /><br />(1) The AI might just listen to a lot of speech, and try to learn which acoustic patterns can be represented by a stable segment regardless of phonetic context.&nbsp; Our paper \"Unsupervised Speech Segmentation and Variable Rate Representation Learning using Segmental Contrastive Predictive Coding\" (Bhati, Villalba, &#379;elasko, Moro-Velazquez and Dehak) demonstrates that, by finding segments that can be summarized by a stable acoustic feature vector regardless of phonetic context, it's possible to find the phoneme boundary times with significantly improved accuracy.&nbsp; This kind of information is not sufficient, by itself, to learn the phoneme inventory of a new language, but it is sufficient to find the phoneme boundary times with greatly improved accuracy.<br /><br />(2) Instead of just listening, the AI could try to use its knowledge of articulatory features to guide its listening.&nbsp; In the example that started this paragraph, Rachel Brown failed to learn Hindi because she failed to learn that Hindi has two different types of \"ka,\" one that is more breathy than the other.&nbsp; Our paper \"Cross-lingual articulatory feature information transfer for speech recognition using recurrent progressive neural networks\" (Morshed and Hasegawa-Johnson) tests the idea that articulatory features can guide an AI as it learns a new language, but found that, by the time the AI has been given 80 hours of transcribed speech in the new language, cross-language transfer of articulatory features is not useful.<br /><br />(3) The AI can try to learn words using multimodal information, i.e., by asking human teachers to describe the contents of a few hundred photographs, and it can use the learned words to help it learn the phonemes of the language.&nbsp; Our paper \"Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition\" (Wang, Feng, Hasegawa-Johnson and Yoo) shows that multimodal learning teaches the AI about 54 percent of the phonemes in a new language.<br /><br />(4) The AI might behave like an adult second-language learner: It can assume that the phonemes of its first language are also phonemes of the second language.&nbsp; Our article \"Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition\" (Zelasko, Feng, Moro-Velazquez, Abavisani, Bhati, Scharenborg, Hasegawa-Johnson and Dehak) demonstrates that simply transferring phonemes from a set of known languages to an unknown language allows the AI to recognize about 67 percent of the phonemes in the unknown language.<br /><br />(5) Instead of just passively recognizing speech, the AI could try to speak.&nbsp; In our paper \"Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition\" (Ni, Wang, Gao, Qian, Zhang, Chang and Hasegawa-Johnson), the AI is given a database of speech, and a database of text, but with no idea which texts match each spoken signal; instead, it must learn to recognize the difference between speech that it generates correctly versus incorrectly by comparing its own pronunciations to those of native speakers.&nbsp; Our unsupervised text-to-speech system is able to generate speech in which about 88 percent of the words are intelligible.<br /><br />Combination of these methods can lead to further performance improvements, as demonstrated in further experiments that have not yet been published.&nbsp; Segmental-CPC (method 1) and articulatory-feature-prognets (method 2) are synergistic; the two methods, used together, outperform an unguided AI trained using 80 hours of transcribed speech in the target language.&nbsp; Segmental-CPC may also be synergistic with multimodal learning, because segmental-CPC significantly improves unsupervised detection of phoneme boundaries.&nbsp; Segmental-CPC has not yet been tested with a multimodal learner, but the use of ground truth phoneme boundaries permits a multimodal learner to learn 82 percent of the phonemes in a previously unknown language.&nbsp; Unsupervised TTS may have similar synergies with the other methods that have not yet been discovered.<br /><br />In summary, this grant has demonstrated five different methods that may be used to automatically learn the phoneme inventory in a previously unknown language.&nbsp; Significant advances in accuracy have been produced, across the board.&nbsp; None of these methods are yet ready for commercial products, but two of them (segmental CPC and unsupervised text-to-speech) may be close.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/02/2023<br>\n\t\t\t\t\tModified by: Mark&nbsp;A&nbsp;Hasegawa-Johnson</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1910319/1910319_10616664_1691024294660_zelasko2022graphicalastract--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1910319/1910319_10616664_1691024294660_zelasko2022graphicalastract--rgov-800width.jpg\" title=\"Graphical Abstract: Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition\"><img src=\"/por/images/Reports/POR/2023/1910319/1910319_10616664_1691024294660_zelasko2022graphicalastract--rgov-66x44.jpg\" alt=\"Graphical Abstract: Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">High-level workflow illustrating how a crosslingual (zero-shot) ASR is applied to phone inventory discovery of an unknown language.</div>\n<div class=\"imageCredit\">&#379;elasko, P., Feng, S., Velazquez, L. M., Abavisani, A., Bhati, S., Scharenborg, O., ... & Dehak, N. (2022). Discovering phonetic inventories with crosslingual automatic speech recognition. Computer Speech & Language, 74, 101358.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mark&nbsp;A&nbsp;Hasegawa-Johnson</div>\n<div class=\"imageTitle\">Graphical Abstract: Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn her memoir \"All the Fishes Come Home to Roost,\" Rachel Manija Brown describes trying to learn Hindi.  Her teacher started with the first two syllables in the Hindi alphabet, which, to her, sounded identical: \"ka\" and \"ka.\"  Unable to distinguish ka from ka, she never progressed to learn any of the rest of the language.  Imagine you are an AI (Artificial Intelligence) tasked with learning a new language.  Suppose, furthermore, that your human teacher doesn't want to write anything down -- perhaps she's too busy, or perhaps the language doesn't have any standard written form.  The NSF grant \"RI: Small: Automatic Creation of New Phone Inventories\" asked the question: How can an AI learn a new language if its teachers don't write anything down?  We developed five methods.\n\n(1) The AI might just listen to a lot of speech, and try to learn which acoustic patterns can be represented by a stable segment regardless of phonetic context.  Our paper \"Unsupervised Speech Segmentation and Variable Rate Representation Learning using Segmental Contrastive Predictive Coding\" (Bhati, Villalba, &#379;elasko, Moro-Velazquez and Dehak) demonstrates that, by finding segments that can be summarized by a stable acoustic feature vector regardless of phonetic context, it's possible to find the phoneme boundary times with significantly improved accuracy.  This kind of information is not sufficient, by itself, to learn the phoneme inventory of a new language, but it is sufficient to find the phoneme boundary times with greatly improved accuracy.\n\n(2) Instead of just listening, the AI could try to use its knowledge of articulatory features to guide its listening.  In the example that started this paragraph, Rachel Brown failed to learn Hindi because she failed to learn that Hindi has two different types of \"ka,\" one that is more breathy than the other.  Our paper \"Cross-lingual articulatory feature information transfer for speech recognition using recurrent progressive neural networks\" (Morshed and Hasegawa-Johnson) tests the idea that articulatory features can guide an AI as it learns a new language, but found that, by the time the AI has been given 80 hours of transcribed speech in the new language, cross-language transfer of articulatory features is not useful.\n\n(3) The AI can try to learn words using multimodal information, i.e., by asking human teachers to describe the contents of a few hundred photographs, and it can use the learned words to help it learn the phonemes of the language.  Our paper \"Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition\" (Wang, Feng, Hasegawa-Johnson and Yoo) shows that multimodal learning teaches the AI about 54 percent of the phonemes in a new language.\n\n(4) The AI might behave like an adult second-language learner: It can assume that the phonemes of its first language are also phonemes of the second language.  Our article \"Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition\" (Zelasko, Feng, Moro-Velazquez, Abavisani, Bhati, Scharenborg, Hasegawa-Johnson and Dehak) demonstrates that simply transferring phonemes from a set of known languages to an unknown language allows the AI to recognize about 67 percent of the phonemes in the unknown language.\n\n(5) Instead of just passively recognizing speech, the AI could try to speak.  In our paper \"Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition\" (Ni, Wang, Gao, Qian, Zhang, Chang and Hasegawa-Johnson), the AI is given a database of speech, and a database of text, but with no idea which texts match each spoken signal; instead, it must learn to recognize the difference between speech that it generates correctly versus incorrectly by comparing its own pronunciations to those of native speakers.  Our unsupervised text-to-speech system is able to generate speech in which about 88 percent of the words are intelligible.\n\nCombination of these methods can lead to further performance improvements, as demonstrated in further experiments that have not yet been published.  Segmental-CPC (method 1) and articulatory-feature-prognets (method 2) are synergistic; the two methods, used together, outperform an unguided AI trained using 80 hours of transcribed speech in the target language.  Segmental-CPC may also be synergistic with multimodal learning, because segmental-CPC significantly improves unsupervised detection of phoneme boundaries.  Segmental-CPC has not yet been tested with a multimodal learner, but the use of ground truth phoneme boundaries permits a multimodal learner to learn 82 percent of the phonemes in a previously unknown language.  Unsupervised TTS may have similar synergies with the other methods that have not yet been discovered.\n\nIn summary, this grant has demonstrated five different methods that may be used to automatically learn the phoneme inventory in a previously unknown language.  Significant advances in accuracy have been produced, across the board.  None of these methods are yet ready for commercial products, but two of them (segmental CPC and unsupervised text-to-speech) may be close.\n\n\t\t\t\t\tLast Modified: 08/02/2023\n\n\t\t\t\t\tSubmitted by: Mark A Hasegawa-Johnson"
 }
}