{
 "awd_id": "1911094",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: A Probabilistic Theory of Deep Learning via Spline Operators",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 542000.0,
 "awd_min_amd_letter_date": "2019-07-16",
 "awd_max_amd_letter_date": "2024-05-15",
 "awd_abstract_narration": "Deep learning has significantly advanced the ability to address a wide range of difficult machine perception tasks, such as recognizing objects from images, activities from videos, or text from speech. As a result, deep learning systems not only are playing a key role in emerging products and services, from conversational assistants to driverless vehicles, but they are also revolutionizing existing ones, from robotics to legal document analysis. Moreover, in the scientific realm, deep learning is enabling new ways to find patterns in large complicated datasets. This success is impressive, but a fundamental question remains: Why does deep learning work? Intuitions abound, but a coherent framework for understanding, analyzing, and designing deep learning architectures has remained elusive. This project will develop a theoretical foundation for deep learning systems by connecting them to classical and recent results from the signal processing, approximation theory, information theory, and statistics. A key goal is the development of new kinds of deep learning systems whose inner workings are explainable and interpretable. This project will have a range of impacts, from developing trustworthy, interpretable models and algorithms for mission-critical applications like autonomous navigation and decision making to advancing machine learning and signal processing education.\r\n\r\nThis project builds on an elegant connection between a wide class of deep (neural) networks based on piecewise-affine, convex nonlinearities and max-affine spline operators (MASOs). The research is organized around two interlocking themes. The first theme revolves around the extension of the MASO framework beyond piecewise-affine, convex nonlinearities by linking deterministic MASOs with probabilistic Gaussian mixture models. The extended, probabilistic MASO will enable the analysis of deep networks with more general nonlinearities than those that are piecewise-affine and convex, such as the sigmoid, hyperbolic tangent, and softmax. The second theme revolves around extending deterministic MASO deep networks to a new class of hierarchical, probabilistic, generative models that generalize the feedforward inference calculations and backpropagation learning of conventional deep networks to optimal Bayesian inference via a closed-form variational expectation-maximization (EM) algorithm. The probabilistic structure will enable the full arsenal of probability and statistics methodology to be applied to deep learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Baraniuk",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Richard G Baraniuk",
   "pi_email_addr": "richb@rice.edu",
   "nsf_id": "000334750",
   "pi_start_date": "2019-07-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 18000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ee183580-7fff-20ed-b49c-057be7150bcd\"> </span></p>\r\n<p><span id=\"docs-internal-guid-9e4cc9ff-7fff-bf92-3aa8-bcf14fbdaab1\">\r\n<p dir=\"ltr\"><span>We have developed a rich theoretical framework for characterizing the geometry of deep networks (DNs) based on piecewise-linear splines. Using this spline-based characterization of DNs, we developed novel approaches to designing architectures for multiple tasks. Here are three research highlights from our work:</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>First, we have exploited the continuous piecewise affine property of modern deep generative models to derive their posterior and marginal distributions as well as the latter's first moments. These findings have enabled us to derive an analytical Expectation-Maximization (EM) algorithm that enables gradient-free deep generative model learning. We have also demonstrated empirically that EM training of deep generative models produces greater likelihood than variational autoencoder training. Our findings will guide the design of new deep generative models that better approximate the true posterior of the data and open avenues to apply standard statistical tools for model comparison, anomaly detection, and missing data imputation.</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>Second, again leveraging the piecewise affine structure of deep networks, we have developed new theoretical results on matrix perturbation to shed light on the impact of architecture on performance. In particular, we have explained analytically what deep learning practitioners have long observed empirically: the parameters of some deep architectures (e.g., residual networks, ResNets, and Dense networks, DenseNets) are easier to optimize than others (e.g., convolutional networks, ConvNets). Our results not only shed new light on the impact of different nonlinear activation functions on a deep network&rsquo;s singular values but also pave the way for new more stable deep network designs.</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>Third, we have made significant progress towards understanding the training dynamics of DNs, most notably the phenomenon of grokking, in which extended training suddenly leads to emergent phenomena such as significantly improved robustness or generalization. The tessellation of a deep network&rsquo;s input space induced by its piecewise affine structure provides new ways and means to both visualize and characterize the evolution of a deep network&rsquo;s input-output mapping.</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>Overall, we have demonstrated how considering the underlying geometry of seemingly impenetrable architectures such as DNs can bring to light useful properties and indicators to improve performance and understanding of such models.</span></p>\r\n</span></p><br>\n<p>\n Last Modified: 02/07/2025<br>\nModified by: Richard&nbsp;G&nbsp;Baraniuk</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\n\r\n\n\nWe have developed a rich theoretical framework for characterizing the geometry of deep networks (DNs) based on piecewise-linear splines. Using this spline-based characterization of DNs, we developed novel approaches to designing architectures for multiple tasks. Here are three research highlights from our work:\r\n\n\r\n\n\nFirst, we have exploited the continuous piecewise affine property of modern deep generative models to derive their posterior and marginal distributions as well as the latter's first moments. These findings have enabled us to derive an analytical Expectation-Maximization (EM) algorithm that enables gradient-free deep generative model learning. We have also demonstrated empirically that EM training of deep generative models produces greater likelihood than variational autoencoder training. Our findings will guide the design of new deep generative models that better approximate the true posterior of the data and open avenues to apply standard statistical tools for model comparison, anomaly detection, and missing data imputation.\r\n\n\r\n\n\nSecond, again leveraging the piecewise affine structure of deep networks, we have developed new theoretical results on matrix perturbation to shed light on the impact of architecture on performance. In particular, we have explained analytically what deep learning practitioners have long observed empirically: the parameters of some deep architectures (e.g., residual networks, ResNets, and Dense networks, DenseNets) are easier to optimize than others (e.g., convolutional networks, ConvNets). Our results not only shed new light on the impact of different nonlinear activation functions on a deep networks singular values but also pave the way for new more stable deep network designs.\r\n\n\r\n\n\nThird, we have made significant progress towards understanding the training dynamics of DNs, most notably the phenomenon of grokking, in which extended training suddenly leads to emergent phenomena such as significantly improved robustness or generalization. The tessellation of a deep networks input space induced by its piecewise affine structure provides new ways and means to both visualize and characterize the evolution of a deep networks input-output mapping.\r\n\n\r\n\n\nOverall, we have demonstrated how considering the underlying geometry of seemingly impenetrable architectures such as DNs can bring to light useful properties and indicators to improve performance and understanding of such models.\r\n\t\t\t\t\tLast Modified: 02/07/2025\n\n\t\t\t\t\tSubmitted by: RichardGBaraniuk\n"
 }
}