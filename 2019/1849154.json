{
 "awd_id": "1849154",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: Uncertainty-Aware Safe Deep Reinforcement Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2019-04-01",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2019-03-25",
 "awd_max_amd_letter_date": "2019-03-25",
 "awd_abstract_narration": "Robot designers cannot always anticipate all real-world eventualities; hence robots will need to use algorithms that can learn to adapt to unexpected changes in their environment. However, if robots are to learn and adapt in the real world, they must adapt in ways that continue to maintain safety while learning.  This project develops methods that ensure that robots continue to be safe even as they learn and adapt to unexpected changes in their environment. By enabling robots to be cognizant of their uncertainty, these methods can help robots operate more safely. Further, the methods will enable robots to be adaptive to unforeseen changes in their environment. These methods will be applied to autonomous driving, to enable robots to operate safely on surfaces with different levels of friction or on uneven terrain that might otherwise be unsafe to operate on. Such methods will enable safer autonomous vehicles for city driving in poor weather conditions, as well as for operating in off-road settings for search and rescue operations, patrol vehicles to detect animal poachers, and for other applications.\r\n\r\nThis research develops a set of methods for uncertainty-aware safe robot learning. These methods will enable a robot to estimate the uncertainty of the effect of its actions; based on the estimated uncertainty, the robot will determine how to improve its performance while operating safely and cautiously. Furthermore, these methods will use the estimated uncertainty to determine how the robot should adapt its parameters to efficiently respond to environmental changes. The methods in this project will operate on complex policy classes such as those represented by neural networks trained with deep reinforcement learning. Such an approach will enable robots to achieve safe and adaptive long-term autonomy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Held",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Held",
   "pi_email_addr": "dheld@andrew.cmu.edu",
   "nsf_id": "000762987",
   "pi_start_date": "2019-03-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d38e9f00-7fff-89c8-0fa7-cb78c443c92d\"> <span id=\"docs-internal-guid-53ed377a-7fff-4c77-48e1-afdd254a786b\"> </span></span></p>\n<p dir=\"ltr\"><span>Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. This novel approach excels in its performance on challenging safety-constrained tasks, achieving significantly higher rewards and near-zero safety violations during inference.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, this work explores the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that senses only along a surface that the user selects. The central challenge is to automatically decide where to place the light curtain to accurately perform this task. This work proposes multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, this work combines these strategies using an online learning framework, using a self-supervised reward function. Further, this work also produces theoretical safety guarantees on the probability of detecting an obstacle using light curtains. This work develops a full stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance, leading to safe navigation.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Reinforcement learning (RL) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. One such class of policies selects actions for a fixed look-ahead horizon. This work investigates a novel instantiation of H-step lookahead with a learned model and provides a theoretical analysis of this method.&nbsp; The analysis suggests a tradeoff between model errors and value function errors and empirically demonstrates this tradeoff to be beneficial in safe reinforcement learning. This work shows the flexibility of the proposed method to incorporate safety constraints during deployment with a set of navigation environments.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The goal of offline reinforcement learning is to learn a policy from a fixed dataset, without further interactions with the environment. This setting will be an increasingly more important paradigm for real-world applications of reinforcement learning such as robotics, in which data collection is slow and potentially dangerous. The proposed method for off-policy reinforcement learning uses a latent action space to reduce extrapolation errors from out-of-distribution actions, leading to improved performance.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>This work allows robots to robustly adapt to changes in their environments.&nbsp; When the environment changes, the robot checks to see if it can still perform its tasks despite the environmental change.&nbsp; If not, the robot tries new actions to relearn to perform its task under the new conditions.&nbsp; This work develops the principles that allow a robot to determine how it should explore new actions in order to most effectively adapt to environmental changes.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>This work can allow robots to more robustly detect objects, while being aware of their uncertainty.&nbsp; Previous methods for teaching robots to detect objects in their environment often result in the robot outputting incorrect detections but with very high confidence.&nbsp; This method allows a robot to be aware of what it knows and what it doesn&rsquo;t know.&nbsp; When it outputs a detection of an object, it verifies that output.&nbsp; If the output does not pass the verification tests, the robot rejects the detection.&nbsp; Thus, the object detection system is aware of its own uncertainties.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Regarding broader impacts, the PI participated in CMU's AI4ALL summer school, a three-week program for underrepresented high school students from around the US who are interested in artificial intelligence.</span></p>\n<p dir=\"ltr\"><span>The PI organized a mentor-matching program, which matches undergraduate students from underrepresented groups with graduate student mentors. The PI advised a post-doc with a hearing disability. The PI is a mentor of the CMU's Tartan Scholars program, which supports first-year students from low-income backgrounds. The role involves meeting with students at least once a month to build a meaningful relationship, and aid the student to explore, grow, and develop toward their future goals.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\">&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/20/2023<br>\n\t\t\t\t\tModified by: David&nbsp;Held</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n  \nSafe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. This novel approach excels in its performance on challenging safety-constrained tasks, achieving significantly higher rewards and near-zero safety violations during inference.\n\n \nTo navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, this work explores the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that senses only along a surface that the user selects. The central challenge is to automatically decide where to place the light curtain to accurately perform this task. This work proposes multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, this work combines these strategies using an online learning framework, using a self-supervised reward function. Further, this work also produces theoretical safety guarantees on the probability of detecting an obstacle using light curtains. This work develops a full stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance, leading to safe navigation. \n\n \nReinforcement learning (RL) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. One such class of policies selects actions for a fixed look-ahead horizon. This work investigates a novel instantiation of H-step lookahead with a learned model and provides a theoretical analysis of this method.  The analysis suggests a tradeoff between model errors and value function errors and empirically demonstrates this tradeoff to be beneficial in safe reinforcement learning. This work shows the flexibility of the proposed method to incorporate safety constraints during deployment with a set of navigation environments.\n\n \nThe goal of offline reinforcement learning is to learn a policy from a fixed dataset, without further interactions with the environment. This setting will be an increasingly more important paradigm for real-world applications of reinforcement learning such as robotics, in which data collection is slow and potentially dangerous. The proposed method for off-policy reinforcement learning uses a latent action space to reduce extrapolation errors from out-of-distribution actions, leading to improved performance.\n\n \nThis work allows robots to robustly adapt to changes in their environments.  When the environment changes, the robot checks to see if it can still perform its tasks despite the environmental change.  If not, the robot tries new actions to relearn to perform its task under the new conditions.  This work develops the principles that allow a robot to determine how it should explore new actions in order to most effectively adapt to environmental changes.\n\n \nThis work can allow robots to more robustly detect objects, while being aware of their uncertainty.  Previous methods for teaching robots to detect objects in their environment often result in the robot outputting incorrect detections but with very high confidence.  This method allows a robot to be aware of what it knows and what it doesn\u2019t know.  When it outputs a detection of an object, it verifies that output.  If the output does not pass the verification tests, the robot rejects the detection.  Thus, the object detection system is aware of its own uncertainties.\n\n \nRegarding broader impacts, the PI participated in CMU's AI4ALL summer school, a three-week program for underrepresented high school students from around the US who are interested in artificial intelligence.\nThe PI organized a mentor-matching program, which matches undergraduate students from underrepresented groups with graduate student mentors. The PI advised a post-doc with a hearing disability. The PI is a mentor of the CMU's Tartan Scholars program, which supports first-year students from low-income backgrounds. The role involves meeting with students at least once a month to build a meaningful relationship, and aid the student to explore, grow, and develop toward their future goals.\n\n \n \n\n \n\n\t\t\t\t\tLast Modified: 07/20/2023\n\n\t\t\t\t\tSubmitted by: David Held"
 }
}