{
 "awd_id": "1907522",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Risk-Averse Control of Markov Systems with Model Uncertainty",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pedro Embid",
 "awd_eff_date": "2019-07-15",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 219996.0,
 "awd_amount": 219996.0,
 "awd_min_amd_letter_date": "2019-07-10",
 "awd_max_amd_letter_date": "2019-07-10",
 "awd_abstract_narration": "This project focuses on mathematical theory and computational methods of decision-making in systems that evolve randomly in time and whose essential characteristics are not precisely known to the observer. The research will address in a coherent way how to model risk in such systems and how to control them within the risk-averse paradigm. This will be accomplished by developing dynamic risk-assessment procedures, called risk filters, and by employing adaptive robust control techniques. The outcome of the project will directly advance and promote the progress of science and engineering, with potential applications in applied areas such as medical sciences, engineering, economics, finance, inventory management and insurance. Special attention will be given to popularizing the proposed research and its impact in these applied fields. In particular, this will be achieved through advising of graduate and undergraduate students, including students from underrepresented groups, presentations at popular, international and local forums, and dissemination of the results via scientific journal and book publications.\r\n\r\nThe classical theory and practice of Markov decision processes have proven to provide a powerful and successful toolkit for generating optimal or sub-optimal decision strategies in situations where the decision maker has access to adequately known (accurate) model of the underlying Markovian dynamical system, and acts so to optimize the expected cumulative cost or reward arising from the decision maker's actions. However, on the one hand, in many decision-making processes the decision maker needs to account for the trade-off between the cumulative award and cumulative risk of the decision. Risk-averse decision criteria underlying this research project and the theory of risk filters are ideally suited for such purposes. On the other hand, it is a typical situation in decision making processes that the model of the underlying Markovian dynamical system is not known exactly. Frequently, such model is a semi-adequate formalization of the underlying Markovian system, in the sense that the structural dynamical features of the system are modeled adequately, but precise knowledge of relevant model parameters is missing. In such cases, we say that the decision maker faces model uncertainty. Part of the proposed research will be devoted to develop methodologies that address this issue through adaptive robust stochastic control framework. Thus, the proposed research addresses in a coherent and novel way two important aspects of decision making in Markov systems: risk-averse decision criteria and model uncertainty. The theory of risk filters will be combined with the adaptive robust control methodology that will lead to novel dynamic programming equations, for which new numerical methods will be established.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrzej",
   "pi_last_name": "Ruszczynski",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrzej Ruszczynski",
   "pi_email_addr": "rusz@business.rutgers.edu",
   "nsf_id": "000195131",
   "pi_start_date": "2019-07-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University Newark",
  "inst_street_address": "123 WASHINGTON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9739720283",
  "inst_zip_code": "071023026",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NJ10",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "T3NGNR66YK89"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "100 Rockefellar Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548081",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 219996.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project was concerned with the control of stochastic dynamical systems, that is, systems that evolve in time as a result of the controller's actions. The focus was on Markov systems whose main characteristic is that one can identify their state at each time: a vector summarizing the effect of the past actions, and allowing us to predict the statistical distribution of the future evolution of the system. Systems of this nature occur in many applications in engineering,&nbsp; economics, and other areas.<br />Most of the theory of controlling Markov systems focuses on the expected value (average) criteria and aims at developing control rules that result in good average performance of the system. The focus of the project was on introducing risk to measure the performance of the system. Risk is broadly understood in the project as the possibility of occurrence of low probability but high cost scenarios. To this end, we advanced the theory of specialized performance criteria, called Markov risk measures, that can be effectively used to develop risk-averse control policies.&nbsp;<br />A particular form of risk occurs in systems whose models are not known exactly. In this case, in addition to the risk of an undesirable evolution of the system, a new risk factor is present: the risk of not knowing the correct model and thus undertaking suboptimal actions. In the project, we developed a new theory of dealing with such situations that is based on the new concept of a risk filter. The new methodology allows for a systematic approach to model uncertainty by exploiting the implied decomposition of risk into two interconnected components: the transition risk, and the model parameter risk. Thanks to this theory, risk-averse policies can be developed for uncertain models as well.<br />In a related line of research, we have considered a situation when the risk-averse policy is developed by simulation or real-time learning. We designed learning algorithms that represent the risk in the system as a function of selected state features, and improve the control policy using the learned representation. We also developed specialized stochastic learning algorithms for on-line risk optimization and we have demonstrated their efficacy in several machine learning applications.<br />We also considered the problem of measuring the distance between Markov systems. We have developed a new theory of distance measures&nbsp; between stochastic transition operators and we have designed methods to estimate the difference of risk evaluations due to the model error. We developed effective algorithms to construct simplified system models without introducing much error in the risk evaluation.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/12/2024<br>\nModified by: Andrzej&nbsp;Ruszczynski</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project was concerned with the control of stochastic dynamical systems, that is, systems that evolve in time as a result of the controller's actions. The focus was on Markov systems whose main characteristic is that one can identify their state at each time: a vector summarizing the effect of the past actions, and allowing us to predict the statistical distribution of the future evolution of the system. Systems of this nature occur in many applications in engineering, economics, and other areas.\nMost of the theory of controlling Markov systems focuses on the expected value (average) criteria and aims at developing control rules that result in good average performance of the system. The focus of the project was on introducing risk to measure the performance of the system. Risk is broadly understood in the project as the possibility of occurrence of low probability but high cost scenarios. To this end, we advanced the theory of specialized performance criteria, called Markov risk measures, that can be effectively used to develop risk-averse control policies.\nA particular form of risk occurs in systems whose models are not known exactly. In this case, in addition to the risk of an undesirable evolution of the system, a new risk factor is present: the risk of not knowing the correct model and thus undertaking suboptimal actions. In the project, we developed a new theory of dealing with such situations that is based on the new concept of a risk filter. The new methodology allows for a systematic approach to model uncertainty by exploiting the implied decomposition of risk into two interconnected components: the transition risk, and the model parameter risk. Thanks to this theory, risk-averse policies can be developed for uncertain models as well.\nIn a related line of research, we have considered a situation when the risk-averse policy is developed by simulation or real-time learning. We designed learning algorithms that represent the risk in the system as a function of selected state features, and improve the control policy using the learned representation. We also developed specialized stochastic learning algorithms for on-line risk optimization and we have demonstrated their efficacy in several machine learning applications.\nWe also considered the problem of measuring the distance between Markov systems. We have developed a new theory of distance measures between stochastic transition operators and we have designed methods to estimate the difference of risk evaluations due to the model error. We developed effective algorithms to construct simplified system models without introducing much error in the risk evaluation.\n\n\n\t\t\t\t\tLast Modified: 02/12/2024\n\n\t\t\t\t\tSubmitted by: AndrzejRuszczynski\n"
 }
}