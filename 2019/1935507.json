{
 "awd_id": "1935507",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "IIBR Multidisciplinary: Locating and counting terrestrial wildlife with an open source, automated acoustic survey platform",
 "cfda_num": "47.074",
 "org_code": "08080000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Robert Fleischmann",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 643272.0,
 "awd_amount": 643272.0,
 "awd_min_amd_letter_date": "2019-08-13",
 "awd_max_amd_letter_date": "2019-08-13",
 "awd_abstract_narration": "An award is made to the University of Pittsburgh to develop an automated acoustic platform for locating and counting terrestrial wildlife. Accurate estimates of wildlife populations are central to research on the effects of global change on biodiversity. Historically, population-level data has been time-consuming and difficult to collect, limiting the number of species and habitats that can be evaluated. Acoustic recorders have the potential to inexpensively gather large-scale data on sound-producing species, including birds, bats, amphibians, and insects. However, most current acoustic surveys are only able to detect the presence or absence of a species and cannot count individual organisms in order to estimate population sizes. This award will support the initial development of an automated, open source acoustic platform to survey terrestrial wildlife populations, with the goal of gathering data at larger scales and with better accuracy than human observers. It will also support the training of at least two graduate students and an educational project that targets undergraduate STEM educator professional development at the interface of biology and data science. The creation of this platform and its release under open source licenses will enable professional ecologists, citizen scientists, and large biodiversity monitoring programs to better detect declines in species populations over time, track shifts in species distributions due to climate change, understand the interacting drivers of biodiversity changes, predict future extinction risks, and develop conservation strategies to protect threatened species.\r\n\r\nThe acoustic survey platform will use GPS time-synchronized recorders to localize sounds in coordinate space. These sound locations will then be used to distinguish and count individual organisms within species. Although applicable to a wide variety of species, the platform will be initially designed for breeding songbirds of the eastern United States. The specific objectives of the award, involving both hardware and software development, include (1) designing an open source, inexpensive field recorder that can collect time-synchronized recordings, (2) developing and training an object-detecting convolutional neural network to identify the boundaries of distinct bird songs in time and frequency, (3) creating a detection-informed time difference of arrival algorithm that localizes each identified bird song in coordinate space and uses this data to count individual birds within species, and (4) completing performance and user testing to evaluate the integrated hardware and software platform. The platform will be evaluated specifically on its ability to estimate breeding bird populations more accurately and precisely than human observers.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "DBI",
 "org_div_long_name": "Division of Biological Infrastructure",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Justin",
   "pi_last_name": "Kitzes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Justin Kitzes",
   "pi_email_addr": "justin.kitzes@pitt.edu",
   "nsf_id": "000801728",
   "pi_start_date": "2019-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "University Club",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "084Y00",
   "pgm_ele_name": "Infrastructure Innovation for"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 643272.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Sensor-based methods for biodiversity surveys, including camera traps, acoustic recorders, and satellite imagery, are playing an increasingly important role in gathering large scale data on species populations. Automated acoustic surveys that combine field recorders with machine learning classifiers have demonstrated the potential to uncover new insights about sound-producing species such as birds, bats, amphibians, and insects. Most automated acoustic surveys today, however, have a critical shortcoming: they are only able to detect the presence or absence of a species, and cannot count individual organisms to generate population estimates. This shortcoming significantly limits the value of these new tools and methods for biodiversity research.</p>\n<p>In this project, we developed and tested an open source, automated acoustic hardware and software platform that can locate and count individual sound-producing organisms in the field. To meet this goal, we completed four specific objectives, focusing on temperate birds as a case study. First, we collaboratively designed, built, and tested an add-on to the popular AudioMoth acoustic recorder that can collect time-synchronized recordings, a feature that is needed to locate bird songs in space. Second, we developed an open source Python package (opensoundscape) that allows users to easily train convolutional neural network classifiers for bird song identification. Third, we developed a conceptual framework and codebase, also included in opensoundcape, that allows classified songs to be associated across recordings, localized in space, and used to count individual vocalizing organisms. Fourth, we completed a playback test in which we were able to locate more than half of recorded songs to within 5 m of true locations with 99% accuracy, as well as a ground-truth study in the field that showed strong correspondence between traditional human surveys and data generated by our platform. We shared our findings and methods with potential users of the platform through a variety of in person discussions and workshops.</p>\n<p>We believe that the development of this platform and its release under open source licenses will help to catalyze a shift in the manner in which biodiversity data are collected in the field, greatly expanding the availability of such data for ecology and conservation research. These new methods and tools will enable professional ecologists, citizen scientists, and large biodiversity monitoring programs to better detect declines in species populations over time, track shifts in species distributions due to climate change, understand the interacting drivers of observed changes in biodiversity, predict future extinction risks, and develop conservation strategies to protect threatened species. Beyond the applications of our platform to support research, this project directly supported the training of undergraduate, graduate, and postdoctoral researchers, several of whom presented their work at national conferences. We also shared our work through multiple training workshops that we delivered to audiences from universities, non-profits, and state and federal government agencies, which will&nbsp;expand the impacts of this work inside and outside of academia.</p><br>\n<p>\n Last Modified: 01/28/2024<br>\nModified by: Justin&nbsp;Kitzes</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nSensor-based methods for biodiversity surveys, including camera traps, acoustic recorders, and satellite imagery, are playing an increasingly important role in gathering large scale data on species populations. Automated acoustic surveys that combine field recorders with machine learning classifiers have demonstrated the potential to uncover new insights about sound-producing species such as birds, bats, amphibians, and insects. Most automated acoustic surveys today, however, have a critical shortcoming: they are only able to detect the presence or absence of a species, and cannot count individual organisms to generate population estimates. This shortcoming significantly limits the value of these new tools and methods for biodiversity research.\n\n\nIn this project, we developed and tested an open source, automated acoustic hardware and software platform that can locate and count individual sound-producing organisms in the field. To meet this goal, we completed four specific objectives, focusing on temperate birds as a case study. First, we collaboratively designed, built, and tested an add-on to the popular AudioMoth acoustic recorder that can collect time-synchronized recordings, a feature that is needed to locate bird songs in space. Second, we developed an open source Python package (opensoundscape) that allows users to easily train convolutional neural network classifiers for bird song identification. Third, we developed a conceptual framework and codebase, also included in opensoundcape, that allows classified songs to be associated across recordings, localized in space, and used to count individual vocalizing organisms. Fourth, we completed a playback test in which we were able to locate more than half of recorded songs to within 5 m of true locations with 99% accuracy, as well as a ground-truth study in the field that showed strong correspondence between traditional human surveys and data generated by our platform. We shared our findings and methods with potential users of the platform through a variety of in person discussions and workshops.\n\n\nWe believe that the development of this platform and its release under open source licenses will help to catalyze a shift in the manner in which biodiversity data are collected in the field, greatly expanding the availability of such data for ecology and conservation research. These new methods and tools will enable professional ecologists, citizen scientists, and large biodiversity monitoring programs to better detect declines in species populations over time, track shifts in species distributions due to climate change, understand the interacting drivers of observed changes in biodiversity, predict future extinction risks, and develop conservation strategies to protect threatened species. Beyond the applications of our platform to support research, this project directly supported the training of undergraduate, graduate, and postdoctoral researchers, several of whom presented their work at national conferences. We also shared our work through multiple training workshops that we delivered to audiences from universities, non-profits, and state and federal government agencies, which willexpand the impacts of this work inside and outside of academia.\t\t\t\t\tLast Modified: 01/28/2024\n\n\t\t\t\t\tSubmitted by: JustinKitzes\n"
 }
}