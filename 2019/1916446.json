{
 "awd_id": "1916446",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Theory and algorithms for computational sufficiency",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 120000.0,
 "awd_amount": 120000.0,
 "awd_min_amd_letter_date": "2019-07-30",
 "awd_max_amd_letter_date": "2019-07-30",
 "awd_abstract_narration": "The extraction of information from data is of fundamental importance. Central to this problem is defining \"information\", determining what in the data is most relevant and deciding which algorithms/procedures should be used.  Technological advances in data acquisition and storage have led to an over  abundance of data, which has only exacerbated the problem. The field of statistics has addressed these issues since its inception; the traditional point of view relies on stringent assumptions about the data and probabilistic models, but has ignored computational aspects. This project will develop a new theory and algorithms based on a concept called computational sufficiency. Rather than making stringent assumptions about the data and models, this concept assumes that many different and complementary procedures might be applied to the data. Then we consider the relevant information in the data to be that which is sufficient for obtaining the results of all the  procedures under consideration. In this way, the theory can (i) reveal hidden commonalities between procedures, (ii) identify meaningful reductions of the data, (iii) provide results that are robust to modeling assumptions, and (iv) allow us to exploit the reductions for efficient computation.\r\n\r\nStatistical sufficiency has been a foundational concept of mathematical statistics since the early 20th century. Implicit in its definition is the specification of a statistical model. However, actual data analysis does not always begin with the specification of a model, and it may not even make explicit use of a statistical model. The abundance of data and advances in computing have made it easier for the data analyst to abandon a single statistical model and instead consider multiple algorithmic models. Computational sufficiency defines information in the context of a collection of procedures that share a common input domain. The basic idea is very simple: we wish to find functions of the data that contain sufficient information for computing every procedure in the collection. In other words, what are the reduced summaries of the data that are sufficient to produce the same answers as would be obtained by applying the procedures to the whole data. This project will broaden the applicability of this theory and its depth. The results will lead to both theoretical insights and practical  computational advances in data analysis. In some cases, the theory will be  able to provide a conceptual link between seemingly unrelated methods and it will also provide a basis for inferences that are less dependent on  assumptions. The computational advances provided by this research can lead to  reduction of computational costs in applying multiple, advanced procedures by an order of magnitude, thus facilitating responsive and multifaceted analysis of large-scale data.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vincent",
   "pi_last_name": "Vu",
   "pi_mid_init": "Q",
   "pi_sufx_name": "",
   "pi_full_name": "Vincent Q Vu",
   "pi_email_addr": "vqv@stat.osu.edu",
   "nsf_id": "000514902",
   "pi_start_date": "2019-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 120000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>\n<div>\n<div><span>The extraction of information and the reduction of data are central concerns of statistics. This project aimed to develop theory and algorithms based on a concept called computational sufficiency that provides a new way to think about data reduction when considering many different possible procedures for analyzing data. The basic idea is very simple: we wish to find functions of the data that contain sufficient information for computing every procedure under consideration.</span></div>\n<br />\n<div><span>The classes of procedures that we studied in this project included matrix-variate and tensor structured estimators that are used for making inferences about relationships between variables in large data sets. These procedures are used in many applications, including genomics, neuroscience, and finance. We were able to find novel data reductions that could be applied to these procedures when the procedures employ sparsity or fusion penalties to encourage parsimonious estimates. This is often the case when the data are high-dimensional, and the procedures are used to estimate large covariance matrices or tensors. The reductions that we found provide both improved conceptual understanding of the role of the penalties in the procedures and computational advantages in terms of improvements to the algorithms for computing the estimates, as the reduction is much easier to compute than applying the procedure to the original data.</span></div>\n<br />\n<div><span>In addition to scientific discoveries, this project has also supported the training of two graduate students. These students contributed to the research in the project and this project also helped prepared the students with skills for successful careers in industry.</span></div>\n</div>\n</p><br>\n<p>\n Last Modified: 05/01/2024<br>\nModified by: Vincent&nbsp;Q&nbsp;Vu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\nThe extraction of information and the reduction of data are central concerns of statistics. This project aimed to develop theory and algorithms based on a concept called computational sufficiency that provides a new way to think about data reduction when considering many different possible procedures for analyzing data. The basic idea is very simple: we wish to find functions of the data that contain sufficient information for computing every procedure under consideration.\n\n\nThe classes of procedures that we studied in this project included matrix-variate and tensor structured estimators that are used for making inferences about relationships between variables in large data sets. These procedures are used in many applications, including genomics, neuroscience, and finance. We were able to find novel data reductions that could be applied to these procedures when the procedures employ sparsity or fusion penalties to encourage parsimonious estimates. This is often the case when the data are high-dimensional, and the procedures are used to estimate large covariance matrices or tensors. The reductions that we found provide both improved conceptual understanding of the role of the penalties in the procedures and computational advantages in terms of improvements to the algorithms for computing the estimates, as the reduction is much easier to compute than applying the procedure to the original data.\n\n\nIn addition to scientific discoveries, this project has also supported the training of two graduate students. These students contributed to the research in the project and this project also helped prepared the students with skills for successful careers in industry.\n\n\t\t\t\t\tLast Modified: 05/01/2024\n\n\t\t\t\t\tSubmitted by: VincentQVu\n"
 }
}