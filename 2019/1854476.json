{
 "awd_id": "1854476",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Distributed Algorithms for Topic Models with Applications to Streaming Document Data and Cancer Genomics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 350000.0,
 "awd_amount": 350000.0,
 "awd_min_amd_letter_date": "2019-08-13",
 "awd_max_amd_letter_date": "2023-02-10",
 "awd_abstract_narration": "There is a growing need for methods that analyze and organize large collections of electronic information, for example a collection of papers on some scientific field, or on some medical question, or a collection of news articles in the New York Times.  Traditional keyword-based searches are very fast but have important deficiencies.  Suppose we are interested in searching for articles that deal with heart attacks.  A search using the keywords \"heart attack\" will not return articles that use \"myocardial infarction\", the medical term for \"heart attack\".  A newer and more powerful approach to the analysis and organization of large collections of electronic documents and for document retrieval is through the use of so-called topic models.  These models work by identifying the hidden topics in the collection (in the New York Times example, these might be Sports, World Events, Politics, etc.)  and by also identifying the topics that each document deals with.  By far the most commonly used topic model is the so-called Latent Dirichlet Allocation (LDA) model.  Unfortunately, all accurate implementations are slow: the algorithms list all the words in all the documents in some order, and then carry out a calculation for each word.  This is done sequentially: the calculation for a given word cannot be carried out before the calculations for all previous words have been completed.  This project will develop a class of algorithms that work in parallel, taking advantage of the massive distributed computation that is now available on multi-core platforms.  Thus, for example, if 1000 processors are available, these algorithms work 1000 times faster than existing algorithms.  These new algorithms will enable the use of the LDA model on very large collections of documents.  Topics can be used to cluster documents into groups. However, at its core, LDA is a \"multi membership model\": a New York Times article about NFL football players kneeling at the national anthem belongs in the Sports section, and also in the Politics section.  Multi-membership models arise in areas other than document retrieval and classification.  For example, in cancer genomics, tumors can be potentially classified as members of several cancer subtypes.  This project will develop other multi-membership models, designed to handle non-textual data, as in the cancer genomics example above, and it will develop parallel algorithms to handle these models.  The output of this project will enable researchers to handle massive collections of documents and medical data.\r\n\r\nLDA and other multi-membership models are inherently Bayesian models, in which topics and topic memberships for each document are unknown parameters.  Posterior distributions are generally estimated by Markov chain Monte Carlo, which has proven convergence guarantees.  This project will develop grouped Gibbs samplers which update variables in groups, where all the variables within a group can be updated simultaneously.  The project will also develop practical convergence diagnostics and also theoretical results on the rates of convergence of the new algorithms.  The theoretical results will enable the user to determine how long the Markov chains need to be run in order to provide a required level of accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hani",
   "pi_last_name": "Doss",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Hani J Doss",
   "pi_email_addr": "doss@stat.ufl.edu",
   "nsf_id": "000183601",
   "pi_start_date": "2019-08-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Michailidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "George Michailidis",
   "pi_email_addr": "gmichail@ucla.edu",
   "nsf_id": "000487374",
   "pi_start_date": "2019-08-13",
   "pi_end_date": "2023-02-10"
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "207 Grinter Hall",
  "perf_city_name": "Gainesville",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326112002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 350000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Topic models are used in many fields in science and engineering.&nbsp; In<br />medicine, for example, in a search for all documents regarding lung<br />removal after a cancer diagnosis, a search using the keywords \"lung<br />removal\" will not return documents in which lung removal is referred<br />to by its medical name of pneumonectomy, but a search by the topic<br />\"lung removal\" will return such articles.&nbsp; Keyword searches are<br />extremely fast, but have the limitation mentioned above.&nbsp; On the<br />other hand, if we use a topic model to do the search then we will<br />not have the limitation mentioned above.&nbsp; In a body of documents to<br />be searched, each document has themes, or topics.&nbsp; For example, in a<br />medical corpus of documents, for some documents the topic may be<br />\"cancer\", for others it may be \"heart attacks\", and for others still<br />it may be \"diabetes\", and there may be more than one topic per<br />document.&nbsp; A topic model aims, among other things, to uncover these<br />themes, and does so without any human intervention.&nbsp; Unsurprisingly,<br />there is a price to pay for the advantage conferred by topic models:<br />the computational burden is far greater.<br /><br />By far the most widely-used topic model is latent Dirichlet<br />allocation.&nbsp; There are currently two general computational methods<br />for implementing the model.&nbsp; One uses so-called variational<br />inference.&nbsp; This is an iterative method.&nbsp; It is very fast, but as<br />the number of iterations goes to infinity, the estimate it produces<br />does not converge to the true and correct answer.&nbsp; The other is<br />Markov chain Monte Carlo (MCMC).&nbsp; This is also an iterative scheme,<br />but in contrast to methods based on variational inference, as the<br />number of iterations goes to infinity, the estimate it produces do<br />in fact converge to the true and correct answer.&nbsp; In current<br />versions of MCMC, a single iteration runs through each word in each<br />document, sequentially, one word at a time, and a word cannot be<br />processed until all the previous words have been processed.&nbsp; For the<br />method to work, a large number of iterations have to be performed.<br />Therefore, MCMC is slow, and can handle only modest-sized document<br />corpora.<br /><br />We have developed an implementation of MCMC in which calculations<br />can be done in parallel, if many processors are available.&nbsp; Thus, on<br />a multi-core platform with 1000 processors, our implementation runs<br />(approximately) 1000 times faster than the current implementations<br />of MCMC.&nbsp; The result is that our implementation can be carried out<br />for a large number of iterations, therefore giving answers that are<br />very close to the true and correct answer, and can do so even for<br />very large document corpora.<br /><br />The results we have obtained in this project are as follows.&nbsp; (1) We<br />have developed the parallel MCMC method.&nbsp; (2) We have shown that the<br />Markov chain is \"uniformly ergodic,\" meaning in non-technical terms<br />that as the number of iterations go to infinity, the answers that it<br />provides converge to the true and correct answers very quickly.&nbsp; (3)<br />We have done extensive empirical work comparing our method with<br />state-of-the-art procedures based on variational inference and with<br />state-of-the-art procedures based on MCMC, and this work has shown<br />that our method outperforms all of these.&nbsp; (4) The computer code for<br />implementation of our methodology was made publicly available in the<br />open-source, free statistical programming language R.<br /><br />Two graduate students worked on the project, and they were involved<br />in all of its aspects.&nbsp; These students have benefited by learning<br />about some cutting edge areas of machine learning, namely computer<br />searches and topic models, and cutting edge techniques in<br />statistics, specifically Monte Carlo methods.</p><br>\n<p>\n Last Modified: 11/16/2023<br>\nModified by: Hani&nbsp;J&nbsp;Doss</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTopic models are used in many fields in science and engineering. In\nmedicine, for example, in a search for all documents regarding lung\nremoval after a cancer diagnosis, a search using the keywords \"lung\nremoval\" will not return documents in which lung removal is referred\nto by its medical name of pneumonectomy, but a search by the topic\n\"lung removal\" will return such articles. Keyword searches are\nextremely fast, but have the limitation mentioned above. On the\nother hand, if we use a topic model to do the search then we will\nnot have the limitation mentioned above. In a body of documents to\nbe searched, each document has themes, or topics. For example, in a\nmedical corpus of documents, for some documents the topic may be\n\"cancer\", for others it may be \"heart attacks\", and for others still\nit may be \"diabetes\", and there may be more than one topic per\ndocument. A topic model aims, among other things, to uncover these\nthemes, and does so without any human intervention. Unsurprisingly,\nthere is a price to pay for the advantage conferred by topic models:\nthe computational burden is far greater.\n\nBy far the most widely-used topic model is latent Dirichlet\nallocation. There are currently two general computational methods\nfor implementing the model. One uses so-called variational\ninference. This is an iterative method. It is very fast, but as\nthe number of iterations goes to infinity, the estimate it produces\ndoes not converge to the true and correct answer. The other is\nMarkov chain Monte Carlo (MCMC). This is also an iterative scheme,\nbut in contrast to methods based on variational inference, as the\nnumber of iterations goes to infinity, the estimate it produces do\nin fact converge to the true and correct answer. In current\nversions of MCMC, a single iteration runs through each word in each\ndocument, sequentially, one word at a time, and a word cannot be\nprocessed until all the previous words have been processed. For the\nmethod to work, a large number of iterations have to be performed.\nTherefore, MCMC is slow, and can handle only modest-sized document\ncorpora.\n\nWe have developed an implementation of MCMC in which calculations\ncan be done in parallel, if many processors are available. Thus, on\na multi-core platform with 1000 processors, our implementation runs\n(approximately) 1000 times faster than the current implementations\nof MCMC. The result is that our implementation can be carried out\nfor a large number of iterations, therefore giving answers that are\nvery close to the true and correct answer, and can do so even for\nvery large document corpora.\n\nThe results we have obtained in this project are as follows. (1) We\nhave developed the parallel MCMC method. (2) We have shown that the\nMarkov chain is \"uniformly ergodic,\" meaning in non-technical terms\nthat as the number of iterations go to infinity, the answers that it\nprovides converge to the true and correct answers very quickly. (3)\nWe have done extensive empirical work comparing our method with\nstate-of-the-art procedures based on variational inference and with\nstate-of-the-art procedures based on MCMC, and this work has shown\nthat our method outperforms all of these. (4) The computer code for\nimplementation of our methodology was made publicly available in the\nopen-source, free statistical programming language R.\n\nTwo graduate students worked on the project, and they were involved\nin all of its aspects. These students have benefited by learning\nabout some cutting edge areas of machine learning, namely computer\nsearches and topic models, and cutting edge techniques in\nstatistics, specifically Monte Carlo methods.\t\t\t\t\tLast Modified: 11/16/2023\n\n\t\t\t\t\tSubmitted by: HaniJDoss\n"
 }
}