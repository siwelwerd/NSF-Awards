{
 "awd_id": "1937134",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Convergence Accelerator Phase I (RAISE): Scalable Knowledge Network to Enable Intelligent Textbooks",
 "cfda_num": "47.084",
 "org_code": "15020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lara Campbell",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2019-09-10",
 "awd_max_amd_letter_date": "2019-09-10",
 "awd_abstract_narration": "The NSF Convergence Accelerator supports team-based, multidisciplinary efforts that address challenges of national importance and show potential for deliverables in the near future. \r\n\r\nProperly educating the STEM leaders of tomorrow requires moving beyond knowledge being transmitted from teacher to learner via paper textbooks and lectures. The broader impact and potential social benefit of this Convergence Accelerator Phase 1 project is to launch the Textbook Open Knowledge Network (TOKN). The TOKN project will be executed by a multidisciplinary team of cognitive scientists, machine learning and artificial intelligence engineers, and educators from Rice University's OpenStax and Stanford University who are committed to creating a library of intelligent textbooks, engaging more partners as the project progresses. To begin, this project will build knowledge graphs that capture the complex relationships between educational concepts. A knowledge graph enables artificial intelligence algorithms to provide enhanced and personalized functionality to intelligent textbooks. Consequently, the intelligent textbook can provide more robust courseware functionality (text, videos, simulations, etc.), learning analytics, and personalized tutoring, such as automatically generating summaries of textbook content, generating useful practice exercises for students, providing interactive dialogues with students to help them better understand and master the underlying source material, and more.  Integrating this intelligent technology into the full OpenStax free and open library has the potential to impact academic outcomes for millions of students in both secondary and higher education, while significantly advancing the state of education worldwide. \r\n\r\nIntelligent textbooks provide an opportunity to facilitate better learning for students. However, they require major investments of time, money, and expertise. An appropriate knowledge graph is at the heart of an intelligent textbook and is often the biggest challenge to intelligent textbook creation due to the need for human subject experts to develop the semantic connectivity of terms and ideas. TOKN aims to develop new, scalable processes and supporting technologies for generating high-quality and extensible knowledge graphs for intelligent textbooks. The proposed research aims to lower both the cost and time required to produce high-quality knowledge graphs. In contrast to using subject matter experts, this project proposes to use a combination of machine learning algorithms and crowdsourcing of knowledge from students. Crowdsourcing will not only provide data for knowledge graphs, but it will also provide an opportunity to evaluate the pedagogical effectiveness of concept mapping on student learning. Phase 1 of this project will include a proof of concept to construct and validate a knowledge graph for one chapter of OpenStax Biology, a free and open-source text used by more than 30% of students in college biology programs. The overarching goal is to eventually apply this approach at scale during Phase 2 to generate knowledge graphs for the entire OpenStax library of 38 general educational textbooks, transforming them into intelligent open textbooks for society.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "ITE",
 "org_div_long_name": "Innovation and Technology Ecosystems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Baraniuk",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Richard G Baraniuk",
   "pi_email_addr": "richb@rice.edu",
   "nsf_id": "000334750",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "H. Craig",
   "pi_last_name": "Heller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "H. Craig Heller",
   "pi_email_addr": "hcheller@stanford.edu",
   "nsf_id": "000207508",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Genesereth",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Michael R Genesereth",
   "pi_email_addr": "genesereth@stanford.edu",
   "nsf_id": "000093365",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Moshe",
   "pi_last_name": "Vardi",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Moshe Y Vardi",
   "pi_email_addr": "vardi@rice.edu",
   "nsf_id": "000462277",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Phillip",
   "pi_last_name": "Grimaldi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Phillip Grimaldi",
   "pi_email_addr": "phillip.grimaldi@rice.edu",
   "nsf_id": "000716832",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "6100 Main St",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "095Y00",
   "pgm_ele_name": "CA-HDR: Convergence Accelerato"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "049Z",
   "pgm_ref_txt": "RAISE-Research Advanced by Interdiscipli"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9eefbe05-7fff-25ef-0779-7e3886d6c655\"> </span></p>\n<p dir=\"ltr\"><span>Intelligent Textbooks (ITB) using Artificial Intelligence (AI) and knowledge graphs (KG) allow students to dynamically interact with the textbook content, increasing their ability to understand concepts, increasing engagement, and thereby, improving academic performance. The potential of ITBs has been difficult to realize because existing KG construction pipelines involve expensive and time consuming creation by subject-matter experts (SMEs), resulting in publishers and edtech providers keeping their KGs proprietary.</span></p>\n<p dir=\"ltr\"><span>During this project we pursued three major activities and made our proposed toolkit and results available for use by others: 1) Automated construction of KG using machine learning and natural language processing, 2) crowdsourcing KG construction, and 3) pedagogical evaluation and plans to scale this project within OpenStax.&nbsp;</span></p>\n<p><span id=\"docs-internal-guid-2e4892b8-7fff-838f-c965-8b61d5d2143f\"> </span></p>\n<p dir=\"ltr\"><span>Creating a KG (Figure 1) involves identifying classes and the relationships between them. Towards this goal, we leverage natural language models for automated term and relationship extraction and crowdsourcing supported by human validation.</span></p>\n<p dir=\"ltr\"><strong>Automated Term Extraction (ATE).&nbsp;</strong>Our initial ATE experiments showed that a commercial tool (AWS Comprehend keyphrase extraction [1]) had a low precision, and therefore, an approach that can learn the domain-specific peculiarities was required. The hand-curated textbook glossary, was thus selected to provide the training data necessary for ATE.</p>\n<p><span id=\"docs-internal-guid-c09cec55-7fff-d194-54ec-f118e7da8af1\"> </span></p>\n<p dir=\"ltr\"><span>We formulated the term extraction problem as a supervised learning task and fine tuned the cased base BERT model [14] for ATE using 10 open-source science textbooks and a proprietary biology textbook [32]. We used the textbook glossaries as training data. For model validation, SMEs hand labeled the reference list of terms.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We compared the performance of ATE with the commercial AWS Comprehend key phrase extraction service [1]. ATE consistently outperformed AWS Comprehend on the F metric (average&nbsp;<em>F</em></span><em><sub>ATE</sub></em><span> = 51 vs. <em>F</em></span><em><sub>AWS_comprehend</sub></em><span> = 32). Preliminary failure analysis revealed that domain based </span><span>lexical knowledge </span><span>(e.g., chromatin vs. chromatid), certain</span><span> general terms</span><span> (often prerequisite knowledge not included in the glossary, e.g., microscope) related to other terms need to be represented in the KG, and identifying and naming unnamed </span><span>terms that are not directly expressed in text</span><span> (e.g., mechanisms that ventilate the environmental side of those surfaces with air or water, can be named ventilation mechanisms) are all essential for good ATE performance.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><strong>Automated Relationship Extraction (ARE). </strong><span>We used the data from KB-Bio-101 and weak labeling methods for training BERT for ARE. In our experiments, the resulting model did not have a high accuracy at this task primarily due to the lack of training data.</span></p>\n<p dir=\"ltr\"><span>We included relationships with the most training data available (taxonomic, structural, and synonyms). We used a new variant of BERT known as BERT-EM [34], which modifies a pre-trained BERT model for supervised relation classification. Our ARE (average </span><span>F1</span><span> = 0.55 vs. </span><span>F1</span><span> = 0.3947) outperformed other taxonomy extraction models [6]. </span><span><br /></span><span><br /></span><strong>Crowdsourcing with Relationship Selection Task (RST). </strong><span>To mitigate the problem of lack of training data for ARE, we developed a relationship labeling task that provided a cost-effective way of gathering relationship labels, and also had the potential to be pedagogically useful to justify no-cost deployment on a textbook reading platform. Therefore, we designed an RST in which a student studying from a textbook engages in an educationally useful concept mapping activity [17, 35].&nbsp;</span></p>\n<p dir=\"ltr\"><span>We retained 20 of over 100 relationships used in KB Bio 101 (including taxonomic, structural, causal relations, and an option for no direct relationship between two terms) that were used most often [16].</span></p>\n<p dir=\"ltr\"><span>The RST participant is first asked to read a section from the textbook and then to undergo a short relational frame training using easy to understand common-sense examples.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We tested the RST in conjunction with the ATE. We used ATE to identify terms (for Sections 4.2 and 10.2 from OpenStax Bio2e, and Section 14.1 from the OpenStax Psychology 2e textbooks) that were validated by SMEs. We then parsed the chosen section into individual sentences and automatically identified all the term pairs that existed in each sentence. A sentence containing N terms would have (n?2)</span><span>&nbsp;</span><span>possible pairings, with each pairing considered to be a single task. Using the RST, the tasks were presented to users on a crowdsourcing platform.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The responses were denoised with an algorithm that accounted for worker competence and task difficulty and showed high agreement with the modal crowd responses on the RST. A subset (n = 50) of these responses were validated by SMEs showing high precision when one of the SMEs agreed with the crowd-workers (&gt;=0.96) but reduced when both SMEs had to agree with the crowd-workers (&gt;=0.67).&nbsp;</span></p>\n<p dir=\"ltr\"><span>Pedagogical evaluation of the task revealed a very small benefit of completing the RST over rereading (</span><span>d</span><span> = 0.14; </span><span>M</span><span><span><sub>RST</sub></span></span><span> = 4.02, </span><span>SD</span><span><span><sub>RST</sub></span></span><span> = 1.8, </span><span>n</span><span> = 98; M</span><span><span><sub>rereading</sub></span></span><span> = 3.76, SD</span><span><sub>rereading</sub> </span><span>= 1.77, n = 100). We plan to continue the pedagogical evaluation on the OpenStax research platform after it becomes available to millions of OpenStax students over the next year.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2021<br>\n\t\t\t\t\tModified by: Richard&nbsp;G&nbsp;Baraniuk</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1937134/1937134_10641208_1632938509255_kg_arch(2)--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1937134/1937134_10641208_1632938509255_kg_arch(2)--rgov-800width.jpg\" title=\"Figure 1. Knowledge graph construction pipeline.\"><img src=\"/por/images/Reports/POR/2021/1937134/1937134_10641208_1632938509255_kg_arch(2)--rgov-66x44.jpg\" alt=\"Figure 1. Knowledge graph construction pipeline.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The components shown in the solid box on the left side are part of the proposed system; whereas the components shown in the right hand side in the dotted box, and the dotted feedback arrows are future work.</div>\n<div class=\"imageCredit\">OpenStax and Stanford Convergence Accelerator Team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Richard&nbsp;G&nbsp;Baraniuk</div>\n<div class=\"imageTitle\">Figure 1. Knowledge graph construction pipeline.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1937134/1937134_10641208_1632938642246_AKG--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1937134/1937134_10641208_1632938642246_AKG--rgov-800width.jpg\" title=\"Figure 2.\"><img src=\"/por/images/Reports/POR/2021/1937134/1937134_10641208_1632938642246_AKG--rgov-66x44.jpg\" alt=\"Figure 2.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Use of data in the proposed KG construction pipeline.</div>\n<div class=\"imageCredit\">OpenStax and Stanford Convergence Accelerator Team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Richard&nbsp;G&nbsp;Baraniuk</div>\n<div class=\"imageTitle\">Figure 2.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nIntelligent Textbooks (ITB) using Artificial Intelligence (AI) and knowledge graphs (KG) allow students to dynamically interact with the textbook content, increasing their ability to understand concepts, increasing engagement, and thereby, improving academic performance. The potential of ITBs has been difficult to realize because existing KG construction pipelines involve expensive and time consuming creation by subject-matter experts (SMEs), resulting in publishers and edtech providers keeping their KGs proprietary.\nDuring this project we pursued three major activities and made our proposed toolkit and results available for use by others: 1) Automated construction of KG using machine learning and natural language processing, 2) crowdsourcing KG construction, and 3) pedagogical evaluation and plans to scale this project within OpenStax. \n\n \nCreating a KG (Figure 1) involves identifying classes and the relationships between them. Towards this goal, we leverage natural language models for automated term and relationship extraction and crowdsourcing supported by human validation.\nAutomated Term Extraction (ATE). Our initial ATE experiments showed that a commercial tool (AWS Comprehend keyphrase extraction [1]) had a low precision, and therefore, an approach that can learn the domain-specific peculiarities was required. The hand-curated textbook glossary, was thus selected to provide the training data necessary for ATE.\n\n \nWe formulated the term extraction problem as a supervised learning task and fine tuned the cased base BERT model [14] for ATE using 10 open-source science textbooks and a proprietary biology textbook [32]. We used the textbook glossaries as training data. For model validation, SMEs hand labeled the reference list of terms. \nWe compared the performance of ATE with the commercial AWS Comprehend key phrase extraction service [1]. ATE consistently outperformed AWS Comprehend on the F metric (average FATE = 51 vs. FAWS_comprehend = 32). Preliminary failure analysis revealed that domain based lexical knowledge (e.g., chromatin vs. chromatid), certain general terms (often prerequisite knowledge not included in the glossary, e.g., microscope) related to other terms need to be represented in the KG, and identifying and naming unnamed terms that are not directly expressed in text (e.g., mechanisms that ventilate the environmental side of those surfaces with air or water, can be named ventilation mechanisms) are all essential for good ATE performance. \n\n \nAutomated Relationship Extraction (ARE). We used the data from KB-Bio-101 and weak labeling methods for training BERT for ARE. In our experiments, the resulting model did not have a high accuracy at this task primarily due to the lack of training data.\nWe included relationships with the most training data available (taxonomic, structural, and synonyms). We used a new variant of BERT known as BERT-EM [34], which modifies a pre-trained BERT model for supervised relation classification. Our ARE (average F1 = 0.55 vs. F1 = 0.3947) outperformed other taxonomy extraction models [6]. \n\nCrowdsourcing with Relationship Selection Task (RST). To mitigate the problem of lack of training data for ARE, we developed a relationship labeling task that provided a cost-effective way of gathering relationship labels, and also had the potential to be pedagogically useful to justify no-cost deployment on a textbook reading platform. Therefore, we designed an RST in which a student studying from a textbook engages in an educationally useful concept mapping activity [17, 35]. \nWe retained 20 of over 100 relationships used in KB Bio 101 (including taxonomic, structural, causal relations, and an option for no direct relationship between two terms) that were used most often [16].\nThe RST participant is first asked to read a section from the textbook and then to undergo a short relational frame training using easy to understand common-sense examples. \nWe tested the RST in conjunction with the ATE. We used ATE to identify terms (for Sections 4.2 and 10.2 from OpenStax Bio2e, and Section 14.1 from the OpenStax Psychology 2e textbooks) that were validated by SMEs. We then parsed the chosen section into individual sentences and automatically identified all the term pairs that existed in each sentence. A sentence containing N terms would have (n?2) possible pairings, with each pairing considered to be a single task. Using the RST, the tasks were presented to users on a crowdsourcing platform. \nThe responses were denoised with an algorithm that accounted for worker competence and task difficulty and showed high agreement with the modal crowd responses on the RST. A subset (n = 50) of these responses were validated by SMEs showing high precision when one of the SMEs agreed with the crowd-workers (&gt;=0.96) but reduced when both SMEs had to agree with the crowd-workers (&gt;=0.67). \nPedagogical evaluation of the task revealed a very small benefit of completing the RST over rereading (d = 0.14; MRST = 4.02, SDRST = 1.8, n = 98; Mrereading = 3.76, SDrereading = 1.77, n = 100). We plan to continue the pedagogical evaluation on the OpenStax research platform after it becomes available to millions of OpenStax students over the next year.\n\n \n\n\t\t\t\t\tLast Modified: 09/29/2021\n\n\t\t\t\t\tSubmitted by: Richard G Baraniuk"
 }
}