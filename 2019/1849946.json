{
 "awd_id": "1849946",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Neuromodulated Deep Vision: Continual Learning and Goal-Oriented Adaptation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-03-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2019-08-30",
 "awd_max_amd_letter_date": "2019-08-30",
 "awd_abstract_narration": "Deep neural networks are revolutionizing many fields and industries. These systems can learn to solve problems ranging from parsing natural language to discovering new drugs. However, current networks can only solve individual, static problems; they cannot learn gradually over time or adapt their computations in real time. This project seeks to bridge the gap between human and machine intelligence by modeling a chemical process in the brain that help humans learn and adjust their behavior based on context. Specifically, the chemical acetylcholine (ACh) has been shown to modulate responses to visual stimuli by selectively raising or lowering the activity level of individual neurons. This chemical is also crucial for visual learning in infants. An insufficient concentration of ACh in the brain leads to poor visual attention and has also been linked to memory deficits and Alzheimer?s disease. Thus, adding ACh-like mechanisms to deep neural networks can potentially allow them to learn over time and adapt their behavior to rapidly changing conditions--a growing need in domains ranging from forest conservation to cybersecurity.\r\n\r\nIn more detail, deep convolutional neural networks (CNNs) are the state-of-the-art approach for a wide variety of image processing tasks, such as object recognition and semantic segmentation. This project will investigate how to add ACh-like regulation to CNNs by modeling the spatial and temporal dynamics of this chemical process. Specifically, the CNN will be connected to a set of spatially and temporally heterogeneous sources of modulation. Each neuron in the CNN will have different receptors that determine how that particular neuron responds to different ambient levels of ACh. The system will use two controllers: one to determine the current level of ACh across different parts of the network and another to set the correct context (i.e., to model top-down feedback from higher cortical areas). The first controller will allow the system to quickly adjust its behavior, while the second will enable long-term learning. This project constitutes an initial step in developing intelligent systems that, like biological organisms, can autonomously cope with changing, dynamic environments. In addition to its technical contributions, this project will allow students at Georgia State University, one of the most diverse institutions in the country, to foster interdisciplinary connections between artificial intelligence, neuroscience, and related STEM disciplines.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rolando",
   "pi_last_name": "Estrada",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Rolando J Estrada",
   "pi_email_addr": "restrada1@gsu.edu",
   "nsf_id": "000782167",
   "pi_start_date": "2019-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia State University Research Foundation, Inc.",
  "inst_street_address": "58 EDGEWOOD AVE NE",
  "inst_street_address_2": "FL 3",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4044133570",
  "inst_zip_code": "303032921",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA STATE UNIVERSITY RESEARCH FOUNDATION INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "MNS7B9CVKDN7"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia State University",
  "perf_str_addr": "25 Park Place",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303023994",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Deep neural networks are revolutionizing our ability to analyze large, complex data. These systems consist of thousands of small processing units arranged in a manner which roughly resembles how biological neurons are connected in the brain. Using a process called backpropagation, we can use data to gradually tune, i.e., adjust the outputs, of each processing unit until the entire system is able to produce useful outputs. For example, we can use a large database of images of cats and dogs to train one of these deep neural networks to reliably identify the correct animal (either cat or dog) in new images, i.e., ones it was not trained on. Beyond image tagging, these networks have shown promise in applications ranging from automatic medical diagnosis and protein discovery to self-driving vehicles.</p>\n<p>&nbsp;However, due to both their architecture and how we train them, these neural networks have a severe limitation: they cannot be trained on multiple tasks over time. In other words, if we trained our cat-and-dog detector to distinguish horse vs. cow images, the network would \"forget\" how to recognize cats and dogs. This is because the way we tune its processing units requires us to change their output for the new task, thus rendering their outputs unsuitable for the old task. Also, another limitation of these systems, which is related to the above, is that they are very sensitive to the exact characteristics of the input (called features in machine learning jargon). For example, if we had only shown our cat-vs-dog detector images of short-haired cats, it might give us the wrong answer for long-haired cats, perhaps because it had learned that long hair is predictive of being a dog. One can partially mitigate this issue by collecting more data, but there will always be combinations of features that will be underrepresented in the data, and so they will not be learned well by the network.</p>\n<p>&nbsp;In this project, we investigated different ways to make these networks more robust to the above problems, loosely inspired by how the chemical acetylcholine helps regulate learning and short-term adaption in the brain. For the first issue of learning tasks over time, we developed two techniques, which we dubbed Self-Nets and Deep Artificial Neurons (DANs). The former consists of storing a compressed representation of the state of the network after it has been trained on a task, similar to saving an image as a JPEG or a song as an MP3 to reduce memory usage. We showed that these compressed versions could be stored in a fraction of the original space, while still being able to perform the original task well. Overall, we showed that it is possible to train and store hundreds of these compressed networks over time in the same space required for a single uncompressed network. Our second idea involved developing a new type of processing unit--which we dubbed Deep Artificial Neurons--that has more sophisticated inputs and outputs than standard units. We showed that if we train networks with these DAN units in the right way, specifically by training them as a population, then these networks will be able to remember old task better than conventional networks, even after training on new tasks.</p>\n<p>&nbsp;For the second issue, we devised a new way of training conventional neural networks in which we randomly alter how much we care about different types of errors (specifically false positives vs. false negatives). In the cat-vs-dog example, one usually adjusts the network by the same amount regardless of whether a cat was labeled a dog or vice versa. In our modified training scheme, though, we randomly make one type of error more important than the other for a while, then switch to making the other one more important and then back again, etc. In our work, we showed that this alternating overcorrection yielded better results on many problems compared to regular, all-errors-are-equal training. Some applications where we showed a benefit of using this scheme include retinal vessel segmentation, artery vs. vein segmentation, and synthetic image generation.</p>\n<p>&nbsp;In total, the funds made available for this project allowed our lab to publish seven journal papers and four conference papers, as well as help establish an international industry partnership that has already yielded tens of thousands of dollars in research investments. In addition, these funds provided full or partial support for five different Ph.D. students, thus helping to strengthen the U.S. STEM labor force.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/27/2022<br>\n\t\t\t\t\tModified by: Rolando&nbsp;J&nbsp;Estrada</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDeep neural networks are revolutionizing our ability to analyze large, complex data. These systems consist of thousands of small processing units arranged in a manner which roughly resembles how biological neurons are connected in the brain. Using a process called backpropagation, we can use data to gradually tune, i.e., adjust the outputs, of each processing unit until the entire system is able to produce useful outputs. For example, we can use a large database of images of cats and dogs to train one of these deep neural networks to reliably identify the correct animal (either cat or dog) in new images, i.e., ones it was not trained on. Beyond image tagging, these networks have shown promise in applications ranging from automatic medical diagnosis and protein discovery to self-driving vehicles.\n\n However, due to both their architecture and how we train them, these neural networks have a severe limitation: they cannot be trained on multiple tasks over time. In other words, if we trained our cat-and-dog detector to distinguish horse vs. cow images, the network would \"forget\" how to recognize cats and dogs. This is because the way we tune its processing units requires us to change their output for the new task, thus rendering their outputs unsuitable for the old task. Also, another limitation of these systems, which is related to the above, is that they are very sensitive to the exact characteristics of the input (called features in machine learning jargon). For example, if we had only shown our cat-vs-dog detector images of short-haired cats, it might give us the wrong answer for long-haired cats, perhaps because it had learned that long hair is predictive of being a dog. One can partially mitigate this issue by collecting more data, but there will always be combinations of features that will be underrepresented in the data, and so they will not be learned well by the network.\n\n In this project, we investigated different ways to make these networks more robust to the above problems, loosely inspired by how the chemical acetylcholine helps regulate learning and short-term adaption in the brain. For the first issue of learning tasks over time, we developed two techniques, which we dubbed Self-Nets and Deep Artificial Neurons (DANs). The former consists of storing a compressed representation of the state of the network after it has been trained on a task, similar to saving an image as a JPEG or a song as an MP3 to reduce memory usage. We showed that these compressed versions could be stored in a fraction of the original space, while still being able to perform the original task well. Overall, we showed that it is possible to train and store hundreds of these compressed networks over time in the same space required for a single uncompressed network. Our second idea involved developing a new type of processing unit--which we dubbed Deep Artificial Neurons--that has more sophisticated inputs and outputs than standard units. We showed that if we train networks with these DAN units in the right way, specifically by training them as a population, then these networks will be able to remember old task better than conventional networks, even after training on new tasks.\n\n For the second issue, we devised a new way of training conventional neural networks in which we randomly alter how much we care about different types of errors (specifically false positives vs. false negatives). In the cat-vs-dog example, one usually adjusts the network by the same amount regardless of whether a cat was labeled a dog or vice versa. In our modified training scheme, though, we randomly make one type of error more important than the other for a while, then switch to making the other one more important and then back again, etc. In our work, we showed that this alternating overcorrection yielded better results on many problems compared to regular, all-errors-are-equal training. Some applications where we showed a benefit of using this scheme include retinal vessel segmentation, artery vs. vein segmentation, and synthetic image generation.\n\n In total, the funds made available for this project allowed our lab to publish seven journal papers and four conference papers, as well as help establish an international industry partnership that has already yielded tens of thousands of dollars in research investments. In addition, these funds provided full or partial support for five different Ph.D. students, thus helping to strengthen the U.S. STEM labor force.\n\n\t\t\t\t\tLast Modified: 07/27/2022\n\n\t\t\t\t\tSubmitted by: Rolando J Estrada"
 }
}