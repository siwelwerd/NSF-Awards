{
 "awd_id": "1904007",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Cortical Motion Coding and Gaze Control in Natural Vision",
 "cfda_num": "47.074",
 "org_code": "08090200",
 "po_phone": "7032924845",
 "po_email": "sraghava@nsf.gov",
 "po_sign_block_name": "Sridhar Raghavachari",
 "awd_eff_date": "2018-01-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 652023.0,
 "awd_amount": 652023.0,
 "awd_min_amd_letter_date": "2019-02-07",
 "awd_max_amd_letter_date": "2019-08-22",
 "awd_abstract_narration": "The human eye sends information to the brain at an estimated rate of approximately 10 megabits per second, roughly the speed of an ethernet connection. Processing such a large bandwidth stream of visual information on behaviorally relevant time scales requires the brain to extract and represent information from visual signals efficiently, i.e. represent the most information for the least cost in time, hardware and energy. In essence, the brain needs to compress the visual stream in much the same way that software compresses the digital representation of a movie. This coding enhancement might arise because the brain has evolved coding strategies that specifically account for the fact that because of both object and eye movements, the visual input to the eye may be correlated in space and time. As a result, the visual signals to the brain from the eye and retina may be quite predictable. One of the primary questions in current sensory-motor systems research is to what extent the brain utilizes prediction to compensate for the fact that it takes a finite amount of time to process information even though the visual scene might change in the interim. This proposal focuses on neural representation of visual motion and gaze behavior for natural motion videos and uses a novel video game environment to simplify the analysis of gaze. The project will also create a publicly available database of natural gaze recordings, analyze the statistics of natural retinal image motion, characterize the representation of naturally correlated motion stimuli in cortical neurons, and to articulate the strategy underlying gaze control. This database will benefit neuroscience, computer vision, media design, and other fields.\r\n\r\nThe experimental approach combines cortical physiology in non-human primates with high-resolution eye movement recording in both humans and monkeys. The PI proposes to use high-resolution videos of natural moving scenes as visual stimuli while recording neural activity in motion-sensitive visual cortex.  By carefully degrading the movies to make them increasingly less natural and measuring the impact on neural responses, the experiments will determine what features of the moving visual scene are represented most precisely.  A second set of experiments will study the interactions between the visual scene and eye movements. The PI will develop an innovative Pong-like video game that actively engages the viewers and creates a common viewing purpose (scoring points) while simplifying the identification of the target of interest to aid analysis, thereby controlling the cognitive state of the viewer. The interdisciplinary nature of the work will provide training opportunities for undergraduate and graduate students crossing over from mathematics and physics to neurobiology, and for students with a biology background to gain skills in computational analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "IOS",
 "org_div_long_name": "Division Of Integrative Organismal Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leslie",
   "pi_last_name": "Osborne",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Leslie C Osborne",
   "pi_email_addr": "leslie.osborne@duke.edu",
   "nsf_id": "000549752",
   "pi_start_date": "2019-02-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main Street",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "724600",
   "pgm_ele_name": "PHYSICS OF LIVING SYSTEMS"
  },
  {
   "pgm_ele_code": "771300",
   "pgm_ele_name": "Activation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 102022.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 247513.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 102487.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The human eye sends information to the brain at an estimated rate of approximately 10 megabits per second, roughly the speed of an ethernet connection. Processing such a large bandwidth stream of visual information on behaviorally relevant time scales requires the brain to extract and represent information from visual signals efficiently, i.e. represent the most information for the least cost in time, hardware and energy. In essence, the brain needs to compress the visual stream in much the same way that software compresses the digital representation of a movie. In previous work, we demonstrated that the brain uses rapid gain adaptation to optimize the information capacity of cortical neurons and the downstream eye movement responses that they drive.&nbsp;&nbsp;This project delved more deeply into how the spatial and temporal dynamics of motion stimuli impact the cortical representation of visual motion, and how behavioral context shapes how the brain responds to moving targets.&nbsp;&nbsp;This project supported combined electrophysiological and eye movement recordings in cortical area MT of monkeys, and eye movement recordings in human subjects.&nbsp;&nbsp;In both humans and monkeys we measured how the brain integrates motion across the visual field to drive smooth pursuit eye movements.&nbsp;&nbsp;We found that motion signals ahead of coming eye movements contribute the most strongly during visual tracking, but this asymmetry persists in the absence of eye movements.&nbsp;&nbsp;Our results demonstrated a congruence between visual motion integration for perception and action, and they suggested a role for anticipation in how visual signals are interpreted.&nbsp;&nbsp;We explored the role of anticipation further by designing a tennis-like video game task based on Pong.&nbsp;&nbsp;We found that subjects eye movements carry predictive information about ball movements ~5s into the future when actively playing the game, but they switch to being reactive (lagging the ball by the latency of visual processing) when observing but not playing.&nbsp;&nbsp;This context-dependent prediction is remarkable both for its long timescale and for its efficiency ? the brain appears to be able to account for nearly all that is predictable about a target?s movements.&nbsp;&nbsp;In monkeys, we performed experiments to understand how the temporal dynamics visual inputs impacts the cortical representation of motion.&nbsp;&nbsp;We found that neurons in motion-sensitive visual cortex adapt their integration timescale based on the frequency of motion fluctuations in a manner that maintains a constant output spectrum.&nbsp;&nbsp;We observed the evidence for the impact of cortical adaptation on smooth pursuit eye movements which undergo the same alternation in response dynamics.&nbsp;&nbsp;More experiments are needed to determine whether the cortical adaptation to input dynamics alone can account for the long timescales of predictive information during Pong play or whether an additional integration step is needed.</p>\n<p>&nbsp;This proposal has supported the creation of a public database of gaze behavior that will benefit neuroscience, computer vision, media design, and other fields, and the development of theoretical tools for the analysis of continuous movement behavior that will generalize across many organisms and behaviors.&nbsp;&nbsp;These projects provided hands-on training in experimental and data analysis techniques for undergraduates from neuroscience, computer science, and applied math, as well as graduate students, medical students, and postdoctoral fellows. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/02/2022<br>\n\t\t\t\t\tModified by: Leslie&nbsp;C&nbsp;Osborne</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe human eye sends information to the brain at an estimated rate of approximately 10 megabits per second, roughly the speed of an ethernet connection. Processing such a large bandwidth stream of visual information on behaviorally relevant time scales requires the brain to extract and represent information from visual signals efficiently, i.e. represent the most information for the least cost in time, hardware and energy. In essence, the brain needs to compress the visual stream in much the same way that software compresses the digital representation of a movie. In previous work, we demonstrated that the brain uses rapid gain adaptation to optimize the information capacity of cortical neurons and the downstream eye movement responses that they drive.  This project delved more deeply into how the spatial and temporal dynamics of motion stimuli impact the cortical representation of visual motion, and how behavioral context shapes how the brain responds to moving targets.  This project supported combined electrophysiological and eye movement recordings in cortical area MT of monkeys, and eye movement recordings in human subjects.  In both humans and monkeys we measured how the brain integrates motion across the visual field to drive smooth pursuit eye movements.  We found that motion signals ahead of coming eye movements contribute the most strongly during visual tracking, but this asymmetry persists in the absence of eye movements.  Our results demonstrated a congruence between visual motion integration for perception and action, and they suggested a role for anticipation in how visual signals are interpreted.  We explored the role of anticipation further by designing a tennis-like video game task based on Pong.  We found that subjects eye movements carry predictive information about ball movements ~5s into the future when actively playing the game, but they switch to being reactive (lagging the ball by the latency of visual processing) when observing but not playing.  This context-dependent prediction is remarkable both for its long timescale and for its efficiency ? the brain appears to be able to account for nearly all that is predictable about a target?s movements.  In monkeys, we performed experiments to understand how the temporal dynamics visual inputs impacts the cortical representation of motion.  We found that neurons in motion-sensitive visual cortex adapt their integration timescale based on the frequency of motion fluctuations in a manner that maintains a constant output spectrum.  We observed the evidence for the impact of cortical adaptation on smooth pursuit eye movements which undergo the same alternation in response dynamics.  More experiments are needed to determine whether the cortical adaptation to input dynamics alone can account for the long timescales of predictive information during Pong play or whether an additional integration step is needed.\n\n This proposal has supported the creation of a public database of gaze behavior that will benefit neuroscience, computer vision, media design, and other fields, and the development of theoretical tools for the analysis of continuous movement behavior that will generalize across many organisms and behaviors.  These projects provided hands-on training in experimental and data analysis techniques for undergraduates from neuroscience, computer science, and applied math, as well as graduate students, medical students, and postdoctoral fellows.  \n\n \n\n\t\t\t\t\tLast Modified: 05/02/2022\n\n\t\t\t\t\tSubmitted by: Leslie C Osborne"
 }
}