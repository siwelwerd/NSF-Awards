{
 "awd_id": "1844186",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Experimental pragmatics and semantics in visual language",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924770",
 "po_email": "rtheodor@nsf.gov",
 "po_sign_block_name": "Rachel M. Theodore",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 1500000.0,
 "awd_amount": 509255.0,
 "awd_min_amd_letter_date": "2019-06-11",
 "awd_max_amd_letter_date": "2022-08-29",
 "awd_abstract_narration": "One thing that makes human language so powerful is that its limited vocabulary can be used to share an unlimited number of ideas, and to say things that have never been said before. Understanding how words combine to express new meanings in a predictable (and therefore unlimited) way has been the focus of decades of research, and has led to increasing success by artificial intelligence systems in responding to direct queries, such as asking for directions, weather, etc. However, the meaning of language in face-to-face interactions is influenced by factors beyond just words, and even beyond intonation: nonlinguistic visual information interacts clearly and regularly with speech. For example, when someone says they \"took a note\" while moving their fingers as if they were typing on a keyboard, this conveys that the note was electronic; as another example, a pointing gesture toward someone while saying \"my neighbor\" identifies the person with the label. Technology currently exists to recognize most of these movements, but they are typically ignored for the purpose of meaning since so little is known about how they contribute and compose with other parts of a sentence. A source of valuable insight for this question comes from languages which are entirely visual: sign languages such as American Sign Language have their own full distinct grammar and their own complex rules for the meanings of word combinations, and at the same time clearly and regularly integrate visual demonstrations and pointing into sentence meaning.   \r\n\r\nThis project compares the role of visual (demonstrations and pointing) information in gestures used in English with well-documented counterparts in American Sign Language. It builds on current work by the project team in theoretical models of sign language linguistics and gesture, expanding to include state-of-the-art experimental methodologies and analysis. English and American Sign Language will be studied at each point in parallel, both by gathering quantitative data using responses to videos presented online and by analyzing fluent language productions. A major challenge is that research on the compositional processes that underlie how words combine into more complex meanings requires a dolid background in logic and computation, and scholars with this skill set tend to have limited overlap with the Deaf, Hard of Hearing, and fluent signing scholars who have the most insight into the quickly growing fields of sign linguistics and gesture. To correct for this gap, the project will include training opportunities for postdoctoral, graduate students and undergraduates whose studies are focused on sign languages and gesture, to provide experience with experimental methodology, statistical analysis, and logic and computation. The project will also include the development of a course and textbook dedicated to bridging this gap.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kathryn",
   "pi_last_name": "Davidson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kathryn Davidson",
   "pi_email_addr": "kathryndavidson@fas.harvard.edu",
   "nsf_id": "000738577",
   "pi_start_date": "2019-06-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "305 Bolyston Hall, 35 Quincy St",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021383834",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 278842.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 0.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 230413.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3d2d27d3-7fff-8d2c-1367-e36cd25a962f\"> </span></p>\n<p dir=\"ltr\"><span>This project targeted a longstanding question in cognitive science, philosophy, and linguistics, namely how meaning from language integrates information from the world outside of language, such as objects in the environment, non-linguistic visual communication like pictures or depicting gestures, etc., through a unique angle: a parallel comparison of context-sensitive expressions in both signed and spoken languages. Both signed and spoken languages make use of highly abstract symbolic meanings that are the hallmark of complex human communication; at the same time they integrate information from iconic gestures and pointing in potentially different ways, given that in spoken language these often occur in separate channels (visual vs. auditory) whereas sign languages often integrate them in the same communicative channel (visual). In this project we applied analytical tools from formal semantics to data we collected through a series of parallel experiments in American Sign Language and English. In the later years of this project, scholars from our group trained in these methods extended this work to additional spoken languages (Mandarin, Turkish, Bangla, German, and Korean) and sign languages (Japanese and Nicaraguan). One of the clear findings that emerged from our series of studies is that both signed and spoken languages impose the same set of highly limited constraints on how language integrates content from outside language. In fact, they can be modeled as falling into just two categories: one which can be referred to in language via demonstratives such as </span><span>this, that, </span><span>pointing, etc, and the other based on contextual concept disambiguation. A clear method for formally modeling these differences is presented in a new textbook </span><em>Formal semantics and pragmatics in sign languages</em><span> in publication at Cambridge University Press, which was written to be accessible to both audiences familiar with sign languages and audience familiar with semantics, as well as audiences in natural language computation and cognitive psychology, and has been used in course instruction at multiple institutions. In addition, a second manuscript </span><em>Compositionality and Iconicity</em><span>, to appear in the Cambridge Elements in Semantics series, zooms out from focusing on sign languages to tie the empirical findings of this project to broader questions in cognition and meaning, especially to large language models, recent cognitive scientific proposals related to a \"language of thought\", and other broader debates at the intersection of meaning and cognition.</span></p>\n<p><br /><span>Beyond the empirical findings and theoretical advances, this project was notable in bringing together researchers and insights from diverse theoretical linguistic frameworks. For example, both major manuscripts mentioned above, as well as the nine published papers and five years of both peer-reviewed and invited presentations from our group supported by this grant model how to integrate cognitive linguistics models of space via metaphor with formal semantic models of meaning via symbolic logic. Similarly, students were trained in understanding insights from both perspectives and integrating the two, which broadened accessibility in this project to scholars coming from both cognitive linguistics and formal perspectives: this work supported a community of deaf and hearing postdoctoral scholars, students, and research assistants who are innovating in these areas. In addition to virtual outreach and pedagogical tools, in the penultimate year we hosted a well-attended workshop at the Linguistics Society of America Summer Institute that brought together national and international deaf and hearing researchers who are leaders in formal semantics, cognitive linguistics, and cognitive psychology to engage a broad audience on advances in understanding iconicity from the perspective of sign languages. Finally, collecting experimental data to address formal semantic theory is an emerging area where we have been one of the leading research groups, and so integrating sign languages in this work has ensured they are included from the beginnings of this subfield, as evidenced by our researcher training and output, including a focused methodology paper (\"</span><span>Is 'experimental' a gradable predicate?\") which brings insights from sign linguistics to advance a general methodology for collecting theoretically important data on linguistic meaning.</span></p><br>\n<p>\n Last Modified: 09/29/2024<br>\nModified by: Kathryn&nbsp;Davidson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThis project targeted a longstanding question in cognitive science, philosophy, and linguistics, namely how meaning from language integrates information from the world outside of language, such as objects in the environment, non-linguistic visual communication like pictures or depicting gestures, etc., through a unique angle: a parallel comparison of context-sensitive expressions in both signed and spoken languages. Both signed and spoken languages make use of highly abstract symbolic meanings that are the hallmark of complex human communication; at the same time they integrate information from iconic gestures and pointing in potentially different ways, given that in spoken language these often occur in separate channels (visual vs. auditory) whereas sign languages often integrate them in the same communicative channel (visual). In this project we applied analytical tools from formal semantics to data we collected through a series of parallel experiments in American Sign Language and English. In the later years of this project, scholars from our group trained in these methods extended this work to additional spoken languages (Mandarin, Turkish, Bangla, German, and Korean) and sign languages (Japanese and Nicaraguan). One of the clear findings that emerged from our series of studies is that both signed and spoken languages impose the same set of highly limited constraints on how language integrates content from outside language. In fact, they can be modeled as falling into just two categories: one which can be referred to in language via demonstratives such as this, that, pointing, etc, and the other based on contextual concept disambiguation. A clear method for formally modeling these differences is presented in a new textbook Formal semantics and pragmatics in sign languages in publication at Cambridge University Press, which was written to be accessible to both audiences familiar with sign languages and audience familiar with semantics, as well as audiences in natural language computation and cognitive psychology, and has been used in course instruction at multiple institutions. In addition, a second manuscript Compositionality and Iconicity, to appear in the Cambridge Elements in Semantics series, zooms out from focusing on sign languages to tie the empirical findings of this project to broader questions in cognition and meaning, especially to large language models, recent cognitive scientific proposals related to a \"language of thought\", and other broader debates at the intersection of meaning and cognition.\n\n\n\nBeyond the empirical findings and theoretical advances, this project was notable in bringing together researchers and insights from diverse theoretical linguistic frameworks. For example, both major manuscripts mentioned above, as well as the nine published papers and five years of both peer-reviewed and invited presentations from our group supported by this grant model how to integrate cognitive linguistics models of space via metaphor with formal semantic models of meaning via symbolic logic. Similarly, students were trained in understanding insights from both perspectives and integrating the two, which broadened accessibility in this project to scholars coming from both cognitive linguistics and formal perspectives: this work supported a community of deaf and hearing postdoctoral scholars, students, and research assistants who are innovating in these areas. In addition to virtual outreach and pedagogical tools, in the penultimate year we hosted a well-attended workshop at the Linguistics Society of America Summer Institute that brought together national and international deaf and hearing researchers who are leaders in formal semantics, cognitive linguistics, and cognitive psychology to engage a broad audience on advances in understanding iconicity from the perspective of sign languages. Finally, collecting experimental data to address formal semantic theory is an emerging area where we have been one of the leading research groups, and so integrating sign languages in this work has ensured they are included from the beginnings of this subfield, as evidenced by our researcher training and output, including a focused methodology paper (\"Is 'experimental' a gradable predicate?\") which brings insights from sign linguistics to advance a general methodology for collecting theoretically important data on linguistic meaning.\t\t\t\t\tLast Modified: 09/29/2024\n\n\t\t\t\t\tSubmitted by: KathrynDavidson\n"
 }
}