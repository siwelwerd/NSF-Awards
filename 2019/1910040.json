{
 "awd_id": "1910040",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Generalizing Learned Manipulation Skills to Unseen Situations by Balancing Uncertainties",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan Wachs",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 334823.0,
 "awd_amount": 334823.0,
 "awd_min_amd_letter_date": "2019-08-31",
 "awd_max_amd_letter_date": "2019-08-31",
 "awd_abstract_narration": "Programming a robot to perform a manipulation task requires a great deal of robotics knowledge and programming skills. Even for an expert, it would take a significant amount of time to carefully craft an algorithm and a controller for a robot to perform a particular task in a specified situation. Giving robots the capability to learn from human demonstrations would remove the need for programming and its cost. However, current approaches can only replicate the human's motion or strategies of the learned task in its demonstrated situations but cannot transfer them to unseen conditions. Since it is unrealistic to teach robots all tasks in every possible situation, robots need to be able to generalize the manipulations kills learned in one situation to unseen situations. The goal of the project is to develop a manipulation skill generalization approach that takes into consideration both the variations in the demonstrations and uncertainties in the unseen situation. By balancing them, the robot can adapt the learned manipulation skills to new situations. The project advances the effort of giving robots the capability to learn and continue their learning in practice. With that capability, robots will be able to perform daily living tasks and provide needed help to people with disabilities and seniors without the cost of programming. The project will also produce new teaching materials for robotics courses and train both graduate and undergraduate students. \r\n \r\nThe main idea of the project is that the robot should learn from the demonstration data in the form of a broader probability representation rather than an optimized but narrowed fitting, so that it is easy to incorporate the probability-based learning from demonstration with the predictions in unseen situations. The project has four objects: learning motion distribution inference model from demonstrations; learning the distributions of two manipulation estimated outcomes and desired outcomes; generating an optimal action by incorporating both motion distributions and outcome distributions; and evaluating the approach on real physical system for daily manipulation tasks. The project not only gives robots a capability to learn and generalize their skills in practice, but also produces new tools for research and applications involving making a decision based on experiences and predictable outcomes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu",
   "pi_last_name": "Sun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Sun",
   "pi_email_addr": "yusun@usf.edu",
   "nsf_id": "000552980",
   "pi_start_date": "2019-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of South Florida",
  "inst_street_address": "4202 E FOWLER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "TAMPA",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "8139742897",
  "inst_zip_code": "336205800",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "FL15",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTH FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NKAZLXLL7Z91"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Florida",
  "perf_str_addr": "4202 E Fowler Ave, Stop AOC 200",
  "perf_city_name": "Tampa",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "336209000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "FL15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 334823.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project has produced a set of algorithms that learn manipulation skills demonstrated in certain conditions and then generalize them to different conditions through self-supervised practicing. The novel approach has been tested by pouring different kinds of materials such as water, oil, rice, beans, and ice cubes. The learned motion model allows the robot to achieve human-like pouring accuracy and speed in a broad range of conditions. A detailed analysis found that the robot pouring motion signature is similar to the spatial-temporal patterns of human pouring, and as a result, the robot pours smoothly and as fast as humans.&nbsp;</p>\n<p>Intellectual merit: The novel generalization by self-supervised practicing approach collects new data from the pouring practices carried out by the robot using the learned model in previous conditions. When the approach fine-tunes the model to the new conditions with the new data, the actual manipulation results are used as the desired targets. This switch allows the self-supervised practice to improve gradually. The project also produced novel representations of manipulation motion called motion code. Instead of relying on verbs, motion code uses the mechanical feature of the motion to represent manipulation motions, which is more meaningful to robots.&nbsp;</p>\n<p>Broader impacts: The project has trained two Ph.D. students and two master's students. The research outcomes have been published in journals and presented at major robotics conferences. They have also been used in the PI's classes as teaching material and outreach activities.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/17/2023<br>\n\t\t\t\t\tModified by: Yu&nbsp;Sun</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1910040/1910040_10639470_1673932567517_outcome1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1910040/1910040_10639470_1673932567517_outcome1--rgov-800width.jpg\" title=\"Generalization by  self-supervised Practicing\"><img src=\"/por/images/Reports/POR/2023/1910040/1910040_10639470_1673932567517_outcome1--rgov-66x44.jpg\" alt=\"Generalization by  self-supervised Practicing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The schematic of the generalization by  self-supervised Practicing approach.</div>\n<div class=\"imageCredit\">Yu Sun</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Sun</div>\n<div class=\"imageTitle\">Generalization by  self-supervised Practicing</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1910040/1910040_10639470_1673932763841_outcome2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1910040/1910040_10639470_1673932763841_outcome2--rgov-800width.jpg\" title=\"Learned Robotic Pouring Behavior\"><img src=\"/por/images/Reports/POR/2023/1910040/1910040_10639470_1673932763841_outcome2--rgov-66x44.jpg\" alt=\"Learned Robotic Pouring Behavior\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The comparison between the learned robotic pouring behavior and the human pouring behavior. They are similar.</div>\n<div class=\"imageCredit\">Yu Sun</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Sun</div>\n<div class=\"imageTitle\">Learned Robotic Pouring Behavior</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe project has produced a set of algorithms that learn manipulation skills demonstrated in certain conditions and then generalize them to different conditions through self-supervised practicing. The novel approach has been tested by pouring different kinds of materials such as water, oil, rice, beans, and ice cubes. The learned motion model allows the robot to achieve human-like pouring accuracy and speed in a broad range of conditions. A detailed analysis found that the robot pouring motion signature is similar to the spatial-temporal patterns of human pouring, and as a result, the robot pours smoothly and as fast as humans. \n\nIntellectual merit: The novel generalization by self-supervised practicing approach collects new data from the pouring practices carried out by the robot using the learned model in previous conditions. When the approach fine-tunes the model to the new conditions with the new data, the actual manipulation results are used as the desired targets. This switch allows the self-supervised practice to improve gradually. The project also produced novel representations of manipulation motion called motion code. Instead of relying on verbs, motion code uses the mechanical feature of the motion to represent manipulation motions, which is more meaningful to robots. \n\nBroader impacts: The project has trained two Ph.D. students and two master's students. The research outcomes have been published in journals and presented at major robotics conferences. They have also been used in the PI's classes as teaching material and outreach activities. \n\n\t\t\t\t\tLast Modified: 01/17/2023\n\n\t\t\t\t\tSubmitted by: Yu Sun"
 }
}