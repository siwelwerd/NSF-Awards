{
 "awd_id": "1908843",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: A Hybrid NVM based Computing Architecture for Machine Learning Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Danella Zhao",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 497576.0,
 "awd_amount": 497576.0,
 "awd_min_amd_letter_date": "2019-07-19",
 "awd_max_amd_letter_date": "2022-02-18",
 "awd_abstract_narration": "Today, enterprises are increasingly looking inward at their huge stores of under-processed or throwaway data, treating them as resources to be mined. As such, an emergent field called memory-intensive computing has ignited interest among industry and academia, largely driven by various emerging non-volatile memory technologies (NVMs). Machine Learning (ML) applications are being targeted by memory-intensive computing, leveraging unique properties of ML applications to improve their distributed performance by orders of magnitude. It is therefore highly desirable for ML applications executing in such memory intensive computing environments to be efficient, flexible and scalable. ML applications crunch a lot of data from disk drives, increasing latency due to disk access delays. This project explores and designs new techniques that let ML applications fully exploit the benefits of persistence for intermediate data in NVMs, which significantly reduces disk I/Os and hence data processing times. This project also involves curriculum development, and provides more avenues to bring women, minority, and underrepresented students into research and graduate programs.\r\n\r\nThis project focuses on an open challenge for memory intensive computing systems how to offer ML applications with high efficiency, low cost and more flexibility, especially under the heterogeneous environment. This project proposes a hybrid NVM based computing architecture with effective data sharing and communication strategy to optimize file management, resource allocation and data communication for ML applications. This research centers on two key designs: 1) a new file and data management system based on the hybrid NVM pool consisting of Byte and Block addressable devices; 2) an efficient data sharing and communication management among memories to guarantee data consistency in the hybrid NVM memory pool.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dong",
   "pi_last_name": "Dai",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dong Dai",
   "pi_email_addr": "dai@udel.edu",
   "nsf_id": "000696153",
   "pi_start_date": "2022-02-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Dazhao",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dazhao Cheng",
   "pi_email_addr": "dcheng3@uncc.edu",
   "nsf_id": "000735222",
   "pi_start_date": "2019-07-19",
   "pi_end_date": "2022-02-18"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yu",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Wang",
   "pi_email_addr": "wangyu@temple.edu",
   "nsf_id": "000317339",
   "pi_start_date": "2019-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Charlotte",
  "inst_street_address": "9201 UNIVERSITY CITY BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTE",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "7046871888",
  "inst_zip_code": "282230001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NC12",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE",
  "org_prnt_uei_num": "NEYCH3CVBTR6",
  "org_uei_num": "JB33DT84JNA5"
 },
 "perf_inst": {
  "perf_inst_name": "UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE",
  "perf_str_addr": "9201 UNIVERSITY CITY BLVD",
  "perf_city_name": "CHARLOTTE",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "282230001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NC12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 497576.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-b50177d1-7fff-5598-c98f-629ee03220c5\"> </span></p>\n<p dir=\"ltr\"><strong>Overview</strong></p>\n<p dir=\"ltr\"><span>The hybrid memory systems make efficiently running data-intensive machine learning (ML) applications challenging. This collaborative project, between the University of North Carolina at Charlotte and Temple University, addresses this key challenge by designing a hybrid non-volatile memory (NVM) computing architecture, consisting of new storage data structures, resource management, and network strategies to optimize the performance of ML workloads in such an environment.</span></p>\n<p dir=\"ltr\"><strong>Project Outcomes and Findings</strong></p>\n<p dir=\"ltr\"><span>We have achieved multiple major outcomes.</span></p>\n<p dir=\"ltr\"><span>First, we analyzed the performance characteristics of key storage data structures in heterogeneous memory devices, such as persistent memory, and identified the key design opportunities of storage systems for data-intensive ML applications.</span></p>\n<p dir=\"ltr\"><span>Second, we designed a novel indexing data structure in DRAM and NVM hybrid environments. Specifically, we leveraged the packed memory array data structure as the foundation of the new indexing data structure. DGAP consists of the new vertex-centric design and various optimizations to best fit NVM devices. DGAP can achieve up to 3.2x better graph update performance and up to 3.77x better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, serving as a solid foundation to support data accesses from ML applications.</span></p>\n<p dir=\"ltr\"><span>Third, to effectively leverage the computing resources to run ML applications, we proposed a novel utilization-aware resource provisioning approach for iterative Apache Spark ML workloads. It allows us to dynamically scale up or down the Spark executors to fully utilize the allocated resources.</span></p>\n<p dir=\"ltr\"><span>Fourth, to allow federated learning, a distributed machine learning method, to run efficiently with limited memory, networking, and computing resources, we studied efficient resource management as well as learning scheduling and task dispatching to optimize the learning cost of federated learning with multiple ML models across different edge servers in an edge network.</span></p>\n<p dir=\"ltr\"><strong>Summary</strong></p>\n<p dir=\"ltr\"><span>Through this project, we have systematically examined the designs of storage systems, resource scheduling, and network management in building an efficient and effective machine learning framework for hybrid NVM architecture. We pinpointed new opportunities and designed innovative solutions to considerably enhance the performance of ML workloads. We believe our findings hold considerable promise for real-world systems and expect them to be used in real-world software.</span></p>\n<p dir=\"ltr\"><span>The outcomes from this project have been published in top-tier conference proceedings and journals, disseminated to research communities through invited talks in both academia and industry, and leveraged to enhance graduate courses and design undergraduate research projects. This project has trained highly qualified researchers and engineers for the computer system, networking, and machine learning industry, and produced excellent PhD graduates with creativity, dedication, and professionalism.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/05/2024<br>\nModified by: Dong&nbsp;Dai</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nOverview\n\n\nThe hybrid memory systems make efficiently running data-intensive machine learning (ML) applications challenging. This collaborative project, between the University of North Carolina at Charlotte and Temple University, addresses this key challenge by designing a hybrid non-volatile memory (NVM) computing architecture, consisting of new storage data structures, resource management, and network strategies to optimize the performance of ML workloads in such an environment.\n\n\nProject Outcomes and Findings\n\n\nWe have achieved multiple major outcomes.\n\n\nFirst, we analyzed the performance characteristics of key storage data structures in heterogeneous memory devices, such as persistent memory, and identified the key design opportunities of storage systems for data-intensive ML applications.\n\n\nSecond, we designed a novel indexing data structure in DRAM and NVM hybrid environments. Specifically, we leveraged the packed memory array data structure as the foundation of the new indexing data structure. DGAP consists of the new vertex-centric design and various optimizations to best fit NVM devices. DGAP can achieve up to 3.2x better graph update performance and up to 3.77x better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, serving as a solid foundation to support data accesses from ML applications.\n\n\nThird, to effectively leverage the computing resources to run ML applications, we proposed a novel utilization-aware resource provisioning approach for iterative Apache Spark ML workloads. It allows us to dynamically scale up or down the Spark executors to fully utilize the allocated resources.\n\n\nFourth, to allow federated learning, a distributed machine learning method, to run efficiently with limited memory, networking, and computing resources, we studied efficient resource management as well as learning scheduling and task dispatching to optimize the learning cost of federated learning with multiple ML models across different edge servers in an edge network.\n\n\nSummary\n\n\nThrough this project, we have systematically examined the designs of storage systems, resource scheduling, and network management in building an efficient and effective machine learning framework for hybrid NVM architecture. We pinpointed new opportunities and designed innovative solutions to considerably enhance the performance of ML workloads. We believe our findings hold considerable promise for real-world systems and expect them to be used in real-world software.\n\n\nThe outcomes from this project have been published in top-tier conference proceedings and journals, disseminated to research communities through invited talks in both academia and industry, and leveraged to enhance graduate courses and design undergraduate research projects. This project has trained highly qualified researchers and engineers for the computer system, networking, and machine learning industry, and produced excellent PhD graduates with creativity, dedication, and professionalism.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 02/05/2024\n\n\t\t\t\t\tSubmitted by: DongDai\n"
 }
}