{
 "awd_id": "1911197",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Understanding Subtle Non-Social Facial Expressivity to Boost Learning and Computer Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499999.0,
 "awd_amount": 499999.0,
 "awd_min_amd_letter_date": "2019-09-06",
 "awd_max_amd_letter_date": "2019-09-06",
 "awd_abstract_narration": "Facial expressions play a significant role in everyday communication among humans. Computer understanding of these complex and subtle expressions will lead to highly capable interactive cyber-human systems with proactive computers that make more appropriate responses to human interactions. This project brings together an interdisciplinary team of investigators to address key challenges associated with spontaneous microexpression recognition in non-social scenarios. The project concentrates on generating bio-feedback from humans while learning skills, such as online learning, and being recorded and analyzed in continuous color and depth video streams. It will develop computer algorithms for human-machine synergy and test how this information can provide for superior learning when training applications are augmented with expression-informed bio-feedback in near real-time. This represents a significant step forward in training machines to recognize and classify facial microexpressions and maximizing the synergy of cyber-human systems that will improve the quality of life experiences. It will provide a computing environment within the reach of common people in which the interests or even the health of people can be detected and predicted, with significant impacts on skill learning, education and information retrieval.\r\n\r\nThe project develops an approach to the understanding of complex and subtle facial microexpressions and bio-feedback where the synergy between cyber and human systems can be fully exploited. It addresses key challenges associated with computational understanding and modeling of intelligence in challenging, realistic contexts. It uses assessment and intervention based on facial microexpressions to maximize synergy of cyber and human systems for skill learning. First, it considers deep learning and closed-loop video analysis for optimized skill learning in a reinforcement learning framework. Second, it develops novel representation of facial microexpressions from color and depth video streams and use them for person independent emotion recognition as well as person-specific emotions recognition when a learning task is adapted. Third, it exploits not only the color camera but also the integrated depth camera for precise measurements, which has not been used for microexpressions. The focus is to determine the extent to which real-time classification of microexpressions can provide for more appropriate interactivity that will facilitate human learning in real applications. The results will be broadly disseminated through a website that will have regular releases of databases and software tools by offering tutorials, workshops and demos at major professional meetings.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bir",
   "pi_last_name": "Bhanu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bir Bhanu",
   "pi_email_addr": "bhanu@ece.ucr.edu",
   "nsf_id": "000126207",
   "pi_start_date": "2019-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Seitz",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron R Seitz",
   "pi_email_addr": "a.seitz@northeastern.edu",
   "nsf_id": "000195088",
   "pi_start_date": "2019-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Riverside",
  "inst_street_address": "200 UNIVERSTY OFC BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "RIVERSIDE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9518275535",
  "inst_zip_code": "925210001",
  "inst_country_name": "United States",
  "cong_dist_code": "39",
  "st_cong_dist_code": "CA39",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE",
  "org_prnt_uei_num": "",
  "org_uei_num": "MR5QC5FCAVH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Riverside",
  "perf_str_addr": "200 University Office Building",
  "perf_city_name": "Riverside",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "925210001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "39",
  "perf_st_cong_dist": "CA39",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Facial expressions play a significant role in everyday communication among humans. Computer understanding of these complex and subtle expressions will lead to highly capable interactive cyber-human systems with proactive computers that make more appropriate responses to human interactions. This award has brought together an interdisciplinary team of investigators to address key challenges associated with spontaneous micro-expression recognition in non-social scenarios. The project concentrates on generating bio-feedback from humans while learning skills, such as game playing and online learning, and being recorded and analyzed in continuous color and depth video streams. It develops computer algorithms for human-machine synergy and test how this information can provide for superior learning when training applications are augmented with expression-informed bio-feedback in near real-time. This represents a significant step forward in training machines to recognize and classify facial micro-expressions and maximizing the synergy of cyber-human systems that will improve the quality of life experiences.</p>\n<p>Human emotions fundamentally and ubiquitously affect human behavior whether in interactions with fellow-humans or when interacting with computers. Hence the ability to accurately measure emotional states and respond appropriately to them is of broad consequence across many domains including those of education and health as well as those of entertainment and commerce. In this project, we explored the contribution of monitoring of subtle emotions and model an initial intervention using emotion based feedback in the context of human skill learning for N-back training game. Following are the major achievements for this project: (a) Developed deep learning and graph-based novel approaches for the detection, classification and evaluation of micro-expressions in color (RGB) and color and depth (RGB-D) streams of video data. (b) Developed a simulation for a closed-loop system for deep reinforcement framework for human skill learning for a memory game playing. (c) Carried out scientific experiments and performance evaluation for RGB-D video data collection and other publicly available datasets (SAAM, CASEME II and SMIC). The techniques developed in (a)-(b) were evaluated and refined further based on the analysis. (d) Collected a dataset from 41 healthy adults, recruited from the UCR student population (mean age = 20.79 years; about half males and females) using an RGB-D camera, while participants watched a series of videos directed to elicit emotions of different types (happy, surprise, fear, anger, disgust, contempt, and sadness). Videos have been hand-scored by at least two research assistants and then the results inspected and verified by a third. We have used this dataset in our publications. The project resulted in publication of 8 peer-reviewed papers.</p>\n<p>Graduate an undergraduate students participated in group meetings and discussions and received training on this project. They made presentations in project group meetings. Graduate student mentored students working on the project. Several undergraduate students received training in research, experimentation, data collection, annotations generation and presentation. Project provided opportunity for research on large video datasets, machine learning and data mining, cognitive psychology and the development of algorithms/tools. It provided many situations for the improvement and refinement of oral/written communication skills.</p>\n<p>Understanding complex and subtle human facial expressions as captured in continuous video streams will have a profound impact on human-computer interaction. It will provide a computing environment within the reach of common people in which the interests or even the health of people can be detected and predicted, with significant impacts on skill learning, education and information retrieval. The link for the website is https://vislab.ucr.edu/RESEARCH/exp/exp.php</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/03/2024<br>\nModified by: Bir&nbsp;Bhanu</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774337718_Micro_expressions_for_RGB_D_videos--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774337718_Micro_expressions_for_RGB_D_videos--rgov-800width.png\" title=\"Micro-expressions for RGB-D videos\"><img src=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774337718_Micro_expressions_for_RGB_D_videos--rgov-66x44.png\" alt=\"Micro-expressions for RGB-D videos\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Examples of VISME (UCR) database with RGB-D images consisting of 6 expressions such as Anger, Happy, Sad, Disgust, Fear and Surprise. The first row consists of RGB images, and the second row belongs to their respective depth images.</div>\n<div class=\"imageCredit\">Ankith Jain Rakesh Kumar, Bir Bhanu, Christopher Casey, Sierra Grace Cheung, and Aaron Seitz.</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu\n<div class=\"imageTitle\">Micro-expressions for RGB-D videos</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714775034399_Node_and_Edge_Features--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714775034399_Node_and_Edge_Features--rgov-800width.JPG\" title=\"Node and Edge Features\"><img src=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714775034399_Node_and_Edge_Features--rgov-66x44.JPG\" alt=\"Node and Edge Features\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The process of capturing optical flow patches involves the following steps: (a) capturing two frames of a video, (b) extracting a 10x10 patch of optical flow features for each landmark point from consecutive frames, (c) aggregating these features into a 10x10 matrix, (d) deriving graph node feature.</div>\n<div class=\"imageCredit\">Ankith Jain Rakesh Kumar and Bir Bhanu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu\n<div class=\"imageTitle\">Node and Edge Features</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774698753_Adaptive_Frame_Selection--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774698753_Adaptive_Frame_Selection--rgov-800width.png\" title=\"Adaptive Frame Selection\"><img src=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774698753_Adaptive_Frame_Selection--rgov-66x44.png\" alt=\"Adaptive Frame Selection\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The process of eliminating lower intensity expression frames from a video using an adaptive frame filtering module is rooted in the analysis of optical flow patterns within a sliding window patch.</div>\n<div class=\"imageCredit\">Ankith Jain Rakesh Kumar and Bir Bhanu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu\n<div class=\"imageTitle\">Adaptive Frame Selection</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774813316_Face_Graph_Structure_Construction--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774813316_Face_Graph_Structure_Construction--rgov-800width.png\" title=\"Face Graph Structure Construction\"><img src=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774813316_Face_Graph_Structure_Construction--rgov-66x44.png\" alt=\"Face Graph Structure Construction\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A locally connected input graph representation with 51 landmark nodes. The black color nodes (14) are additional nodes over the eyebrow and close to the mouth region to capture the subtle movements in these regions.</div>\n<div class=\"imageCredit\">Ankith Jain Rakesh Kumar and Bir Bhanu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu\n<div class=\"imageTitle\">Face Graph Structure Construction</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774563083_Motion_Magnification_for_ME_RGB_Videos--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774563083_Motion_Magnification_for_ME_RGB_Videos--rgov-800width.JPG\" title=\"Motion Magnification for micro-expression RGB Videos\"><img src=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714774563083_Motion_Magnification_for_ME_RGB_Videos--rgov-66x44.JPG\" alt=\"Motion Magnification for micro-expression RGB Videos\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Eulerian Motion Magnification for micro-expression videos using various values of magnification from 1 to 4</div>\n<div class=\"imageCredit\">Ankith Jain Rakesh Kumar and Bir Bhanu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu\n<div class=\"imageTitle\">Motion Magnification for micro-expression RGB Videos</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714775162255_Masking_of_node_and_node_features--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714775162255_Masking_of_node_and_node_features--rgov-800width.JPG\" title=\"Masking of node and node features\"><img src=\"/por/images/Reports/POR/2024/1911197/1911197_10640507_1714775162255_Masking_of_node_and_node_features--rgov-66x44.JPG\" alt=\"Masking of node and node features\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In the MAGPOOL process, MaskGAT calculates self-attention scores for each node in the input graph. We then select crucial nodes using Top-K selection based on attention scores and mask the graph accordingly. The final graph contains only nodes relevant for micro-expression classification.</div>\n<div class=\"imageCredit\">Ankith Jain Rakesh Kumar and Bir Bhanu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu\n<div class=\"imageTitle\">Masking of node and node features</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nFacial expressions play a significant role in everyday communication among humans. Computer understanding of these complex and subtle expressions will lead to highly capable interactive cyber-human systems with proactive computers that make more appropriate responses to human interactions. This award has brought together an interdisciplinary team of investigators to address key challenges associated with spontaneous micro-expression recognition in non-social scenarios. The project concentrates on generating bio-feedback from humans while learning skills, such as game playing and online learning, and being recorded and analyzed in continuous color and depth video streams. It develops computer algorithms for human-machine synergy and test how this information can provide for superior learning when training applications are augmented with expression-informed bio-feedback in near real-time. This represents a significant step forward in training machines to recognize and classify facial micro-expressions and maximizing the synergy of cyber-human systems that will improve the quality of life experiences.\n\n\nHuman emotions fundamentally and ubiquitously affect human behavior whether in interactions with fellow-humans or when interacting with computers. Hence the ability to accurately measure emotional states and respond appropriately to them is of broad consequence across many domains including those of education and health as well as those of entertainment and commerce. In this project, we explored the contribution of monitoring of subtle emotions and model an initial intervention using emotion based feedback in the context of human skill learning for N-back training game. Following are the major achievements for this project: (a) Developed deep learning and graph-based novel approaches for the detection, classification and evaluation of micro-expressions in color (RGB) and color and depth (RGB-D) streams of video data. (b) Developed a simulation for a closed-loop system for deep reinforcement framework for human skill learning for a memory game playing. (c) Carried out scientific experiments and performance evaluation for RGB-D video data collection and other publicly available datasets (SAAM, CASEME II and SMIC). The techniques developed in (a)-(b) were evaluated and refined further based on the analysis. (d) Collected a dataset from 41 healthy adults, recruited from the UCR student population (mean age = 20.79 years; about half males and females) using an RGB-D camera, while participants watched a series of videos directed to elicit emotions of different types (happy, surprise, fear, anger, disgust, contempt, and sadness). Videos have been hand-scored by at least two research assistants and then the results inspected and verified by a third. We have used this dataset in our publications. The project resulted in publication of 8 peer-reviewed papers.\n\n\nGraduate an undergraduate students participated in group meetings and discussions and received training on this project. They made presentations in project group meetings. Graduate student mentored students working on the project. Several undergraduate students received training in research, experimentation, data collection, annotations generation and presentation. Project provided opportunity for research on large video datasets, machine learning and data mining, cognitive psychology and the development of algorithms/tools. It provided many situations for the improvement and refinement of oral/written communication skills.\n\n\nUnderstanding complex and subtle human facial expressions as captured in continuous video streams will have a profound impact on human-computer interaction. It will provide a computing environment within the reach of common people in which the interests or even the health of people can be detected and predicted, with significant impacts on skill learning, education and information retrieval. The link for the website is https://vislab.ucr.edu/RESEARCH/exp/exp.php\n\n\n\t\t\t\t\tLast Modified: 06/03/2024\n\n\t\t\t\t\tSubmitted by: BirBhanu\n"
 }
}