{
 "awd_id": "1937599",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RTML: Large: Acceleration to Graph-Based Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 1499997.0,
 "awd_amount": 1799897.0,
 "awd_min_amd_letter_date": "2019-09-10",
 "awd_max_amd_letter_date": "2021-08-27",
 "awd_abstract_narration": "Graphs are ubiquitous, and often the fundamental data structure in many applications including bioinformatics, chemistry, healthcare, social networks, recommender systems and systems analysis.  Machine learning (ML) using graphs is receiving increasing attention, both where graphs are a representation of data, as in graph neural networks (GNN) algorithms, and where graphs are an efficient ML model representation, as in arithmetic circuits representation of probabilistic graphical models.  While useful, graph-based ML poses unique challenges to existing computation hardware (Central Processing Units and Graphics Processing Units) due to the combination of irregular memory access and dynamic parallelism imposed by the graph structure and the dense computation required for relevant learning algorithms, though hardware-based implementations are highly desirable to enable real-time processing of streams of data generated by such applications.  The project addresses these challenges with a novel accelerator architecture for graph-based ML, along with a supporting open source software stack, simulator, and field-programmable gate-array (FPGA) prototype.  Beyond the technical contributions, the project will integrate the latest research into several graduate and upper-division undergraduate courses. The project will also work with the UCLA Center for Excellence in Engineering and Diversity (CEED) and Women in Engineering to recruit highly diversified undergraduate and graduate students to participate in the research. \r\n\r\nThe project targets a programmable and heterogeneous multi-accelerator architecture, with software-controlled compute and memory resources. It is specialized in the following ways to meet the needs of graph-based machine learning.  First, it supports composing accelerator engines for efficient  pipelining of graph-based prefetching with dense computation units. Second, the prefetching hardware will be co-designed with GNN algorithms to support recent and upcoming advances in graph sampling and graph-coarsening algorithms. Third, it will include a high bandwidth scratchpad architecture optimized for indirect access, and spatial compute fabrics (e.g. systolic arrays) optimized for dense computation. Finally, the execution model will be based on an architecture-aware task-parallel model, which has rich-enough primitives to take advantage of heterogeneous hardware, while being flexible enough to load balance for dynamic parallelism.  The key components of the proposed architecture will be prototyped on an FPGA. Overall, the goal of the work is to greatly advance the state-of-the-art of graph-based ML in terms of model accuracy, efficiency, and real-time inference and learning. The project will also collaborate with a synergistic DARPA program for related hardware development.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Cong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jason Cong",
   "pi_email_addr": "cong@cs.ucla.edu",
   "nsf_id": "000301151",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yizhou",
   "pi_last_name": "Sun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yizhou Sun",
   "pi_email_addr": "yzsun@cs.ucla.edu",
   "nsf_id": "000629910",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Nowatzki",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony Nowatzki",
   "pi_email_addr": "tjn@cs.ucla.edu",
   "nsf_id": "000752659",
   "pi_start_date": "2019-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California, Los Angeles, Computer Science Dept.",
  "perf_str_addr": "468A Engineering VI, Phase 2",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "082Z",
   "pgm_ref_txt": "RTML-Real Time Machine Learning"
  },
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  },
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1499997.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 299900.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong id=\"docs-internal-guid-b0630113-7fff-4099-29ce-f7a8fddeb4f3\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\"> </strong></p>\r\n<p style=\"line-height: 1.38; margin-left: -1pt; text-align: justify; margin-top: 0pt; margin-bottom: 1pt; padding: 0pt 0pt 26pt;\" dir=\"ltr\"><strong id=\"docs-internal-guid-b0630113-7fff-4099-29ce-f7a8fddeb4f3\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project focuses on acceleration of graph-based machine learning (ML) with hardware accelerators and hardware-algorithm co-optimization. While much progress has been made in the past decade to provide hardware acceleration for machine learning (ML) on images, speech, and video, there has been limited focus on accelerating ML for </span><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: italic; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">graphs</span><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">. Graphs are ubiquitous, and often the fundamental data structure in applications ranging from bioinformatics, chemistry, healthcare, recommender systems, social network study, to program analysis and network security.</span></strong></p>\r\n<p><strong id=\"docs-internal-guid-b0630113-7fff-4099-29ce-f7a8fddeb4f3\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\">\r\n<p style=\"line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">One emphasis of this project is to understand the power of graph-based ML models, where the team has achieved many important results. One successful example is the development of the Heterogeneous Graph Transformer (HGT, WWW&rsquo;20), which was the first graph transformer designed for heterogeneous graphs.&nbsp; It could handle the heterogeneity of the graph as well as the temporal perspective of many graphs. This project provides foundation solutions to heterogeneous graphs, which has been included in courses and books. It has been successfully applied to different applications, including healthcare, e-commerce, and academic graph research. It has been cited 1364 times since its publication, and is the most cited paper in WWW&rsquo;20.&nbsp;</span></p>\r\n</strong><br class=\"Apple-interchange-newline\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none;\" /><br class=\"Apple-interchange-newline\" /><strong id=\"docs-internal-guid-ef5b3944-7fff-9dc9-7e0e-2c6b45f8f8aa\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Another emphasis of this project is to accelerate various graph-based ML algorithms. Given that most graphs in real life applications are sparse, the core of ML-based graph models involve sparse-matrix dense-matrix multiplication (SpMM), as well as sparse-matrix dense-vector computation (SpMV). So, we developed Sextans (FPGA&rsquo;22) and Serpens (DAC&rsquo;22), the first SpMM and SpMV accelerators on FPGAs with high-bandwidth memory (HBM), and achieved comparable performance to GPUs with the same memory bandwidth for data center scale problems. Both Sextans and Serpens are developed in high-level synthesis (HLS) and have been open-sourced to the community. Sextans and Serpens are now used as a baseline for many graph applications and sparse scientific computing on FPGAs. Since its publication in 2022, Sextans has the highest download and citation counts among all works in FPGA&rsquo;22. Sextans and Serpens have received more than 100 stars on Github and inspired works published in the top conferences in&nbsp; many areas including FPGA (FPGA/FCCM/FPL), computer </span></strong><strong id=\"docs-internal-guid-8c048355-7fff-c578-deef-5e1aa974b22f\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">architecture (ISCA/MICRO), design automation (DAC/ICCAD/DATE/TCAD), high-performance computing (SC/IPDPS), and multiple recent master's and doctoral theses.</span></strong></p>\r\n<p>&nbsp;</p>\r\n<p><strong id=\"docs-internal-guid-6ece94a7-7fff-7251-e769-a4cfda3a8696\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The third focus of this project is to develop a general framework to make it easier to deploy FPGA-based acceleration for various ML models.&nbsp; With this goal in mind, the team developed OverGen (Micro&rsquo; 22), which presented an alternative programming strategy to High-level synthesis (HLS) for FPGAs, and won the best-paper runner-up award. Traditional HLS requires FPGA physical design, which can take hours of compilation time, and FPGA reconfiguration time limits HLS from targeting complex workloads.&nbsp; Thus, we developed a new FPGA programming framework called OverGen, where an overlay architecture is automatically specialized to a set of representative applications.&nbsp; OverGen overcomes the limitations of prior overlay architectures through automated specialization.&nbsp; While it requires more resources to support more general architectures, experimental results show that OverGen achieves nearly equivalent performance as state-of-the-art HLS solutions, while requiring 10,000x less compile and reconfiguration time.&nbsp; </span></strong><strong id=\"docs-internal-guid-7ead68b6-7fff-b86d-c30d-9649a1bb3ba7\" style=\"caret-color: #000000; color: #000000; font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; text-decoration: none; font-weight: normal;\"><span style=\"font-size: 11pt; font-family: Arial, sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-alternates: normal; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-position: normal; font-variant-emoji: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This framework was also a platform for developing novel execution models for task parallelism, which were of key importance in high-performance execution of graph-based ML models.&nbsp; PolyGraph (ISCA 2021) and TaskStream (ASPLOS 2022) developed a hierarchical dataflow model to capture task-parallelism and specialize inter-task parallelism.</span></strong></p><br>\n<p>\n Last Modified: 12/20/2024<br>\nModified by: Jason&nbsp;Cong</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1937599/1937599_10641273_1734734016474_Sextans_and_Serpens_Architectures--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1937599/1937599_10641273_1734734016474_Sextans_and_Serpens_Architectures--rgov-800width.png\" title=\"Sextans and Serpens Architectures\"><img src=\"/por/images/Reports/POR/2024/1937599/1937599_10641273_1734734016474_Sextans_and_Serpens_Architectures--rgov-66x44.png\" alt=\"Sextans and Serpens Architectures\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The architectures of Sextans accelerator for SpMM and Serpens accelerator for SpMV.</div>\n<div class=\"imageCredit\">Sextans(FPGA?22) and Serpens(DAC?22)</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Cong\n<div class=\"imageTitle\">Sextans and Serpens Architectures</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThis project focuses on acceleration of graph-based machine learning (ML) with hardware accelerators and hardware-algorithm co-optimization. While much progress has been made in the past decade to provide hardware acceleration for machine learning (ML) on images, speech, and video, there has been limited focus on accelerating ML for graphs. Graphs are ubiquitous, and often the fundamental data structure in applications ranging from bioinformatics, chemistry, healthcare, recommender systems, social network study, to program analysis and network security.\r\n\n\n\r\n\n\nOne emphasis of this project is to understand the power of graph-based ML models, where the team has achieved many important results. One successful example is the development of the Heterogeneous Graph Transformer (HGT, WWW20), which was the first graph transformer designed for heterogeneous graphs. It could handle the heterogeneity of the graph as well as the temporal perspective of many graphs. This project provides foundation solutions to heterogeneous graphs, which has been included in courses and books. It has been successfully applied to different applications, including healthcare, e-commerce, and academic graph research. It has been cited 1364 times since its publication, and is the most cited paper in WWW20.\r\n\n\nAnother emphasis of this project is to accelerate various graph-based ML algorithms. Given that most graphs in real life applications are sparse, the core of ML-based graph models involve sparse-matrix dense-matrix multiplication (SpMM), as well as sparse-matrix dense-vector computation (SpMV). So, we developed Sextans (FPGA22) and Serpens (DAC22), the first SpMM and SpMV accelerators on FPGAs with high-bandwidth memory (HBM), and achieved comparable performance to GPUs with the same memory bandwidth for data center scale problems. Both Sextans and Serpens are developed in high-level synthesis (HLS) and have been open-sourced to the community. Sextans and Serpens are now used as a baseline for many graph applications and sparse scientific computing on FPGAs. Since its publication in 2022, Sextans has the highest download and citation counts among all works in FPGA22. Sextans and Serpens have received more than 100 stars on Github and inspired works published in the top conferences in many areas including FPGA (FPGA/FCCM/FPL), computer architecture (ISCA/MICRO), design automation (DAC/ICCAD/DATE/TCAD), high-performance computing (SC/IPDPS), and multiple recent master's and doctoral theses.\r\n\n\n\r\n\n\nThe third focus of this project is to develop a general framework to make it easier to deploy FPGA-based acceleration for various ML models. With this goal in mind, the team developed OverGen (Micro 22), which presented an alternative programming strategy to High-level synthesis (HLS) for FPGAs, and won the best-paper runner-up award. Traditional HLS requires FPGA physical design, which can take hours of compilation time, and FPGA reconfiguration time limits HLS from targeting complex workloads. Thus, we developed a new FPGA programming framework called OverGen, where an overlay architecture is automatically specialized to a set of representative applications. OverGen overcomes the limitations of prior overlay architectures through automated specialization. While it requires more resources to support more general architectures, experimental results show that OverGen achieves nearly equivalent performance as state-of-the-art HLS solutions, while requiring 10,000x less compile and reconfiguration time. This framework was also a platform for developing novel execution models for task parallelism, which were of key importance in high-performance execution of graph-based ML models. PolyGraph (ISCA 2021) and TaskStream (ASPLOS 2022) developed a hierarchical dataflow model to capture task-parallelism and specialize inter-task parallelism.\t\t\t\t\tLast Modified: 12/20/2024\n\n\t\t\t\t\tSubmitted by: JasonCong\n"
 }
}