{
 "awd_id": "1913006",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Information-Based Complexity Analysis for Large-Scale Nonlinear Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 243756.0,
 "awd_amount": 243756.0,
 "awd_min_amd_letter_date": "2019-06-21",
 "awd_max_amd_letter_date": "2019-06-21",
 "awd_abstract_narration": "Recent years have seen rapid advances in the fields of data analysis, which has impacts in various disciplines. Optimization has been an important tool in solving problems arising from data analysis applications. Modern applications with large volume datasets and sophisticated data structures bring great challenges to designing scalable and efficient numerical optimization algorithms for large-scale nonlinear optimization problems. While the goal of optimization algorithm design is to solve the problems of interest as efficiently as possible, it is equally important to study the complexity of the problems of interest themselves. In particular, the complexity analysis of a class of nonlinear optimization problems reveals the fundamental performance limits of any algorithms for solving problems in such class. The discovered performance limits would then encourage one to design efficient algorithms that reach such performance limits. \r\n \r\nFirst-order methods are a class of numerical optimization algorithms that only need to access the information on function value and first-order derivatives. Due to the computational efficiency and scalability, first-order methods have been widely used to solve large-scale nonlinear optimization problems. This proposal addresses the important question of performance limits of first-order methods through the information-based complexity theory. The problems of interests are nonlinear optimization with different special structures. In order to accelerate computation, many special problem structures have been explored in the literature on algorithm design. By utilizing the problem structure, several newly designed algorithms are able to achieve improved computational performance. However, comparing with the rapidly growing number of new and novel algorithms on structured nonlinear optimization, the complexity analysis and the design of worse-case instances does not match with the advancement in algorithm design. This proposal aims to close some of the aforementioned gaps by constructing several worst-case examples that demonstrate the performance limits of first-order methods, in the hope of broaden the understanding of efficiency of first-order methods and the difficulty of certain nonlinear optimization models.\r\n\r\nThis project is jointly funded by Computational Mathematics Program, DMS/MPS, and the Established Program to Stimulate Competitive Research (EPSCoR).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuyuan",
   "pi_last_name": "Ouyang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuyuan Ouyang",
   "pi_email_addr": "yuyuano@clemson.edu",
   "nsf_id": "000756042",
   "pi_start_date": "2019-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Clemson University",
  "inst_street_address": "201 SIKES HALL",
  "inst_street_address_2": "",
  "inst_city_name": "CLEMSON",
  "inst_state_code": "SC",
  "inst_state_name": "South Carolina",
  "inst_phone_num": "8646562424",
  "inst_zip_code": "296340001",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "SC03",
  "org_lgl_bus_name": "CLEMSON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "H2BMNX7DSKU8"
 },
 "perf_inst": {
  "perf_inst_name": "Clemson University",
  "perf_str_addr": "Martin Hall O-207, Clemson Unive",
  "perf_city_name": "Clemson",
  "perf_st_code": "SC",
  "perf_st_name": "South Carolina",
  "perf_zip_code": "296340001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "SC03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 243756.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this research we study the information-based complexity bounds of first-order methods on solving large-scale convex optimization and the more general monotone variational inequalities problems. The research project has enhanced our understanding of the theoretical performance limit of first-order methods for such problems. The understanding of such bounds further motivated novel algorithm design of first-order methods with better theoretical performance. We have shown that on solving affine constrained convex smooth optimization, first-order methods could not generally accelerate from the known slower convergence rate O(t<sup>-1</sup>) to the faster O(t<sup>-2</sup>), and in addition, O(t<sup>-1</sup>) is optimal for such problems. This result distinguishes affine constrained problems from unconstrained convex smooth problems from the information-based complexity perspective. We have also designed worst-case datasets for demonstrating the performance limits of first-order methods on solving the binary logistic regression machine learning model. Our result on lower complexity bounds motivates us to design new algorithms with better theoretical performance for solving unconstrained and constrained convex optimization, as well as for the more general problem of monotone variational inequalities.</p>\n<p>&nbsp;</p>\n<p>Beyond technical contributions, this research has a broad impact on education. Two PhD students actively participated in the proposed research projects and learned optimization theory, algorithm design, and complexity analysis of optimization algorithms. Both students successfully defended their PhD dissertation. The PhD training has also enabled them to pursue their careers, and both students have landed job placements in fields directly related to this research. New graduate student courses are designed and taught to train a new generation of graduate students on information-based complexity analysis for optimization and machine learning. The PI also led undergraduate research and K-12 summer research programs on complexity analysis and optimization algorithm design on machine learning. These educational efforts aim to inspire the younger generation to pursue their direction of study in fields related to this research.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/19/2024<br>\nModified by: Yuyuan&nbsp;Ouyang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this research we study the information-based complexity bounds of first-order methods on solving large-scale convex optimization and the more general monotone variational inequalities problems. The research project has enhanced our understanding of the theoretical performance limit of first-order methods for such problems. The understanding of such bounds further motivated novel algorithm design of first-order methods with better theoretical performance. We have shown that on solving affine constrained convex smooth optimization, first-order methods could not generally accelerate from the known slower convergence rate O(t-1) to the faster O(t-2), and in addition, O(t-1) is optimal for such problems. This result distinguishes affine constrained problems from unconstrained convex smooth problems from the information-based complexity perspective. We have also designed worst-case datasets for demonstrating the performance limits of first-order methods on solving the binary logistic regression machine learning model. Our result on lower complexity bounds motivates us to design new algorithms with better theoretical performance for solving unconstrained and constrained convex optimization, as well as for the more general problem of monotone variational inequalities.\n\n\n\n\n\nBeyond technical contributions, this research has a broad impact on education. Two PhD students actively participated in the proposed research projects and learned optimization theory, algorithm design, and complexity analysis of optimization algorithms. Both students successfully defended their PhD dissertation. The PhD training has also enabled them to pursue their careers, and both students have landed job placements in fields directly related to this research. New graduate student courses are designed and taught to train a new generation of graduate students on information-based complexity analysis for optimization and machine learning. The PI also led undergraduate research and K-12 summer research programs on complexity analysis and optimization algorithm design on machine learning. These educational efforts aim to inspire the younger generation to pursue their direction of study in fields related to this research.\n\n\n\t\t\t\t\tLast Modified: 09/19/2024\n\n\t\t\t\t\tSubmitted by: YuyuanOuyang\n"
 }
}