{
 "awd_id": "1838808",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SCH: INT: Adaptive Partnership for the Robotic Treatment of Autism",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 1196361.0,
 "awd_amount": 1228361.0,
 "awd_min_amd_letter_date": "2019-09-16",
 "awd_max_amd_letter_date": "2022-06-16",
 "awd_abstract_narration": "This project targets research on Human-Robot Interaction (HRI) between children with Autism Spectrum Disorder (ASD) and engineered, autonomous, facially-expressive, human-like robots that can assess and adapt to personalized therapeutic goals. Adaptive partner robots specifically targeting ASD hold the potential to revolutionize treatment and improve the lives and productivity of millions of Americans. We envision that the next generation ASD robot will be used primarily in the home, in addition to specialized treatment facilities. The envisioned robotic partners will serve as therapists, instructors, passive observers, or active learning peers, thus supplementing the role of important individuals: parents, caregivers, or teachers. The further exploration of the application of groups of robots and children in therapy will enrich and deepen our research understanding and could produce meaningful future outcomes for individuals with ASD and their families. The proposed interactions represent a unique opportunity to capture and objectively measure the natural behavior of ASD children in multiple contexts, irrespective of the age, language or cognitive ability, because it is based on both motor and emotional responses. \r\n\r\nThe research undertaken in this project has several novel components, including formulating new quantitative, non-invasive markers, eventually integrated into a new ASD scale for diagnosis and treatment based on real-time evaluation of motions and emotions during interaction between robot and child. The ASD severity scale will be based on a dynamic time-warping motion metric and on a continuous emotion metric assessed by the robot during interaction. The scale will be used by the robot to adapt therapeutic activities in a personalized manner for each child in one-one one therapy sessions, and for several children and robots in group therapy sessions. With the help of industry partners, we will embed the resulting novel motor and cognitive behaviors into a nonthreatening, partner robot, specifically designed for children with ASD, endowed with a friendly appearance, behavioral switching and neuroadaptive controllers. The robot will be capable of collecting large amounts of data that may be useful to elucidate unique connections between the motor, sensory and emotional brain functions of the ASD population.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dan",
   "pi_last_name": "Popa",
   "pi_mid_init": "O",
   "pi_sufx_name": "",
   "pi_full_name": "Dan O Popa",
   "pi_email_addr": "dan.popa@louisville.edu",
   "nsf_id": "000085594",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karla",
   "pi_last_name": "Welch",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Karla C Welch",
   "pi_email_addr": "karla.welch@louisville.edu",
   "nsf_id": "000563059",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Pennington",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Robert Pennington",
   "pi_email_addr": "robert.pennington@louisville.edu",
   "nsf_id": "000614315",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Barnes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory Barnes",
   "pi_email_addr": "gregory.barnes@louisville.edu",
   "nsf_id": "000780205",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Louisville Research Foundation Inc",
  "inst_street_address": "2301 S 3RD ST",
  "inst_street_address_2": "",
  "inst_city_name": "LOUISVILLE",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "5028523788",
  "inst_zip_code": "402081838",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "KY03",
  "org_lgl_bus_name": "UNIVERSITY OF LOUISVILLE",
  "org_prnt_uei_num": "",
  "org_uei_num": "E1KJM4T54MK6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Louisville Research Foundation Inc",
  "perf_str_addr": "WS Speed 200",
  "perf_city_name": "Louisville",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "402920001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "KY03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801800",
   "pgm_ele_name": "Smart and Connected Health"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  },
  {
   "pgm_ref_code": "8062",
   "pgm_ref_txt": "SCH Type II: INT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1196361.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, a team of researchers from University of Louisville created social robotics tools that will be used by researchers, practitioners, and families affected by Autism Spectrum Disorders (ASD). The research will lead to robot partners with &ldquo;embodied intelligence,&rdquo; that can collaborate with individuals and groups of humans aiding in connecting people (ASD patients and caregivers in group therapy sessions) and connecting ASD therapy systems by sharing information between robot controllers and automating the delivery of standard ASD diagnosis tests. The project offered a quantitative approach for capturing and adaptively using data with ASD treatment relevance and had&nbsp;4 goals:</p>\r\n<ol>\r\n<li>Establish and validate a new ASD quantitative severity scale.</li>\r\n<li>Propose neuro-inspired adaptive robot therapy.</li>\r\n<li>Engineering of ASD robot for the home, learning, or therapy facility.&nbsp;</li>\r\n<li>Conduct Human-Robot Interaction studies in one to one and in multiple robots to multiple children group therapy.</li>\r\n</ol>\r\n<p>During this project we collected data from over 300 subjects, including adults, children aged 8-12, both&nbsp;neuro-typical and also with ASD diagnoses. The purpose of experiments was to develop and validate robot motion as well as emotional imitation metrics, by engaging with&nbsp;the robot and child's upper body gestures, facial expressions, speech, and psysiological signals. For each interaction modality we developed appropriate Machine Learning algorithms to understand and classify&nbsp;the quality of interaction, and have the robot adapt their repertoire to make it easier for children to follow instructions.&nbsp;</p>\r\n<p>We validated our metrics with ASD populations recruited by our partner organization, the Norton Children's Autism Center, and showcased our robots in several community outreach events such as the Louisville Feat 2024, Makefaire 2024, and the St. Aloysius Catholic School. The robots employed in our experiments included facial expression capable units Zeno, a custom robot, and Milo, a commercial robot. It also included NAO commercial robot units.</p>\r\n<p>The project resulted in numerous journal and conference publications, and helped train 3 postdocs, 3 Ph.D. students, 3 M.S. students, and over 12 undergraduate researchers who worked with&nbsp;the investigators to conduct and process experimental data in a convergent research environment at the Louisville Automation and Robotics Research Institute (LARRI).</p>\r\n<p>Our findings confirm that children with ASD have motion and emotional imitation scores lower than neurotypical children, and these metrics can serve as guidelines for evaluating the efficacy of robot-assisted therapy interventions. Results also indicate that it is possible to adapt the robot intervention to the needs of all children, not just children with ASD. Furthermore, preliminary findings also point&nbsp;to&nbsp;to the fact&nbsp;that some interventions, such as motion imitation, are more appropriate than other interventions, such as social stories, for nonverbal ASD children.</p>\r\n<p>The results from this study will be used to guide further clinical research to validate these findings, and to serve as guidelines for developing future robot-assisted interventions for the ASD population.</p><br>\n<p>\n Last Modified: 01/13/2025<br>\nModified by: Dan&nbsp;O&nbsp;Popa</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803271692_multimodal_pipeline--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803271692_multimodal_pipeline--rgov-800width.jpg\" title=\"Multimodal Pipeline\"><img src=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803271692_multimodal_pipeline--rgov-66x44.jpg\" alt=\"Multimodal Pipeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Machine Learning pipeline for fusing visual (Vision Transformers) and emotional (CNNs for Blood Volume Pulse) responses to stories for children with ASD.</div>\n<div class=\"imageCredit\">Ruchik Mishra</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Multimodal Pipeline</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736802984591_Aloysius_2--rgov-214x142.JPG\" original=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736802984591_Aloysius_2--rgov-800width.JPG\" title=\"Aloysius 2024\"><img src=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736802984591_Aloysius_2--rgov-66x44.JPG\" alt=\"Aloysius 2024\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Team showcasing the Milo and Nao robots and research to K-8 students at St. Aloysius Catholic School in Louisville, KY, 2024.</div>\n<div class=\"imageCredit\">Irina Kondaurova</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Aloysius 2024</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803397450_Speech_pipeline--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803397450_Speech_pipeline--rgov-800width.jpg\" title=\"Speech Analysis with NAO\"><img src=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803397450_Speech_pipeline--rgov-66x44.jpg\" alt=\"Speech Analysis with NAO\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Team validated a Machine Learning approach to classifying emotional responses from speech data using MEL Spectrograms.</div>\n<div class=\"imageCredit\">Ruchik Mishra</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Speech Analysis with NAO</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803114788_Zeno_SODTW--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803114788_Zeno_SODTW--rgov-800width.jpg\" title=\"Zeno experiments\"><img src=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803114788_Zeno_SODTW--rgov-66x44.jpg\" alt=\"Zeno experiments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The team proposed a metric for motor robot imitation called Segment Online Dynamic Time Warping, and a Machine Learning approach to adapting robot motion based on this metric to make it easier to follow the robot during interaction.</div>\n<div class=\"imageCredit\">Ali Ashary</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Zeno experiments</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803835894_Zeno_LARRI--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803835894_Zeno_LARRI--rgov-800width.jpg\" title=\"Zeno Robot\"><img src=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736803835894_Zeno_LARRI--rgov-66x44.jpg\" alt=\"Zeno Robot\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Zeno robot at Louisville Automation and Robotics Research Institute Social Robotics Lab was used to validate imitation performance for children with ASD.</div>\n<div class=\"imageCredit\">Ali Ashary</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Zeno Robot</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736802881881_lou_feat--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736802881881_lou_feat--rgov-800width.jpg\" title=\"Louisville Feat 2024\"><img src=\"/por/images/Reports/POR/2025/1838808/1838808_10642299_1736802881881_lou_feat--rgov-66x44.jpg\" alt=\"Louisville Feat 2024\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Team showing robots to community event for ASD population in Louisville KY.</div>\n<div class=\"imageCredit\">Dan Popa</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Louisville Feat 2024</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, a team of researchers from University of Louisville created social robotics tools that will be used by researchers, practitioners, and families affected by Autism Spectrum Disorders (ASD). The research will lead to robot partners with embodied intelligence, that can collaborate with individuals and groups of humans aiding in connecting people (ASD patients and caregivers in group therapy sessions) and connecting ASD therapy systems by sharing information between robot controllers and automating the delivery of standard ASD diagnosis tests. The project offered a quantitative approach for capturing and adaptively using data with ASD treatment relevance and had4 goals:\r\n\r\nEstablish and validate a new ASD quantitative severity scale.\r\nPropose neuro-inspired adaptive robot therapy.\r\nEngineering of ASD robot for the home, learning, or therapy facility.\r\nConduct Human-Robot Interaction studies in one to one and in multiple robots to multiple children group therapy.\r\n\r\n\n\nDuring this project we collected data from over 300 subjects, including adults, children aged 8-12, bothneuro-typical and also with ASD diagnoses. The purpose of experiments was to develop and validate robot motion as well as emotional imitation metrics, by engaging withthe robot and child's upper body gestures, facial expressions, speech, and psysiological signals. For each interaction modality we developed appropriate Machine Learning algorithms to understand and classifythe quality of interaction, and have the robot adapt their repertoire to make it easier for children to follow instructions.\r\n\n\nWe validated our metrics with ASD populations recruited by our partner organization, the Norton Children's Autism Center, and showcased our robots in several community outreach events such as the Louisville Feat 2024, Makefaire 2024, and the St. Aloysius Catholic School. The robots employed in our experiments included facial expression capable units Zeno, a custom robot, and Milo, a commercial robot. It also included NAO commercial robot units.\r\n\n\nThe project resulted in numerous journal and conference publications, and helped train 3 postdocs, 3 Ph.D. students, 3 M.S. students, and over 12 undergraduate researchers who worked withthe investigators to conduct and process experimental data in a convergent research environment at the Louisville Automation and Robotics Research Institute (LARRI).\r\n\n\nOur findings confirm that children with ASD have motion and emotional imitation scores lower than neurotypical children, and these metrics can serve as guidelines for evaluating the efficacy of robot-assisted therapy interventions. Results also indicate that it is possible to adapt the robot intervention to the needs of all children, not just children with ASD. Furthermore, preliminary findings also pointtoto the factthat some interventions, such as motion imitation, are more appropriate than other interventions, such as social stories, for nonverbal ASD children.\r\n\n\nThe results from this study will be used to guide further clinical research to validate these findings, and to serve as guidelines for developing future robot-assisted interventions for the ASD population.\t\t\t\t\tLast Modified: 01/13/2025\n\n\t\t\t\t\tSubmitted by: DanOPopa\n"
 }
}