{
 "awd_id": "1850335",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Experimental Studies of Human Trust in Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-04-15",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 174992.0,
 "awd_amount": 174992.0,
 "awd_min_amd_letter_date": "2019-04-03",
 "awd_max_amd_letter_date": "2019-04-03",
 "awd_abstract_narration": "Machine learning (ML) has been applied to various domains including finance, healthcare, urban operations, and targeted news and advertising.  They achieve great success uncovering insights from massive data and advancing decision making.  Despite widespread applications, scientific understanding of peoples' trust or lack of trust in ML approaches is lacking, while the success of human-machine collaborations requires a deeper understanding of trust.  This project advances the understanding of lay people's trust in ML through large-scale randomized human-subject experiments. The project will result in theoretical insights on the formation and maintenance of trust between humans and ML systems, and practical insights for designing systems acceptable to people.\r\n\r\nLeveraging existing theoretical models of trust in automation, this project will answer three fundamental questions related to lay people's trust in machine learning.  How does the performance of an ML system (correctness, reliability, and predictability) affect people's trust in it?  How does its interpretability affect people's trust in it?  How do the performance and interpretability interact with each other to influence trust? The project will:(1) advance theoretical and empirical understanding about the development and maintenance of trust in ML systems (and compare with trust in conventional automated systems), (2) provide design guidelines that help instill trust in ML systems, and (3) develop an experimental framework for evaluating and benchmarking trustworthiness of ML systems in a systematic manner.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ming",
   "pi_last_name": "Yin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ming Yin",
   "pi_email_addr": "mingyin@purdue.edu",
   "nsf_id": "000784929",
   "pi_start_date": "2019-04-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "155 South Grant Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072114",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 174992.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we have obtained a comprehensive understanding of how people's trust in and reliance on machine learning (ML) models are affected by the model's performance and interpretability through conducting a series of randomized human-subject experiments. For example, we empirically analyzed how the ML model's performance measurements or estimates, such as its stated accuracy, observed accuracy, and confidence scores, influence people's trust in the model. We also explored the heursitics that people utilize to adjust their trust in ML models when performance information about the ML model is absent. Moreover, we investigated into whether and how common types of explanation methods of ML models can help people better understand the models, better recognize the uncertainty underlying the model's prediction, and trust the model in a more appropriate way. This is complemented by additional studies on understanding how people's trust in ML models changes with the changes in ML models' explanations due to the model updates.&nbsp;</p>\n<p>Beyond the research originally planned, we have also extended our investigations of people's trust in machine learning models into a few new and critical directions, including (1) understanding&nbsp;how people trust machine learning models on the out-of-distribution data that is different from those data on which the model is trained; (2) understanding how people trust machine learning models when they interact with the model as a group rather than an individual; (3) building computational models to quantatitively characterize and predict the dynamics of people's trust in ML models and their reliance behavior; and (4) design intervention methods to train people to trust and rely on ML models in a more appropriate manner.</p>\n<p>This research has led to more than 10 publications at top-tier human-computer interaction and AI conferences, including 3 award-winning papers (one best paper at CHI, one best paper at CSCW, and one best paper honoranle mention at CHI). Our research results have also been incorporated to human-AI interaction curriculum at multiple institutions worldwide.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/27/2023<br>\n\t\t\t\t\tModified by: Ming&nbsp;Yin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, we have obtained a comprehensive understanding of how people's trust in and reliance on machine learning (ML) models are affected by the model's performance and interpretability through conducting a series of randomized human-subject experiments. For example, we empirically analyzed how the ML model's performance measurements or estimates, such as its stated accuracy, observed accuracy, and confidence scores, influence people's trust in the model. We also explored the heursitics that people utilize to adjust their trust in ML models when performance information about the ML model is absent. Moreover, we investigated into whether and how common types of explanation methods of ML models can help people better understand the models, better recognize the uncertainty underlying the model's prediction, and trust the model in a more appropriate way. This is complemented by additional studies on understanding how people's trust in ML models changes with the changes in ML models' explanations due to the model updates. \n\nBeyond the research originally planned, we have also extended our investigations of people's trust in machine learning models into a few new and critical directions, including (1) understanding how people trust machine learning models on the out-of-distribution data that is different from those data on which the model is trained; (2) understanding how people trust machine learning models when they interact with the model as a group rather than an individual; (3) building computational models to quantatitively characterize and predict the dynamics of people's trust in ML models and their reliance behavior; and (4) design intervention methods to train people to trust and rely on ML models in a more appropriate manner.\n\nThis research has led to more than 10 publications at top-tier human-computer interaction and AI conferences, including 3 award-winning papers (one best paper at CHI, one best paper at CSCW, and one best paper honoranle mention at CHI). Our research results have also been incorporated to human-AI interaction curriculum at multiple institutions worldwide.\n\n\t\t\t\t\tLast Modified: 07/27/2023\n\n\t\t\t\t\tSubmitted by: Ming Yin"
 }
}