{
 "awd_id": "1932377",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Small: Real-time spatial audio on the Internet of Things",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499944.0,
 "awd_amount": 515944.0,
 "awd_min_amd_letter_date": "2019-09-09",
 "awd_max_amd_letter_date": "2020-03-24",
 "awd_abstract_narration": "As the Internet of Things proliferates, many networked devices fill the home, the office, and public spaces, often with speakers for auditory response. This project strives to use such speakers to perceptually place auditory sounds in arbitrary virtual spaces. Towards this goal, the project studies networked software systems for auditory crosstalk cancellation. The research project investigates practical foundational issues regarding: (i) the design of a visual tracking-driven feedback system for calibration and utilization of sound propagation models, (ii) the design of sounds that optimally spatialize on the spatial audio system, and (iii) the design of a networked system infrastructure to generate spatialized sounds from a computationally heterogeneous variety of Internet of Things devices. To approach such issues, the project designs and characterizes a family of system methods that invoke human-in-the-loop calibration of acoustical models through augmented reality technologies and interfaces, as well as leveraging cloud-based virtual audio propagation simulations for real-time decision-making. The project evaluates designs and methods through user studies on a prototype testbed that includes simulation frameworks, prototyped wireless audio systems and integration with existing Internet of Things devices and environments.\r\n\r\nThe research offers transformative capabilities to the Internet of Things by enabling audio-based spatial guidance systems and spatial personal assistance. Such systems would be beneficial in assistive technologies for visually impaired and memory impaired individuals. The spatial guidance could also assist first responders in navigating emergency personnel towards spatially tracked individuals in foreign environments. Finally, spatial audio systems could greatly enhance commercial Internet of Things capabilities, offering the ability to place responsive sounds to allow users to e.g., locate misplaced objects, respond to spatially positioned virtual assistants, and provide auditory feedback from speaker-less devices, e.g., books, electrical outlets, exercise equipment, etc. To promote broader utility, the project integrates findings in custom open source software frameworks. Towards broadening access to computing, the project exercises outreach activities, including integrating the research into a high school summer camp, university curricula, and web-based and/or augmented reality demonstrations of audio propagation models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "LiKamWa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Robert LiKamWa",
   "pi_email_addr": "likamwa@asu.edu",
   "nsf_id": "000724741",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Visar",
   "pi_last_name": "Berisha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Visar Berisha",
   "pi_email_addr": "visar@asu.edu",
   "nsf_id": "000672841",
   "pi_start_date": "2019-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "P.O. Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499944.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The \"CPS: Small: Real-time spatial audio on the Internet of Things\" project sought to integrate spatial audio integration within IoT infrastructures. The team aimed to transform conventional spaces into immersive auditory environments, thereby redefining human-technology and space interaction dynamics.</p>\n<h4>Intellectual Merit</h4>\n<p>The core objective of the project was to utilize IoT infrastructure capabilities to deliver spatial audio via a network of speakers, overcoming the limitations of traditional audio processing techniques within dynamic IoT environments. The team devised a geometry-based dynamic crosstalk cancellation algorithm, enabling precise audio positioning in space, marking a significant advancement in the field.</p>\n<p>Inspired by the progressive capabilities of speaker-enabled IoT devices, advancements in tracking/localization, and the demand for sophisticated auditory processing in virtual environments, the project introduced innovative system methods. These included human-in-the-loop calibration of acoustical models through augmented reality and cloud-based simulations for real-time audio propagation decision-making. Such methodologies not only enriched interactions with everyday objects but also paved new pathways for in-home navigation assisted by spatial audio.</p>\n<h4>Achievements</h4>\n<ul>\n<li><span style=\"font-weight: normal;\">The research team successfully implemented the Xblock algorithm, integrating it into the Unity platform as a plugin. This integration facilitated the creation of virtual environments (VEs) with enhanced spatial audio capabilities, tailored to operate efficiently across a broad spectrum of acoustic conditions, both in simulations and real-world settings.</span></li>\n<li>Through simulations and live testing, it was evidenced that audio processed via the Xblock algorithm achieved superior acoustic localization compared to traditional methods. The empirical data showcased a marked reduction in error rates for Interaural Level Differences (ILD) and Interaural Time Differences (ITD), especially within specific spatial arrangements.</li>\n<li>The efficacy of Xblock was evaluated through spatial tasks and listening tests involving 15 participants. The findings demonstrated that participants could localize sound sources more swiftly and accurately when audio was processed through Xblock, particularly with certain audio stimuli. Statistical analyses further validated the effectiveness of the Xblock algorithm in crafting more immersive and intuitive auditory environments.</li>\n</ul>\n<h4>Broader Impacts</h4>\n<p>The outcomes of this project have wide-reaching implications:</p>\n<ul>\n<li><span>Accessibility Improvements:</span> The project's innovations promise to enhance navigability for individuals with visual impairments, employing spatial audio cues to guide them through spaces with unparalleled precision.</li>\n<li><span>Intellectual Property and Technology Transfer:</span> The successful application of the Xblock algorithm has led to a patent application, aiming for a shift toward more immersive audio generation from infrastructure in both consumer products and professional settings.</li>\n</ul><br>\n<p>\n Last Modified: 02/13/2024<br>\nModified by: Robert&nbsp;Likamwa</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1932377/1932377_10640874_1707810281324_User_Experimentation_Setup--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1932377/1932377_10640874_1707810281324_User_Experimentation_Setup--rgov-800width.png\" title=\"Experimentation setup for Xblock localization study\"><img src=\"/por/images/Reports/POR/2024/1932377/1932377_10640874_1707810281324_User_Experimentation_Setup--rgov-66x44.png\" alt=\"Experimentation setup for Xblock localization study\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Experimentation setup to evaluate how quickly a user could localize a specific sound source. Comparisons between spatialized sound generated from Xblock vs. sound produced directly from headset-computed stereo audio.</div>\n<div class=\"imageCredit\">Frank Liu</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Robert&nbsp;Likamwa\n<div class=\"imageTitle\">Experimentation setup for Xblock localization study</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe \"CPS: Small: Real-time spatial audio on the Internet of Things\" project sought to integrate spatial audio integration within IoT infrastructures. The team aimed to transform conventional spaces into immersive auditory environments, thereby redefining human-technology and space interaction dynamics.\nIntellectual Merit\n\n\nThe core objective of the project was to utilize IoT infrastructure capabilities to deliver spatial audio via a network of speakers, overcoming the limitations of traditional audio processing techniques within dynamic IoT environments. The team devised a geometry-based dynamic crosstalk cancellation algorithm, enabling precise audio positioning in space, marking a significant advancement in the field.\n\n\nInspired by the progressive capabilities of speaker-enabled IoT devices, advancements in tracking/localization, and the demand for sophisticated auditory processing in virtual environments, the project introduced innovative system methods. These included human-in-the-loop calibration of acoustical models through augmented reality and cloud-based simulations for real-time audio propagation decision-making. Such methodologies not only enriched interactions with everyday objects but also paved new pathways for in-home navigation assisted by spatial audio.\nAchievements\n\nThe research team successfully implemented the Xblock algorithm, integrating it into the Unity platform as a plugin. This integration facilitated the creation of virtual environments (VEs) with enhanced spatial audio capabilities, tailored to operate efficiently across a broad spectrum of acoustic conditions, both in simulations and real-world settings.\nThrough simulations and live testing, it was evidenced that audio processed via the Xblock algorithm achieved superior acoustic localization compared to traditional methods. The empirical data showcased a marked reduction in error rates for Interaural Level Differences (ILD) and Interaural Time Differences (ITD), especially within specific spatial arrangements.\nThe efficacy of Xblock was evaluated through spatial tasks and listening tests involving 15 participants. The findings demonstrated that participants could localize sound sources more swiftly and accurately when audio was processed through Xblock, particularly with certain audio stimuli. Statistical analyses further validated the effectiveness of the Xblock algorithm in crafting more immersive and intuitive auditory environments.\n\nBroader Impacts\n\n\nThe outcomes of this project have wide-reaching implications:\n\nAccessibility Improvements: The project's innovations promise to enhance navigability for individuals with visual impairments, employing spatial audio cues to guide them through spaces with unparalleled precision.\nIntellectual Property and Technology Transfer: The successful application of the Xblock algorithm has led to a patent application, aiming for a shift toward more immersive audio generation from infrastructure in both consumer products and professional settings.\n\t\t\t\t\tLast Modified: 02/13/2024\n\n\t\t\t\t\tSubmitted by: RobertLikamwa\n"
 }
}