{
 "awd_id": "1908258",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Adversarially Robust Statistical Inference",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032928910",
 "po_email": "jafowler@nsf.gov",
 "po_sign_block_name": "James Fowler",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499996.0,
 "awd_amount": 499996.0,
 "awd_min_amd_letter_date": "2019-07-19",
 "awd_max_amd_letter_date": "2019-07-19",
 "awd_abstract_narration": "With machine learning and statistical inference algorithms increasingly used in safety-critical and security-related applications, there arises a pressing need to study the robustness of these algorithms in adversarial environments. The large existing body of work on robust statistical inference mainly addresses issues such as outliers or model uncertainties. However, in many recent data analytical applications (including safety-critical applications), one faces situations which are more severe than those addressed thus far. One such scenario occurs when an adversary can observe the whole data stream, and then devise its attack vector to modify all entries in the data set so as to inflict maximum inference errors. The existence of such powerful adversaries calls for new models and methodologies to carry out adversary-robust inference. The successful development of these models and methodologies will expand scenarios where machine learning algorithms can be safely applied, and thus expand the application of machine learning into safety-critical domains. This project will support educational activities to attract members from underrepresented groups into research. \r\n\r\nIn this project, the investigator will address robust inference in the presence of powerful adversaries. In particular, after the data points are generated, the adversary can observe the whole dataset, and then modify all data points with the hope of inflicting maximum inference errors. This project aims to answer the following questions: 1) What is the attacker's optimal attack strategy in choosing the attack vectors?; 2) What are the possible impacts of these attacks?; and 3) How should inference algorithms be designed to minimize the impact of an attack? These questions will be addressed through two related thrusts. In Thrust 1, the fundamental limits of adversarially-robust inference algorithms will be characterized; this include characterizing the optimal attack strategy and its impacts on the inference performance. The investigator will then seek to identify inference algorithms that minimize the corresponding impact. In Thrust 2, the adversarial robustness of practical inference algorithms will be studied. In practice, many inference problems are formulated as optimization problems, which are then solved using various optimization algorithms. The implementation of these optimization algorithms are often distributed in nature, and this introduces several new threats that will be addressed in this thrust.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lifeng",
   "pi_last_name": "Lai",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lifeng Lai",
   "pi_email_addr": "lflai@ucdavis.edu",
   "nsf_id": "000541215",
   "pi_start_date": "2019-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California, Davis",
  "perf_str_addr": "1 Shields Ave, University of Cal",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186134",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499996.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we investigated the fundamental limits of and designed efficient algorithms for adversarially robust statistical inference. As machine learning and statistical inference algorithms are being increasingly used in safety critical applications and security related applications, there is a pressing need to study the robustness of machine learning and statistical inference algorithms in adversarial environments, in which data might be modified in an adversarial manner. Specfically, we addressed the following questions: 1) What is the optimal attack strategy of the attacker in choosing the attack vectors?; 2) What are the impacts of these attacks?; and 3) How shall we design inference algorithms to minimize the impact? In particular, we investigated and designed adversarially robust algorithms for a variety of signal processing and statistical inference problems, including hypothesis testing, linear regression, sparse linear regression, fair classifications etc under adversarial enviroments. We show that one can properly formulate these inference problems under adversarial attacks as minmax problems. Depending on the structure of each individual problem, we developed efficient algorithms to arrive in the minmax solution, which characterizes the optimal attack strategy and the corresponding defense strategy. We also developed tools to analyze the impact of these adversarial attacks as compared to the scenario where the environment is benign and hence there is no attacker. Our research findings provide insights and guidelines for the design of practical algorithms that are robust in adversarial environment.</p><br>\n<p>\n Last Modified: 01/14/2024<br>\nModified by: Lifeng&nbsp;Lai</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we investigated the fundamental limits of and designed efficient algorithms for adversarially robust statistical inference. As machine learning and statistical inference algorithms are being increasingly used in safety critical applications and security related applications, there is a pressing need to study the robustness of machine learning and statistical inference algorithms in adversarial environments, in which data might be modified in an adversarial manner. Specfically, we addressed the following questions: 1) What is the optimal attack strategy of the attacker in choosing the attack vectors?; 2) What are the impacts of these attacks?; and 3) How shall we design inference algorithms to minimize the impact? In particular, we investigated and designed adversarially robust algorithms for a variety of signal processing and statistical inference problems, including hypothesis testing, linear regression, sparse linear regression, fair classifications etc under adversarial enviroments. We show that one can properly formulate these inference problems under adversarial attacks as minmax problems. Depending on the structure of each individual problem, we developed efficient algorithms to arrive in the minmax solution, which characterizes the optimal attack strategy and the corresponding defense strategy. We also developed tools to analyze the impact of these adversarial attacks as compared to the scenario where the environment is benign and hence there is no attacker. Our research findings provide insights and guidelines for the design of practical algorithms that are robust in adversarial environment.\t\t\t\t\tLast Modified: 01/14/2024\n\n\t\t\t\t\tSubmitted by: LifengLai\n"
 }
}