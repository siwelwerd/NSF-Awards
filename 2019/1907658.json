{
 "awd_id": "1907658",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Distributed Machine Learning in the Age of Fast Data Streams",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2019-08-21",
 "awd_max_amd_letter_date": "2019-08-21",
 "awd_abstract_narration": "Recent technological advances have resulted in the emergence of many decentralized/distributed systems comprising interconnected components that communicate among themselves over wireless links and the internet backbone for coordination and decision making. Examples of such systems include sensor networks, Internet-of-Things (IoT) systems, multiagent systems, high-performance computing clusters, and federated computing systems. One defining characteristics of many of these distributed systems is the continuous gathering of new data samples by the individual system components (e.g., motes, robots, IoT devices, cell towers, GPU nodes, etc.). Several use cases of these systems, which range from smart agriculture and smart homes to smart grids and smart transportation, are being envisioned that extract actionable information in real time from the incoming distributed \"data streams\" through adoption of sophisticated machine learning techniques. But the world's growing appetite for data coupled with the price and capacity projections for sensing, storage, computation, and bandwidth point to a near future in which (affordable) bandwidth capacity will start lagging behind the rate at which distributed systems gather new data samples. Such a future does not bode well for distributed systems that are expected to rely on machine learning advances for effective decision making. As such, it is crucial to develop distributed learning strategies that accommodate high volumes of data while operating over (relatively) low-throughput communication links. This project addresses this challenge and delivers a comprehensive set of analytical and algorithmic frameworks for communications-aware and optimization-based distributed machine learning from (possibly corrupted) data arriving in the form of (extremely) fast streams at multiple interconnected entities. In doing so, the project directly benefits the national economy through advances in the state-of-the-art in distributed systems, which will lead to reduction in both energy costs and wastage, increase in industrial efficiency, better containment of environmental disasters, efficient monitoring of nation's infrastructure, etc. Further, this project will also help address the shortage of talent in the critical areas of machine learning and data science by training two graduate and several undergraduate students.\r\n \r\nThis project develops and analyzes an algorithmic framework for real-time, in-network machine learning that acknowledges and accounts for the mismatch between the communications rate and the rate of distributed data streams in many emerging applications, where continuous data gathering is cheap and communications is over infrastructure-free device-to-device and/or machine-to-machine links. The investigator formalizes this setting as a distributed stochastic approximation problem, in which the optimum machine learning model is iteratively trained using the random data streaming into individual devices and machines. The research then focuses on the design and analysis of collaborative strategies that operate in the regime of (extremely) fast streaming rates. These strategies account for the topology of the network, the severity of the mismatch between communications and data streaming rates, and the convexity and structure of the learning problem. Further, they account for the challenges of real-world networks and data gathering, including intermittent communications links, heterogeneous data modalities, correlated data streams, and corrupt or missing data. The result is a comprehensive set of techniques and analysis that provide fast, reliable learning and a thorough understanding of network learning performance.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Waheed",
   "pi_last_name": "Bajwa",
   "pi_mid_init": "U",
   "pi_sufx_name": "",
   "pi_full_name": "Waheed U Bajwa",
   "pi_email_addr": "waheed.bajwa@rutgers.edu",
   "nsf_id": "000601209",
   "pi_start_date": "2019-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "94 Brett Rd",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7939",
   "pgm_ref_txt": "WIRELESS COMM & SIGNAL PROCESS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project made significant strides in distributed machine learning by tackling core challenges in streaming data processing, decentralized learning, and optimization. The advancements achieved have the potential to drive innovation across a range of fields, from IoT and next-generation wireless networks to autonomous systems, ensuring robust and efficient machine learning even in constrained environments.</p>\r\n<p>These advancements were driven by three key technological trends: (1) the growing reliance on machine learning for real-time decision-making, (2) the exponential increase in data generated by mobile phones, autonomous vehicles, sensor networks, and edge computing nodes, and (3) the widening gap between data generation rates and communication bandwidth. The project developed distributed learning strategies that enhance scalability, resilience, and computational efficiency, enabling high-speed processing of streaming data while optimizing resource use and minimizing delays in bandwidth-limited settings.</p>\r\n<h2>Key Technical Outcomes</h2>\r\n<p>The intellectual merit of this project is reflected in the development and validation of novel algorithms that enhance the efficiency and scalability of distributed machine learning systems. Major contributions include:</p>\r\n<ul>\r\n<li><strong>Development of streaming PCA algorithms:</strong> A significant focus was on designing and analyzing principal component analysis (PCA) techniques for distributed environments, enabling real-time data processing across networks with minimal communication overhead. The project advanced existing methodologies by proposing efficient, communication-aware PCA techniques suited for decentralized systems. These methods allow for scalable eigenvector computation while reducing the overhead of message-passing in networks with limited bandwidth. The research also provided theoretical guarantees for convergence and robustness against data inconsistency in distributed settings.</li>\r\n<li><strong>Resilience against adversarial and unreliable agents:</strong> The project introduced robust decentralized learning algorithms designed to withstand adversarial interference, including Byzantine faults and man-in-the-middle attacks. These frameworks ensure that malicious or unreliable nodes cannot corrupt the overall learning process. Specifically, the project developed robust gradient descent methods designed to improve stability and reliability in decentralized learning environments. One key contribution was the use of redundancy-aware statistical filtering, which detects and mitigates anomalies injected by adversarial agents, preserving model integrity even in environments with compromised nodes. Additionally, adaptive trust mechanisms were explored to dynamically adjust node influence based on historical reliability.</li>\r\n<li><strong>Optimization and acceleration techniques:</strong> Advanced gradient-based optimization methods were analyzed, providing new insights into escaping saddle points in nonconvex optimization problems, which are crucial for improving machine learning model training efficiency. The research explored acceleration techniques that balance convergence speed with computational stability, including adaptive momentum-based strategies and second-order approximation methods.</li>\r\n</ul>\r\n<h2>Broader Impacts</h2>\r\n<p><strong>Impact on Technology and Applications</strong></p>\r\n<p>The project's outcomes have broad implications across various domains, including:</p>\r\n<ul>\r\n<li><strong>Internet of Things (IoT):</strong> The research contributes to the development of intelligent, energy-efficient, and autonomous IoT systems that can learn and adapt in real time.</li>\r\n<li><strong>Multi-agent Systems:</strong> Multi-agent systems benefit from decentralized learning techniques that allow them to collaborate and make data-driven decisions without centralized control.</li>\r\n<li><strong>5G and 6G Wireless Networks:</strong> The project's insights support distributed machine learning applications in next-generation 5G and 6G wireless networks, enabling advancements in real-time processing for autonomous systems and smart cities.</li>\r\n</ul>\r\n<p><strong>Educational Contributions</strong></p>\r\n<p>A significant portion of the project was dedicated to training and education, including:</p>\r\n<ul>\r\n<li>Direct or indirect training of two MS students and four PhD students, along with minor support for two additional PhD students as they explored side projects. Two of the graduate students are expected to complete their degrees within the next year, while five have successfully graduated and are either employed in the machine learning industry or pursuing academic careers.</li>\r\n<li>Training of several undergraduate students, some of whom have transitioned into the machine learning industry, while two have pursued graduate studies, one in an MS program and one in a PhD program.</li>\r\n<li>Development of an undergraduate-level machine learning course to increase accessibility to fundamental AI concepts.</li>\r\n<li>Dissemination of research findings through publications, seminars, and industry collaborations, ensuring insights are shared with both academic and professional communities.</li>\r\n</ul><br>\n<p>\n Last Modified: 03/16/2025<br>\nModified by: Waheed&nbsp;U&nbsp;Bajwa</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project made significant strides in distributed machine learning by tackling core challenges in streaming data processing, decentralized learning, and optimization. The advancements achieved have the potential to drive innovation across a range of fields, from IoT and next-generation wireless networks to autonomous systems, ensuring robust and efficient machine learning even in constrained environments.\r\n\n\nThese advancements were driven by three key technological trends: (1) the growing reliance on machine learning for real-time decision-making, (2) the exponential increase in data generated by mobile phones, autonomous vehicles, sensor networks, and edge computing nodes, and (3) the widening gap between data generation rates and communication bandwidth. The project developed distributed learning strategies that enhance scalability, resilience, and computational efficiency, enabling high-speed processing of streaming data while optimizing resource use and minimizing delays in bandwidth-limited settings.\r\nKey Technical Outcomes\r\n\n\nThe intellectual merit of this project is reflected in the development and validation of novel algorithms that enhance the efficiency and scalability of distributed machine learning systems. Major contributions include:\r\n\r\nDevelopment of streaming PCA algorithms: A significant focus was on designing and analyzing principal component analysis (PCA) techniques for distributed environments, enabling real-time data processing across networks with minimal communication overhead. The project advanced existing methodologies by proposing efficient, communication-aware PCA techniques suited for decentralized systems. These methods allow for scalable eigenvector computation while reducing the overhead of message-passing in networks with limited bandwidth. The research also provided theoretical guarantees for convergence and robustness against data inconsistency in distributed settings.\r\nResilience against adversarial and unreliable agents: The project introduced robust decentralized learning algorithms designed to withstand adversarial interference, including Byzantine faults and man-in-the-middle attacks. These frameworks ensure that malicious or unreliable nodes cannot corrupt the overall learning process. Specifically, the project developed robust gradient descent methods designed to improve stability and reliability in decentralized learning environments. One key contribution was the use of redundancy-aware statistical filtering, which detects and mitigates anomalies injected by adversarial agents, preserving model integrity even in environments with compromised nodes. Additionally, adaptive trust mechanisms were explored to dynamically adjust node influence based on historical reliability.\r\nOptimization and acceleration techniques: Advanced gradient-based optimization methods were analyzed, providing new insights into escaping saddle points in nonconvex optimization problems, which are crucial for improving machine learning model training efficiency. The research explored acceleration techniques that balance convergence speed with computational stability, including adaptive momentum-based strategies and second-order approximation methods.\r\n\r\nBroader Impacts\r\n\n\nImpact on Technology and Applications\r\n\n\nThe project's outcomes have broad implications across various domains, including:\r\n\r\nInternet of Things (IoT): The research contributes to the development of intelligent, energy-efficient, and autonomous IoT systems that can learn and adapt in real time.\r\nMulti-agent Systems: Multi-agent systems benefit from decentralized learning techniques that allow them to collaborate and make data-driven decisions without centralized control.\r\n5G and 6G Wireless Networks: The project's insights support distributed machine learning applications in next-generation 5G and 6G wireless networks, enabling advancements in real-time processing for autonomous systems and smart cities.\r\n\r\n\n\nEducational Contributions\r\n\n\nA significant portion of the project was dedicated to training and education, including:\r\n\r\nDirect or indirect training of two MS students and four PhD students, along with minor support for two additional PhD students as they explored side projects. Two of the graduate students are expected to complete their degrees within the next year, while five have successfully graduated and are either employed in the machine learning industry or pursuing academic careers.\r\nTraining of several undergraduate students, some of whom have transitioned into the machine learning industry, while two have pursued graduate studies, one in an MS program and one in a PhD program.\r\nDevelopment of an undergraduate-level machine learning course to increase accessibility to fundamental AI concepts.\r\nDissemination of research findings through publications, seminars, and industry collaborations, ensuring insights are shared with both academic and professional communities.\r\n\t\t\t\t\tLast Modified: 03/16/2025\n\n\t\t\t\t\tSubmitted by: WaheedUBajwa\n"
 }
}