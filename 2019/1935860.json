{
 "awd_id": "1935860",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Evaluate the use of a portable multi-modal physiological monitoring system in the virtual reality environment for the assessment of mild traumatic brain injury",
 "cfda_num": "47.041",
 "org_code": "07020000",
 "po_phone": "7032927014",
 "po_email": "szehnder@nsf.gov",
 "po_sign_block_name": "Steve Zehnder",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 99969.0,
 "awd_amount": 99969.0,
 "awd_min_amd_letter_date": "2019-08-30",
 "awd_max_amd_letter_date": "2019-08-30",
 "awd_abstract_narration": "UCSD and FDA will jointly develop and test a wearable and easily deployable system, based on virtual-reality goggles, to collect synchronized brain and biometric signals from mild traumatic brain injury (TBI) patients and healthy participants. They will also develop advanced computational approaches to identify abnormal biomedical signals associated with TBI. The goal of this project is to test the feasibility of using a portable, compact, and deployable system as a pre- and post-hospital diagnostic and monitoring tool of TBI.\r\n\r\n\r\nUCSD and FDA jointly propose an intense, twelve-month program to design, develop, fabricate and test a wearable and easily deployable system, based on a Virtual-Reality (VR) Head-Mounted Display (HMD), to collect synchronized neural and physiological data from TBI patients and healthy participants. The use of VR HMD allows the use a compact form factor for collecting multi-modal bio-sensing data in a comfortably wearable manner, providing unprecedented opportunities to assess and evaluate the brain and mind under various cognitive tasks immediately after an injury (i.e. pre-hospital care)\r\nThe objective of this project is to determine the utility of a portable and multi-modal physiological signal monitoring system as a diagnostic and monitoring tool of TBI.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CBET",
 "org_div_long_name": "Division of Chemical, Bioengineering, Environmental, and Transport Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tzyy-Ping",
   "pi_last_name": "Jung",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tzyy-Ping Jung",
   "pi_email_addr": "jung@sccn.ucsd.edu",
   "nsf_id": "000278042",
   "pi_start_date": "2019-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164200",
   "pgm_ele_name": "Special Initiatives"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7236",
   "pgm_ref_txt": "BIOPHOTONICS, IMAGING &SENSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 99969.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this CBET project was to develop a portable and multi-modal physiological signal monitoring system in the compact form factor of a Virtual-Reality (VR) Head-mounted Display (HMD) as a diagnostic and monitoring tool for mild TBI (mTBI). The VR HMD enables us to collect multi-modal bio-sensing data in a comfortably wearable manner, providing unprecedented opportunities to assess and evaluate the brain and mind under various cognitive tasks immediately following TBI diagnosis, as well as ubiquitously and continuously monitoring patients during and after hospital evaluations and/or treatments.</p>\n<p>The project's outcomes included an unobtrusive VR Head-mounted Display (HMD) for multi-modal data acquisition, synchronization, visualization, and processing. The new VR HMD can deliver visual and auditory stimuli at the same time, track eye movement with infrared cameras, and monitor brain activity (brainwaves). More importantly, the stimulus presentation, eye tracking, and neurophysiological recording are all perfectly synchronized. Furthermore, we developed and validated a new signal-processing algorithm that reduces the memory requirements and computational complexity of automatic subspace construction (ASR). Our efforts resulted in eight research articles (Wei et al., FiCN, 2021; Van, et al., IEEE TCAS 2021; Chiang et al., JNE, 2021; Gu et al, IEEE TCBB, 2021; Jeng et al, 2021; Tsai, et al., 2022; Lee &amp; Jung, CCIS, 2021) and a conference paper (Xiao et al., IEEE NER 2021).</p>\n<p>We have used the VR HMD to diagnose patients with multiple sclerosis (MS). The VR HMD, according to preliminary findings, can be used to objectively evaluate afferent and efferent visual pathway abnormalities in MS patients (Nakanishi <em>et al</em>, under review and US provisional patent application). Furthermore, we used the platform to collect pilot data from TBI patients to assess abnormalities in TBI patients' (neuro)physiological signals.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/19/2022<br>\n\t\t\t\t\tModified by: Tzyy-Ping&nbsp;Jung</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1935860/1935860_10639174_1658232013580_Figureforoutcomesreport--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1935860/1935860_10639174_1658232013580_Figureforoutcomesreport--rgov-800width.jpg\" title=\"Translating EEG laboratory to a VR HMD\"><img src=\"/por/images/Reports/POR/2022/1935860/1935860_10639174_1658232013580_Figureforoutcomesreport--rgov-66x44.jpg\" alt=\"Translating EEG laboratory to a VR HMD\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This figure compares a standard EEG/psychophysiological experimental setup to a new-generation portable and fieldable VR EEG/eye-tracking Head-mounted Display. The new VR HMD can simultaneously deliver visual and auditory stimuli, track eye movements, and monitor (neuro)physiological signals.</div>\n<div class=\"imageCredit\">Tzyy-Ping Jung</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Tzyy-Ping&nbsp;Jung</div>\n<div class=\"imageTitle\">Translating EEG laboratory to a VR HMD</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of this CBET project was to develop a portable and multi-modal physiological signal monitoring system in the compact form factor of a Virtual-Reality (VR) Head-mounted Display (HMD) as a diagnostic and monitoring tool for mild TBI (mTBI). The VR HMD enables us to collect multi-modal bio-sensing data in a comfortably wearable manner, providing unprecedented opportunities to assess and evaluate the brain and mind under various cognitive tasks immediately following TBI diagnosis, as well as ubiquitously and continuously monitoring patients during and after hospital evaluations and/or treatments.\n\nThe project's outcomes included an unobtrusive VR Head-mounted Display (HMD) for multi-modal data acquisition, synchronization, visualization, and processing. The new VR HMD can deliver visual and auditory stimuli at the same time, track eye movement with infrared cameras, and monitor brain activity (brainwaves). More importantly, the stimulus presentation, eye tracking, and neurophysiological recording are all perfectly synchronized. Furthermore, we developed and validated a new signal-processing algorithm that reduces the memory requirements and computational complexity of automatic subspace construction (ASR). Our efforts resulted in eight research articles (Wei et al., FiCN, 2021; Van, et al., IEEE TCAS 2021; Chiang et al., JNE, 2021; Gu et al, IEEE TCBB, 2021; Jeng et al, 2021; Tsai, et al., 2022; Lee &amp; Jung, CCIS, 2021) and a conference paper (Xiao et al., IEEE NER 2021).\n\nWe have used the VR HMD to diagnose patients with multiple sclerosis (MS). The VR HMD, according to preliminary findings, can be used to objectively evaluate afferent and efferent visual pathway abnormalities in MS patients (Nakanishi et al, under review and US provisional patent application). Furthermore, we used the platform to collect pilot data from TBI patients to assess abnormalities in TBI patients' (neuro)physiological signals.\n\n \n\n\t\t\t\t\tLast Modified: 07/19/2022\n\n\t\t\t\t\tSubmitted by: Tzyy-Ping Jung"
 }
}