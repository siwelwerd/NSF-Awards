{
 "awd_id": "1931686",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: New Techniques for Optimizing Accuracy in Differential Privacy Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Nan Zhang",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2019-08-16",
 "awd_max_amd_letter_date": "2023-06-26",
 "awd_abstract_narration": "Differential Privacy is an important advance in the modern toolkit for protecting privacy and confidentiality. It allows organizations such as government agencies and private companies to collect data and publish statistics about it without leaking personal information about people -- no matter how sophisticated an attacker is. The project's novelties are in the careful design of new differentially private tools that provide more accurate population statistics while maintaining strong privacy guarantees. The project's impacts are in the ability to create datasets for social science and policy research without sacrificing privacy of individuals. The project includes both graduate and undergraduate students in this research. \r\n   \r\nThe technical ideas behind this project are that a careful analysis of privacy proofs for many differentially private algorithms can identify additional (noisy) information that can be released without changing the privacy guarantees. In addition to this, noise that is correlated between different stages of a differentially private algorithm can further reduce the variance of the final result. These techniques will allow smaller organizations to optimize the accuracy of their privacy preserving algorithms for practical deployment, and customize existing algorithms by swapping in building blocks that have more accuracy and that allow them to take better advantage of background knowledge.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Kifer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Kifer",
   "pi_email_addr": "duk17@psu.edu",
   "nsf_id": "000519235",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "W333 Westgate Building",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br />When detailed statistics about a population are being published, there <br />is a risk that this information could be used to reconstruct responses<br />provided by the individuals whose data were used. To protect against<br />this reconstruction, public and private organizations are turning towards<br />using differential privacy to carefully add the right amount of noise to<br />provide plausible deniability for the data provided by individuals. There <br />is a natural tradeoff between privacy loss (as measured mathematically by<br />a privacy loss budget) and accuracy of the published statistics.<br /><br />The goal of this project was to design differentially private algorithms<br />that provide more utility at a lower privacy cost. The intellectual merit<br />of the project includes:<br />1) The ability to optimize the fitness for use of the publsihed statistics under <br />a wide variety of user-defined notions of fitness for use.<br />2) The design of optimal algorithms that provide the best possible fitness for use<br />under a pre-specified privacy loss budget.<br />3) The design of highly scalable algorithms that provide optimal results for large<br />datasets.<br /><br />The broader impact of the work is the development of tools that allow organizations to publish the best possible research datasets for any chosen level of privacy risk. The noise distributions in these datasets can also be revealed to researchers and they consist of multivariate Gaussians (i.e., noise distributions that are easy to work with and are compatible with a large body of existing statistical analysis techniques).</p><br>\n<p>\n Last Modified: 11/17/2024<br>\nModified by: Daniel&nbsp;Kifer</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\nWhen detailed statistics about a population are being published, there \nis a risk that this information could be used to reconstruct responses\nprovided by the individuals whose data were used. To protect against\nthis reconstruction, public and private organizations are turning towards\nusing differential privacy to carefully add the right amount of noise to\nprovide plausible deniability for the data provided by individuals. There \nis a natural tradeoff between privacy loss (as measured mathematically by\na privacy loss budget) and accuracy of the published statistics.\n\nThe goal of this project was to design differentially private algorithms\nthat provide more utility at a lower privacy cost. The intellectual merit\nof the project includes:\n1) The ability to optimize the fitness for use of the publsihed statistics under \na wide variety of user-defined notions of fitness for use.\n2) The design of optimal algorithms that provide the best possible fitness for use\nunder a pre-specified privacy loss budget.\n3) The design of highly scalable algorithms that provide optimal results for large\ndatasets.\n\nThe broader impact of the work is the development of tools that allow organizations to publish the best possible research datasets for any chosen level of privacy risk. The noise distributions in these datasets can also be revealed to researchers and they consist of multivariate Gaussians (i.e., noise distributions that are easy to work with and are compatible with a large body of existing statistical analysis techniques).\t\t\t\t\tLast Modified: 11/17/2024\n\n\t\t\t\t\tSubmitted by: DanielKifer\n"
 }
}