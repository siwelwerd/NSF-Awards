{
 "awd_id": "1908866",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Sparse Reconfigurable Artificial Neural Systems: Optimal Neuron Selection and Generalization",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 449999.0,
 "awd_amount": 449999.0,
 "awd_min_amd_letter_date": "2019-09-04",
 "awd_max_amd_letter_date": "2019-09-04",
 "awd_abstract_narration": "Machine learning and Artificial Intelligence are fueling a revolution that is making it possible to do better prediction from data, to better search for images on the internet, and even to better talk to computers using natural language.  This project introduces novel machine learning methods for training artificial networks of neurons. This research also has the potential to contribute to neural science and the understanding of biological brain function. Humans display fast and flexible learning. How are our brains wired to do what they do?  During normal brain development, the process of programmed cell death represents a form of \"neuron selection\" that helps to shape the size and configuration of different information processing centers in the brain.  In effect, this wires our brain to do particular tasks.  This is also thought to represent one of the most basic forms of learning.   This research introduces new methods for \"neuron selection\" as a form of learning by machines.\r\n\r\nCurrent learning methods for artificial networks of neurons largely focus on adjusting signal strength between neural cells.   Adjusting the strength of these signals is a slow and repetitive process.    However, human learning is often spontaneous. This project looks at how artificial networks of neurons can learn by turning neurons on and off, enabling the same network to be reconfigured for multiple learning tasks.  Preliminary experiments show that this can be highly effective and can result in good generalization, even when using neurons with fixed randomly generated signals.  The proposed methods do not just identify \"useful neurons.\"  Instead, these methods can identify \"coalitions of neurons\" that work together as a team to achieve a particular goal.   Neuron selection can be executed much more rapidly than learning signal strength between neurons.   The proposed methods guarantee a linear time bound on learning.   The methods are also guaranteed to converge to the optimal \"team of neurons\" relative to a given starting configuration and learning task.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Darrell",
   "pi_last_name": "Whitley",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Darrell Whitley",
   "pi_email_addr": "darrell.whitley@gmail.com",
   "nsf_id": "000342224",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado State University",
  "inst_street_address": "601 S HOWES ST",
  "inst_street_address_2": "",
  "inst_city_name": "FORT COLLINS",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "9704916355",
  "inst_zip_code": "805212807",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "COLORADO STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LT9CXX8L19G1"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado State University",
  "perf_str_addr": "200 W Lake Street",
  "perf_city_name": "Fort Collins",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "805214593",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 449999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project was aimed at developing sparse low-cost neural networks that have the potential to be reused for applications in AI. The most direct outcome of this project was the creation of a family of sparse low cost neural networks that can operate as an ensemble.&nbsp; These &ldquo;Prune and Tune&rdquo; ensembles are created by partially training one deep learning AI model, then randomly splitting it to create many smaller sparse neural networks that function as a team of experts. Our work suggests that this approach to low-cost ensembles is superior to previous methods due to its ability to create diversity in the ensemble.&nbsp; We introduced several interpretability methods to qualitatively analyze ensemble diversity. This project also looked at new methods for creating smaller, more energy efficient neural networks for AI. Some methods were used to maximize the contribution of all neurons in the neural networks. Other work focused on gradually shrinking the size of the neural network matrices during training in order to find a much smaller network capable of learning the same classification task. Our experiments show that the fielded energy cost could be reduced by as much as 85% without loss of accuracy.&nbsp; Finally, we also discovered that local optima in some keystone combinatorial optimization problems are organized into related subsets, such that all of the local optima in a related subset can be evaluated with a single linear equation.&nbsp; This new mathematical result has applications in both machine learning and combinatorial optimization by allowing optimization methods to directly move between local optima in linear time.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/14/2025<br>\nModified by: Darrell&nbsp;Whitley</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project was aimed at developing sparse low-cost neural networks that have the potential to be reused for applications in AI. The most direct outcome of this project was the creation of a family of sparse low cost neural networks that can operate as an ensemble. These Prune and Tune ensembles are created by partially training one deep learning AI model, then randomly splitting it to create many smaller sparse neural networks that function as a team of experts. Our work suggests that this approach to low-cost ensembles is superior to previous methods due to its ability to create diversity in the ensemble. We introduced several interpretability methods to qualitatively analyze ensemble diversity. This project also looked at new methods for creating smaller, more energy efficient neural networks for AI. Some methods were used to maximize the contribution of all neurons in the neural networks. Other work focused on gradually shrinking the size of the neural network matrices during training in order to find a much smaller network capable of learning the same classification task. Our experiments show that the fielded energy cost could be reduced by as much as 85% without loss of accuracy. Finally, we also discovered that local optima in some keystone combinatorial optimization problems are organized into related subsets, such that all of the local optima in a related subset can be evaluated with a single linear equation. This new mathematical result has applications in both machine learning and combinatorial optimization by allowing optimization methods to directly move between local optima in linear time.\r\n\n\n\t\t\t\t\tLast Modified: 03/14/2025\n\n\t\t\t\t\tSubmitted by: DarrellWhitley\n"
 }
}