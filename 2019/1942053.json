{
 "awd_id": "1942053",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID: A Smart and Mobile Sensor Fusion Framework for Earthquake Hazard Reduction, Situational Assessment, and Relief Efforts",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 98516.0,
 "awd_amount": 98516.0,
 "awd_min_amd_letter_date": "2019-08-27",
 "awd_max_amd_letter_date": "2019-08-27",
 "awd_abstract_narration": "Natural disasters including earthquakes, tsunamis, hurricanes and anthropogenic disasters, such as wildfires, are dynamic situations requiring constant monitoring as numerous hazards are constantly emerging that hinder humanitarian efforts and create deadly conditions for rescue workers and victims. Most disaster area assessment, and search and rescue efforts, rely heavily on visual imagery captured from cell phones, aerial video and other forms of media. Currently, real-time assessment for disasters depends entirely on the human operator's ability to visually identify the subject of interest in the video, or images captured. Further, damaged buildings and roadways, debris, smoke, fire, and environmental conditions such as rain complicate the human observer's ability to monitor situations for detecting victims and hazards. It is essential that rescuers can assess hazardous conditions before risking their lives and be armed with smart technologies that enable them to respond to dynamic situations.   The recent events in Southern California (earthquakes) and Louisiana (Hurricane Barry) provide perishable data that will enable maturation of the proposed research activities.  This plus the integration within the Smart Mobile Disaster Data Collection unit for response provides further justification of the RAPID award mechanism. \r\n\r\nThe researchers will create a Smart Mobile Disaster Data Collection and Assessment tool for detecting and mapping out situational hazards using intelligent data fused information collected from multiple sensor technologies. Furthermore, this project aims to create a first-of-its-kind public database of collected imagery and media from disasters to help researchers and agencies share information or to assist in developing best practices and collaborative strategies. This project will also enable better visualization of disparate sources of data, thus making it conducive for human operator to detect hazards and victims. This research will also enhance safety and security applications across a wide range of areas including firefighting, disaster relief, and search-and-rescue. The approach of seamlessly utilizing and visualizing sensor information in a situational map to guide responders' actions and enhance their efficiency reduces the risk factor for responders.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Panetta",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Karen A Panetta",
   "pi_email_addr": "Karen@eecs.tufts.edu",
   "nsf_id": "000094986",
   "pi_start_date": "2019-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Tufts University",
  "inst_street_address": "80 GEORGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "MEDFORD",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6176273696",
  "inst_zip_code": "021555519",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "TRUSTEES OF TUFTS COLLEGE",
  "org_prnt_uei_num": "WL9FLBRVPJJ7",
  "org_uei_num": "WL9FLBRVPJJ7"
 },
 "perf_inst": {
  "perf_inst_name": "Tufts University",
  "perf_str_addr": "200 College Ave",
  "perf_city_name": "Medford",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021555530",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 98516.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Natural disasters including earthquakes, hurricanes, and anthropogenic disasters, such as wild-fires overwhelming California, are dynamic situations where numerous hazards are constantly emerging that hinder humanitarian efforts and create deadly conditions for rescue workers and victims. Despite the availability of low-cost information sensor and imaging technologies, robust and low-cost smart mobile tools for collecting and automatically analyzing data to help in disaster assessment, monitoring, and relief efforts are nearlynon-existent.</p>\n<p>Following a catastrophic failure of a structure, rescue workers and emergency responders may be required to enter the collapsed structure. Emergency workers may be responsible for assisting survivors, extinguishing fires, shutting down utilities, evaluating structural instabilities, identifying safe paths into the structure and assessment of other hazards such as loose electrical wires and airborne contaminants. Most disaster area assessment, and search and rescue efforts, rely heavily on visual imagery captured from cell phones, aerial video and other forms of media. Currently, real-time assessment for disasters depends entirely on the human operator&rsquo;s ability to visually identify the subject of interest in the video, or images captured. Further, damaged buildings and roadways, debris, smoke, fire, and environmental conditions such as rain complicate the human observer&rsquo;s ability to monitor situations for detecting victims and hazards. Victims often cite seeing and hearing helicopters or vehicles nearby, yet are often missed due to the structural damages, haze, fire, rain or caught among raging floods. Victims or injured rescue workers may be too weak and wounded or obstructed by debris or water to make enough movement to bring attention to themselves.</p>\n<p><strong>We created a Smart Mobile Disaster Data Collection and Assessment tool for detecting and mapping out situational hazards using intelligent data fused information collected from multiple sensor technologies. </strong></p>\n<p>Using advanced imaging and data fusion methods for images captured from various imaging sensors such as visible, thermal and multispectral cameras and radiation sensors, the framework will help to monitor the situations. It will also have the capability to detect victims/rescue workers in the collapsed structures, motorways, water, in smoke and fire engulfed areas while detecting and mapping live power lines, mapping out hazardous radiation levels, and structural failures, to safeguard emergency workers life.</p>\n<p>The research focused on five key aspects of sensor data fusion for use in disaster assessment and relief. 1) Real-time image enhancement of images subjected to environmental conditions, such as rain, fog, fire, and poor illumination, 2) Detection of victims/survivors in damaged structures and motorways, 3) Incorporating thermal imaging to detect human/animal life and utilizing thermal and radiation sensing for assessing danger, 4) Underwater image enhancement for detecting hazards and victims both under the water and within turbulent flows, 5) Fusing the sensor data collected into a visually informative map with overlaid GPS coordinates to help locate victims/emergency workers, create an assessment map of damage area and monitor developing situations, such as risingwaters, debris, electrical dangers and collapsing structures. Furthermore, we have continued working with our partner, the IEEE MOVE truck, which has already been used to aid the Red Cross in relief efforts from many hurricanes. This uniquely positions our team to ensure the final mobile sensor platform is not only provided directly in the hands of relief workers and first responders but also provides our team with real user input and feedback on both the design and deployment aspects of the final deliverables. A second benefit is the IEEE MOVE vehicle provides recharging and a direct satellite communications link, when no other forms of communication are available. One important goal is that <strong>the resulting framework itself is mobile </strong>and can be hand-held or mounted to a vehicle or other robotic platform.</p>\n<p>While existing individual sensor technologies can enable the capture of video and thermal images during and after disasters, the assessment, detection, recognition, and analysis of this valuable data are primarily left to a human observer. Conditions such as rain, haze, smoke due to fires, poor lighting, and structural damage due to earthquakes, floods, and fire affect the image quality and will ultimately impact the observer&rsquo;s ability to &ldquo;see&rdquo; victims or assess hazards. Rescue workers may have to enter and analyze structures without the knowledge of its current stability. Therefore,another goal is that the resulting framework can be used in conjunction with any platform on which the sensors are mounted, including underwater robots, rescue vehicles or helicopters or aerial drones or even hand-held. The prototype RAPID framework has been created and deployed as a mobile and modular system, that can be used with various sensors and interfaced with the PI&rsquo;s state-of-the-art image processing and computer vision algorithms. This work resulted in 10 publications and a patent application for firefighting, search and rescue and is the focus of two new start-up ventures, SeaDeep and Tessera Intelligence.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/15/2021<br>\n\t\t\t\t\tModified by: Karen&nbsp;A&nbsp;Panetta</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNatural disasters including earthquakes, hurricanes, and anthropogenic disasters, such as wild-fires overwhelming California, are dynamic situations where numerous hazards are constantly emerging that hinder humanitarian efforts and create deadly conditions for rescue workers and victims. Despite the availability of low-cost information sensor and imaging technologies, robust and low-cost smart mobile tools for collecting and automatically analyzing data to help in disaster assessment, monitoring, and relief efforts are nearlynon-existent.\n\nFollowing a catastrophic failure of a structure, rescue workers and emergency responders may be required to enter the collapsed structure. Emergency workers may be responsible for assisting survivors, extinguishing fires, shutting down utilities, evaluating structural instabilities, identifying safe paths into the structure and assessment of other hazards such as loose electrical wires and airborne contaminants. Most disaster area assessment, and search and rescue efforts, rely heavily on visual imagery captured from cell phones, aerial video and other forms of media. Currently, real-time assessment for disasters depends entirely on the human operator\u2019s ability to visually identify the subject of interest in the video, or images captured. Further, damaged buildings and roadways, debris, smoke, fire, and environmental conditions such as rain complicate the human observer\u2019s ability to monitor situations for detecting victims and hazards. Victims often cite seeing and hearing helicopters or vehicles nearby, yet are often missed due to the structural damages, haze, fire, rain or caught among raging floods. Victims or injured rescue workers may be too weak and wounded or obstructed by debris or water to make enough movement to bring attention to themselves.\n\nWe created a Smart Mobile Disaster Data Collection and Assessment tool for detecting and mapping out situational hazards using intelligent data fused information collected from multiple sensor technologies. \n\nUsing advanced imaging and data fusion methods for images captured from various imaging sensors such as visible, thermal and multispectral cameras and radiation sensors, the framework will help to monitor the situations. It will also have the capability to detect victims/rescue workers in the collapsed structures, motorways, water, in smoke and fire engulfed areas while detecting and mapping live power lines, mapping out hazardous radiation levels, and structural failures, to safeguard emergency workers life.\n\nThe research focused on five key aspects of sensor data fusion for use in disaster assessment and relief. 1) Real-time image enhancement of images subjected to environmental conditions, such as rain, fog, fire, and poor illumination, 2) Detection of victims/survivors in damaged structures and motorways, 3) Incorporating thermal imaging to detect human/animal life and utilizing thermal and radiation sensing for assessing danger, 4) Underwater image enhancement for detecting hazards and victims both under the water and within turbulent flows, 5) Fusing the sensor data collected into a visually informative map with overlaid GPS coordinates to help locate victims/emergency workers, create an assessment map of damage area and monitor developing situations, such as risingwaters, debris, electrical dangers and collapsing structures. Furthermore, we have continued working with our partner, the IEEE MOVE truck, which has already been used to aid the Red Cross in relief efforts from many hurricanes. This uniquely positions our team to ensure the final mobile sensor platform is not only provided directly in the hands of relief workers and first responders but also provides our team with real user input and feedback on both the design and deployment aspects of the final deliverables. A second benefit is the IEEE MOVE vehicle provides recharging and a direct satellite communications link, when no other forms of communication are available. One important goal is that the resulting framework itself is mobile and can be hand-held or mounted to a vehicle or other robotic platform.\n\nWhile existing individual sensor technologies can enable the capture of video and thermal images during and after disasters, the assessment, detection, recognition, and analysis of this valuable data are primarily left to a human observer. Conditions such as rain, haze, smoke due to fires, poor lighting, and structural damage due to earthquakes, floods, and fire affect the image quality and will ultimately impact the observer\u2019s ability to \"see\" victims or assess hazards. Rescue workers may have to enter and analyze structures without the knowledge of its current stability. Therefore,another goal is that the resulting framework can be used in conjunction with any platform on which the sensors are mounted, including underwater robots, rescue vehicles or helicopters or aerial drones or even hand-held. The prototype RAPID framework has been created and deployed as a mobile and modular system, that can be used with various sensors and interfaced with the PI\u2019s state-of-the-art image processing and computer vision algorithms. This work resulted in 10 publications and a patent application for firefighting, search and rescue and is the focus of two new start-up ventures, SeaDeep and Tessera Intelligence.\n\n\t\t\t\t\tLast Modified: 12/15/2021\n\n\t\t\t\t\tSubmitted by: Karen A Panetta"
 }
}