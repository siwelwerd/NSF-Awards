{
 "awd_id": "1909683",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Algebraic Methods in Codes and Computation",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 299956.0,
 "awd_amount": 299956.0,
 "awd_min_amd_letter_date": "2019-07-29",
 "awd_max_amd_letter_date": "2021-06-08",
 "awd_abstract_narration": "This project investigates the power and limitations of algebraic computation and algebraic techniques in computer science. Arithmetic circuits are a very natural model of algebraic computation and most natural algebraic algorithms such as matrix multiplication, fast Fourier transforms, algorithms for computing the determinant, and others can be implemented via arithmetic circuits. This project will investigate the complexity of algebraic computation via the model of arithmetic circuits. In recent years algebraic techniques have shown up as an extremely powerful tool influencing all areas of computer science, including even the understanding of problems that are not inherently algebraic. Algebraic techniques have also found striking applications in combinatorics, coding theory, and pseudorandomness. Due to the profound impact of algebraic techniques and algebraic algorithms, this naturally motivates a deeper understanding of the power and limits of these methods, and this project aims to develop new tools and techniques in order to do this. One of the focuses of the project will be to harness algebraic techniques for the construction of efficient and local error-correcting codes. Mentoring and training of young researchers is an important part of the educational component of the project. Additionally, the investigator will co-organize the Women in Theory workshop in the summer of 2020, which will bring together women researchers in theoretical computer science from around the world.\r\n\r\nThe two main problems that this project will explore are those of proving lower bounds for arithmetic circuits and the problem of polynomial identity testing. These are some of the most central questions in algebraic complexity theory, and this project aims to understand these via the study of bounded-depth arithmetic circuits. The project will also study two other very related problems of polynomial factoring and polynomial reconstruction. In addition, the project will use algebraic techniques to construct new families of error-correcting codes with extremely efficient encodings and with sublinear-time decoding and testing algorithms. In particular, the project will attempt to understand the optimal rate/query complexity tradeoffs in the construction of locally decodable codes and locally testable codes. This project will bring together ideas and tools from a broad array of disciplines and has the potential to have significant practical applications to information storage and retrieval.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Allender",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Eric W Allender",
   "pi_email_addr": "Allender@cs.rutgers.edu",
   "nsf_id": "000391611",
   "pi_start_date": "2021-06-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Shubhangi",
   "pi_last_name": "Saraf",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shubhangi Saraf",
   "pi_email_addr": "shubhangi.saraf@gmail.com",
   "nsf_id": "000618097",
   "pi_start_date": "2019-07-29",
   "pi_end_date": "2021-06-08"
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers, The State University of New Jersey",
  "perf_str_addr": "110 Frelinghuysen Rd",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 299956.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"moz-text-html\" lang=\"x-unicode\">\n<p>Computation can be studied in the digital framework (where the       input and output are viewed as strings of binary digits) or in the       algebraic framework (where the only operations allowed are       arithmetic operations such as + and x).&nbsp; In principle, it is       easier to rule out the existence of efficient algorithms in the       algebraic setting (where the class of algorithms under       consideration is restricted) than in the unrestricted digital       setting.&nbsp; Since proving the nonexistence of efficient algorithms       has frustrated the best efforts of researchers thus far, it has       been fruitful to develop techniques that work in the algebraic       setting.&nbsp; The twenty-one publications that were supported by this       grant made progress in both the algebraic and the digital       framework.&nbsp;</p>\n<p>Let us first discuss the problems addressed in the digital       framework:</p>\n<p>Which functions are hard to compute, and which are easy? Which       strings contain a lot of information, and which do not? Although       these questions may seem unrelated, there are actually close       connections between the two. The best known approaches to       measuring the amount of information in a string center on the       question of how much a string can be compressed. More precisely,       algorithmic information theory (also known as Kolmogorov       complexity) equates the information content of a string with the       length of its shortest description.&nbsp; A variant of Kolmogorov       complexity, called KT, takes into account not only the length of       the shortest description of a string, but also the time required       to obtain the string from its description. The KT complexity of       the graph of a function turns out to be an approximation of the       circuit size that is required to compute the function.&nbsp; Thus the       functions whose truth tables have a lot of information (in the KT       sense) are the functions that are hard to compute.</p>\n<p>Work supported by the grant draws some surprising connections       between KT complexity and the study of cryptography -- especially       to the notion of zero-knowledge proofs.&nbsp; A series of these led to       the result that the set of strings with high KT complexity is hard       (under randomized reductions) for a class known as NISZK:       (Non-Interactive Statistical Zero Knowledge).&nbsp; More strikingly,       NISZK is PRECISELY the set of problems that are randomly-reducible       to the set of strings with high Kolmogorov complexity (with a       moderate approximation error).&nbsp; Another of the papers showed that       cryptographically-secure one-way functions that are computable in       small space exist if and only if computing KT(x|y) is hard on       average, whereas this same problem (computing KT(x|y)) is       NP-complete (under randomized reductions).&nbsp; This arguably gives       the first example of a natural NP-complete problem whose       complexity characterizes the existence of a class of one-way       functions.</p>\n<p>Work supported by the grant also resulted in improved parallel       algorithms for Depth-First Search and for Integer Division.&nbsp; The       improved division algorithm provided improved upper bounds on the       complexity of computing the bits of the binary representation of       algebraic numbers.</p>\n<p>Let us now turn to the problems addressed in the algebraic       framework.</p>\n<p>In addition to proving the non-existence of efficient algebraic       algorithms, it is equally important to develop new algorithms in       the algebraic setting.&nbsp; Some of the most useful error-correcting       codes make use of algebraic techniques, and advances in algebraic       computing have led to improved codes.&nbsp; A sequence of two papers       supported by the grant greatly improved the efficiency of       algorithms for evaluating a multivariate polynomial at multiple       points.&nbsp; Another sequence of two papers yielded new insights about       one of the most intensely-studied class of codes: Reed-Solomon       Codes, by showing that, for any affine space of bitstrings, either       all of the strings in the space are \"close\" to the set of       codewords, or only a tiny fraction of the strings is close to the       set of codewords.&nbsp; This insight yielded improvements in a variety       of different application areas, ranging from distributed storage,       to multiparty cryptographic protocols, to efficient proof       systems.&nbsp; Other work yielded improved algorithms for learning       arithmetic circuits (also known as the \"reconstruction problem\").&nbsp; &nbsp; &nbsp;</p>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/14/2023<br>\n\t\t\t\t\tModified by: Eric&nbsp;W&nbsp;Allender</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nComputation can be studied in the digital framework (where the       input and output are viewed as strings of binary digits) or in the       algebraic framework (where the only operations allowed are       arithmetic operations such as + and x).  In principle, it is       easier to rule out the existence of efficient algorithms in the       algebraic setting (where the class of algorithms under       consideration is restricted) than in the unrestricted digital       setting.  Since proving the nonexistence of efficient algorithms       has frustrated the best efforts of researchers thus far, it has       been fruitful to develop techniques that work in the algebraic       setting.  The twenty-one publications that were supported by this       grant made progress in both the algebraic and the digital       framework. \n\nLet us first discuss the problems addressed in the digital       framework:\n\nWhich functions are hard to compute, and which are easy? Which       strings contain a lot of information, and which do not? Although       these questions may seem unrelated, there are actually close       connections between the two. The best known approaches to       measuring the amount of information in a string center on the       question of how much a string can be compressed. More precisely,       algorithmic information theory (also known as Kolmogorov       complexity) equates the information content of a string with the       length of its shortest description.  A variant of Kolmogorov       complexity, called KT, takes into account not only the length of       the shortest description of a string, but also the time required       to obtain the string from its description. The KT complexity of       the graph of a function turns out to be an approximation of the       circuit size that is required to compute the function.  Thus the       functions whose truth tables have a lot of information (in the KT       sense) are the functions that are hard to compute.\n\nWork supported by the grant draws some surprising connections       between KT complexity and the study of cryptography -- especially       to the notion of zero-knowledge proofs.  A series of these led to       the result that the set of strings with high KT complexity is hard       (under randomized reductions) for a class known as NISZK:       (Non-Interactive Statistical Zero Knowledge).  More strikingly,       NISZK is PRECISELY the set of problems that are randomly-reducible       to the set of strings with high Kolmogorov complexity (with a       moderate approximation error).  Another of the papers showed that       cryptographically-secure one-way functions that are computable in       small space exist if and only if computing KT(x|y) is hard on       average, whereas this same problem (computing KT(x|y)) is       NP-complete (under randomized reductions).  This arguably gives       the first example of a natural NP-complete problem whose       complexity characterizes the existence of a class of one-way       functions.\n\nWork supported by the grant also resulted in improved parallel       algorithms for Depth-First Search and for Integer Division.  The       improved division algorithm provided improved upper bounds on the       complexity of computing the bits of the binary representation of       algebraic numbers.\n\nLet us now turn to the problems addressed in the algebraic       framework.\n\nIn addition to proving the non-existence of efficient algebraic       algorithms, it is equally important to develop new algorithms in       the algebraic setting.  Some of the most useful error-correcting       codes make use of algebraic techniques, and advances in algebraic       computing have led to improved codes.  A sequence of two papers       supported by the grant greatly improved the efficiency of       algorithms for evaluating a multivariate polynomial at multiple       points.  Another sequence of two papers yielded new insights about       one of the most intensely-studied class of codes: Reed-Solomon       Codes, by showing that, for any affine space of bitstrings, either       all of the strings in the space are \"close\" to the set of       codewords, or only a tiny fraction of the strings is close to the       set of codewords.  This insight yielded improvements in a variety       of different application areas, ranging from distributed storage,       to multiparty cryptographic protocols, to efficient proof       systems.  Other work yielded improved algorithms for learning       arithmetic circuits (also known as the \"reconstruction problem\").     \n\n\n \n\n\t\t\t\t\tLast Modified: 10/14/2023\n\n\t\t\t\t\tSubmitted by: Eric W Allender"
 }
}