{
 "awd_id": "1900681",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Towards Practical Encoderless Robotics Through Vision-Based Training and Adaptation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 425000.0,
 "awd_amount": 425000.0,
 "awd_min_amd_letter_date": "2019-08-02",
 "awd_max_amd_letter_date": "2019-08-02",
 "awd_abstract_narration": "As robots branch out into unstructured and dynamic human environments (such as homes, offices, and hospitals), they require a new design methodology. These robots need to be safe to operate next to humans; they are expected to handle frequent changes and uncertainties that are inherent in human environments; and they should be as inexpensive as possible to enable wide-spread dissemination. Such criteria have lead to the emergence of compliant/soft robots, 3D printed robots, and inexpensive consumer-grade hardware, all of which constitute a major shift from heavy and rigid robots with tight tolerances used in industry. Traditional measurement devices that are suitable for sensing and controlling the motion of the rigid robots, i.e. joint encoders, are incompatible or impractical for many of these new types of robot. Alternative approaches that do not rely on encoders are largely missing from robotics technology and must be developed for these novel design models. This project investigates ways of using only cameras for sensing and controlling the robot's motion. Vision-based algorithms for robotic walking, object grasping and manipulation will be derived. Such algorithms will not only enable the use of the new-wave robots in unstructured environments but will also significantly lower the cost of traditional robotic systems, and therefore, boost their dissemination for industry and educational purposes.\r\n\r\nThe project will focus on utilizing vision-based estimation schemes and learning methods for acquiring both robot configuration information and task models within a framework where modeling inaccuracies and environment uncertainties are dealt with by robust visual servoing approaches. Visual observations will be used to model the relationship between actuator inputs, manipulator configuration, and task states, and they will be combined with adaptive vision-based control schemes that are robust to modeling uncertainties and disturbances. The framework will fundamentally rely on using convolutional neural networks (CNNs) to build the models from observation alone, both for a low-dimensional representation of configuration and for an image segmentation of the manipulator. Reinforcement learning methods will also be applied to assess the practicality of a modular combination of such methods with the offline learned representations to perform complex positioning and control tasks. These approaches will be evaluated in the context of within-hand manipulation, compliant surgical tool control, locomotion of a 3D-printed multi-legged robot, and force-controlled grasping and peg-insertion using a soft continuum manipulator. The contributions of our proposed work are that no prior model of a robot's configuration is needed because it is explicitly observed and inferred up-front (system identification); uncertainty affecting task performance is addressed by adapting the robot dynamics on-the-fly (model-through-confirmation); and the broad applicability of our methods will be demonstrated through application to a wide variety of platforms. Work done on this project will help to enable lower cost robotic and mechatronic hardware across a range of domains and will particularly impact the ability to control compliant and under-actuated structures.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Dollar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron Dollar",
   "pi_email_addr": "aaron.dollar@yale.edu",
   "nsf_id": "000525237",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "15 Prospect Street",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208284",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 425000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project primarily focused on investigating practical means of  controlling &ldquo;non-traditional&rdquo; robot hardware, such as highly compliant  &ldquo;soft&rdquo; and underactuated systems, low-cost and low-precision robots such  as 3D-printed or mass-produced consumer-grade hardware, among others.  For these systems, the traditional method of relying on robust  proprioceptive information from encoders is not possible or practical,  motivating the exploration of alternative or complementary approaches.</p>\n<p>We focused on investigating the  use of computer vision as the primary (and perhaps sole) basis for control of  those non-traditional robotic systems. We focused on synergistic collaborations of  vision-based model estimation and control techniques for realizing  robust and efficient control &ndash; seeking to achieve high performance  systems with minimal proprioceptive configuration sensors.&nbsp;</p>\n<p>We were able to demonstrate a range of impressive results supported by this grant:</p>\n<p>- we developed a \"self-identification\" framework that allows us to randomly  manipulate objects and while doing so, estimate a  manipulation model of the hand and then use this model to  complete difficult dexterous within-hand manipulation actions.</p>\n<p>- we developed a simple robot hand that is able to manipulate objects using what we call \"caging-based manipulation\", where the hand is wrapped around the object and slides it to the desired configuration.</p>\n<p>- we showed that we could accomplish tight-tolerance object insertion  controlled primarily through computer vision, leveraging the compliance&nbsp;</p>\n<p>Through our work on this project, we are beginning to get the robotics research community to think more seriously about  questioning the typical approach to robotics: sensing everything with as  much precision as possible, and then trying to control all aspects of  the system to accomplish a task. Instead, along with passively adaptive  hardware that facilitates dealing with uncertainty, we are showing that you can do a lot with minimal sensing, and vision is  undoubtedly the best option for inexpensive, high resolution, robust  sensing. If successful, this approach will lead to robots that are much more commercially-viable as they do not need to be as stiff/precise and will not need the large suite of fragile sensors that most approaches today rely upon.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/12/2023<br>\n\t\t\t\t\tModified by: Aaron&nbsp;Dollar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project primarily focused on investigating practical means of  controlling \"non-traditional\" robot hardware, such as highly compliant  \"soft\" and underactuated systems, low-cost and low-precision robots such  as 3D-printed or mass-produced consumer-grade hardware, among others.  For these systems, the traditional method of relying on robust  proprioceptive information from encoders is not possible or practical,  motivating the exploration of alternative or complementary approaches.\n\nWe focused on investigating the  use of computer vision as the primary (and perhaps sole) basis for control of  those non-traditional robotic systems. We focused on synergistic collaborations of  vision-based model estimation and control techniques for realizing  robust and efficient control &ndash; seeking to achieve high performance  systems with minimal proprioceptive configuration sensors. \n\nWe were able to demonstrate a range of impressive results supported by this grant:\n\n- we developed a \"self-identification\" framework that allows us to randomly  manipulate objects and while doing so, estimate a  manipulation model of the hand and then use this model to  complete difficult dexterous within-hand manipulation actions.\n\n- we developed a simple robot hand that is able to manipulate objects using what we call \"caging-based manipulation\", where the hand is wrapped around the object and slides it to the desired configuration.\n\n- we showed that we could accomplish tight-tolerance object insertion  controlled primarily through computer vision, leveraging the compliance \n\nThrough our work on this project, we are beginning to get the robotics research community to think more seriously about  questioning the typical approach to robotics: sensing everything with as  much precision as possible, and then trying to control all aspects of  the system to accomplish a task. Instead, along with passively adaptive  hardware that facilitates dealing with uncertainty, we are showing that you can do a lot with minimal sensing, and vision is  undoubtedly the best option for inexpensive, high resolution, robust  sensing. If successful, this approach will lead to robots that are much more commercially-viable as they do not need to be as stiff/precise and will not need the large suite of fragile sensors that most approaches today rely upon.\n\n\t\t\t\t\tLast Modified: 01/12/2023\n\n\t\t\t\t\tSubmitted by: Aaron Dollar"
 }
}