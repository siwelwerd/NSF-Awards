{
 "awd_id": "1910213",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "OAC Core: Small: Sust-CI: A Machine Learning based Approach to Make Advanced Cyberinfrastructure Applications More Efficient and Sustainable",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927116",
 "po_email": "sghafoor@nsf.gov",
 "po_sign_block_name": "Sheikh Ghafoor",
 "awd_eff_date": "2019-05-01",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 499998.0,
 "awd_amount": 499998.0,
 "awd_min_amd_letter_date": "2019-04-22",
 "awd_max_amd_letter_date": "2019-04-22",
 "awd_abstract_narration": "Many high-end engineering and scientific applications routinely employ advanced cyber infrastructure (CI). CI is formed of a combination of high performance computing (HPC) systems, software, application developers, and application users. Among these high-end applications, a growing number require repeated runs on HPC systems that are not designed optimally for their executions. These challenges are further exacerbated by the continuously changing hardware landscape. Faced with these challenges how is a CI application developer expected to develop and deploy applications in an efficient and sustainable manner? This is the central research question that this project seeks to address. The overarching goal is to develop a systematic and structured way to explore design spaces of CI configurations using machine learning techniques, and to demonstrate value in application and discovery potential through real-world applications. Other project activities integrate and leverage upon the research outcomes of this project, while preparing the next generation scientific workforce. The project is also leading to the development of curricular modules in parallel algorithms/applications and machine learning, and conference tutorials for broader outreach. The project will lead to the training of two PhD students in performing interdisciplinary research.\r\n\r\nThis project lays the foundations for a novel computational framework referred as Sust-CI that enables the developers to design and optimize cyber infrastructures for efficiency. This framework synergistically combines algorithmic abstractions, programming tools, and machine learning techniques to enable adaptive cyber infrastructures. This approach will automatically learn policies to make design decisions to optimize an objective specified by the developer (e.g., performance) in a data-driven manner. The project is leading to the development of sample-efficient machine learning algorithms for CI design space exploration and optimization. The key idea is to provide advanced CI applications a new capability to derive knowledge by exploring different execution traces (computational behavior) on the given training problem instances. The research will lead to a first-of-its-kind design space exploration framework to enable a sustainable use of CI resources toward leadership applications in science and engineering.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Janardhan Rao",
   "pi_last_name": "Doppa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Janardhan Rao Doppa",
   "pi_email_addr": "jana@eecs.wsu.edu",
   "nsf_id": "000679215",
   "pi_start_date": "2019-04-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anantharaman",
   "pi_last_name": "Kalyanaraman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anantharaman Kalyanaraman",
   "pi_email_addr": "ananth@eecs.wsu.edu",
   "nsf_id": "000289075",
   "pi_start_date": "2019-04-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington State University",
  "inst_street_address": "240 FRENCH ADMINISTRATION BLDG",
  "inst_street_address_2": "",
  "inst_city_name": "PULLMAN",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "5093359661",
  "inst_zip_code": "991640001",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "WA05",
  "org_lgl_bus_name": "WASHINGTON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XRJSGX384TD6"
 },
 "perf_inst": {
  "perf_inst_name": "Washington State university",
  "perf_str_addr": "Washington State University, Sch",
  "perf_city_name": "Pullman",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "991641227",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "WA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "090Y00",
   "pgm_ele_name": "OAC-Advanced Cyberinfrast Core"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many high-end engineering and scientific applications routinely employ advanced cyber infrastructure (CI). CI is formed of a combination of high performance computing (HPC) systems, software, application developers, and application users. Among these high-end applications, a growing number require repeated runs on HPC systems that are not designed optimally for their executions. These challenges are further exacerbated by the continuously changing hardware landscape. Faced with these challenges how is a CI application developer expected to develop and deploy applications in an efficient and sustainable manner? This is the central research question that this project seeks to address. The overarching goal is to develop a systematic and structured way to explore design spaces of CI configurations using machine learning techniques, and to demonstrate value in application and discovery potential through real-world applications.</p>\n<p><br />The intellectual merits of the work was to significantly advance the theory and practice of machine learning (ML) to create optimized cyberinfrastructures to optimally trade-off performance and accuracy. One approach was to create ML-enabled dynamic resource management policies to execute applications on mobile, manycore, and HPC systems. This also allowed us to configure and execute a variety of deep neural networks to perform adaptive inference depending on the hardness of inputs to enable energy-efficient edge ML. We also developed novel ML-enabled adaptive experiment design algorithms to reduce the number of expensive computational simulations in a wide range of engineering and scientific applications. Another approach was to create adaptive algorithms to process combinatorial inputs (e.g., graphs and sequences) to significantly improve scalability for processing large-scale data to enable high-end scientific applications in computational chemistry and computational biology. Throughout the project new benchmarks and software were created for the research community to support further work by other researchers.</p>\n<p><br />The broader impact of the project was to investigate the application of machine learning for a variety of high-end engineering and scientific applications. This included work on large-scale graph processing for computational chemistry and large-scale genome data processing for genome assembly; reducing the number of expensive simulations for hardware design and nanoporous materials design optimization; and energy-efficient deployment of large deep models on resource-constrained mobile platforms. Our work demonstrated that machine learning can be an effective approach to design and optimize a wide-variety of cyberinfrastructures. This experience has set the stage for similar approaches to be used for designing future cyberinfrastructures where adaptive decisions are required.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/31/2023<br>\n\t\t\t\t\tModified by: Janardhan Rao&nbsp;Doppa</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany high-end engineering and scientific applications routinely employ advanced cyber infrastructure (CI). CI is formed of a combination of high performance computing (HPC) systems, software, application developers, and application users. Among these high-end applications, a growing number require repeated runs on HPC systems that are not designed optimally for their executions. These challenges are further exacerbated by the continuously changing hardware landscape. Faced with these challenges how is a CI application developer expected to develop and deploy applications in an efficient and sustainable manner? This is the central research question that this project seeks to address. The overarching goal is to develop a systematic and structured way to explore design spaces of CI configurations using machine learning techniques, and to demonstrate value in application and discovery potential through real-world applications.\n\n\nThe intellectual merits of the work was to significantly advance the theory and practice of machine learning (ML) to create optimized cyberinfrastructures to optimally trade-off performance and accuracy. One approach was to create ML-enabled dynamic resource management policies to execute applications on mobile, manycore, and HPC systems. This also allowed us to configure and execute a variety of deep neural networks to perform adaptive inference depending on the hardness of inputs to enable energy-efficient edge ML. We also developed novel ML-enabled adaptive experiment design algorithms to reduce the number of expensive computational simulations in a wide range of engineering and scientific applications. Another approach was to create adaptive algorithms to process combinatorial inputs (e.g., graphs and sequences) to significantly improve scalability for processing large-scale data to enable high-end scientific applications in computational chemistry and computational biology. Throughout the project new benchmarks and software were created for the research community to support further work by other researchers.\n\n\nThe broader impact of the project was to investigate the application of machine learning for a variety of high-end engineering and scientific applications. This included work on large-scale graph processing for computational chemistry and large-scale genome data processing for genome assembly; reducing the number of expensive simulations for hardware design and nanoporous materials design optimization; and energy-efficient deployment of large deep models on resource-constrained mobile platforms. Our work demonstrated that machine learning can be an effective approach to design and optimize a wide-variety of cyberinfrastructures. This experience has set the stage for similar approaches to be used for designing future cyberinfrastructures where adaptive decisions are required. \n\n\t\t\t\t\tLast Modified: 07/31/2023\n\n\t\t\t\t\tSubmitted by: Janardhan Rao Doppa"
 }
}