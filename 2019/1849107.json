{
 "awd_id": "1849107",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: Context-Aware Active Data Gathering for Complex Outdoor Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-02-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 599962.0,
 "awd_amount": 599962.0,
 "awd_min_amd_letter_date": "2019-01-29",
 "awd_max_amd_letter_date": "2019-01-29",
 "awd_abstract_narration": "Traditional agents are programmed to acquire information by recognizing and attending to predetermined areas and targets in a given environment. Recent advances in deep learning models and miniature hardware platforms are providing artificial agents unprecedented capability in processing and interpreting visual data. These advancements create an exciting opportunity to build intelligent machines running with greater autonomy and adaptability. Toward this goal, this project investigates new methods that enable multiple unmanned aerial systems to understand and explore complex outdoor environment by actively seeking, acquiring, integrating, and processing visual information across space and time. The developed framework with enhanced adaptability, self-awareness, and generalizability will be applicable to autonomous systems in broad applications such as environmental monitoring, search and rescue, self-driving cars, smart health, and manufacturing domains. Throughout the project, the principal investigators will make project results including created datasets, trained models, code, and papers publicly available. The new integrative research combining vision, planning and actuation will be incorporated into teaching materials, underrepresented and undergraduate research projects, as well as K-12 outreach activities.\r\n\r\nThe project seeks to develop algorithms for context-aware active sensing which also incorporate energy constraints. This will be achieved by: First, proposing new deep learning models for holistic attention prediction with multiple aerial views. The models will leverage external knowledge to enable inference and generalization in unseen contexts. Second, by developing new view and path planning methods that are efficient and aware of systems' energy, mobility and sensing constraints. Third, by contributing novel online learning methods that adapt based on uncertainty to implement adaptiveness and awareness to changing environment. Experiments to validate the findings will take place both indoors in the newly-renovated Shepherd UAV Lab at the University of Minnesota and in the field at the Cedar Creek Ecosystem Reserve. The results of this project have the potential to inspire further research into intelligent and integrative perceptual, planning and actuation systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qi",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qi Zhao",
   "pi_email_addr": "qzhao@umn.edu",
   "nsf_id": "000753440",
   "pi_start_date": "2019-01-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ibrahim",
   "pi_last_name": "Isler",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Ibrahim V Isler",
   "pi_email_addr": "isler@cs.utexas.edu",
   "nsf_id": "000233463",
   "pi_start_date": "2019-01-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota",
  "perf_str_addr": "200 Union Street SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550167",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 599962.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9167d718-7fff-351b-ec5e-7ae5c0c3d8f9\"> </span></p>\n<p dir=\"ltr\"><span>The project focuses on advancing UAV capabilities to intelligently explore complex environments using recent advances in deep learning and miniature hardware. We aim to create a dynamic multi-UAV system that enhances self-awareness and generalizability by actively integrating and processing visual data across space and time. The system optimizes view plans and path planning for energy efficiency, with applications spanning environmental monitoring, search and rescue, autonomous vehicles, smart health, and manufacturing.</span></p>\n<p dir=\"ltr\"><span>To develop advanced attention models capable of integrating video data from multiple UAVs comprehensively, we curated extensive attention and reasoning datasets. Our novel attention model considered both correct and incorrect attention patterns to refine machine learning. Additionally, we conducted the first research on predicting scanpaths during diverse tasks, achieving superior accuracy in capturing human attention's spatio-temporal dynamics compared to existing methods. We introduced novel datasets and models to facilitate the integration of multi-modal inputs, enhancing decision-making transparency and performance across varied applications. Furthermore, our development of a trustworthiness predictor enhances prediction accuracy by distinguishing distributions of positive and negative examples, thereby optimizing decision-making under complex real-world conditions.</span></p>\n<p dir=\"ltr\"><span>To enhance the adaptability and performance of attention models in dynamic environments, we integrated external knowledge directly into our models. We introduced the first explicit and explainable visual reasoning method that integrated external knowledge through dynamic scene graphs and functional programs, augmented by a novel reinforcement learning method. We also defined new attention mechanisms tailored for embodied settings and developed methods that tightly integrated perception with action for embodied navigation. Experimental results demonstrated substantial improvements in navigation across diverse, previously unseen environments.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We have developed cutting-edge view and path planning algorithms tailored for UAVs to enhance the efficiency and quality of image capture. Our research focused on optimizing view planning in complex scenarios such as disaster response and urban environments, ensuring high-quality 3D geometry and texture representation while minimizing path length deviations. By converting quality requirements into mathematical constraints, we introduced adaptive viewing planes that adapt naturally to scene geometries, significantly reducing reconstruction errors compared to traditional methods. Addressing scalability challenges, we innovatively transformed the coverage problem into a cone-based variant of the Traveling Salesman Problem (cone-TSPN), achieving polynomial runtime and constant memory usage for large-scale city coverage, a significant improvement over previous techniques. Additionally, we developed algorithms that compute visual coverage trajectories optimizing total length while maintaining specified detection probabilities. Finally, we introduced a novel route-finding approach integrating perception and travel costs, employing an entropy-based viewing score to generate diameter-bounded viewing neighborhoods.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Furthermore, this project also involves developing methods to enhance online learning in dynamic environments. We measured agreement between new and prior knowledge, implementing Direction Concentration Learning (DCL) to optimize updates in deep neural networks. Additionally, our gradient-based few-shot learning methods enabled efficient knowledge transfer from large-scale datasets to domains with limited samples. We proposed a learning approach leveraging privileged information from multiple modalities during training, enhancing testing performance without necessitating one-to-one modal relationships. Finally, our Gradient Adjustment Learning technique optimized gradients by incorporating knowledge from previous iterations, demonstrating significant improvements across diverse applications.</span></p>\n<p dir=\"ltr\"><span>The project has shaped artificial intelligence and robotics by enhancing autonomous systems' capabilities to navigate and adapt in complex environments. It has also established frameworks for transparent decision-making and improved generalizability across diverse applications, focusing on attention, generalization, and adaptive action planning. Public accessibility of datasets, analyses, and code fostered transparency and community engagement, reflected in 26 publications at conferences like CVPR, ECCV, ICCV, ICRA, and NeurIPS. The project involved seven PhD students in computer science and robotics, fostering their expertise and skills in these fields.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/28/2024<br>\nModified by: Qi&nbsp;Zhao</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515796184_Slide5--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515796184_Slide5--rgov-800width.jpeg\" title=\"Category-Level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondences\"><img src=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515796184_Slide5--rgov-66x44.jpeg\" alt=\"Category-Level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondences\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The method divides a template point cloud into 18 parts, samples keypoints, constructs an affinity matrix, and optimizes matching for pose determination.</div>\n<div class=\"imageCredit\">Volkan Isler</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Category-Level Global Camera Pose Estimation with Multi-Hypothesis Point Cloud Correspondences</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515274097_Slide1--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515274097_Slide1--rgov-800width.jpeg\" title=\"Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention\"><img src=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515274097_Slide1--rgov-66x44.jpeg\" alt=\"Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Task-driven visual attention. Correct attention provides essential information for answering questions, while incorrect attention identifies distracting features to avoid in intelligent visual systems.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515347929_Slide2--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515347929_Slide2--rgov-800width.jpeg\" title=\"Predicting human scanpaths in visual question answering\"><img src=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515347929_Slide2--rgov-66x44.jpeg\" alt=\"Predicting human scanpaths in visual question answering\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visual scanpaths encode and illustrate decision-making strategies and performance.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Predicting human scanpaths in visual question answering</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515589361_Slide6--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515589361_Slide6--rgov-800width.jpeg\" title=\"Stochastic Traveling Salesperson Problem with Neighborhoods for Object Detection\"><img src=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515589361_Slide6--rgov-66x44.jpeg\" alt=\"Stochastic Traveling Salesperson Problem with Neighborhoods for Object Detection\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The trajectory of an aerial vehicle for observing a set of cars in the Unreal Engine simulator. Images are captured along the trajectory toward the objects of interest.</div>\n<div class=\"imageCredit\">Volkan Isler</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Stochastic Traveling Salesperson Problem with Neighborhoods for Object Detection</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515466261_Slide4--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515466261_Slide4--rgov-800width.jpeg\" title=\"Attention to action: Leveraging attention for object navigation\"><img src=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515466261_Slide4--rgov-66x44.jpeg\" alt=\"Attention to action: Leveraging attention for object navigation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">New attention-based method that enables a tight integration of perception and action&#12290;</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Attention to action: Leveraging attention for object navigation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515425649_Slide3--rgov-214x142.jpeg\" original=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515425649_Slide3--rgov-800width.jpeg\" title=\"Query and attention augmentation for knowledge-based explainable reasoning\"><img src=\"/por/images/Reports/POR/2024/1849107/1849107_10589083_1719515425649_Slide3--rgov-66x44.jpeg\" alt=\"Query and attention augmentation for knowledge-based explainable reasoning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Query and attention augmentation methods enhance explainable deep learning models with knowledge.</div>\n<div class=\"imageCredit\">Qi Zhao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Qi&nbsp;Zhao\n<div class=\"imageTitle\">Query and attention augmentation for knowledge-based explainable reasoning</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThe project focuses on advancing UAV capabilities to intelligently explore complex environments using recent advances in deep learning and miniature hardware. We aim to create a dynamic multi-UAV system that enhances self-awareness and generalizability by actively integrating and processing visual data across space and time. The system optimizes view plans and path planning for energy efficiency, with applications spanning environmental monitoring, search and rescue, autonomous vehicles, smart health, and manufacturing.\n\n\nTo develop advanced attention models capable of integrating video data from multiple UAVs comprehensively, we curated extensive attention and reasoning datasets. Our novel attention model considered both correct and incorrect attention patterns to refine machine learning. Additionally, we conducted the first research on predicting scanpaths during diverse tasks, achieving superior accuracy in capturing human attention's spatio-temporal dynamics compared to existing methods. We introduced novel datasets and models to facilitate the integration of multi-modal inputs, enhancing decision-making transparency and performance across varied applications. Furthermore, our development of a trustworthiness predictor enhances prediction accuracy by distinguishing distributions of positive and negative examples, thereby optimizing decision-making under complex real-world conditions.\n\n\nTo enhance the adaptability and performance of attention models in dynamic environments, we integrated external knowledge directly into our models. We introduced the first explicit and explainable visual reasoning method that integrated external knowledge through dynamic scene graphs and functional programs, augmented by a novel reinforcement learning method. We also defined new attention mechanisms tailored for embodied settings and developed methods that tightly integrated perception with action for embodied navigation. Experimental results demonstrated substantial improvements in navigation across diverse, previously unseen environments.\n\n\nWe have developed cutting-edge view and path planning algorithms tailored for UAVs to enhance the efficiency and quality of image capture. Our research focused on optimizing view planning in complex scenarios such as disaster response and urban environments, ensuring high-quality 3D geometry and texture representation while minimizing path length deviations. By converting quality requirements into mathematical constraints, we introduced adaptive viewing planes that adapt naturally to scene geometries, significantly reducing reconstruction errors compared to traditional methods. Addressing scalability challenges, we innovatively transformed the coverage problem into a cone-based variant of the Traveling Salesman Problem (cone-TSPN), achieving polynomial runtime and constant memory usage for large-scale city coverage, a significant improvement over previous techniques. Additionally, we developed algorithms that compute visual coverage trajectories optimizing total length while maintaining specified detection probabilities. Finally, we introduced a novel route-finding approach integrating perception and travel costs, employing an entropy-based viewing score to generate diameter-bounded viewing neighborhoods.\n\n\nFurthermore, this project also involves developing methods to enhance online learning in dynamic environments. We measured agreement between new and prior knowledge, implementing Direction Concentration Learning (DCL) to optimize updates in deep neural networks. Additionally, our gradient-based few-shot learning methods enabled efficient knowledge transfer from large-scale datasets to domains with limited samples. We proposed a learning approach leveraging privileged information from multiple modalities during training, enhancing testing performance without necessitating one-to-one modal relationships. Finally, our Gradient Adjustment Learning technique optimized gradients by incorporating knowledge from previous iterations, demonstrating significant improvements across diverse applications.\n\n\nThe project has shaped artificial intelligence and robotics by enhancing autonomous systems' capabilities to navigate and adapt in complex environments. It has also established frameworks for transparent decision-making and improved generalizability across diverse applications, focusing on attention, generalization, and adaptive action planning. Public accessibility of datasets, analyses, and code fostered transparency and community engagement, reflected in 26 publications at conferences like CVPR, ECCV, ICCV, ICRA, and NeurIPS. The project involved seven PhD students in computer science and robotics, fostering their expertise and skills in these fields.\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 06/28/2024\n\n\t\t\t\t\tSubmitted by: QiZhao\n"
 }
}