{
 "awd_id": "1916717",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Detecting and Mitigating Unintended Learning",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 499997.0,
 "awd_amount": 499997.0,
 "awd_min_amd_letter_date": "2019-07-11",
 "awd_max_amd_letter_date": "2023-08-07",
 "awd_abstract_narration": "Machine learning is fueling major advances in biomedical research, natural language processing, image recognition, self-driving vehicles, etc.  These advances depend on the continuing availability of data.  By assuring the integrity and privacy of both the data and the machine learning models based on this data, this project aims to bring the benefits of machine learning to all data holders. However, machine learning can unintentionally reveal sensitive data such as the identity of specific persons. This project will investigate unintended learning, i.e., what machine learning models are discovering beyond their stated objectives, and its consequences for the data on which the models are trained and to which they are applied.  \r\n\r\nIn this project, the first focus area is developing inference techniques that detect leakage of sensitive data and, more generally, determine what models are actually learning. This research will help identify the root causes of training-data memorization in deep models, develop methodology for detecting and measuring it, and help prevent deep models from unintentionally learning privacy-sensitive features of the data.  The second focus area is developing and analyzing methods for mitigating unintended learning and ensuring that models do not contain unwanted or malicious functionality.  In addition to improving privacy of the training and test data, this research will help detect and prevent backdoors in models trained on smartphones and other edge devices. All technologies developed as part of this project will be evaluated on state-of-the-art image-analysis and natural-language models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vitaly",
   "pi_last_name": "Shmatikov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vitaly Shmatikov",
   "pi_email_addr": "shmat@cs.cornell.edu",
   "nsf_id": "000109107",
   "pi_start_date": "2019-07-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell Tech",
  "perf_str_addr": "2 West Loop Road",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100441501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated unintended learning in AI models and systems.&nbsp; Unintended learning means that the model has learned more than the task specified by the model creator.&nbsp; Main goals of this project included analyzing and reducing sensitive information captured and leaked by ML models, in parrticular large language models and their components; developing integrity protections to prevent poisoning of ML models with unwanted functionality; and designing practical auditing and data provenance methods to help users detect unwanted and unintended uses of their data.</p>\n<p>Outcomes include:</p>\n<ul>\n<li>Several auditing and provenance methods that help users check if their data was used to train machine learning models.</li>\n<li>First investigation of security and privacy of federated learning, a popular privacy-preserving framework for learning from data on users' devices.</li>\n<li>A study of the interaction between privacy protection and bias, demonstrating that differential privacy, a popular approach to data privacy, has a negative impact on fairness in machine learning models.</li>\n<li>Investigation of unwanted learning in AI-based frameworks for generating computer code.</li>\n<li>First demonstration of \"overlearning\", where a model created for a simple objective unintentionally learns to recognize attributes and concepts that are sensitive from the privacy or bias perspective.</li>\n<li>A new framework for protecting machine learning models from poisoning without disruptive changes to model-training pipelines.</li>\n<li>Investigations of how embeddings, logit representations, and outputs of large language models unintentionally leak information about sensitive inputs.</li>\n<li>An investigation of how large language models can be abused to generate propaganda and disinformation.</li>\n</ul>\n<p>Results of this research were published in top, peer-reviewed security, privacy, machine learning, and natural language processing venues, including IEEE Symposium on Security and Privacy, USENIX Security Symposium, ACM Conference on Computer and Communication Security, Conference on Neural Information Processing Systems, International Conference on Learning Representations, and Empirical Methods in Natural Language Processing.&nbsp; These papers have already garnered several thousand citations.&nbsp; Three papers received Distinguished Paper Awards.</p>\n<p>All software developed by this project was released as open source.&nbsp; Two of projects have several hundred stars each on GitHub, indicating that they are actively used by the research community.</p>\n<p>The project supported several PhD students, enabling them to participate in advanced research at an early stage of their careers.&nbsp; After graduation, these students continued working on AI security and privacy research in tenure-track academic positions and industrial research labs.</p><br>\n<p>\n Last Modified: 10/31/2024<br>\nModified by: Vitaly&nbsp;Shmatikov</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project investigated unintended learning in AI models and systems. Unintended learning means that the model has learned more than the task specified by the model creator. Main goals of this project included analyzing and reducing sensitive information captured and leaked by ML models, in parrticular large language models and their components; developing integrity protections to prevent poisoning of ML models with unwanted functionality; and designing practical auditing and data provenance methods to help users detect unwanted and unintended uses of their data.\n\n\nOutcomes include:\n\nSeveral auditing and provenance methods that help users check if their data was used to train machine learning models.\nFirst investigation of security and privacy of federated learning, a popular privacy-preserving framework for learning from data on users' devices.\nA study of the interaction between privacy protection and bias, demonstrating that differential privacy, a popular approach to data privacy, has a negative impact on fairness in machine learning models.\nInvestigation of unwanted learning in AI-based frameworks for generating computer code.\nFirst demonstration of \"overlearning\", where a model created for a simple objective unintentionally learns to recognize attributes and concepts that are sensitive from the privacy or bias perspective.\nA new framework for protecting machine learning models from poisoning without disruptive changes to model-training pipelines.\nInvestigations of how embeddings, logit representations, and outputs of large language models unintentionally leak information about sensitive inputs.\nAn investigation of how large language models can be abused to generate propaganda and disinformation.\n\n\n\nResults of this research were published in top, peer-reviewed security, privacy, machine learning, and natural language processing venues, including IEEE Symposium on Security and Privacy, USENIX Security Symposium, ACM Conference on Computer and Communication Security, Conference on Neural Information Processing Systems, International Conference on Learning Representations, and Empirical Methods in Natural Language Processing. These papers have already garnered several thousand citations. Three papers received Distinguished Paper Awards.\n\n\nAll software developed by this project was released as open source. Two of projects have several hundred stars each on GitHub, indicating that they are actively used by the research community.\n\n\nThe project supported several PhD students, enabling them to participate in advanced research at an early stage of their careers. After graduation, these students continued working on AI security and privacy research in tenure-track academic positions and industrial research labs.\t\t\t\t\tLast Modified: 10/31/2024\n\n\t\t\t\t\tSubmitted by: VitalyShmatikov\n"
 }
}