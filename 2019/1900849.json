{
 "awd_id": "1900849",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Physics and Learning Integration Using Differentiable Rendering",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 399924.0,
 "awd_amount": 415924.0,
 "awd_min_amd_letter_date": "2019-08-19",
 "awd_max_amd_letter_date": "2021-08-10",
 "awd_abstract_narration": "Using image measurements to understand and acquire properties of the physical world (such as the shape of an object, the reflectance of a surface, or the lighting in a room) is a critical capability for many sciences including medicine, material fabrication, remote sensing, robotics, autonomous navigation, architectural design, computer graphics, and computer vision. At a high level, these properties can be found by taking images and using computational algorithms to infer unknown parameters from the measurements. Broadly, the inference algorithms can be classified into two categories. On the one hand, physics-based algorithms try to analytically model and then invert the physics underlying the process of how a scene of certain parameters produces measured images; these algorithms are generally accurate but require a lot of computation. On the other hand, data-driven algorithms use supervised datasets to learn how to directly map measurements to unknowns; these algorithms are computationally efficient but are not guaranteed to produce accurate predictions.  This project aims to transform physical acquisition pipelines, by creating general-purpose computational tools that combine the advantages of physics-based and machine-learning-based techniques, and that are simultaneously efficient, accurate and robust. By developing the theory and computational tools for this integration of simulation and learning, the project has the potential for transformative impact in application areas like industrial quality control, material science, oceanography, and biomedical imaging. Widespread adoption of project outcomes will be encouraged by making new software publicly available, as well as by offering tutorials and workshops in computer graphics, vision, and imaging conferences.  The project also includes an education and outreach program that is tightly coupled to the research objectives, and which takes the form of courses, summer workshops, and lab visits for K-12 students intended to introduce them to science at an early stage and encourage STEM education. Additionally, the project will contribute towards broadening participation in computing through targeted involvement in existing programs in the participating institutions that focus on outreach to female students, first-generation students, and students from traditionally underrepresented minorities.\r\n\r\nThis project aims to transform physical acquisition pipelines by creating general-purpose computational tools that enable efficient and robust inference. This will be achieved by coupling physics-based and learning-based approaches, in a way that combines their complementary strengths of accuracy, generality, and efficiency. Three core areas of research will contribute to this.  First, the project will develop inference pipelines that synergistically combine neural networks with analysis by synthesis optimization, in order to efficiently produce high-fidelity estimates of physical parameters. Neural networks will be trained in a physics-aware manner, by using physically-accurate renderers as layers in their architecture; this will allow the neural networks to simultaneously leverage supervised information and physical knowledge when making predictions.  Second, a new class of physically accurate differentiable renderers will be created, which will enable this tight integration of physics and learning without the need to sacrifice computational efficiency. Instead of images, differentiable renderers will estimate their derivatives with respect to scene parameters; this estimation will be performed in a physically accurate way, using physical simulation algorithms derived from first principles and benefiting from innovations targeting improved efficiency.  Finally, the advantages of the developed inference tools will be demonstrated in a diverse range of applications such as autonomous sensing, material science and fabrication, and biomedical imaging.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ioannis",
   "pi_last_name": "Gkioulekas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ioannis Gkioulekas",
   "pi_email_addr": "igkioule@andrew.cmu.edu",
   "nsf_id": "000743510",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 148502.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 136941.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 130481.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-401f519d-7fff-fa07-50cc-954a0bc54a10\"> </span></p>\n<p dir=\"ltr\"><span>Developing digital representations of the world requires high quality models of the shapes and materials in the world. To produce these models, this project developed general-purpose computational tools to acquire shapes and materials through measurement, combined with novel methods that bring together our understanding of the physics of how light interacts with these shapes and materials with AI learning methods to reconstruct the shapes and materials accurately.</span></p>\n<p dir=\"ltr\"><strong>Intellectual merit</strong></p>\n<p dir=\"ltr\"><span>The project has contributed techniques to improve not the accuracy, efficiency and generality of these techniques to capture the rich visual variety of the real world.&nbsp; There are three major thrusts of innovation. The efforts in these thrusts have led to more than 40 publications, four PhD theses, and multiple public releases of software and acquired data.</span></p>\n<p dir=\"ltr\"><em>Thrust 1: Differentiable rendering.</em><span> The project developed a new class of physically accurate light simulation tools, in the form of differentiable rendering. Compared to conventional physically accurate rendering techniques, which take as input a digital scene specification and produce&nbsp; as output photorealistic images of that scene, differentiable rendering outputs derivatives of images with respect to scene parameters; for example the geometry and materials of the objects comprising the scene.</span></p>\n<p dir=\"ltr\"><span>To this end, the project introduced the theoretical foundations for differential light transport and differentiable rendering algorithms derived from these foundations. This theory and these algorithms generalize their classical rendering counterparts to the differentiable setting, achieving comparable practicality and generality.</span></p>\n<p dir=\"ltr\"><em>Thrust 2: Integration of physics and learning.</em><span> The project developed new inferential pipelines that combine data-driven deep learning techniques with physics-based optimization. These pipelines use neural networks trained in a physics-aware manner, by using differentiable renderers as layers in their architecture. This allows the neural networks to leverage simultaneously supervised information and physical knowledge when making predictions.</span></p>\n<p dir=\"ltr\"><span>To this end, the project introduced neural rendering techniques that leverage neural networks and differentiable renderers in various ways: for example, by using neural networks as trainable scene representations (for geometry, material, or illumination) that can then be differentiably rendered to produce images matching physical measurements; or using neural networks to produce efficient initializations for subsequent use with differentiable rendering.</span></p>\n<p dir=\"ltr\"><em>Thrust 3: Applications.</em><span> The project demonstrated the advantages of the developed inference tools in a diverse range of applications. Within computer graphics, the project applied the developed inference tools to improve the acquisition of 3D shape, reflectance, and scattering properties of real world objects. In particular, the project enabled the acquisition of high-fidelity representations of these properties thousands of times faster than previously possible at this fidelity, through techniques such as kaleidoscopic 3D scanning, reflectometry from interreflections, and learning-based inverse scattering. The common theme of all these techniques is that they leverage differentiable rendering to invert global illumination measurements (multiple reflections in a kaleidoscope, interreflections inside a concave object, or multiple scattering in a translucent object).</span></p>\n<p dir=\"ltr\"><span>Outside computer graphics, the project applied the developed inference tools in applications in medical imaging, robotics, and environmental conservation. In medical imaging, the project facilitated the use of differentiable rendering to design optimized acousto-optic waveguides, deployed in tissue imaging and brain imaging applications. In robotics, the project demonstrated the use of differentiable and neural rendering techniques to enable new robotics navigation capabilities, such as non-line-of-sight imaging and underwater 3D imaging using SONAR. Additionally, the project enabled the design of optimized vision-based tactile sensors for robotic inspection and manipulation tasks. Lastly, in environmental conservation, the project led to the development of smoke reconstruction and monitoring techniques, which assist firefighters during wildfire management efforts.</span></p>\n<p dir=\"ltr\"><strong>Broader impacts</strong></p>\n<p dir=\"ltr\"><span>This project has established <em>differentiable rendering</em> and <em>neural rendering</em> as core research areas within computer graphics, computer vision. Whereas both areas were nascent at the start of the project, nowadays they feature dedicated sessions at all major computer graphics and vision disciplines, with hundreds of papers published in these areas every year. Both research areas are also the subject matter for multiple workshops, tutorials, and courses in computer graphics and vision. The project has additionally resulted in the adoption of differentiable and neural rendering techniques well outside of computer graphics and vision, in application areas spanning medical imaging, remote sensing, robotics, seismic imaging, and astronomy. These outcomes have created new points of convergence between computer graphics, physics, robotics, and medicine.</span></p>\n<p dir=\"ltr\"><span>The project has resulted in the development of educational and outreach resources that help increase engagement and improve representation in STEM. In particular, the project has resulted in the creation of new courses in the three participating institutions (Carnegie Mellon University, Cornell University, and University of California Irvine), as well as at major computer graphics and vision venues (SIGGRAPH, CVPR) that cover differentiable rendering and neural rendering techniques. The project has additionally facilitated the development of a series of weekend courses and summer workshops that introduce middle-school and high-school students from underrepresented minorities to computer graphics.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/26/2024<br>\nModified by: Ioannis&nbsp;Gkioulekas</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nDeveloping digital representations of the world requires high quality models of the shapes and materials in the world. To produce these models, this project developed general-purpose computational tools to acquire shapes and materials through measurement, combined with novel methods that bring together our understanding of the physics of how light interacts with these shapes and materials with AI learning methods to reconstruct the shapes and materials accurately.\n\n\nIntellectual merit\n\n\nThe project has contributed techniques to improve not the accuracy, efficiency and generality of these techniques to capture the rich visual variety of the real world. There are three major thrusts of innovation. The efforts in these thrusts have led to more than 40 publications, four PhD theses, and multiple public releases of software and acquired data.\n\n\nThrust 1: Differentiable rendering. The project developed a new class of physically accurate light simulation tools, in the form of differentiable rendering. Compared to conventional physically accurate rendering techniques, which take as input a digital scene specification and produce as output photorealistic images of that scene, differentiable rendering outputs derivatives of images with respect to scene parameters; for example the geometry and materials of the objects comprising the scene.\n\n\nTo this end, the project introduced the theoretical foundations for differential light transport and differentiable rendering algorithms derived from these foundations. This theory and these algorithms generalize their classical rendering counterparts to the differentiable setting, achieving comparable practicality and generality.\n\n\nThrust 2: Integration of physics and learning. The project developed new inferential pipelines that combine data-driven deep learning techniques with physics-based optimization. These pipelines use neural networks trained in a physics-aware manner, by using differentiable renderers as layers in their architecture. This allows the neural networks to leverage simultaneously supervised information and physical knowledge when making predictions.\n\n\nTo this end, the project introduced neural rendering techniques that leverage neural networks and differentiable renderers in various ways: for example, by using neural networks as trainable scene representations (for geometry, material, or illumination) that can then be differentiably rendered to produce images matching physical measurements; or using neural networks to produce efficient initializations for subsequent use with differentiable rendering.\n\n\nThrust 3: Applications. The project demonstrated the advantages of the developed inference tools in a diverse range of applications. Within computer graphics, the project applied the developed inference tools to improve the acquisition of 3D shape, reflectance, and scattering properties of real world objects. In particular, the project enabled the acquisition of high-fidelity representations of these properties thousands of times faster than previously possible at this fidelity, through techniques such as kaleidoscopic 3D scanning, reflectometry from interreflections, and learning-based inverse scattering. The common theme of all these techniques is that they leverage differentiable rendering to invert global illumination measurements (multiple reflections in a kaleidoscope, interreflections inside a concave object, or multiple scattering in a translucent object).\n\n\nOutside computer graphics, the project applied the developed inference tools in applications in medical imaging, robotics, and environmental conservation. In medical imaging, the project facilitated the use of differentiable rendering to design optimized acousto-optic waveguides, deployed in tissue imaging and brain imaging applications. In robotics, the project demonstrated the use of differentiable and neural rendering techniques to enable new robotics navigation capabilities, such as non-line-of-sight imaging and underwater 3D imaging using SONAR. Additionally, the project enabled the design of optimized vision-based tactile sensors for robotic inspection and manipulation tasks. Lastly, in environmental conservation, the project led to the development of smoke reconstruction and monitoring techniques, which assist firefighters during wildfire management efforts.\n\n\nBroader impacts\n\n\nThis project has established differentiable rendering and neural rendering as core research areas within computer graphics, computer vision. Whereas both areas were nascent at the start of the project, nowadays they feature dedicated sessions at all major computer graphics and vision disciplines, with hundreds of papers published in these areas every year. Both research areas are also the subject matter for multiple workshops, tutorials, and courses in computer graphics and vision. The project has additionally resulted in the adoption of differentiable and neural rendering techniques well outside of computer graphics and vision, in application areas spanning medical imaging, remote sensing, robotics, seismic imaging, and astronomy. These outcomes have created new points of convergence between computer graphics, physics, robotics, and medicine.\n\n\nThe project has resulted in the development of educational and outreach resources that help increase engagement and improve representation in STEM. In particular, the project has resulted in the creation of new courses in the three participating institutions (Carnegie Mellon University, Cornell University, and University of California Irvine), as well as at major computer graphics and vision venues (SIGGRAPH, CVPR) that cover differentiable rendering and neural rendering techniques. The project has additionally facilitated the development of a series of weekend courses and summer workshops that introduce middle-school and high-school students from underrepresented minorities to computer graphics.\n\n\n\t\t\t\t\tLast Modified: 03/26/2024\n\n\t\t\t\t\tSubmitted by: IoannisGkioulekas\n"
 }
}