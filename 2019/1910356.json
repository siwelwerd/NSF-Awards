{
 "awd_id": "1910356",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III:Small: Data Exploration Framework for Augmented Reality",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 499978.0,
 "awd_amount": 515978.0,
 "awd_min_amd_letter_date": "2019-08-16",
 "awd_max_amd_letter_date": "2024-01-29",
 "awd_abstract_narration": "With recent advances in computer vision and widespread availability of capable hardware, augmented reality has become a consumer-grade technology. It has enabled a new paradigm of digital interaction using camera-based devices, presenting novel ways to interact with the real world. Simultaneously, another emerging trend is the prevalence of data exploration in the real world. One challenge in exploring real-world data is that it is often found in isolation, lacking any contextual information. Further, such data is commonly found in ad-hoc formats, making it hard to combine with digital data. Building on these trends, this project aims to enable augmented reality (AR)-driven discovery and decision making over ad-hoc real-world data. This approach towards data exploration and enrichment using AR-based interaction will open up possibilities in a wide variety of domains, including education, healthcare, architecture design, and emergency services. AR broadens the scope of participation for groups who would otherwise be unable to use traditional computing interfaces such as keyboards, addressing challenges of literacy and motor abilities.\r\n\r\nThis project will develop a principled framework for exploration and understanding of real-world data using AR. The primary goals of this project are as follows. As a first step, the project will design a new grammar and visual encoding that allows for both querying and layering of structured data results in AR space. Secondly, it will devise methods to enrich real-world images and video streams with schema-rich information using cloud-based data stores by considering not just optical character recognition, but also visual features and context. Third, the project will develop a data storage and query processing backend that is optimized specifically for highly interactive, AR-specific workloads.  The outcomes of this research will not only advance the understanding of building data exploration systems for AR, but it will additionally help reduce the barrier to entry for software developers to build data-rich AR applications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Arnab",
   "pi_last_name": "Nandi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Arnab Nandi",
   "pi_email_addr": "arnab@cse.ohio-state.edu",
   "nsf_id": "000623779",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1960 Kenny Road",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 336099.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 163879.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to develop a new, structured approach for data exploration in Augmented Reality (AR). By integrating real-time querying, enrichment, and visualization, this research project aimed to bridge the gap between traditional data systems and AR-driven interaction. Our work addressed three key research challenges: (1) enriching real-world data with knowledge bases, (2) enabling interactive querying in AR, and (3) developing scalable data infrastructure for AR collaboration. Through these contributions, this project advanced AR-based data processing techniques, improving efficiency and accessibility across multiple domains.</p>\r\n<p>One core challenge in AR-based data interaction is the augmentation of real-world data with relevant digital information. This project introduced methods to integrate structured and unstructured data from cloud-backed databases, enabling schema-aware extraction and contextual enrichment. As part of this effort, layout-aware data understanding methods were developed to improve the processing of visually complex documents and structured data scenes. Traditional optical character recognition (OCR) systems struggle with preserving structured relationships in tables, forms, and multi-column layouts. The project addressed this challenge by integrating data management and computer vision-based techniques to recognize record structure, enabling accurate extraction and classification of structured data in visual scenes. This approach not only improves data extraction but also facilitates the fusion of real-world and digital data through fuzzy joins.</p>\r\n<p>The ability to natively and interactively explore structured data is a fundamental requirement for next-generation AR platforms. ARQuery, a system that enables direct touch-based engagement with overlaid digital data, was developed to eliminate the need for traditional indirect interaction. With ARQuery, users can point an AR-enabled device, such as a headset or a tablet, at structured data sources such as flight schedules, inventory systems, or industrial control panels and apply gesture-driven interactions to filter, sort, or analyze the information dynamically. ARQuery makes data retrieval seamless and intuitive, particularly in settings that require rapid decision-making.&nbsp;</p>\r\n<p>While ARQuery enables intuitive interaction with data, data creation itself remains a critical source of friction, particularly in dynamic or high-stakes environments. A new approach, camera-first form filling, uses image understanding and artificial intelligence model-driven extraction to automate structured data entry, reducing the manual effort required to input information. Instead of filling out complex forms, users can simply point a camera-enabled device at documents, control panels, or disaster recovery scenes, allowing the system to extract and populate relevant information. This approach is particularly impactful in sectors such as emergency management, where rapid and accurate data capture can improve decision-making and resource deployment.</p>\r\n<p>While these ideas are compelling, incorporating them into data-rich AR applications can be complex and time-consuming. To simplify this development, Quill, a declarative framework that makes it easier for AR applications to incorporate structured data, was introduced. Quill enables developers to define AR-oriented data enrichment at a high level, abstracting the complexities of low-level coding. This framework allows developers to easily create interactive AR overlays for real-time data visualization. By reducing technical barriers, Quill can accelerate the adoption of AR-powered decision-support systems across a multitude of applications and industries. &nbsp;</p>\r\n<p>When adopted at scale, AR environments will need to support a large number of users. This requires efficient data synchronization and query processing to ensure seamless collaboration. Traditional AR systems often suffer from inconsistencies, network latency, and difficulty in managing real-time data updates across multiple users. This project addressed these challenges by developing DreamStore, a scalable AR data management platform optimized for high-frequency query workloads. DreamStore enhances AR experiences by prefetching relevant data, prioritizing critical queries, and synchronizing updates across all users. DreamStore incorporates methods that enable real-time collaboration, making multi-user AR applications more practical and effective in settings such as industrial operations, emergency response coordination, and training simulations.&nbsp;</p>\r\n<p>By advancing AR-driven querying, record understanding, and scalable AR infrastructure, this project significantly improves how users interact with structured and unstructured data in augmented environments. The ability to query, retrieve, and manipulate information in AR has practical applications in areas such as consumer applications, industrial monitoring, logistics, medical diagnostics, emergency management, and disaster response. These innovations establish a foundation for a future where AR-based data exploration is seamlessly integrated into professional and operational workflows, enhancing efficiency, accessibility, and user experience.</p><br>\n<p>\n Last Modified: 02/07/2025<br>\nModified by: Arnab&nbsp;Nandi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project was to develop a new, structured approach for data exploration in Augmented Reality (AR). By integrating real-time querying, enrichment, and visualization, this research project aimed to bridge the gap between traditional data systems and AR-driven interaction. Our work addressed three key research challenges: (1) enriching real-world data with knowledge bases, (2) enabling interactive querying in AR, and (3) developing scalable data infrastructure for AR collaboration. Through these contributions, this project advanced AR-based data processing techniques, improving efficiency and accessibility across multiple domains.\r\n\n\nOne core challenge in AR-based data interaction is the augmentation of real-world data with relevant digital information. This project introduced methods to integrate structured and unstructured data from cloud-backed databases, enabling schema-aware extraction and contextual enrichment. As part of this effort, layout-aware data understanding methods were developed to improve the processing of visually complex documents and structured data scenes. Traditional optical character recognition (OCR) systems struggle with preserving structured relationships in tables, forms, and multi-column layouts. The project addressed this challenge by integrating data management and computer vision-based techniques to recognize record structure, enabling accurate extraction and classification of structured data in visual scenes. This approach not only improves data extraction but also facilitates the fusion of real-world and digital data through fuzzy joins.\r\n\n\nThe ability to natively and interactively explore structured data is a fundamental requirement for next-generation AR platforms. ARQuery, a system that enables direct touch-based engagement with overlaid digital data, was developed to eliminate the need for traditional indirect interaction. With ARQuery, users can point an AR-enabled device, such as a headset or a tablet, at structured data sources such as flight schedules, inventory systems, or industrial control panels and apply gesture-driven interactions to filter, sort, or analyze the information dynamically. ARQuery makes data retrieval seamless and intuitive, particularly in settings that require rapid decision-making.\r\n\n\nWhile ARQuery enables intuitive interaction with data, data creation itself remains a critical source of friction, particularly in dynamic or high-stakes environments. A new approach, camera-first form filling, uses image understanding and artificial intelligence model-driven extraction to automate structured data entry, reducing the manual effort required to input information. Instead of filling out complex forms, users can simply point a camera-enabled device at documents, control panels, or disaster recovery scenes, allowing the system to extract and populate relevant information. This approach is particularly impactful in sectors such as emergency management, where rapid and accurate data capture can improve decision-making and resource deployment.\r\n\n\nWhile these ideas are compelling, incorporating them into data-rich AR applications can be complex and time-consuming. To simplify this development, Quill, a declarative framework that makes it easier for AR applications to incorporate structured data, was introduced. Quill enables developers to define AR-oriented data enrichment at a high level, abstracting the complexities of low-level coding. This framework allows developers to easily create interactive AR overlays for real-time data visualization. By reducing technical barriers, Quill can accelerate the adoption of AR-powered decision-support systems across a multitude of applications and industries. \r\n\n\nWhen adopted at scale, AR environments will need to support a large number of users. This requires efficient data synchronization and query processing to ensure seamless collaboration. Traditional AR systems often suffer from inconsistencies, network latency, and difficulty in managing real-time data updates across multiple users. This project addressed these challenges by developing DreamStore, a scalable AR data management platform optimized for high-frequency query workloads. DreamStore enhances AR experiences by prefetching relevant data, prioritizing critical queries, and synchronizing updates across all users. DreamStore incorporates methods that enable real-time collaboration, making multi-user AR applications more practical and effective in settings such as industrial operations, emergency response coordination, and training simulations.\r\n\n\nBy advancing AR-driven querying, record understanding, and scalable AR infrastructure, this project significantly improves how users interact with structured and unstructured data in augmented environments. The ability to query, retrieve, and manipulate information in AR has practical applications in areas such as consumer applications, industrial monitoring, logistics, medical diagnostics, emergency management, and disaster response. These innovations establish a foundation for a future where AR-based data exploration is seamlessly integrated into professional and operational workflows, enhancing efficiency, accessibility, and user experience.\t\t\t\t\tLast Modified: 02/07/2025\n\n\t\t\t\t\tSubmitted by: ArnabNandi\n"
 }
}