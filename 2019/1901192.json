{
 "awd_id": "1901192",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: Photonic Neural Network Accelerator for Energy-efficient Heterogeneous Multicore Architectures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 499999.0,
 "awd_amount": 539999.0,
 "awd_min_amd_letter_date": "2019-07-26",
 "awd_max_amd_letter_date": "2021-05-24",
 "awd_abstract_narration": "Deep learning architectures such as convolutional neural networks and recurrent neural networks have achieved unprecedented, sometimes super-human accuracy on many modern applications in artificial intelligence, such as image classification and speech recognition. Power dissipation is however a major concern in these energy-hungry machine-learning architectures, and decreasing it requires designs that provide a more energy-efficient combination of hardware and machine-learning algorithms. There is an increased emphasis to leverage parallelism and specialization to improve performance and energy efficiency. To dramatically reduce power consumption, silicon photonics has been proposed to improve performance-per-Watt compared to electrical implementation.\r\n\r\nThis project leverages photonic technology and heterogeneous multicores for the design of deep-neural network accelerators that improve parallelism, concurrency, energy efficiency and scalability in various machine-learning applications. The first task of the project is concerned with the characterization and identification of photonic devices that can implement accelerator functionalities such as multiply-and-accumulate, summation, and other arithmetic operations. The characterized devices are then inserted into single-layer and multi-layer photonic topologies for implementing accelerator functionality. The second task of the project implements various types of deep-learning architectures on the proposed photonic neural network accelerator to maximize the gains offered by the photonic technology. The third task of the project builds an extensive simulation and modeling infrastructure that combines the photonic technology, network architectures, accelerator functionality, and machine-learning algorithms developed in the previous two steps, in order to validate the significant reduction in energy consumption enabled by the photonic neural-network accelerator.\r\n\r\nThe proposed research bridges a very important gap between photonic technology, hardware architecture, and machine learning. As such, and due to its cross-cutting nature, it is expected to have far-reaching impacts on the design of next-generation multicore architectures. It will foster new research directions in several areas, spanning computer architecture, optical technology, machine learning algorithms and applications. The research will also play a major role in education by integrating discovery with teaching and training. All the research findings and simulation toolkits will be disseminated to the community via conference and journal publications, and a dedicated website.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Avinash",
   "pi_last_name": "Karanth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Avinash Karanth",
   "pi_email_addr": "karanth@ohio.edu",
   "nsf_id": "000495031",
   "pi_start_date": "2019-07-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Razvan",
   "pi_last_name": "Bunescu",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Razvan C Bunescu",
   "pi_email_addr": "rbunescu@uncc.edu",
   "nsf_id": "000515843",
   "pi_start_date": "2019-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio University",
  "inst_street_address": "1 OHIO UNIVERSITY",
  "inst_street_address_2": "",
  "inst_city_name": "ATHENS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "7405932857",
  "inst_zip_code": "457012979",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "OH12",
  "org_lgl_bus_name": "OHIO UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LXHMMWRKN5N8"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio University",
  "perf_str_addr": "329 Stocker Center",
  "perf_city_name": "Athens",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "457012979",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "OH12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499999.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 24000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>To dramatically reduce power consumption, silicon photonics has been proposed to improve performance-per-Watt compared to electrical implementation of embedded systems and system-on-chips. This project leverages photonic technology and heterogeneous multicores for the design of deep neural network accelerators that improve parallelism, concurrency, energy-efficiency and scalability in various machine learning applications. The first task of the project is concerned with the characterization and identification of photonic devices that can implement accelerator functionalities such as multiply-and-accumulate, summation, and other arithmetic operations. The second task of the project implements various types of deep learning architectures on the proposed photonic neural network accelerator to maximize the gains offered by the photonic technology. The third task of the project builds an extensive simulation and modeling infrastructure that combines the photonic technology, network architectures, accelerator functionality, and machine learning algorithms.</p>\r\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We leverage the unique advantages of photonic technology to design PIXEL: Photonic Neural Network Accelerator for inference by designing the basic Optical Multiply and Accumulate (OMAC) units and integrating the OMAC units with digital processing systems. The proposed work is based on the effective use of microring resonators (MRRs), Mach-Zehnder Interferometers (MZIs), optical waveguides and lasers for multiply and accumulate functionality and integrating the photonic components with electronic processing. Both MRRs and MZIs are mature technologies that have the required form factor (area-efficiency) as well as bandwidth-density for optical processing and integration.</p>\r\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; By characterizing photonic devices such as MRRs and MZMs using photonic simulators, we develop a realistic expectation of device capability for system level acceleration. The precision limitations imposed by MRRs and MZMs are quantified, which is the driving factor for the Albiero architecture. We present a set of photonic computation schemes that naturally exploit the shared parameters and multicast data distribution found in CNNs, which reduces energy consumption and significantly increases throughput for CNN applications. Albireo implements an efficient broadcast combined with multicast data distribution, and leverages parameter sharing through unique WDM dot product processing in the proposed photonic locally-connected units (PLCU). Albireo improves upon prior work by not only leveraging shared parameters, but by doing so through passively overlapping receptive fields, significantly increasing computation parallelism.</p>\r\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We propose SPACX, a silicon photonics-based accelerator for DNN inference applications. Specifically, SPACX includes a photonic network design that enables seamless two-dimensional broadcast communications, and a tailored dataflow that promotes data broadcast and maximizes parallelism. Furthermore, we explore the broadcast granularity (number of broadcast destinations) of the proposed photonic network and its implications on system performance and energy efficiency. We propose to support simultaneous bi-directional communication between on-chip memory and processing elements using space-division multiplexing technique. A flexible bandwidth allocation scheme is also proposed to dynamically adjust communication bandwidths for different types of data. Simulation studies have shown that SPACX outperforms other state-of-the-art chiplet-based DNN accelerators in terms of execution time and energy consumption.</p>\r\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We propose Flumen, a dual-purpose interconnect that provides communication at the package-level while also doubling as an accelerator, performing parallel linear computation when network load is low. The proposed architecture utilizes the inherent parallelism of light to create energy-efficient interconnects that support en route computation with minimal changes to the network. By dynamically adjusting the topology, Flumen can change the communication and compute sections of the architecture to adapt to workload fluctuations. Performance evaluation on linear algebra applications shows that Flumen achieves significant speedup and energy saving when compared to an electrical mesh network that is used exclusively for communication.</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Publications</strong>:</p>\r\n<ul>\r\n<li>Andy Wolff and Avinash Karanth, &ldquo;Training Photonic Mach Zehnder Meshes for Neural Network Acceleration,&rdquo; Accepted to appear in <em>31<sup>st</sup> IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC), </em>Bengaluru, India, December 18-21, 2024.</li>\r\n<li>Yuan Li, Ahmed Louri and Avinash Karanth, &ldquo;A Silicon Photonic Multi-DNN Accelerator,&rdquo; <em>32<sup>nd</sup> International Conference on Parallel Architectures and Compilation Techniques (PACT-23)</em>, Vienna, Austria, Oct 21-25, 2023.</li>\r\n<li>Kyle Shiflett, Avinash Karanth, Ahmed Louri and Razvan Bunescu, &ldquo;Flumen: Dynamic Processing in the Photonic Interconnect,&rdquo; <em>50<sup>th</sup> IEEE International Symposium on Computer Architecture (ISCA)</em>, Orlando, Florida, June 17-23, 2023.</li>\r\n<li>Yuan Li, Ahmed Louri and Avinash Karanth, &ldquo;SPACX: Silicon Photonics-based Chiplet Architecture for DNN Inference,&rdquo; <em>28<sup>th</sup></em> <em>IEEE International Symposium on High-Performance Computer Architecture (HPCA), </em>Seoul, South Korea,<em> </em>February 12-16, 2022.</li>\r\n<li>Yuan Li, Ahmed Louri and Avinash Karanth, &ldquo;Scaling Deep Learning Inference with Chiplet-based Architecture and Photonic Interconnects,&rdquo; <em>Design Automation Conference (DAC)</em>, San Francisco, CA, Dec, 2021. </li>\r\n<li>Kyle Shiflett, Avinash Karanth, Ahmed Louri and Razvan Bunescu, &ldquo;Albireo: Energy-Efficient Acceleration of Convolutional Neural Networks via Silicon Photonics,&rdquo; <em>48<sup>th</sup> IEEE International Symposium on Computer Architecture (ISCA-48)</em>, Valencia, Spain June 14-18, 2021<em>.</em></li>\r\n<li>Kyle Shiflett, Dylan Wright, Avinash Karanth and Ahmed Louri, &ldquo;PIXEL: Photonic Neural Network Accelerator,&rdquo; <em>26<sup>th</sup> IEEE International Symposium on High-Performance Computer Architecture (HPCA-2020)</em>, San Diego, CA, Feb 22-26, 2020.</li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2024<br>\nModified by: Avinash&nbsp;Karanth</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTo dramatically reduce power consumption, silicon photonics has been proposed to improve performance-per-Watt compared to electrical implementation of embedded systems and system-on-chips. This project leverages photonic technology and heterogeneous multicores for the design of deep neural network accelerators that improve parallelism, concurrency, energy-efficiency and scalability in various machine learning applications. The first task of the project is concerned with the characterization and identification of photonic devices that can implement accelerator functionalities such as multiply-and-accumulate, summation, and other arithmetic operations. The second task of the project implements various types of deep learning architectures on the proposed photonic neural network accelerator to maximize the gains offered by the photonic technology. The third task of the project builds an extensive simulation and modeling infrastructure that combines the photonic technology, network architectures, accelerator functionality, and machine learning algorithms.\r\n\n\n- We leverage the unique advantages of photonic technology to design PIXEL: Photonic Neural Network Accelerator for inference by designing the basic Optical Multiply and Accumulate (OMAC) units and integrating the OMAC units with digital processing systems. The proposed work is based on the effective use of microring resonators (MRRs), Mach-Zehnder Interferometers (MZIs), optical waveguides and lasers for multiply and accumulate functionality and integrating the photonic components with electronic processing. Both MRRs and MZIs are mature technologies that have the required form factor (area-efficiency) as well as bandwidth-density for optical processing and integration.\r\n\n\n- By characterizing photonic devices such as MRRs and MZMs using photonic simulators, we develop a realistic expectation of device capability for system level acceleration. The precision limitations imposed by MRRs and MZMs are quantified, which is the driving factor for the Albiero architecture. We present a set of photonic computation schemes that naturally exploit the shared parameters and multicast data distribution found in CNNs, which reduces energy consumption and significantly increases throughput for CNN applications. Albireo implements an efficient broadcast combined with multicast data distribution, and leverages parameter sharing through unique WDM dot product processing in the proposed photonic locally-connected units (PLCU). Albireo improves upon prior work by not only leveraging shared parameters, but by doing so through passively overlapping receptive fields, significantly increasing computation parallelism.\r\n\n\n- We propose SPACX, a silicon photonics-based accelerator for DNN inference applications. Specifically, SPACX includes a photonic network design that enables seamless two-dimensional broadcast communications, and a tailored dataflow that promotes data broadcast and maximizes parallelism. Furthermore, we explore the broadcast granularity (number of broadcast destinations) of the proposed photonic network and its implications on system performance and energy efficiency. We propose to support simultaneous bi-directional communication between on-chip memory and processing elements using space-division multiplexing technique. A flexible bandwidth allocation scheme is also proposed to dynamically adjust communication bandwidths for different types of data. Simulation studies have shown that SPACX outperforms other state-of-the-art chiplet-based DNN accelerators in terms of execution time and energy consumption.\r\n\n\n- We propose Flumen, a dual-purpose interconnect that provides communication at the package-level while also doubling as an accelerator, performing parallel linear computation when network load is low. The proposed architecture utilizes the inherent parallelism of light to create energy-efficient interconnects that support en route computation with minimal changes to the network. By dynamically adjusting the topology, Flumen can change the communication and compute sections of the architecture to adapt to workload fluctuations. Performance evaluation on linear algebra applications shows that Flumen achieves significant speedup and energy saving when compared to an electrical mesh network that is used exclusively for communication.\r\n\n\n\r\n\n\nPublications:\r\n\r\nAndy Wolff and Avinash Karanth, Training Photonic Mach Zehnder Meshes for Neural Network Acceleration, Accepted to appear in 31st IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC), Bengaluru, India, December 18-21, 2024.\r\nYuan Li, Ahmed Louri and Avinash Karanth, A Silicon Photonic Multi-DNN Accelerator, 32nd International Conference on Parallel Architectures and Compilation Techniques (PACT-23), Vienna, Austria, Oct 21-25, 2023.\r\nKyle Shiflett, Avinash Karanth, Ahmed Louri and Razvan Bunescu, Flumen: Dynamic Processing in the Photonic Interconnect, 50th IEEE International Symposium on Computer Architecture (ISCA), Orlando, Florida, June 17-23, 2023.\r\nYuan Li, Ahmed Louri and Avinash Karanth, SPACX: Silicon Photonics-based Chiplet Architecture for DNN Inference, 28th IEEE International Symposium on High-Performance Computer Architecture (HPCA), Seoul, South Korea, February 12-16, 2022.\r\nYuan Li, Ahmed Louri and Avinash Karanth, Scaling Deep Learning Inference with Chiplet-based Architecture and Photonic Interconnects, Design Automation Conference (DAC), San Francisco, CA, Dec, 2021. \r\nKyle Shiflett, Avinash Karanth, Ahmed Louri and Razvan Bunescu, Albireo: Energy-Efficient Acceleration of Convolutional Neural Networks via Silicon Photonics, 48th IEEE International Symposium on Computer Architecture (ISCA-48), Valencia, Spain June 14-18, 2021.\r\nKyle Shiflett, Dylan Wright, Avinash Karanth and Ahmed Louri, PIXEL: Photonic Neural Network Accelerator, 26th IEEE International Symposium on High-Performance Computer Architecture (HPCA-2020), San Diego, CA, Feb 22-26, 2020.\r\n\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 12/30/2024\n\n\t\t\t\t\tSubmitted by: AvinashKaranth\n"
 }
}