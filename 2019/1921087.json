{
 "awd_id": "1921087",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AI-DCL: Collaborative Research: EAGER: Understanding and Alleviating Potential Biases in Large Scale Employee Selection Systems: The Case of Automated Video Interviews",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frederick Kronz",
 "awd_eff_date": "2019-09-15",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 144994.0,
 "awd_amount": 144994.0,
 "awd_min_amd_letter_date": "2019-08-05",
 "awd_max_amd_letter_date": "2019-08-05",
 "awd_abstract_narration": "The goal of this project is to use machine learning to understand and mitigate bias in interviewer evaluations. The researchers will do so by examining gender differences in expressed behavior during interviews; they will focus on behaviors that can lead to different interviewer evaluations. More specifically, they will use unsupervised video interviews to assess gender differences in terms of signaling behavior, such as facial expressions and language style; they will do so by studying how these differences are perceived by human interviewers in their indexing of personality and cognitive ability. For their research design, they rely on a large sample of men and women interviewees matched on standardized test scores of Graduate and Managerial Assessment (GMA), self-reported personality ratings, age, race, and ethnicity. The research will provide new opportunities for interdisciplinary training of students with an emphasis on recruiting underrepresented groups to work on this project. This research will provide information and guidance for developing bias-free machine-learning systems for personnel selection. By identifying and accounting for behavioral differences between genders that lead to predictive bias in machine learning selection systems, the proposed research will advance our understanding of the differences in gender expression of behaviors, methods for dealing with bias in machine learning, and bias reduction strategies in personnel selection and assessment.\r\n\r\nThis project focuses on two scenarios of assessing interviewee attributes to train machine-learning algorithms: Algorithms trained on interviewee information (GMA test scores and self-reported personality), and algorithms trained on observer (interviewer) assessment of attributes. The matched sample ensures machine-learning model differences are not based on difference in underlying sample attributes. The two main goals of the project are: To understand gender differences in expressed behaviors and interviewer ratings (trained and untrained interviewers) using machine-learning techniques, and then to use that understanding to reduce predictive discrepancies between men and women by accounting for it in the models. The findings will have several significant societal impacts. They will improve our ability to predict and mitigate biases, bring to light new methodologies for mitigating bias in machine learning; and (provide strategies and tools for reducing social inequalities in employment outcomes. This research also has potential to advance both social science and machine learning. It will provide insights that can advance our understanding of social role theory by uncovering objective differences in behavior exhibited by men and women and how these behaviors are interpreted differently. Further, it can advance machine learning in developing new techniques for addressing bias at all stages of the machine-learning pipeline from instance selection and weighting, to model fitting, and then to model selection and optimization.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sidney",
   "pi_last_name": "D'Mello",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sidney D'Mello",
   "pi_email_addr": "sidney.dmello@gmail.com",
   "nsf_id": "000744176",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Colorado at Boulder",
  "inst_street_address": "3100 MARINE ST",
  "inst_street_address_2": "STE 481 572 UCB",
  "inst_city_name": "Boulder",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3034926221",
  "inst_zip_code": "803090001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "THE REGENTS OF THE UNIVERSITY OF COLORADO",
  "org_prnt_uei_num": "",
  "org_uei_num": "SPVKK1RC2MZ3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Colorado at Boulder",
  "perf_str_addr": "3100 Marine Street, Room 481, 57",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803031058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 144994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the current EAGER project&nbsp; ?AI-DCL: Collaborative Research: EAGER: Understanding and Alleviating Potential Biases in Large Scale Employee Selection Systems: The Case of Automated Video Interviews? was to identify different sources of potential biases in hiring through the use of artificial intelligence (AI) systems. The intellectual merits include creating and testing a conceptual framework for assessing and mitigating different sources of bias in AI systems. This interdisciplinary work between psychology and computer science provides a shared basis for various fields to better understand ways to assess and address biases in AI systems used for assessing individuals. This is aligned with the broader societal goal of reducing inadvertent bias and discrimination against underrepresented minorities when implementing and using AI systems. The following video showcases this framework to a general audience: <a href=\"https://youtu.be/CGhaUm6oCuI\">https://youtu.be/CGhaUm6oCuI</a></p>\n<p>Different aspects of the framework were empirically tested in the context of identifying and mitigating gender bias in automated video interviews in simulated employment selection. We collected over 1,500 video interviews from participants and annotated them for dimensions of personality, intelligence, and hireability. A range of multimodal machine learning models was developed to automate the interview annotation process, which provided test cases for identifying and reducing gender bias.</p>\n<p>The findings improve our ability to predict and mitigate perceptual biases that disadvantage women and other minorities; develop methodologies for mitigating bias in ML; and provide strategies and tools for reducing social inequalities in employment outcomes, which can address broader inequalities.</p>\n<p>In addition to the aforementioned dataset, the project yielded multiple presentations, 9 publications, 1 doctoral dissertation, supported the interdisciplinary training of 1 postdoctoral researcher, 3 masters students, 2 graduate students, and 20 undergraduate students, and spanned a second project focusing on face-based racial bias in automated video interviews funded by Society for Industrial and Organizational Psychology (SIOP).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2022<br>\n\t\t\t\t\tModified by: Sidney&nbsp;D'mello</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of the current EAGER project  ?AI-DCL: Collaborative Research: EAGER: Understanding and Alleviating Potential Biases in Large Scale Employee Selection Systems: The Case of Automated Video Interviews? was to identify different sources of potential biases in hiring through the use of artificial intelligence (AI) systems. The intellectual merits include creating and testing a conceptual framework for assessing and mitigating different sources of bias in AI systems. This interdisciplinary work between psychology and computer science provides a shared basis for various fields to better understand ways to assess and address biases in AI systems used for assessing individuals. This is aligned with the broader societal goal of reducing inadvertent bias and discrimination against underrepresented minorities when implementing and using AI systems. The following video showcases this framework to a general audience: https://youtu.be/CGhaUm6oCuI\n\nDifferent aspects of the framework were empirically tested in the context of identifying and mitigating gender bias in automated video interviews in simulated employment selection. We collected over 1,500 video interviews from participants and annotated them for dimensions of personality, intelligence, and hireability. A range of multimodal machine learning models was developed to automate the interview annotation process, which provided test cases for identifying and reducing gender bias.\n\nThe findings improve our ability to predict and mitigate perceptual biases that disadvantage women and other minorities; develop methodologies for mitigating bias in ML; and provide strategies and tools for reducing social inequalities in employment outcomes, which can address broader inequalities.\n\nIn addition to the aforementioned dataset, the project yielded multiple presentations, 9 publications, 1 doctoral dissertation, supported the interdisciplinary training of 1 postdoctoral researcher, 3 masters students, 2 graduate students, and 20 undergraduate students, and spanned a second project focusing on face-based racial bias in automated video interviews funded by Society for Industrial and Organizational Psychology (SIOP).\n\n\t\t\t\t\tLast Modified: 11/22/2022\n\n\t\t\t\t\tSubmitted by: Sidney D'mello"
 }
}