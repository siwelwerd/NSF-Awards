{
 "awd_id": "1852841",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Advancing Record Linkage Research: Optimal Linkage Decisions and Propagating Linkage Uncertainty",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927269",
 "po_email": "ceavey@nsf.gov",
 "po_sign_block_name": "Cheryl Eavey",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2019-07-18",
 "awd_max_amd_letter_date": "2019-07-18",
 "awd_abstract_narration": "This project will advance research on record linkage. It is increasingly common to find complementary information on individuals scattered across multiple data sources. To take full advantage of these data sources, researchers need to be able to link information on the same individuals. In many applications, however, there are no unique identifiers of the individuals in the datafiles. This makes it difficult to recognize which records correspond to the same individuals. Statistical methodology will be developed for creating merged datafiles and for improving analyses of the linked data.  These data linkages will allow richer data analyses and potentially substitute for or facilitate new data collection efforts. Researchers across disciplines will benefit from being able to use statistically rigorous procedures to merge datasets and to carry out analyses with linked data. The ability to create and analyze richer datasets will facilitate understanding of policy options in important areas such as education and health, thus furthering societal interests. A graduate student will be trained as part of this project, and the techniques will be made available as part of free software packages along with tutorials.\r\n\r\nThis project will use the output of probabilistic record linkage procedures to develop rigorous statistical methodology for creating merged datafiles. Coherent approaches for propagating linkage uncertainty into subsequent analyses will be explored. To create merged datasets, the investigator will derive an estimator of the true linkage of the datafiles. A loss function will be developed through which researchers will be able to give different weights to different types of linkage errors. The linkage estimator will be derived by minimizing the expected value of the researcher-defined loss function. The point estimators also will include the option of \"abstaining\" from linking records for which the correct links are highly uncertain. To perform statistical analyses with merged data, the investigator will explore procedures in which researchers carry out the statistical analysis they are interested in for each of several plausible linkages of the data and then combine the output from these analyses. The procedures will be validated theoretically, via simulation studies, and using real data analyses.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mauricio",
   "pi_last_name": "Sadinle Garcia-Ruiz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mauricio Sadinle Garcia-Ruiz",
   "pi_email_addr": "msadinle@uw.edu",
   "nsf_id": "000779552",
   "pi_start_date": "2019-07-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "1705 NE Pacific St, F600 HSB",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981957232",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "133300",
   "pgm_ele_name": "Methodology, Measuremt & Stats"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Finding multiple data sources that contain complementary information on individuals is becoming increasingly common across academia, government, industry, and non-profit sectors. To take full advantage of multiple data sources, analysts need to be able to conduct data merges by linking the information that come from the same individuals. These merges allow richer data analyses and can substitute or facilitate new data collection efforts. However, in many applications there are rarely unique identifiers of the individuals in the datafiles, which makes it difficult to recognize which records correspond to the same individuals across data sources. Record linkage techniques use partially-identifying information common across data sources, such as names or dates of birth, to identify unique individuals. However, in practice it is common to have a limited amount of such partial identifiers, as well as there being errors and missing data, all of which lead to uncertainty in the correct way of linking the datafiles. Probabilistic approaches to record linkage aim at quantifying this uncertainty, which is typically represented in terms of probabilities or distributions on the possible ways of linking the datafiles. Having a quantification of the linkage uncertainty is important, but we need to acknowledge that data analysts typically want to achieve one of two goals. First, they might be interested in simply creating a unified datafile, for example for administrative purposes. Second, they might want to carry out a statistical analysis. In the first case, the output of probabilistic record linkage needs to be transformed into a final linkage decision, and in the second case, the linkage uncertainty needs to be propagated into the statistical analysis. Currently, these two tasks are done in largely ad-hoc or restrictive ways, as there does not exist sufficient statistical methodology nor software implementations. In this project we proposed to advance record linkage methodology in those two fronts. First, we developed rigorous statistical methodology for creating merged datafiles using the output of probabilistic record linkage procedures. Second, we explored a coherent approach for propagating linkage uncertainty into subsequent analyses.</p>\n<p>Using probabilistic record linkage to create a final merged datafile amounts to deriving a point estimate of the linkage of the datafiles. To do this in a principled manner, we proposed to create a loss function where analysts can give different weights to different types of linkage errors. The proposed linkage point estimator is derived by minimizing the expected value of the analyst-defined loss function. The estimator corresponds to the solution of an optimization problem for which we propose different algorithms. Our point estimators can also &ldquo;abstain&rdquo; from linking records for which the correct links are highly uncertain.&nbsp;&nbsp;<span>The methodology was extensively evaluated against other competitors, using both simulation studies and existing real datasets. The performance of the new approach is superior in terms of the precision and recall metrics, although the amount of improvement depends on the specific scenario.</span></p>\n<p>These results were presented at multiple invited talks at different conferences, and published in the premier statistics journal, with the RA as the first author.&nbsp; The methodology was implemented as a freely available package for the R computing software, distributed via GitHub.&nbsp; The software produced from this project will allow researchers across disciplines to have access to easy to use, high-quality data linkage techniques.&nbsp;</p>\n<p><span>We also trained four PhD students as part of related exploratory projects.&nbsp; Specific skills that they gained as part of their training in record linkage include: computer coding, probabilistic modeling, Bayesian inference, missing data theory, presentation and writing skills.&nbsp; Among these students, one became the project's RA, and completed his PhD dissertation on this topic.&nbsp;&nbsp;</span></p>\n<p>Additonally, the PI directed an investigation by another PhD student who compared existing record linkage algorithms, which led to a publication.&nbsp; The PI also co-wrote two discussion pieces of papers related to this project, co-wrote a book chapter on a related topic, wrote an R software package implementing the two-file record linkage methodology on which this project builds, and along with the RA got a manuscript in a related topic accepted as a discussion paper in the premier biostatistics journal.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2023<br>\n\t\t\t\t\tModified by: Mauricio&nbsp;Sadinle Garcia-Ruiz</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFinding multiple data sources that contain complementary information on individuals is becoming increasingly common across academia, government, industry, and non-profit sectors. To take full advantage of multiple data sources, analysts need to be able to conduct data merges by linking the information that come from the same individuals. These merges allow richer data analyses and can substitute or facilitate new data collection efforts. However, in many applications there are rarely unique identifiers of the individuals in the datafiles, which makes it difficult to recognize which records correspond to the same individuals across data sources. Record linkage techniques use partially-identifying information common across data sources, such as names or dates of birth, to identify unique individuals. However, in practice it is common to have a limited amount of such partial identifiers, as well as there being errors and missing data, all of which lead to uncertainty in the correct way of linking the datafiles. Probabilistic approaches to record linkage aim at quantifying this uncertainty, which is typically represented in terms of probabilities or distributions on the possible ways of linking the datafiles. Having a quantification of the linkage uncertainty is important, but we need to acknowledge that data analysts typically want to achieve one of two goals. First, they might be interested in simply creating a unified datafile, for example for administrative purposes. Second, they might want to carry out a statistical analysis. In the first case, the output of probabilistic record linkage needs to be transformed into a final linkage decision, and in the second case, the linkage uncertainty needs to be propagated into the statistical analysis. Currently, these two tasks are done in largely ad-hoc or restrictive ways, as there does not exist sufficient statistical methodology nor software implementations. In this project we proposed to advance record linkage methodology in those two fronts. First, we developed rigorous statistical methodology for creating merged datafiles using the output of probabilistic record linkage procedures. Second, we explored a coherent approach for propagating linkage uncertainty into subsequent analyses.\n\nUsing probabilistic record linkage to create a final merged datafile amounts to deriving a point estimate of the linkage of the datafiles. To do this in a principled manner, we proposed to create a loss function where analysts can give different weights to different types of linkage errors. The proposed linkage point estimator is derived by minimizing the expected value of the analyst-defined loss function. The estimator corresponds to the solution of an optimization problem for which we propose different algorithms. Our point estimators can also \"abstain\" from linking records for which the correct links are highly uncertain.  The methodology was extensively evaluated against other competitors, using both simulation studies and existing real datasets. The performance of the new approach is superior in terms of the precision and recall metrics, although the amount of improvement depends on the specific scenario.\n\nThese results were presented at multiple invited talks at different conferences, and published in the premier statistics journal, with the RA as the first author.  The methodology was implemented as a freely available package for the R computing software, distributed via GitHub.  The software produced from this project will allow researchers across disciplines to have access to easy to use, high-quality data linkage techniques. \n\nWe also trained four PhD students as part of related exploratory projects.  Specific skills that they gained as part of their training in record linkage include: computer coding, probabilistic modeling, Bayesian inference, missing data theory, presentation and writing skills.  Among these students, one became the project's RA, and completed his PhD dissertation on this topic.  \n\nAdditonally, the PI directed an investigation by another PhD student who compared existing record linkage algorithms, which led to a publication.  The PI also co-wrote two discussion pieces of papers related to this project, co-wrote a book chapter on a related topic, wrote an R software package implementing the two-file record linkage methodology on which this project builds, and along with the RA got a manuscript in a related topic accepted as a discussion paper in the premier biostatistics journal. \n\n \n\n\t\t\t\t\tLast Modified: 01/03/2023\n\n\t\t\t\t\tSubmitted by: Mauricio Sadinle Garcia-Ruiz"
 }
}