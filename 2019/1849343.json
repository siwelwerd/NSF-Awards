{
 "awd_id": "1849343",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS:INT:Learning and Planning for Dynamic Locomotion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-02-01",
 "awd_exp_date": "2025-01-31",
 "tot_intn_awd_amt": 820000.0,
 "awd_amount": 836000.0,
 "awd_min_amd_letter_date": "2019-01-29",
 "awd_max_amd_letter_date": "2019-05-09",
 "awd_abstract_narration": "Despite years of work on robot locomotion, we still do not have robots that can reliably and flexibly move around in homes, workspaces, and natural terrain. For many of these environments, legged robots, as opposed to wheel-based robots, appear to be the most viable option for achieving the desired level of locomotion autonomy. Prior work has produced ATRIAS, a two-legged robot, which was designed to replicate the dynamic properties of human and animal legs, and Cassie, which retains this dynamics-first approach but improves upon ATRIAS by adding steering capability and ankles, along with engineering improvements. Compared to conventional robot-leg designs, the designs of ATRIAS and Cassie carefully incorporate \"passive dynamics\" into the mechanism, essentially bringing the dynamic behavior of the hardware into partnership with the software control system. This approach has the potential to exhibit locomotion capabilities much closer to humans. However, the flexibility and \"springiness\" of these human-like legs creates new challenges for locomotion control. While ATRIAS and Cassie are currently able to walk and run outdoors over moderate terrain using basic balance control methods, the methods are still not able to support more complex locomotion activities, such as navigating stairs or rocky terrain. The proposed research will develop new control methods for dynamic legged locomotion, which will enable robots such as ATRIAS and Cassie to effectively move around in our homes, workplaces, and other complex natural environments with much more flexibility, while using much less energy. This will significantly expand on the application domains for which autonomous robot locomotion can be applied. \r\n\r\nThe primary technical contribution of the project will be twofold: First, the research will study machine learning techniques to dramatically improve the existing hand-crafted controllers for dynamic locomotion, and create a rich action space composed of behavior policies that produce robust walking, standing, running, and leaping behaviors with various speeds, step/jump heights, and other characteristics. This action space provides an expressive and compact means of controlling the motion of a legged robot, greatly surpassing direct torque control in expressiveness while also dramatically reducing the dimensionality of the problem. Second, the research will design a fast and efficient sampling-based planning architecture, which also uses machine learning to speed up the planning process to allow for real-time fulfillment of movement goals while avoiding collisions and falls. This work adds new knowledge in research on legged locomotion planning by considering obstacle planning and robot dynamics as an integrated problem. Most prior work attempts to decouple the two pieces, for example by using a planner to find footholds in kinematic space and handing them to a dynamics controller that tries to maintain balance as the robot follows the kinematic goals. For human-like performance in two-legged locomotion, the project considers foothold choice to be intrinsically linked to robot dynamics, and considers foot placement in an integrated way.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Fern",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Fern",
   "pi_email_addr": "afern@eecs.oregonstate.edu",
   "nsf_id": "000088242",
   "pi_start_date": "2019-01-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Hurst",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan W Hurst",
   "pi_email_addr": "jonathan.hurst@oregonstate.edu",
   "nsf_id": "000528997",
   "pi_start_date": "2019-01-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973305501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 836000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Before this project, bipedal robots&mdash;robots that walk on two legs like humans&mdash;could only move reliably in tightly controlled environments. Their movements were brittle and prone to failure in real-world settings. This research helped change that. The approach we developed&mdash;training robots in simulation and transferring those skills to the physical world&mdash;is now widely used by companies and universities working on humanoid robotics. It has become a standard practice in the field.</p>\r\n<p>The core idea behind our work is called sim-to-real reinforcement learning. It starts with building a realistic computer simulation of the robot and its environment. A deep neural network (DNN)&mdash;a type of artificial brain&mdash;serves as the robot's controller. This network receives input about the robot&rsquo;s current state (like joint positions and movement) and sends motor commands to control its limbs.</p>\r\n<p>Initially, the robot falls a lot. But with each failed attempt, the system learns. A reward signal tells the network how well it&rsquo;s doing: it gets negative rewards for falling and positive rewards for better walking, running, or balancing. Over millions of training steps, the neural network improves its control, gradually learning how to move smoothly and stay upright.</p>\r\n<p>However, simulations are never a perfect match for the real world. Small mismatches in friction, motor strength, or weight distribution can cause a trained robot to fail once deployed. To address this, we developed a technique called domain randomization. During training, we intentionally vary aspects of the simulation&mdash;such as surface slipperiness or motor power&mdash;so that the robot learns to succeed across a wide range of conditions. This makes the final controller more robust and better able to handle the messiness of the real world.</p>\r\n<p>Throughout the project, we applied these techniques to increasingly complex challenges. We developed new types of reward signals and improved domain randomization methods, enabling bipedal robots to perform difficult locomotion tasks never before demonstrated.</p>\r\n<p>Some key accomplishments include:</p>\r\n<ol>\r\n<li>The first bipedal controller capable of all      common human-like gaits: standing, walking, running, hopping, and      skipping.</li>\r\n<li>The first bipedal robot to complete a 5K outdoor      race.</li>\r\n<li>A Guinness World Record for the fastest      100-meter dash by a bipedal robot.</li>\r\n<li>The first robust demonstration of blind stair      climbing, where the robot climbs stairs without using vision.</li>\r\n<li>The first sim-to-real reinforcement learning      demonstration of humanoid box manipulation, in which a robot moves      boxes while walking.</li>\r\n</ol>\r\n<p>These results demonstrated that reinforcement learning in simulation, when combined with domain randomization, can reliably transfer to real-world robots&mdash;even for highly dynamic and challenging tasks.</p>\r\n<p>In addition to technical advances, the project had significant educational and workforce development impacts. It trained 4 Ph.D. students and involved more than 10 undergraduates, helping to prepare the next generation of roboticists and AI researchers.</p><br>\n<p>\n Last Modified: 04/09/2025<br>\nModified by: Alan&nbsp;Fern</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744235085775_digit_w_dean--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744235085775_digit_w_dean--rgov-800width.jpg\" title=\"Digit Handing off Game Ball\"><img src=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744235085775_digit_w_dean--rgov-66x44.jpg\" alt=\"Digit Handing off Game Ball\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Digit handing the ball to the Dean of Engineering at the \"Beavers Beyond the Classroom\ufffd event at Oregon State University where 7000+ middle school students from around the state are bused in for a field trip to attend a women\ufffds basketball game and experience engineering exhibits.</div>\n<div class=\"imageCredit\">Oregon State University</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Alan&nbsp;Fern\n<div class=\"imageTitle\">Digit Handing off Game Ball</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744234644287_far_view_5k--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744234644287_far_view_5k--rgov-800width.png\" title=\"Cassie Robot Running a 5k\"><img src=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744234644287_far_view_5k--rgov-66x44.png\" alt=\"Cassie Robot Running a 5k\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Aerial image of Cassie during the running of a 5k race. Cassie was the first bipedal robot to complete a full 5k in a natural outdoor environment.</div>\n<div class=\"imageCredit\">Oregon State University</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Alan&nbsp;Fern\n<div class=\"imageTitle\">Cassie Robot Running a 5k</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744234121103_52386009346_1a4ed19388_k--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744234121103_52386009346_1a4ed19388_k--rgov-800width.jpg\" title=\"Bipedal 100m Dash World Record\"><img src=\"/por/images/Reports/POR/2025/1849343/1849343_10589070_1744234121103_52386009346_1a4ed19388_k--rgov-66x44.jpg\" alt=\"Bipedal 100m Dash World Record\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Cassie robot running the Guinness World Record 100m dash for bipedal robots. The robot is controlled by a neural network that is trained in simulation using reinforcement learning and then transferred to the real robot.</div>\n<div class=\"imageCredit\">Oregon State University</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Alan&nbsp;Fern\n<div class=\"imageTitle\">Bipedal 100m Dash World Record</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nBefore this project, bipedal robotsrobots that walk on two legs like humanscould only move reliably in tightly controlled environments. Their movements were brittle and prone to failure in real-world settings. This research helped change that. The approach we developedtraining robots in simulation and transferring those skills to the physical worldis now widely used by companies and universities working on humanoid robotics. It has become a standard practice in the field.\r\n\n\nThe core idea behind our work is called sim-to-real reinforcement learning. It starts with building a realistic computer simulation of the robot and its environment. A deep neural network (DNN)a type of artificial brainserves as the robot's controller. This network receives input about the robots current state (like joint positions and movement) and sends motor commands to control its limbs.\r\n\n\nInitially, the robot falls a lot. But with each failed attempt, the system learns. A reward signal tells the network how well its doing: it gets negative rewards for falling and positive rewards for better walking, running, or balancing. Over millions of training steps, the neural network improves its control, gradually learning how to move smoothly and stay upright.\r\n\n\nHowever, simulations are never a perfect match for the real world. Small mismatches in friction, motor strength, or weight distribution can cause a trained robot to fail once deployed. To address this, we developed a technique called domain randomization. During training, we intentionally vary aspects of the simulationsuch as surface slipperiness or motor powerso that the robot learns to succeed across a wide range of conditions. This makes the final controller more robust and better able to handle the messiness of the real world.\r\n\n\nThroughout the project, we applied these techniques to increasingly complex challenges. We developed new types of reward signals and improved domain randomization methods, enabling bipedal robots to perform difficult locomotion tasks never before demonstrated.\r\n\n\nSome key accomplishments include:\r\n\r\nThe first bipedal controller capable of all      common human-like gaits: standing, walking, running, hopping, and      skipping.\r\nThe first bipedal robot to complete a 5K outdoor      race.\r\nA Guinness World Record for the fastest      100-meter dash by a bipedal robot.\r\nThe first robust demonstration of blind stair      climbing, where the robot climbs stairs without using vision.\r\nThe first sim-to-real reinforcement learning      demonstration of humanoid box manipulation, in which a robot moves      boxes while walking.\r\n\r\n\n\nThese results demonstrated that reinforcement learning in simulation, when combined with domain randomization, can reliably transfer to real-world robotseven for highly dynamic and challenging tasks.\r\n\n\nIn addition to technical advances, the project had significant educational and workforce development impacts. It trained 4 Ph.D. students and involved more than 10 undergraduates, helping to prepare the next generation of roboticists and AI researchers.\t\t\t\t\tLast Modified: 04/09/2025\n\n\t\t\t\t\tSubmitted by: AlanFern\n"
 }
}