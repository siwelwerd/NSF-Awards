{
 "awd_id": "1925178",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Creating Trust Between Groups of Humans and Robots Using a Novel Music Driven Robotic Emotion Generator",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922633",
 "po_email": "aleoness@nsf.gov",
 "po_sign_block_name": "Alex Leonessa",
 "awd_eff_date": "2019-11-01",
 "awd_exp_date": "2023-10-31",
 "tot_intn_awd_amt": 669912.0,
 "awd_amount": 803892.0,
 "awd_min_amd_letter_date": "2019-08-20",
 "awd_max_amd_letter_date": "2020-07-31",
 "awd_abstract_narration": "This project will perform fundamental research contributing to the establishment of trust between humans and robots through the development of novel emotional communication channels.  As co-robots become prevalent at home, work, and public spaces, they need to become trust-worthy and socially believable agents if they are to be integrated into and accepted by society.  The research will utilize the latest developments in Artificial Intelligence to gain knowledge about of the role of non-linguistic expressions in trust building. Findings from studies about non-linguistic emotional expressions such as prosody and gestures in music - one of the most emotionally meaningful human experiences - will be implemented in a group of newly developed personal robots. User experiments will be conducted to explore humans' reactions to - and trust building with - these prosody-driven robots. Results of the study will lead to novel approaches for creating open and meaningful interactions between groups of humans and robots.  The research will advance national prosperity by increasing engagement, relatability, and trust in large scale human-robot interactive scenarios such as personal robots in private and public spaces, work place training, education, and combat.  The project takes an interdisciplinary approach, which will address fields such as cognitive science, communication, and music, while leading to progress in both science and engineering.\r\n\r\nProsodic features such as pitch, loudness, tempo, timbre, and rhythm bear strong resemblance to musical features, which can inform a novel approach for generative emotion-driven robotic prosody. The first phase of this project will focus on developing machine learning techniques to derive features from a newly created emotionally labeled musical dataset. It will use these features to drive a non-linguistic robotic voice synthesizer that conveys emotional content and builds trust. The results of this study will be integrated with previous work on conveying robotic emotions through physical gestures. The second phase of the project will focus on user experiments that will study subjects' preference to a variety of robotic emotional responses when interacting with a single robot. It will use the learned features to design a larger scale robotic emotional contagion engine in an effort to improve and enrich emotion-driven human interaction with large groups of robots.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gil",
   "pi_last_name": "Weinberg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gil Weinberg",
   "pi_email_addr": "gil.weinberg@coa.gatech.edu",
   "nsf_id": "000105700",
   "pi_start_date": "2019-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 669912.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 133980.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As co-robots become prevalent at home, work, and public spaces, they need to become trust-worthy and socially believable agents if they are to be integrated into and accepted by society.&nbsp;The main goal of this project was to develop a new paradigm for trust building between humans and co-robots through novel emotional communication channels that utilize non-linguistic expressions such as vocal prosody and physical gestures.</p>\n<p>To address this goal, a large dataset of vocal and instrumental audio phrases was created, labeled for emotional content by musicians, and validated in a large-scale user survey.&nbsp;&nbsp;Deep learning models were then developed and trained by this dataset to generate audio that carry Emotional Musical Prosody (EMP). A new rule-based model for emotion conveyance through gestures was also developed for multiple robotic platforms.&nbsp;&nbsp;The emotional generated audio was then integrated with the emotional robotic gesture generator in a number of robotic platforms.&nbsp;</p>\n<p>Multiple Human-Robot Interaction studies have been conducted to evaluate the effectiveness of the integrated system in improving trust between humans and robots.&nbsp;&nbsp;The studies showed that our integrated sound-gestural system not only improved robotic emotion detection by humans but also significantly improved the perception of human trust in these robots. The system also presented significant results in improving the perception robotic animacy, anthropomorphism, likability, and intelligence.&nbsp;</p>\n<p>Over the life of the project, two PhD students and 8 MS students have been trained and contributed to the development of different modules of the project. The graduate students were helped by more than 20 undergrad students as part of the Vertical Integrated Project class offered by Georgia Tech, where undergraduate students from all 5 colleges across campus are working with professors and grad students on their research.</p>\n<p>Results from the project were disseminated through academic publications as well as performances, workshops, and concerts, where the general public learned about the importance of sound and gestures in creating trust between humans and robots.&nbsp;&nbsp;&nbsp;One of these performances titled FOREST received the international Falling Walls award in the category of Arts and Sciences, expanding the reach of the project to international audiences.</p><br>\n<p>\n Last Modified: 12/25/2023<br>\nModified by: Gil&nbsp;Weinberg</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1925178/1925178_10636126_1703517800419_Forest_photo--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1925178/1925178_10636126_1703517800419_Forest_photo--rgov-800width.jpg\" title=\"FOREST Performance\"><img src=\"/por/images/Reports/POR/2023/1925178/1925178_10636126_1703517800419_Forest_photo--rgov-66x44.jpg\" alt=\"FOREST Performance\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Two dancers interact with emotional sound and gesture driven robotic arms as part the performance of FOREST.</div>\n<div class=\"imageCredit\">Gioconda Barral-Secchi</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Gil&nbsp;Weinberg\n<div class=\"imageTitle\">FOREST Performance</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/1925178/1925178_10636126_1703518709785_Trust_EMP--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1925178/1925178_10636126_1703518709785_Trust_EMP--rgov-800width.jpg\" title=\"Emotional Music Prosody for HRI Trust\"><img src=\"/por/images/Reports/POR/2023/1925178/1925178_10636126_1703518709785_Trust_EMP--rgov-66x44.jpg\" alt=\"Emotional Music Prosody for HRI Trust\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Box plot demonstrating the effect of Emotional Musical Prosody on the perception of trust in human-robot-interaction. Published in  \"Emotional Musical Prosody for the Enhancement of Trust in Robotic Arm Communication\" Paladyn, Journal of Behavioral Robotics, Savery R., Zahary, L, Weinberg G, 2020</div>\n<div class=\"imageCredit\">Richard Savery</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Gil&nbsp;Weinberg\n<div class=\"imageTitle\">Emotional Music Prosody for HRI Trust</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs co-robots become prevalent at home, work, and public spaces, they need to become trust-worthy and socially believable agents if they are to be integrated into and accepted by society.The main goal of this project was to develop a new paradigm for trust building between humans and co-robots through novel emotional communication channels that utilize non-linguistic expressions such as vocal prosody and physical gestures.\n\n\nTo address this goal, a large dataset of vocal and instrumental audio phrases was created, labeled for emotional content by musicians, and validated in a large-scale user survey.Deep learning models were then developed and trained by this dataset to generate audio that carry Emotional Musical Prosody (EMP). A new rule-based model for emotion conveyance through gestures was also developed for multiple robotic platforms.The emotional generated audio was then integrated with the emotional robotic gesture generator in a number of robotic platforms.\n\n\nMultiple Human-Robot Interaction studies have been conducted to evaluate the effectiveness of the integrated system in improving trust between humans and robots.The studies showed that our integrated sound-gestural system not only improved robotic emotion detection by humans but also significantly improved the perception of human trust in these robots. The system also presented significant results in improving the perception robotic animacy, anthropomorphism, likability, and intelligence.\n\n\nOver the life of the project, two PhD students and 8 MS students have been trained and contributed to the development of different modules of the project. The graduate students were helped by more than 20 undergrad students as part of the Vertical Integrated Project class offered by Georgia Tech, where undergraduate students from all 5 colleges across campus are working with professors and grad students on their research.\n\n\nResults from the project were disseminated through academic publications as well as performances, workshops, and concerts, where the general public learned about the importance of sound and gestures in creating trust between humans and robots.One of these performances titled FOREST received the international Falling Walls award in the category of Arts and Sciences, expanding the reach of the project to international audiences.\t\t\t\t\tLast Modified: 12/25/2023\n\n\t\t\t\t\tSubmitted by: GilWeinberg\n"
 }
}