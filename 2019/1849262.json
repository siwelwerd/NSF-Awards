{
 "awd_id": "1849262",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS: FND: COLLAB: Learning from Stories: Practical Value Alignment and Taskability for Autonomous Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 308696.0,
 "awd_amount": 308696.0,
 "awd_min_amd_letter_date": "2019-05-17",
 "awd_max_amd_letter_date": "2019-05-17",
 "awd_abstract_narration": "In the near future we are likely to see increasingly-capable autonomous systems operating in proximity to humans and immersed in society. As these systems become more sophisticated, they will interact increasingly with humans. With this increased human-agent interaction comes an increased obligation to ensure that autonomous systems do not cause even unintentional harm to a human. Creating systems that cannot intentionally or unintentionally harm humans in not an easy task. This is because there are infinitely many undesirable outcomes that can be achieved in an open world, making it impossible to instruct these systems to avoid each one. If the desired behavior cannot be directly specified, then it must be learned. Past approaches to learn these types of behaviors have focused on learning from human examples, but these methods are unlikely to scale. This research uses natural language explanations of behavior as a scalable alternative for training autonomous agents for safe operation. Naturalistic descriptions contain vast amounts of information about sociocultural norms, which make them rich sources for such training. Enabling systems to better understand and learn from such descriptions will enable human operators to more naturally specify goals or tasks for the agent to complete.\r\n\r\nThis research explores the concept of learning via natural language descriptions of desired behavior. This technique uses procedural knowledge contained in natural language explanations to help train autonomous agents. Concretely, this approach learns utility functions that can be used to guide autonomous agents towards behaviors that are aligned with the description used for training. To accomplish this, researchers will create computational models capable of extracting both knowledge about sociocultural norms as well as procedural knowledge from naturally occurring corpora. These models will then be used to create behavior policies that are both aligned with sociocultural norms and procedurally plausible. To further ensure that these models can be practically deployed, researchers will enable their models to incorporate a \"human in the loop\" to provide online feedback about the quality of these learned behavior policies in terms of their social acceptability and appropriateness. Safeguards will also be investigated to protect the learned behavior policies against the effects of adversarial or malicious training examples.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Riedl",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mark Riedl",
   "pi_email_addr": "riedl@cc.gatech.edu",
   "nsf_id": "000077574",
   "pi_start_date": "2019-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue, NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 308696.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As we make progress building increasingly capable AI systems that spend significant amounts of time interacting with people, we seek to ensure that they are safe and <span>do not cause even unintentional harms. </span><span>We introduce the concept of \"normative alignment\", a new research problem in which human values that are encoded in their societal norms are understood and enacted by AI systems. Stories, in particular, encode human values specialized to the culture or social group that created the stories. We demonstrated that AI models (prior to GPT3) can be taught to classify textual descriptions of behavior as being consistent with norms from a story or not. With this capability, we can achieve a number of practical effects on AI systems. We demonstrated that language models could be fine tuned to avoid language that is not consistent with sociocultural norms. We invented a technique for training virtual agents to solve problems in ways that are consistent with sociocultural norms. These agents learned concepts such as altruism from the stories provided as data without being explicitly instructed about the concept. We also wanted to know if norms can be fed back into new stories. We invented a technique for generating narrative text that could be controlled by preferences about the norms that story characters followed or violated. Normative alignment creates a conceptual framework that spans multiple paradigms of AI safety, allowing a single problem formulation under which to tackle safety concerns such as bias mitigation, agent and robot safe operation, and control of language models. <br /></span></p><br>\n<p>\n Last Modified: 01/09/2024<br>\nModified by: Mark&nbsp;O&nbsp;Riedl</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs we make progress building increasingly capable AI systems that spend significant amounts of time interacting with people, we seek to ensure that they are safe and do not cause even unintentional harms. We introduce the concept of \"normative alignment\", a new research problem in which human values that are encoded in their societal norms are understood and enacted by AI systems. Stories, in particular, encode human values specialized to the culture or social group that created the stories. We demonstrated that AI models (prior to GPT3) can be taught to classify textual descriptions of behavior as being consistent with norms from a story or not. With this capability, we can achieve a number of practical effects on AI systems. We demonstrated that language models could be fine tuned to avoid language that is not consistent with sociocultural norms. We invented a technique for training virtual agents to solve problems in ways that are consistent with sociocultural norms. These agents learned concepts such as altruism from the stories provided as data without being explicitly instructed about the concept. We also wanted to know if norms can be fed back into new stories. We invented a technique for generating narrative text that could be controlled by preferences about the norms that story characters followed or violated. Normative alignment creates a conceptual framework that spans multiple paradigms of AI safety, allowing a single problem formulation under which to tackle safety concerns such as bias mitigation, agent and robot safe operation, and control of language models. \n\t\t\t\t\tLast Modified: 01/09/2024\n\n\t\t\t\t\tSubmitted by: MarkORiedl\n"
 }
}