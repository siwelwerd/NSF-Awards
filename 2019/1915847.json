{
 "awd_id": "1915847",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC: Early-Stage Interdisciplinary Collaboration: Privacy Enhancing Framework to Advance Behavior Models",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2019-06-15",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 299471.0,
 "awd_amount": 315471.0,
 "awd_min_amd_letter_date": "2019-06-05",
 "awd_max_amd_letter_date": "2019-06-25",
 "awd_abstract_narration": "This project is designed to advance research on problematic eating behavior. The project investigates wearable sensors to measure eating behavior and developing models of behavior that comprise multiple observable behaviors such as eating alone or with friends, or chewing speed. These data can help scientists improve upon current traditional methods such as self-reported eating diaries, which tend to be inconsistent, sparse, and rarely timely. We capture human behavior using a custom wearable augmented camera. Wearable cameras provide rich data, but raise privacy concerns. The project will address these concerns by building a framework using machine learning and information theory while including human-reported privacy concerns. The framework will address wearers' concerns that may limit recording authentic behavior in real-world settings and will optimize algorithms to enhance the detection and classification of human behavior.\r\n \r\nThe project explores the acceptability of obfuscation techniques on varied activities and their requisite tasks. The proposed research will design a suite of computationally efficient task-specific algorithms that use raw images in computationally restrictive (in situ) and obfuscated images in unrestrictive environments (offline) to build information-performance curves for the scalable development of personalized ground truth wearable cameras. The project also will develop a modular, plug-and-play, low-complexity and efficient obfuscation computing hardware device to facilitate and accelerate the use of the proposed methods and algorithms. This work will validate an overeating behavior model in a real-world setting using the design framework and device, providing visual confirmation of eating behaviors, showing how it can be used to test existing models. This project is likely to be useful to other domains in the social sciences, fundamentally changing the way researchers build and validate behavioral models in real-world settings. There are potential applications in health (especially preventive medicine), social, and economic sciences: energy balance, infant development, medication adherence, consumer behavior, and human-environment interaction.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nabil",
   "pi_last_name": "Alshurafa",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nabil Alshurafa",
   "pi_email_addr": "nabil@northwestern.edu",
   "nsf_id": "000718284",
   "pi_start_date": "2019-06-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aggelos",
   "pi_last_name": "Katsaggelos",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Aggelos K Katsaggelos",
   "pi_email_addr": "aggk@eecs.northwestern.edu",
   "nsf_id": "000258523",
   "pi_start_date": "2019-06-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bonnie",
   "pi_last_name": "Spring",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Bonnie J Spring",
   "pi_email_addr": "bspring@northwestern.edu",
   "nsf_id": "000624371",
   "pi_start_date": "2019-06-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Josiah",
   "pi_last_name": "Hester",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Josiah D Hester",
   "pi_email_addr": "josiah@gatech.edu",
   "nsf_id": "000753638",
   "pi_start_date": "2019-06-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Graham",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea K Graham",
   "pi_email_addr": "andrea.graham@northwestern.edu",
   "nsf_id": "000794821",
   "pi_start_date": "2019-06-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University at Chicago",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "KG76WYENL5K1",
  "org_uei_num": "KG76WYENL5K1"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University at Chicago",
  "perf_str_addr": "680 North Lake Shore Drive, Suit",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606114579",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "IL05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "114Z",
   "pgm_ref_txt": "SaTC-CISE-SBE New Collabs"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 315471.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Studying human behavior has traditionally been theory based, attempting to explain or predict behavior through a set of constructs or complex concepts (e.g., self-efficacy or attitude toward behavior) and then subjecting specific concepts to empirical testing. Ecological momentary assessments (EMAs) allow participants to report behavior when events occur using their smartphones, but they are still prone to bias and prevent contextual understanding of behavior in free-living settings. Wearable mobile sensors enable quantitative representation of coarse- and fine-grained behaviors, facilitating the development of new behavioral models that explain and predict problematic behaviors such as overeating. These wearable devices will allow behaviorists to construct models that more closely represent reality and enable the improved design of interventions that change behavior. Building models of these behaviors requires a reliable, consistent, and constant source of ground truth for development and verification. Researchers can learn contextual information about human behavior in their natural habitat using wearable cameras combined with infrared (IR) sensor arrays and advanced computational methods. However, wearer perception of bystander privacy concerns is the leading reason people are unwilling to continuously wear cameras in daily life.</p>\n<p>In this project we designed and developed multiple wearable RGB cameras equipped with an IR-sensing system that evaluates various obfuscation methods to verify human behavior in a real-world setting. In addition, we studied and tested the tradeoff between the wearer's acceptability of different obfuscation techniques before recording the video footage and the performance of various online and offline algorithms to detect specific human behavior. Our proposed method addresses the challenges for monitoring human behavior that can enable the development of behavioral models. Our approach supports how we build behavioral models based on objective, real-world behavior in free-living people. With this EAGER research grant, we (1) prototyped a modular and configurable hardware platform for energy-constrained human behavior monitoring, (2) developed intelligent obfuscation algorithms for enhancing privacy, and (3) ran research studies to identify user's preferred obfuscation methods to evaluate the general utility of the wearable device for human behavior monitoring.</p>\n<p>We built a suite of wearable cameras equipped with a low-resolution thermal camera for all-day monitoring of four activities: eating, smoking, socializing, and using a screen. We conducted multiple experiments to test acceptability, privacy concerns, and utility of different wearable cameras in free-living settings. We tested the device in 367 Amazon Mechanical Turkers, 20 people in a 2-day free-living study aimed at detecting eating, and 60 people in a 2-week study of people with obesity to detect overeating. As our cameras evolved, so did the number of hours of video footage collected per day by the participants as privacy considerations continued to be addressed. The device is miniaturized (most recent iteration was approximately golf-ball sized), can continuously collect RGB and thermal sensing data on-device, can transmit data via Bluetooth to a smartphone app, and incorporates night-vision. Furthermore, several case designs are available to address different body shapes and sizes. We have also tested several algorithms that can run offline on the obfuscated image and on-device to detect human gestures for applications related to eating and smoking. Our wearable device is configurable, and we have designed five different types of wearable devices depending on the application and need. Specifically, the device can be configured based on privacy concerns, desired battery lifetime, types of activity that needs to be detected, and necessity for real-time processing.</p>\n<p>Our wearable camera suite opens a new research program for recognizing human behavior. This research enables the computational means for low-cost, continuous monitoring of human behavior. In the future, these devices can operate independently and extract all the required information using on-device machine learning for human behavior at a reduced size and cost. The devices will deliver actionable data, including notifications and suggestions to users, to control and observe their daily habits. In addition, the devices will provide human behavioralists with a tool that supports the understanding of human behaviors, enabling visual confirmation of human behavior using a tool that a person is much more likely to wear in free-living settings.</p>\n<p>We have designed a scalable, low-cost system that is capable of informing computational constraints and requirements in situ and performing post-processing of obfuscation methods. This will allow future information theory and resource allocation experts to optimize where algorithms should run, taking into consideration wearer levels of concern. Our platform could be applied to nearly every study of human behavior to obtain precise predictors and determinants of behavior. This also significantly advances behavioral science to build more precise models and interventions that improve behavior in many fields.</p>\n<p>The main outcome of the project has been published, and a website detailing the suite of wearable cameras is available: <a href=\"https://habitslab.github.io/grants/nsf-eager-/\">NSF EAGER Privacy Conscious Wearable Camera Suite</a>. The website includes links for the online release of the hardware and software the were developed through the project.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2022<br>\n\t\t\t\t\tModified by: Nabil&nbsp;Alshurafa</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279645134_1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279645134_1--rgov-800width.jpg\" title=\"Overview of the smart wearable.\"><img src=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279645134_1--rgov-66x44.jpg\" alt=\"Overview of the smart wearable.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">One application of our system. (a) A wearable camera is given to participants\ufffdwho (b) wear it, allowing it to detect and quantify health-risk events, such as eating and smoking. Data are stored in (c) a cloud server, retrievable by (d) a clinician who uses the data to inform treatment.</div>\n<div class=\"imageCredit\">Tommy Cohen and Christopher Romano</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nabil&nbsp;Alshurafa</div>\n<div class=\"imageTitle\">Overview of the smart wearable.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279794265_2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279794265_2--rgov-800width.jpg\" title=\"First prototype of the wearable camera.\"><img src=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279794265_2--rgov-66x44.jpg\" alt=\"First prototype of the wearable camera.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">First prototype of the privacy conscious wearable camera system, from schematic to test unit.</div>\n<div class=\"imageCredit\">Rawan Alharbi and Mahdi Pedram</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nabil&nbsp;Alshurafa</div>\n<div class=\"imageTitle\">First prototype of the wearable camera.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279906647_3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279906647_3--rgov-800width.jpg\" title=\"Final prototype of the wearable camera.\"><img src=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666279906647_3--rgov-66x44.jpg\" alt=\"Final prototype of the wearable camera.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Prototyping of the final privacy conscious wearable camera system, from schematic to test unit.</div>\n<div class=\"imageCredit\">Mahdi Pedram</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nabil&nbsp;Alshurafa</div>\n<div class=\"imageTitle\">Final prototype of the wearable camera.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666280046332_4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666280046332_4--rgov-800width.jpg\" title=\"Different Obfuscation Methods\"><img src=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666280046332_4--rgov-66x44.jpg\" alt=\"Different Obfuscation Methods\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Various masking techniques applied to activity-oriented video data to increase privacy protection, ranging from fully masking background information to applying cartoonization effects.</div>\n<div class=\"imageCredit\">Rawan Alharbi and Glenn Fernandes</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nabil&nbsp;Alshurafa</div>\n<div class=\"imageTitle\">Different Obfuscation Methods</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666280128635_5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666280128635_5--rgov-800width.jpg\" title=\"Smart activation algorithm to save power.\"><img src=\"/por/images/Reports/POR/2022/1915847/1915847_10609198_1666280128635_5--rgov-66x44.jpg\" alt=\"Smart activation algorithm to save power.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The onboard trigger system. (a) The device rests on the torso with the sensor array facing upwards towards the\ufffdwearer\ufffds face and runs in a (b) low-power state, collecting only thermal images, triggering a (c) high-power state to detect RGB\ufffdimages or run ML models only when necessary.</div>\n<div class=\"imageCredit\">Glenn Fernandes and Soroush Shahi</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Nabil&nbsp;Alshurafa</div>\n<div class=\"imageTitle\">Smart activation algorithm to save power.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nStudying human behavior has traditionally been theory based, attempting to explain or predict behavior through a set of constructs or complex concepts (e.g., self-efficacy or attitude toward behavior) and then subjecting specific concepts to empirical testing. Ecological momentary assessments (EMAs) allow participants to report behavior when events occur using their smartphones, but they are still prone to bias and prevent contextual understanding of behavior in free-living settings. Wearable mobile sensors enable quantitative representation of coarse- and fine-grained behaviors, facilitating the development of new behavioral models that explain and predict problematic behaviors such as overeating. These wearable devices will allow behaviorists to construct models that more closely represent reality and enable the improved design of interventions that change behavior. Building models of these behaviors requires a reliable, consistent, and constant source of ground truth for development and verification. Researchers can learn contextual information about human behavior in their natural habitat using wearable cameras combined with infrared (IR) sensor arrays and advanced computational methods. However, wearer perception of bystander privacy concerns is the leading reason people are unwilling to continuously wear cameras in daily life.\n\nIn this project we designed and developed multiple wearable RGB cameras equipped with an IR-sensing system that evaluates various obfuscation methods to verify human behavior in a real-world setting. In addition, we studied and tested the tradeoff between the wearer's acceptability of different obfuscation techniques before recording the video footage and the performance of various online and offline algorithms to detect specific human behavior. Our proposed method addresses the challenges for monitoring human behavior that can enable the development of behavioral models. Our approach supports how we build behavioral models based on objective, real-world behavior in free-living people. With this EAGER research grant, we (1) prototyped a modular and configurable hardware platform for energy-constrained human behavior monitoring, (2) developed intelligent obfuscation algorithms for enhancing privacy, and (3) ran research studies to identify user's preferred obfuscation methods to evaluate the general utility of the wearable device for human behavior monitoring.\n\nWe built a suite of wearable cameras equipped with a low-resolution thermal camera for all-day monitoring of four activities: eating, smoking, socializing, and using a screen. We conducted multiple experiments to test acceptability, privacy concerns, and utility of different wearable cameras in free-living settings. We tested the device in 367 Amazon Mechanical Turkers, 20 people in a 2-day free-living study aimed at detecting eating, and 60 people in a 2-week study of people with obesity to detect overeating. As our cameras evolved, so did the number of hours of video footage collected per day by the participants as privacy considerations continued to be addressed. The device is miniaturized (most recent iteration was approximately golf-ball sized), can continuously collect RGB and thermal sensing data on-device, can transmit data via Bluetooth to a smartphone app, and incorporates night-vision. Furthermore, several case designs are available to address different body shapes and sizes. We have also tested several algorithms that can run offline on the obfuscated image and on-device to detect human gestures for applications related to eating and smoking. Our wearable device is configurable, and we have designed five different types of wearable devices depending on the application and need. Specifically, the device can be configured based on privacy concerns, desired battery lifetime, types of activity that needs to be detected, and necessity for real-time processing.\n\nOur wearable camera suite opens a new research program for recognizing human behavior. This research enables the computational means for low-cost, continuous monitoring of human behavior. In the future, these devices can operate independently and extract all the required information using on-device machine learning for human behavior at a reduced size and cost. The devices will deliver actionable data, including notifications and suggestions to users, to control and observe their daily habits. In addition, the devices will provide human behavioralists with a tool that supports the understanding of human behaviors, enabling visual confirmation of human behavior using a tool that a person is much more likely to wear in free-living settings.\n\nWe have designed a scalable, low-cost system that is capable of informing computational constraints and requirements in situ and performing post-processing of obfuscation methods. This will allow future information theory and resource allocation experts to optimize where algorithms should run, taking into consideration wearer levels of concern. Our platform could be applied to nearly every study of human behavior to obtain precise predictors and determinants of behavior. This also significantly advances behavioral science to build more precise models and interventions that improve behavior in many fields.\n\nThe main outcome of the project has been published, and a website detailing the suite of wearable cameras is available: NSF EAGER Privacy Conscious Wearable Camera Suite. The website includes links for the online release of the hardware and software the were developed through the project.\n\n \n\n\t\t\t\t\tLast Modified: 10/20/2022\n\n\t\t\t\t\tSubmitted by: Nabil Alshurafa"
 }
}