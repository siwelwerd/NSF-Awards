{
 "awd_id": "1909847",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Role-Based Norm Violation Response in Human-Robot Teams",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499967.0,
 "awd_amount": 499967.0,
 "awd_min_amd_letter_date": "2019-08-26",
 "awd_max_amd_letter_date": "2020-08-12",
 "awd_abstract_narration": "Robots may need to carefully decide when and how to reject commands given to them, if the actions required to carry out those commands are not morally permissible.  Most previous work on this topic takes a norm-based ethical approach, where a robot would operate under a set of rules describing what states or actions are morally wrong, and use those rules to explain its actions. In contrast, this project explores a role-based perspective, in which the robot reasons about the relationships it holds with others, the roles it plays in those relationships, and whether the actions requested of it are benevolent with respect to those roles and relationships. Specifically, the researchers will develop a framework to allow robots to reason in this way and generate explanations of its actions based on this reasoning.  The researchers will then explore how role-based and norm-based command rejections compare in terms of how they affect human-robot teamwork, and design algorithms to allow robots to automatically decide what type of rejection to generate based on their context. These algorithms and explanations will be evaluated in two very different contexts with different types of relationships, roles, and rules: with civilian undergraduates at the Colorado School of Mines, and with Air Force cadets at the US Air Force Academy. This work will not only increase robots' ability to behave ethically and act as good teammates, but will also advance moral philosophy by providing experimental evidence for the relative importance and effectiveness of different tenets of role-based moral philosophy.\r\n\r\nMore formally, the goals of this research are to investigate context-sensitive tradeoffs between rule-based and role-based responses, and the representations and mechanisms needed to facilitate role-based responses.  The research team will do this by identifying metrics to assess response acceptability, quality, and effectiveness; modeling the generation of role-based responses and selection between role-based and rule-based responses; conducting experiments to validate those models and responses; and using the results to articulate novel moral and philosophical arguments.  The team will start with exploratory studies at each experimental site contrasting the effectiveness of different command rejection phrasings formed by crossing different Speech-Act Theoretic communication strategies paired with different moral philosophical backgrounds, with respect to (a) field-standard survey measures of trust, likability, mindfulness, workload, and norm strength; (b) qualitative analysis of video data; and (c) statistical linguistic analyses from the multimodal interaction community. The researchers will then develop a framework for robots to generate these responses that will involve formal computational model development of inter-team relations, roles, and actions; machine learning-based modeling of norm violation response strategy selection using features such as task context characterization, role-theoretic proposed action benevolence, and expected responses to the responses; and integration into the DIARC robot cognitive architecture.  Based on this work, the team will both advance traditional moral philosophical arguments and develop a novel framework in which moral philosophical arguments are justified based on the effects of computational models on moral psychology.  Overall, the project will lead to foundational, interdisciplinary knowledge into norm violation response, consisting of algorithms for selecting between norm violation responses grounded in different ethical theories, guidelines for and insights into the design of morally competent language capable robots, novel computational accounts of role-based robot ethics, and novel empirically informed moral philosophical arguments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Williams",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Williams",
   "pi_email_addr": "twilliams@mines.edu",
   "nsf_id": "000762830",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Qin",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qin Zhu",
   "pi_email_addr": "qinzhu@vt.edu",
   "nsf_id": "000763167",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado School of Mines",
  "inst_street_address": "1500 ILLINOIS ST",
  "inst_street_address_2": "",
  "inst_city_name": "GOLDEN",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3032733000",
  "inst_zip_code": "804011887",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "CO07",
  "org_lgl_bus_name": "TRUSTEES OF THE COLORADO SCHOOL OF MINES",
  "org_prnt_uei_num": "JW2NGMP4NMA3",
  "org_uei_num": "JW2NGMP4NMA3"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado School of Mines",
  "perf_str_addr": "1500 Illinois Street",
  "perf_city_name": "Golden",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "804011887",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "CO07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "081Y00",
   "pgm_ele_name": "NSF 2026 Fund"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499967.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ac6f9646-7fff-de41-a103-c9dc287f9072\">\n<p dir=\"ltr\"><span>Social robots deployed into human-like environments will need moral competence to avoid negatively impacting human moral ecosystems. Robots will need to be able to tell when what is asked of them is wrong, and reject those commands in appropriate ways. Most previous work on moral competence has been grounded in norm-based theories of morality. In contrast, this project provided an initial exploration of role-based theories of morality that emphasize robots and humans&rsquo; positions within a broader social and relational moral ecosystem. To understand how robots might be given role-based moral competence, the team explored several distinct research thrusts.</span></p>\n<br />\n<p dir=\"ltr\"><span>Overall, much of the project&rsquo;s work focused on developing a theory of Confucian Robot Ethics grounded in Confucian Role Ethics, an ethical framework that emphasizes the cultivation of the moral self. The team took several approaches to achieving this goal, including developing algorithms for role-based moral reasoning; psychologically studying the effectiveness of role-based moral communication; and analyzing more broadly how robots should behave from a Confucian Perspective. The project&rsquo;s philosophical work involved a number of new explorations of robot ethics, and encouraged a shift from designing robots that do good to designing robots that encourage humans to be good. The project&rsquo;s algorithmic work led to new algorithms for learning moral representations, and for the first knowledge representations for role-based moral reasoning and moral communication.</span></p>\n<br />\n<p dir=\"ltr\"><span>The project&rsquo;s psychological work explored how robots&rsquo; moral guidance phrased from different moral perspectives might have different effectiveness at encouraging moral behavior from human users. Our results showed that role-based moral advice can be particularly effective, but that it depends how the interaction is structured. In particular, our results&nbsp; showed the importance of moral reflection and moral practice for effective moral communication: opportunities for reflection on ethical principles may increase the efficacy of robots&rsquo; role-based moral language; and following robots&rsquo; moral language with opportunities for moral practice may facilitate role-based moral cultivation. This work also showed the ways that different cultural orientations led to differences in moral behavior under robot moral advising.</span></p>\n<br />\n<p dir=\"ltr\"><span>The project also explored more generally how robots should give moral advice. Our results showed that people hold different expectations for both humans and robots as to how to give moral advice vs act in moral situations, and even differences between how people should morally act when advised vs not advised. Finally, this work showed the ways that people are blamed more for disobeying human and robot advice than for acting against moral norms.</span></p>\n<br />\n<p dir=\"ltr\"><span>The project also explored the ways that robot interaction design could shape human politeness both towards robots and toward other people, showing that dominant strategies taken by tech companies were likely to backfire and decrease politeness, and instead suggesting easily adoptable alternate approaches.</span></p>\n<br />\n<p dir=\"ltr\"><span>Moreover, the project led to new ethical arguments beyond role ethics, and to new paradigms of engineering education. From the ethical perspective, the project led to new examinations of the types of perceptual capabilities robots are given, sometimes in the name of norm violation response, and the ethical problems with those capabilities. From the pedagogical perspective, the project explored experimental robot ethics as a key in-class activity that could be used to cultivate practical ethical reasoning among students.</span></p>\n<div><span><br /></span></div>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/07/2023<br>\nModified by: Thomas&nbsp;Williams</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nSocial robots deployed into human-like environments will need moral competence to avoid negatively impacting human moral ecosystems. Robots will need to be able to tell when what is asked of them is wrong, and reject those commands in appropriate ways. Most previous work on moral competence has been grounded in norm-based theories of morality. In contrast, this project provided an initial exploration of role-based theories of morality that emphasize robots and humans positions within a broader social and relational moral ecosystem. To understand how robots might be given role-based moral competence, the team explored several distinct research thrusts.\n\n\n\n\nOverall, much of the projects work focused on developing a theory of Confucian Robot Ethics grounded in Confucian Role Ethics, an ethical framework that emphasizes the cultivation of the moral self. The team took several approaches to achieving this goal, including developing algorithms for role-based moral reasoning; psychologically studying the effectiveness of role-based moral communication; and analyzing more broadly how robots should behave from a Confucian Perspective. The projects philosophical work involved a number of new explorations of robot ethics, and encouraged a shift from designing robots that do good to designing robots that encourage humans to be good. The projects algorithmic work led to new algorithms for learning moral representations, and for the first knowledge representations for role-based moral reasoning and moral communication.\n\n\n\n\nThe projects psychological work explored how robots moral guidance phrased from different moral perspectives might have different effectiveness at encouraging moral behavior from human users. Our results showed that role-based moral advice can be particularly effective, but that it depends how the interaction is structured. In particular, our results showed the importance of moral reflection and moral practice for effective moral communication: opportunities for reflection on ethical principles may increase the efficacy of robots role-based moral language; and following robots moral language with opportunities for moral practice may facilitate role-based moral cultivation. This work also showed the ways that different cultural orientations led to differences in moral behavior under robot moral advising.\n\n\n\n\nThe project also explored more generally how robots should give moral advice. Our results showed that people hold different expectations for both humans and robots as to how to give moral advice vs act in moral situations, and even differences between how people should morally act when advised vs not advised. Finally, this work showed the ways that people are blamed more for disobeying human and robot advice than for acting against moral norms.\n\n\n\n\nThe project also explored the ways that robot interaction design could shape human politeness both towards robots and toward other people, showing that dominant strategies taken by tech companies were likely to backfire and decrease politeness, and instead suggesting easily adoptable alternate approaches.\n\n\n\n\nMoreover, the project led to new ethical arguments beyond role ethics, and to new paradigms of engineering education. From the ethical perspective, the project led to new examinations of the types of perceptual capabilities robots are given, sometimes in the name of norm violation response, and the ethical problems with those capabilities. From the pedagogical perspective, the project explored experimental robot ethics as a key in-class activity that could be used to cultivate practical ethical reasoning among students.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 12/07/2023\n\n\t\t\t\t\tSubmitted by: ThomasWilliams\n"
 }
}