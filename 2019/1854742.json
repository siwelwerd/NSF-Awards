{
 "awd_id": "1854742",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AitF: Collaborative Research: A Framework of Simultaneous Acceleration and Storage Reduction on Deep Neural Networks Using Structured Matrices",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "A. Funda Ergun",
 "awd_eff_date": "2018-08-15",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 367884.0,
 "awd_amount": 367884.0,
 "awd_min_amd_letter_date": "2018-10-25",
 "awd_max_amd_letter_date": "2018-10-25",
 "awd_abstract_narration": "\u00a0 \u00a0 \u00a0\u00a0Deep neural networks (DNNs) have emerged as a class of powerful techniques for learning solutions in a number of challenging problem domains, including computer vision, natural language processing and bioinformatics.  These solutions have been enabled mainly because we now have computational accelerators able to sift through the myriad of data required to train a neural network.   As the size of DNN models continues to grow, computational and memory resource requirements for training will also grow, limiting deployment of deep learning in many practical applications. \r\n\r\n      Leveraging the theory of structured matrices, this project will develop a general framework for efficient DNN training and inference, providing a significant reduction in algorithmic complexity measures in terms of both computation and storage.  \r\nThe project, if successful, should fundamentally impact a broad class of deep learning applications.  It will explore accelerating this new structure for deep learning algorithms targeting emerging accelerator architectures, and will evaluate the benefits of these advances across a number of application domains, including big data analytics, cognitive systems, unmanned vehicles and aerial systems, and wearable devices.  The interdisciplinary nature of this project bridges the areas of matrix theory, machine learning, and computer architecture, and will affect education at both Northeastern and CCNY, including the involvement of underrepresented and undergraduate students in the rich array of research tasks.\r\n\r\n\u00a0 \u00a0 \u00a0The project will: (1) for the first time, develop a general theoretical framework for structured matrix-based DNN models and perform detailed analysis and investigation of error bounds, convergence, fast training algorithms, etc.; (2) develop low-space-cost and high-speed inference and training schemes for the fully connected layers of DNNs; (3) impose a weight tensor with structure and enable low computational and space cost convolutional layers; (4) develop high-performance and energy-efficient implementations of deep learning systems on high-performance parallel platforms, low-power embedded platforms, as well as emerging computing paradigms and devices; (5) perform a comprehensive evaluation of the proposed approaches on different performance metrics in a variety of platforms.\u00a0\u00a0The project will deliver tuned implementations targeting a range of computational platforms, including ASICs, FPGAs, GPUs and cloud servers. The hardware optimizations will focus on producing high-speed and low-cost implementations of deep learning systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bo",
   "pi_last_name": "Yuan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bo Yuan",
   "pi_email_addr": "bo.yuan@soe.rutgers.edu",
   "nsf_id": "000704451",
   "pi_start_date": "2018-10-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088543925",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723900",
   "pgm_ele_name": "Algorithms in the Field"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 367883.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This award has delivered the research outcomes on both algorithm and hardware across multiple domains. On the algorithm side, the theoretical analysis of structured matrix based neural network is performed. Meanwhile, the performance of neural network is examined from the perspective of low-rank matrix theory. In addition, novel low structured matrix-based neural network model design and training schemes are developed to reduce memory and computational cost. On the hardware side, new hardware architecture for the structured matrix based neural network accelerator is developed. The customized design is able to fully leverage the property of the new network model and unlock the promising potentials to deliver high throughput, low latency and low energy consumption. Such algorithm/hardware co-design scheme brings significant algorithmic and hardware performance improvement, and thereby facilitating the widespread deployment of modern AI technique in a more affordable way and efficient way on many embedded and edge devices. This award also promotes the STEM education and workforce. Several postdoc researcher and graduate students are supported and trained by this award. The research outcomes are published and disseminated in top mathematics, computer science, computer engineering and electrical engineering journals and conferences, thereby further attracting more graduate and undergraduate students to participate in the research and engineering activities in mathematics, artificial intelligence and semiconductor.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/22/2022<br>\n\t\t\t\t\tModified by: Bo&nbsp;Yuan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis award has delivered the research outcomes on both algorithm and hardware across multiple domains. On the algorithm side, the theoretical analysis of structured matrix based neural network is performed. Meanwhile, the performance of neural network is examined from the perspective of low-rank matrix theory. In addition, novel low structured matrix-based neural network model design and training schemes are developed to reduce memory and computational cost. On the hardware side, new hardware architecture for the structured matrix based neural network accelerator is developed. The customized design is able to fully leverage the property of the new network model and unlock the promising potentials to deliver high throughput, low latency and low energy consumption. Such algorithm/hardware co-design scheme brings significant algorithmic and hardware performance improvement, and thereby facilitating the widespread deployment of modern AI technique in a more affordable way and efficient way on many embedded and edge devices. This award also promotes the STEM education and workforce. Several postdoc researcher and graduate students are supported and trained by this award. The research outcomes are published and disseminated in top mathematics, computer science, computer engineering and electrical engineering journals and conferences, thereby further attracting more graduate and undergraduate students to participate in the research and engineering activities in mathematics, artificial intelligence and semiconductor. \n\n\t\t\t\t\tLast Modified: 03/22/2022\n\n\t\t\t\t\tSubmitted by: Bo Yuan"
 }
}