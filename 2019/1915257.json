{
 "awd_id": "1915257",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Exploring Second-Chance Testing as a Practical Form of Mastery Learning",
 "cfda_num": "47.076",
 "org_code": "11040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Eleanor Sayre",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2019-08-02",
 "awd_max_amd_letter_date": "2019-08-02",
 "awd_abstract_narration": "With support from the NSF Improving Undergraduate STEM Education Program: Education and Human Resources (IUSE: EHR), this project aims to serve the national interest by investigating how second-chance testing improves student learning and retention. The significance of this work is rooted in the observation that some students fail courses, even though they have to ability to succeed and willingness to work. These students often fail because they overestimate the adequacy of their preparation for an exam or misunderstand course expectations. This failure can prevent these qualified students from persisting in STEM majors and joining the STEM workforce. Second-chance testing is the practice of allowing students to take an exam and then giving them feedback on their performance. They are then allowed to further study the material and then take a second, equivalent exam for some form of grade replacement. Preliminary data show that second-chance testing improves average exam scores and reduces exam failure rates. This project will investigate best practices with respect to second-chance testing and characterize its benefits. The project will also study whether second-chance testing disproportionately helps students who are traditionally under-served by existing course structures. It will also study what can be done to help instructors adopt this practice.\r\n\r\nSecond-chance testing can be a powerful technique because it exploits both the \"testing effect\" (increased long-term memory resulting from deliberate retrieval practice) and \"test-potentiated learning\" (increased ability of a learner to benefit from an opportunity to restudy a concept). In addition, it offers the potential to mitigate students' failures on exams due to their inability to correctly assess course expectations and their level of readiness. Preliminary data at the University of Illinois on second-chance testing has shown improvements in student performance measured by mean exam scores and a reduction in exam failure rates. In addition, this data showed that variations in the grading policy can exert considerable influence on which students choose to take the second chance exam. The goals of this project are to more rigorously measure the benefits of second-chance testing, map out the scope of grading policies and their impacts, and better understand the factors involved in faculty adoption of second-chance testing. The project will focus on three research questions. First, how much does second-chance testing improve students' cognitive and non-cognitive outcomes? Second, how do second-chance grading policies affect students' cognitive and non-cognitive outcomes on first-chance and subsequent second-chance exams? Third, does second-chance testing disproportionately help students who are traditionally under-served by single opportunity exam structures? The NSF IUSE: EHR Program supports research and development projects to improve the effectiveness of STEM education for all students. Through the Engaged Student Learning track, the program supports the creation, exploration, and implementation of promising practices and tools.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Craig",
   "pi_last_name": "Zilles",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Craig Zilles",
   "pi_email_addr": "zilles@illinois.edu",
   "nsf_id": "000107009",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "West",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew West",
   "pi_email_addr": "mwest@illinois.edu",
   "nsf_id": "000491061",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Bretl",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy Bretl",
   "pi_email_addr": "tbretl@illinois.edu",
   "nsf_id": "000380917",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Geoffrey",
   "pi_last_name": "Herman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Geoffrey L Herman",
   "pi_email_addr": "glherman@illinois.edu",
   "nsf_id": "000592249",
   "pi_start_date": "2019-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "1901 S. First Street, Suite A",
  "perf_city_name": "Champaign",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0419",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001920DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we explored various aspects of \"second-chance testing\", the idea of routinely offering an optional, second version of an exam, usually a week after the first exam, after a student has received feedback and had a chance to remediate the material.&nbsp; Through classroom experiments, student and instructor interviews, and student surveys, we characterized its impact on student learning, affect (how students feel about it), and behavior.&nbsp; Our findings can be summarized as follows:&nbsp;<br />1. Second-chance testing is effective.&nbsp; In all studies, we saw increased student performance resulting from second-chance testing.&nbsp; In particular, through improving the performance of lower-performing students, we narrow the score distribution.&nbsp; Students report that second-chance testing gives them the motivation/incentive to go back and learn missed material in ways that one-shot testing does not.&nbsp;<br />2. Most students study broadly for first-chance exams and then focus their study before the second-chance exam on topics that they missed on the first-chance exam.&nbsp; We observed no statistically significant reduction in studying for the first-chance exam relative to a course that only offered exams once.&nbsp; Students are most likely to use course office hours between first- and second-chance exams, when they have a clearer understanding of where they aren't meeting course expectations and have an opportunity to rectify it.&nbsp;<br />3. Second-chance testing reduces test anxiety.&nbsp; Students report that knowing that they have an opportunity for a second chance makes studying for exams less stressful; they can focus on learning the material and less on performance anxiety.&nbsp; Interestingly, this is true even for students that never end up taking the second-chance exams.&nbsp; In addition, students report that offering second chances make exams more fair.&nbsp;<br />4. Grading policies should put some weight on first-chance exam scores.&nbsp; In theory, we should give students grades equal to their highest achieved score, but this approach seems not to maximize learning.&nbsp; Student behavior is shaped by incentives.&nbsp; We've found that when no full grade replacement is offered (i.e., the score isn't tied to the first-chance exam performance), some students put less effort into studying for the first exam or will skip it entirely, mitigating the benefit from second-chance testing.&nbsp; Instead, we advocate for grading policies that put some \"token\" weight on the first-chance exam; one such policy is \"90% of your maximum score + 10% of your minimum score\".&nbsp; Policies like this seem to be enough to shape student behavior to treat the first-chance exam seriously.<br />5. The specific grading policy appears not to be very important, as long as it puts some weight on the first-chance exam score.&nbsp; The exact balance of scores between the two exams and how they are combined seems secondary.&nbsp; As such, we'd advocate for a simple policy (like the aforementioned 90 max/10 min policy), which can be easily understood by students.<br /><br />The main challenge to adopting second-chance testing for most faculty is the additional workload of administering and grading additional exams.&nbsp; On our campus, the adoption of second-chance testing has been facilitated by the creation of computer-based testing facilities (CBTFs) and the development of question generators using the PrairieLearn platform.&nbsp; CBTFs are a low-overhead, trustworthy assessment solution consisting of two main parts: (1) proctored computer labs where students take exams on locked-down institutional computers, and (2) a web-based scheduling tool that students use to schedule their exam in a 3-day window, which avoids the administrative overhead of conflicts and missed exams.&nbsp; Exams are constructed from pools of questions and question generators (small programs that use randomization to generate a range of questions on a given topic) to reduce the effort of exam creation and to mitigate cheating on asynchronous exams.&nbsp; In the past year, almost 200,000 exams were run in our CBTF.&nbsp; Each semester, 6-8 large enrollment courses use second-chance testing in the CBTF that this research has influenced to start or change how they implement second-chance testing.&nbsp;&nbsp;</p><br>\n<p>\n Last Modified: 07/14/2024<br>\nModified by: Craig&nbsp;Zilles</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we explored various aspects of \"second-chance testing\", the idea of routinely offering an optional, second version of an exam, usually a week after the first exam, after a student has received feedback and had a chance to remediate the material. Through classroom experiments, student and instructor interviews, and student surveys, we characterized its impact on student learning, affect (how students feel about it), and behavior. Our findings can be summarized as follows:\n1. Second-chance testing is effective. In all studies, we saw increased student performance resulting from second-chance testing. In particular, through improving the performance of lower-performing students, we narrow the score distribution. Students report that second-chance testing gives them the motivation/incentive to go back and learn missed material in ways that one-shot testing does not.\n2. Most students study broadly for first-chance exams and then focus their study before the second-chance exam on topics that they missed on the first-chance exam. We observed no statistically significant reduction in studying for the first-chance exam relative to a course that only offered exams once. Students are most likely to use course office hours between first- and second-chance exams, when they have a clearer understanding of where they aren't meeting course expectations and have an opportunity to rectify it.\n3. Second-chance testing reduces test anxiety. Students report that knowing that they have an opportunity for a second chance makes studying for exams less stressful; they can focus on learning the material and less on performance anxiety. Interestingly, this is true even for students that never end up taking the second-chance exams. In addition, students report that offering second chances make exams more fair.\n4. Grading policies should put some weight on first-chance exam scores. In theory, we should give students grades equal to their highest achieved score, but this approach seems not to maximize learning. Student behavior is shaped by incentives. We've found that when no full grade replacement is offered (i.e., the score isn't tied to the first-chance exam performance), some students put less effort into studying for the first exam or will skip it entirely, mitigating the benefit from second-chance testing. Instead, we advocate for grading policies that put some \"token\" weight on the first-chance exam; one such policy is \"90% of your maximum score + 10% of your minimum score\". Policies like this seem to be enough to shape student behavior to treat the first-chance exam seriously.\n5. The specific grading policy appears not to be very important, as long as it puts some weight on the first-chance exam score. The exact balance of scores between the two exams and how they are combined seems secondary. As such, we'd advocate for a simple policy (like the aforementioned 90 max/10 min policy), which can be easily understood by students.\n\nThe main challenge to adopting second-chance testing for most faculty is the additional workload of administering and grading additional exams. On our campus, the adoption of second-chance testing has been facilitated by the creation of computer-based testing facilities (CBTFs) and the development of question generators using the PrairieLearn platform. CBTFs are a low-overhead, trustworthy assessment solution consisting of two main parts: (1) proctored computer labs where students take exams on locked-down institutional computers, and (2) a web-based scheduling tool that students use to schedule their exam in a 3-day window, which avoids the administrative overhead of conflicts and missed exams. Exams are constructed from pools of questions and question generators (small programs that use randomization to generate a range of questions on a given topic) to reduce the effort of exam creation and to mitigate cheating on asynchronous exams. In the past year, almost 200,000 exams were run in our CBTF. Each semester, 6-8 large enrollment courses use second-chance testing in the CBTF that this research has influenced to start or change how they implement second-chance testing.\t\t\t\t\tLast Modified: 07/14/2024\n\n\t\t\t\t\tSubmitted by: CraigZilles\n"
 }
}