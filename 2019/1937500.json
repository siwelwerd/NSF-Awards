{
 "awd_id": "1937500",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RTML: Large: Efficient and Adaptive Real-Time Learning for Next Generation Wireless Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1199000.0,
 "awd_min_amd_letter_date": "2019-09-16",
 "awd_max_amd_letter_date": "2021-08-27",
 "awd_abstract_narration": "Emerging wireless standards and the promise of 5G communication are driven by the need to attain faster data rates and ultra-low latency. Many incredible bleeding-edge applications, such as community/shared virtual reality experiences and self-driving cars, crucially rely on the ubiquitous availability and real-time reconfigurability of high-speed wireless links, which in turn strongly relies on the ability of next-generation wireless devices to perform a broad variety of inference tasks in real-time. The latency requirements associated with these applications imply the need for improved and accelerated machine learning through dedicated hardware.  Moreover, due to the unpredictable nature of the wireless channel, inference algorithms must be able to adapt and evolve in the presence of an unfamiliar environment. This project seeks to solve this foundational challenge, with successful outcomes being able to achieve unprecedented efficiency improvements in next generation wireless systems. Ideas and findings from the project are incorporated into a number of accessible seminar talks geared at high-school and undergraduate students, to encourage further interest in engineering and science, as well as through multi-disciplinary tutorials aimed at both the wireless networking and machine learning communities.\r\n\r\nThe project, executed by a multidisciplinary team of machine learning, systems, and networking researchers, advances the state of the art through novel deep learning architectures tailored to inference tasks pertinent to next generation wireless devices. It also incorporates novel model compression techniques, producing a hardware-friendly structured pruning approach for fully-connected and convolutional layers of deep neural networks, combined with a novel quantization scheme learned jointly during training. The project's quantization scheme and its hyper-parameter tuning is co-designed with an field programmable gate array (FPGA) hardware implementation and determined via deep reinforcement learning. The adaptation of parts of the network in the presence of new samples is enabled by blending lifelong learning approaches like dynamic networks and complementary learning as new objectives during training.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stratis",
   "pi_last_name": "Ioannidis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stratis Ioannidis",
   "pi_email_addr": "IOANNIDIS@ECE.NEU.EDU",
   "nsf_id": "000711788",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jennifer",
   "pi_last_name": "Dy",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Jennifer G Dy",
   "pi_email_addr": "jdy@ece.neu.edu",
   "nsf_id": "000286912",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kaushik",
   "pi_last_name": "Chowdhury",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Kaushik R Chowdhury",
   "pi_email_addr": "kaushik@utexas.edu",
   "nsf_id": "000541186",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tommaso",
   "pi_last_name": "Melodia",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tommaso Melodia",
   "pi_email_addr": "melodia@northeastern.edu",
   "nsf_id": "000676111",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yanzhi",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yanzhi Wang",
   "pi_email_addr": "yanzhiwang@northeastern.edu",
   "nsf_id": "000695637",
   "pi_start_date": "2019-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 HUNTINGTON AVE",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "082Z",
   "pgm_ref_txt": "RTML-Real Time Machine Learning"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 1000000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 199000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-db06c8f8-7fff-d3f8-5458-182d2d3c5a37\"> <span id=\"docs-internal-guid-70e8532a-7fff-8f94-5bf6-0c85b041bfe2\"> </span></span></p>\n<p dir=\"ltr\"><strong>Intellectual Merit:</strong></p>\n<p dir=\"ltr\"><strong>Real Time Inference on Edge Devices. </strong><span>We introduced sparsification methods for diverse deep neural network architectures (such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and&nbsp; object detection networks), enabling their fast deployment on edge devices. We proposed several novel compression techniques, including compression-compilation co-design, compressed structured block pruning, stochastic rounding, and compiler-aware unified network pruning and neural architecture search. Our methods led to&nbsp; real-time implementation of large networks over several target edge hardware, including FPGAs and mobile devices.</span></p>\n<p dir=\"ltr\"><strong>ML for Wireless.</strong><span> We proposed a data augmentation step within the training pipeline of RF fingerprinting machine learning algorithms that exposes them to many simulated channel and noise variations that are not present in the original dataset. We also showed how such perturbations can be designed at the receiver side to improve classification accuracy.&nbsp; We also proposed machine learning approaches for beamforming that camera images, to (i) rapidly identify the locations of the transmitter and receiver nodes, and then (ii) return the optimal beam pair. This reduces beamforming related exploration time under different ambient lighting conditions. We further improved predictive performance through a multi-modal approach that combined image, lidar, and positioning data, as well as federating learning across multiple edge sensors.</span></p>\n<p dir=\"ltr\"><strong>Adversarial Robustness on the Edge.</strong><span> We investigated the use of regularizers during training adversarially robust deep neural networks. In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful information for output prediction while reducing redundant information: we capture both through the use of the Hilbert Schmidt Independence Criterion (HSIC) as a proxy for mutual information.&nbsp; We showed that this so-called HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. We also demonstrated that it can be used to prune pre-trained networks for edge deployment without any additional use of adversarial examples, significantly reducing pruning time while maintaining robustness.</span></p>\n<p dir=\"ltr\"><strong>Continual Learning.</strong><span> We proposed a novel deep learning framework named learn-prune-share for the continual learning problem. Our scheme learns sequential tasks without experiencing catastrophic forgetting, by partitioning the neural network and dedicating portions to each task. It also prunes the neural network, thereby maintaining parsimony and avoiding overfitting. Finally, it selectively shares knowledge from old tasks and reuses them on new tasks. All of these happen simultaneously, in a unified optimization framework trained in an end-to-end fashion. We also propose a continual learning method to boost the performance of existing rehearsal-based methods by mitigating inter-task interference and promoting task-invariant knowledge sharing: both happen using appropriately defined HSIC-bottlenecks as regularizers. We show that this outperforms state-of-the-art regularization-enhanced rehearsal methods.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><strong>Broader Impacts:</strong></p>\n<p dir=\"ltr\"><span>Ten Ph.D. students and one M.Sc. student were involved in the project during its duration. Research findings were disseminated in 21 conference and 6&nbsp; journal publications. Results were further disseminated through invited seminars and keynotes at academic institutions in the US as well as internationally. Our code was made publicly available and shared via github repositories.</span></p>\n<p><br /><br /></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/17/2024<br>\nModified by: Stratis&nbsp;Ioannidis</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n  \n\n\nIntellectual Merit:\n\n\nReal Time Inference on Edge Devices. We introduced sparsification methods for diverse deep neural network architectures (such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and object detection networks), enabling their fast deployment on edge devices. We proposed several novel compression techniques, including compression-compilation co-design, compressed structured block pruning, stochastic rounding, and compiler-aware unified network pruning and neural architecture search. Our methods led to real-time implementation of large networks over several target edge hardware, including FPGAs and mobile devices.\n\n\nML for Wireless. We proposed a data augmentation step within the training pipeline of RF fingerprinting machine learning algorithms that exposes them to many simulated channel and noise variations that are not present in the original dataset. We also showed how such perturbations can be designed at the receiver side to improve classification accuracy. We also proposed machine learning approaches for beamforming that camera images, to (i) rapidly identify the locations of the transmitter and receiver nodes, and then (ii) return the optimal beam pair. This reduces beamforming related exploration time under different ambient lighting conditions. We further improved predictive performance through a multi-modal approach that combined image, lidar, and positioning data, as well as federating learning across multiple edge sensors.\n\n\nAdversarial Robustness on the Edge. We investigated the use of regularizers during training adversarially robust deep neural networks. In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful information for output prediction while reducing redundant information: we capture both through the use of the Hilbert Schmidt Independence Criterion (HSIC) as a proxy for mutual information. We showed that this so-called HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. We also demonstrated that it can be used to prune pre-trained networks for edge deployment without any additional use of adversarial examples, significantly reducing pruning time while maintaining robustness.\n\n\nContinual Learning. We proposed a novel deep learning framework named learn-prune-share for the continual learning problem. Our scheme learns sequential tasks without experiencing catastrophic forgetting, by partitioning the neural network and dedicating portions to each task. It also prunes the neural network, thereby maintaining parsimony and avoiding overfitting. Finally, it selectively shares knowledge from old tasks and reuses them on new tasks. All of these happen simultaneously, in a unified optimization framework trained in an end-to-end fashion. We also propose a continual learning method to boost the performance of existing rehearsal-based methods by mitigating inter-task interference and promoting task-invariant knowledge sharing: both happen using appropriately defined HSIC-bottlenecks as regularizers. We show that this outperforms state-of-the-art regularization-enhanced rehearsal methods.\n\n\n\n\n\nBroader Impacts:\n\n\nTen Ph.D. students and one M.Sc. student were involved in the project during its duration. Research findings were disseminated in 21 conference and 6 journal publications. Results were further disseminated through invited seminars and keynotes at academic institutions in the US as well as internationally. Our code was made publicly available and shared via github repositories.\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 01/17/2024\n\n\t\t\t\t\tSubmitted by: StratisIoannidis\n"
 }
}