{
 "awd_id": "1910154",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Towards Explainable Recommendation Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 499735.0,
 "awd_amount": 507735.0,
 "awd_min_amd_letter_date": "2019-09-03",
 "awd_max_amd_letter_date": "2021-11-15",
 "awd_abstract_narration": "Recommendation systems are essential components of our daily life. Today, intelligent recommendation systems are used in many Web-based systems. These systems provide personalized information to help human decisions. Leading examples include e-commerce recommendations for everyday shopping, job recommendations for employment markets, and social recommendations to make people better connected. However, most recommendation systems merely suggest recommendations to users. They rarely tell users why such recommendations are provided. This is primarily due to the closed nature algorithms behind the systems that are difficult to explain. The lack of good explainability sacrifices transparency, effectiveness, persuasiveness, and trustworthiness of recommendation systems. This research will allow for personalized recommendations to be provided in more explainable manners, improving search performance and transparency. The research will benefit users in real systems through researchers? industry collaboration with e-commerce and social networks. New algorithms and datasets developed in the project will supplement courses in computer science and iSchool programs. Presentation of the work and demos will help to engage with wider audiences that are interested in computational research. Ultimately, the project will make it easier for humans to understand and trust the machine decisions.\r\n\r\nThis project will explore a new framework for explainable recommendation that involves both system designers and end users. The system designers will benefit from structured explanations that are generated for model diagnostics. The end users will benefit from receiving natural language explanations for various algorithmic decisions. This project will address three fundamental research challenges. First, it will create new machine learning methods for explainable decision making. Second, it will develop new models to generate free-text natural language explanations. Third, it will identify key factors to evaluate the quality of explanations. In the process, the project will also develop aggregated explainability measures and release evaluation benchmarks to support reproducible explainable recommendation research. The project will result in the dissemination of shared data and benchmarks to the Information Retrieval, Data Mining, Recommender System, and broader AI communities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yongfeng",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yongfeng Zhang",
   "pi_email_addr": "yongfeng.zhang@rutgers.edu",
   "nsf_id": "000763753",
   "pi_start_date": "2019-09-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chirag",
   "pi_last_name": "Shah",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chirag Shah",
   "pi_email_addr": "chirags@uw.edu",
   "nsf_id": "000573357",
   "pi_start_date": "2019-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088543925",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499735.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to develop explainable machine learning algorithms and evaluation protocols for recommender systems, so that system designers and users can understand how and why a particular decision is made by the system, which helps to improve the trustworthiness of the system. More broadly, the project aims to highlight the importance of transparency and explainability in modern information systems such as recommender systems and search engines, which helps to build the trustworthiness between human and AI.</p>\n<p>The project has resulted in 28 research papers in related conferences and journals, and the results have been disseminated in various research communities such as Information Retrieval, Recommender Systems, Machine Learning, Data Mining, Natural Language Processing, and the general AI community.&nbsp;</p>\n<p>The research of the project helps intelligent system to generate natural language explanations so that the AI decisions can be intuitively explained to average users who has few or no knowledge about AI or computer science. The causal and counterfactual explanation approach developed in this project helps the community to better define the evaluation protocol for recommendation explanations, because counterfactual explanations are easy for offline evaluation due to their intuitive meanings. The counterfactual explanations also give users better control of the recommender system by telling the users what would happen if they had or had not clicked some recommended items.&nbsp;</p>\n<p>The neural logical reasoning approach to explainable recommendation developed in this project inspires a new paradigm for recommendation or search systems. Recommendation is a highly cognitive intelligence problem since users practice concrete reasoning when deciding what to do. As a result, to predict user behaviors, the model must be able to conduct logical reasoning. The project developed a framework to integrate symbolic reasoning and differentiable learning to demonstrate the idea of how reasoning can be conducted using neural networks, which has long been considered as a challenge for neural networks.&nbsp;</p>\n<p>Furthermore, evaluating explanations has long been a challenge for the community. This project has made contributions to the evaluation of explanations. The project developed the Sufficiency and Necessity based evaluation protocol, and generalized it so that the protocol can be used to evaluate both factual and counterfactual explanations. This makes the evaluation of counterfactual explanation convenient even when the ground-truth explanation is not available. The project also developed a learning to evaluate framework, which is not only applicable to evaluate counterfactual explanations but basically any explanation. These efforts benefit researchers by making it possible to evaluate explanations with standard and easy to use metrics.</p>\n<p>Except for technical contributions, the project also involved extensive efforts on research dissemination. To help the community better evaluate explanations, the project has constructed datasets for explanation evaluation and publicly shared the datasets to the community. The PIs also organized several workshops on SIGIR 2018 through 2020 to disseminate the importance and research efforts of explainable AI in recommender systems to the community. The PIs also delivered tutorials on explainable recommendation and search at WWW, SIGIR, ICTIR conferences to better disseminate the research.</p>\n<p>The project also contributed to the teaching and education efforts. Explainability is a very important perspective of trustworthy and responsible AI. The explainable recommendation datasets and framework developed from the project have been used as the course project of the PI's courses. Explainability has been taught as one of the perspectives in the trustworthy AI lecture in the PI's class. This helps students to gain hands-on experience in developing explainable AI methods and building concrete understandings about how to use the techniques they learn in class in responsible ways.</p>\n<p>Finally, since recommender systems are widely deployed on the web and in many intelligent systems, it more and more influences people's decision-making process. The project and its research outcomes highlight the importance of explaining to users about how and why certain algorithm decisions are made. Through the research, the project helps to trigger discussions about the explainable AI in legal, business and politics perspectives, and benefits the society at large by making AI decisions more transparent for everyday users.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/29/2023<br>\n\t\t\t\t\tModified by: Yongfeng&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aims to develop explainable machine learning algorithms and evaluation protocols for recommender systems, so that system designers and users can understand how and why a particular decision is made by the system, which helps to improve the trustworthiness of the system. More broadly, the project aims to highlight the importance of transparency and explainability in modern information systems such as recommender systems and search engines, which helps to build the trustworthiness between human and AI.\n\nThe project has resulted in 28 research papers in related conferences and journals, and the results have been disseminated in various research communities such as Information Retrieval, Recommender Systems, Machine Learning, Data Mining, Natural Language Processing, and the general AI community. \n\nThe research of the project helps intelligent system to generate natural language explanations so that the AI decisions can be intuitively explained to average users who has few or no knowledge about AI or computer science. The causal and counterfactual explanation approach developed in this project helps the community to better define the evaluation protocol for recommendation explanations, because counterfactual explanations are easy for offline evaluation due to their intuitive meanings. The counterfactual explanations also give users better control of the recommender system by telling the users what would happen if they had or had not clicked some recommended items. \n\nThe neural logical reasoning approach to explainable recommendation developed in this project inspires a new paradigm for recommendation or search systems. Recommendation is a highly cognitive intelligence problem since users practice concrete reasoning when deciding what to do. As a result, to predict user behaviors, the model must be able to conduct logical reasoning. The project developed a framework to integrate symbolic reasoning and differentiable learning to demonstrate the idea of how reasoning can be conducted using neural networks, which has long been considered as a challenge for neural networks. \n\nFurthermore, evaluating explanations has long been a challenge for the community. This project has made contributions to the evaluation of explanations. The project developed the Sufficiency and Necessity based evaluation protocol, and generalized it so that the protocol can be used to evaluate both factual and counterfactual explanations. This makes the evaluation of counterfactual explanation convenient even when the ground-truth explanation is not available. The project also developed a learning to evaluate framework, which is not only applicable to evaluate counterfactual explanations but basically any explanation. These efforts benefit researchers by making it possible to evaluate explanations with standard and easy to use metrics.\n\nExcept for technical contributions, the project also involved extensive efforts on research dissemination. To help the community better evaluate explanations, the project has constructed datasets for explanation evaluation and publicly shared the datasets to the community. The PIs also organized several workshops on SIGIR 2018 through 2020 to disseminate the importance and research efforts of explainable AI in recommender systems to the community. The PIs also delivered tutorials on explainable recommendation and search at WWW, SIGIR, ICTIR conferences to better disseminate the research.\n\nThe project also contributed to the teaching and education efforts. Explainability is a very important perspective of trustworthy and responsible AI. The explainable recommendation datasets and framework developed from the project have been used as the course project of the PI's courses. Explainability has been taught as one of the perspectives in the trustworthy AI lecture in the PI's class. This helps students to gain hands-on experience in developing explainable AI methods and building concrete understandings about how to use the techniques they learn in class in responsible ways.\n\nFinally, since recommender systems are widely deployed on the web and in many intelligent systems, it more and more influences people's decision-making process. The project and its research outcomes highlight the importance of explaining to users about how and why certain algorithm decisions are made. Through the research, the project helps to trigger discussions about the explainable AI in legal, business and politics perspectives, and benefits the society at large by making AI decisions more transparent for everyday users. \n\n\t\t\t\t\tLast Modified: 08/29/2023\n\n\t\t\t\t\tSubmitted by: Yongfeng Zhang"
 }
}