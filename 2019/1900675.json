{
 "awd_id": "1900675",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "FET: Medium: Memory Processing Unit (MPU) - An Efficient, Reconfigurable In-memory Computing Fabric",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-07-15",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 952552.0,
 "awd_amount": 960352.0,
 "awd_min_amd_letter_date": "2019-07-15",
 "awd_max_amd_letter_date": "2021-07-12",
 "awd_abstract_narration": "Artificial Intelligence (AI) is expected to become a disruptive force for both emerging and mature industry sectors. However, current AI progress is mostly driven by software and algorithm advances, while physical AI implementation is limited by hardware systems that were primarily developed to perform conventional computing tasks. Large scale implementation of AI in smart homes, robotics and autonomous vehicles will only become possible with hardware innovations, through new computing architectures and devices that can overcome the limits of today's systems in terms of power efficiency and speed. This project aims at precisely addressing these problems through the development of a new in-memory computing architecture that is naturally suited for AI applications. Undergraduate and graduate students will be trained to become experts at the interface of nanoelectronic devices and computing architecture, and be able to join the workforce of computer engineering and semiconductor research and development. The knowledge developed in this project will also be widely disseminated to the general public through publications, tutorials, course modules, high-school visits and industry partnerships. \r\n\r\nThe proposed project will lead to a new computing architecture that is fundamentally efficient, parallel, modular and reconfigurable. Unlike specialized accelerators designed for specific algorithms, the project aims at developing a general memory-centric hardware platform that can be used for a broad range of computing tasks. The program will be carried out through multidisciplinary research efforts organized around five central thrusts that cover small scale prototype and circuit verification, uniform module development, scalable chip design, algorithm mapping, and system benchmarking and optimization. Key performance parameters will be measured and optimized, while new devices, circuit components, design tools and simulation packages will be developed and shared with the research community and the general public to help broaden the impact of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wei",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wei Lu",
   "pi_email_addr": "wluee@eecs.umich.edu",
   "nsf_id": "000492897",
   "pi_start_date": "2019-07-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zhengya",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhengya Zhang",
   "pi_email_addr": "zhengya@eecs.umich.edu",
   "nsf_id": "000542768",
   "pi_start_date": "2019-07-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "3003 South State St. Room 1062",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "089Y00",
   "pgm_ele_name": "FET-Fndtns of Emerging Tech"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "076Z",
   "pgm_ref_txt": "FET: Foundations of Emerging Technologie"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 352551.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 307802.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 299999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to develop a highly efficient in-memory computing fabric (Memory Processing Unit &ndash; MPU) that can potentially be used for a broad range of data-intensive tasks, where essential computing functions, including memory, digital logic and arithmetic, and neuromorphic operations can be performed using the same underlying devices and circuits. With co-located memory and logic at the smallest grain &ndash; the device level, the proposed system can fundamentally address the von Neumann bottleneck, while offering the flexibility to be optimally reconfigured to perform different computing and data storage tasks in a massively parallel fashion. &nbsp;</p>\n<p>The research was carried out through multidisciplinary efforts organized around five central thrusts: R1: Heterogeneously Integrated Prototype Demonstration which seeks initial demonstration of the proposed architecture by building small-scale prototypes with programmable resistive random access memory (RRAM) arrays directly integrated on CMOS periphery and control circuitry; R2: MPU Functional Module Development which aims to develop the uniform interface that allow the MPU modules to perform data storage, arithmetic computing and neuromorphic computing using the same physical fabric with high efficiency and programmability; R3: Dual-Mode High-Speed Network-on-Chip (NoC) Design to enable circuit-switched NoC to support large-scale integration; R4: Algorithm Mapping Methodology to develop design templates and tools to allow users to readily map different workloads on MPU architecture with optimized data flow; R5. System Benchmarking and Optimization to benchmark the MPU architecture against the state-of-the-art using a broad range of metrics including performance (throughput, efficiency, compute density), programmability and reusability, and design effort, and to achieve further improvement through optimizations from physical parameters and microarchitecture to design templates and tools.</p>\n<p>During the past 5 years, the PIs and their students have made significant progress in the proposed tasks, and obtained the following outcomes:</p>\n<ul>\n<li>In collaboration with industry partners, we have successfully demonstrated RRAM-based MPU modules integrated with CMOS periphery circuitry in a standard 300mm fab. Fully functional chips with integrated MPU modules, processors, mixed-signal circuitry and data links have been designed, fabricated, and tested. These chips were in turn used to successfully perform neural network inference and training tasks. </li>\n<li>We have successfully developed the proposed tiled-based MPU architecture. Through architecture analysis and circuit synthesis, we showed the tiled MPU architecture can map a broad range of neural network models with high energy efficiency and throughput.</li>\n<li>We extended the design beyond RRAM systems, and showed that by leveraging data locality and performing expensive vector-matrix multiplication tasks in memory, very large models such as GPT can be accelerated using a DRAM-based process-in-memory (PIM) architecture, with orders of magnitude improvements in energy efficiency and throughput compared with state-of-the-art. </li>\n<li>We developed new algorithms that can leverage the internal device physics to effectively process spatiotemporal data, which not only shows high accuracy at a small network size, but also offers the ability to process multi-modality inputs and can effectively tolerate input errors.</li>\n<li>We performed device-system co-designs and developed new devices whose internal dynamics can be tailored through material composition control. Networks based on such devices offer richer dynamics and can more effectively process spatiotemporal inputs compared with prior implementations.</li>\n<li>We also expanded the application space of MPU beyond neural networks. In particular, end-to-end encryption, e.g. through homomorphic encryption (HE), is highly desirable but orders of magnitude times slower than plain text operation. We studied the feasibility of using compute-in-memory to accelerate HE operations, and developed techniques to improve throughput and mitigate device variations in these very high precision operations. &nbsp;</li>\n</ul>\n<p>The outcomes have been broadly disseminated to the research community and the general public. The PIs and their students have published 21 papers in prestigious journals and conferences, and given multiple invited talks at major conferences on findings of this project. 6 Ph.D. students (3 of which female) have been systematically trained based on funding from NSF. All 6 students have successfully obtained their Ph.D. degrees and either joined US-based chip design companies or continued research in academia. The findings have also been incorporated in two graduate-level course the PIs developed, and shared with the general public through publications, presentations, and online resources. Collaborations with industry have been formed. Several new research directions have been established, and hardware and software infrastructure has been developed during the project. &nbsp;The findings have also inspired several new students who have since become actively engaged in research in related topics.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/29/2024<br>\nModified by: Wei&nbsp;Lu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aims to develop a highly efficient in-memory computing fabric (Memory Processing Unit  MPU) that can potentially be used for a broad range of data-intensive tasks, where essential computing functions, including memory, digital logic and arithmetic, and neuromorphic operations can be performed using the same underlying devices and circuits. With co-located memory and logic at the smallest grain  the device level, the proposed system can fundamentally address the von Neumann bottleneck, while offering the flexibility to be optimally reconfigured to perform different computing and data storage tasks in a massively parallel fashion. \n\n\nThe research was carried out through multidisciplinary efforts organized around five central thrusts: R1: Heterogeneously Integrated Prototype Demonstration which seeks initial demonstration of the proposed architecture by building small-scale prototypes with programmable resistive random access memory (RRAM) arrays directly integrated on CMOS periphery and control circuitry; R2: MPU Functional Module Development which aims to develop the uniform interface that allow the MPU modules to perform data storage, arithmetic computing and neuromorphic computing using the same physical fabric with high efficiency and programmability; R3: Dual-Mode High-Speed Network-on-Chip (NoC) Design to enable circuit-switched NoC to support large-scale integration; R4: Algorithm Mapping Methodology to develop design templates and tools to allow users to readily map different workloads on MPU architecture with optimized data flow; R5. System Benchmarking and Optimization to benchmark the MPU architecture against the state-of-the-art using a broad range of metrics including performance (throughput, efficiency, compute density), programmability and reusability, and design effort, and to achieve further improvement through optimizations from physical parameters and microarchitecture to design templates and tools.\n\n\nDuring the past 5 years, the PIs and their students have made significant progress in the proposed tasks, and obtained the following outcomes:\n\nIn collaboration with industry partners, we have successfully demonstrated RRAM-based MPU modules integrated with CMOS periphery circuitry in a standard 300mm fab. Fully functional chips with integrated MPU modules, processors, mixed-signal circuitry and data links have been designed, fabricated, and tested. These chips were in turn used to successfully perform neural network inference and training tasks. \nWe have successfully developed the proposed tiled-based MPU architecture. Through architecture analysis and circuit synthesis, we showed the tiled MPU architecture can map a broad range of neural network models with high energy efficiency and throughput.\nWe extended the design beyond RRAM systems, and showed that by leveraging data locality and performing expensive vector-matrix multiplication tasks in memory, very large models such as GPT can be accelerated using a DRAM-based process-in-memory (PIM) architecture, with orders of magnitude improvements in energy efficiency and throughput compared with state-of-the-art. \nWe developed new algorithms that can leverage the internal device physics to effectively process spatiotemporal data, which not only shows high accuracy at a small network size, but also offers the ability to process multi-modality inputs and can effectively tolerate input errors.\nWe performed device-system co-designs and developed new devices whose internal dynamics can be tailored through material composition control. Networks based on such devices offer richer dynamics and can more effectively process spatiotemporal inputs compared with prior implementations.\nWe also expanded the application space of MPU beyond neural networks. In particular, end-to-end encryption, e.g. through homomorphic encryption (HE), is highly desirable but orders of magnitude times slower than plain text operation. We studied the feasibility of using compute-in-memory to accelerate HE operations, and developed techniques to improve throughput and mitigate device variations in these very high precision operations. \n\n\n\nThe outcomes have been broadly disseminated to the research community and the general public. The PIs and their students have published 21 papers in prestigious journals and conferences, and given multiple invited talks at major conferences on findings of this project. 6 Ph.D. students (3 of which female) have been systematically trained based on funding from NSF. All 6 students have successfully obtained their Ph.D. degrees and either joined US-based chip design companies or continued research in academia. The findings have also been incorporated in two graduate-level course the PIs developed, and shared with the general public through publications, presentations, and online resources. Collaborations with industry have been formed. Several new research directions have been established, and hardware and software infrastructure has been developed during the project. The findings have also inspired several new students who have since become actively engaged in research in related topics.\n\n\n\t\t\t\t\tLast Modified: 07/29/2024\n\n\t\t\t\t\tSubmitted by: WeiLu\n"
 }
}