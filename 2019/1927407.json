{
 "awd_id": "1927407",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:AI-DCL: Understanding the Relationship between Algorithmic Transparency and Filter Bubbles in Online Media",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 299871.0,
 "awd_amount": 299871.0,
 "awd_min_amd_letter_date": "2019-08-15",
 "awd_max_amd_letter_date": "2021-02-04",
 "awd_abstract_narration": "Computer algorithms are widely used by online sites to determine the content users see, for example, by curating news articles or recommending social media posts. These algorithms are primarily designed to improve user experience by showing to users the content that they are likely to be interested in. However, there is growing evidence that these algorithms may have unintended side effects. For example, by showing users only content that conforms with their preexisting perceptions and beliefs, users may receive a biased subset of all content, possibly increasing intellectual isolation, a phenomenon known as a \"filter bubble.\" This project promotes the progress of computational science by investigating how and why filter bubbles form and developing new algorithms to prevent them. Additionally, this award supports the cross-disciplinary training of two PhD students at Illinois Tech, jointly advised by computer science and political science faculty, and will result in new curricula for courses in online social network analysis, algorithmic transparency, and public policy.\r\n\r\nThe technical approach of the project focuses on two enhancements to content recommendation algorithms: 1) improving transparency by informing the users of their reading habits, what the recommendation model thinks of them, and why particular items are recommended; and 2) supporting rich user interactions by enabling the user to provide feedback on model predictions and explanations. New algorithms are developed to support transparency and interaction for modern, neural network-based recommendation systems, scalable to high-dimensional text domains. The project conducts extensive user studies to measure the impact that transparency and interactions have on the formation and severity of filter bubbles. A key aspect of this project is the development of an open-source platform and accompanying datasets that will foster additional research to better understand and ultimately mitigate filter bubbles. This platform includes tools not only for identifying user preferences and making content recommendations, but also for conducting user studies to measure how changes to the recommendation system affect filter bubble formation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mustafa",
   "pi_last_name": "Bilgic",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mustafa Bilgic",
   "pi_email_addr": "mbilgic@iit.edu",
   "nsf_id": "000578383",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Aron",
   "pi_last_name": "Culotta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aron Culotta",
   "pi_email_addr": "aculotta@tulane.edu",
   "nsf_id": "000536164",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Shapiro",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew A Shapiro",
   "pi_email_addr": "mshapir2@iit.edu",
   "nsf_id": "000584567",
   "pi_start_date": "2019-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Illinois Institute of Technology",
  "inst_street_address": "10 W 35TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125673035",
  "inst_zip_code": "606163717",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "ILLINOIS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E2NDENMDUEG8"
 },
 "perf_inst": {
  "perf_inst_name": "Illinois Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606163717",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 299871.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;Algorithmic personalization of news and social media content aims to improve user experience; however, this filtering can have the unintended side effect of creating homogeneous \"filter bubbles,\" where users are over-exposed to ideas that conform with their preexisting beliefs. In this project, we collect a large news dataset, run large-scale simulations to study the formation and evolution of filter bubbles, conduct a user study investigating the effects of transparency and interaction interfaces on filter bubbles, and develop an algorithmic approach to prevent cross-topic political homogenization. Specific outcomes of this project are as follows:</p>\n<p><span id=\"docs-internal-guid-4c2c7964-7fff-d66d-19f1-0b810dc4946e\"> </span></p>\n<p><strong>Dataset</strong>: We identified 41 news sources from www.allsides.com, annotated with political stances ranging from very liberal to very conservative. We collected over 900K news articles from these sources. We annotated these articles (through a combination of manual and automated annotation) with one or more of 14 political topics (e.g., abortion, environment, etc.). This dataset serves as the basis for the outcomes below.</p>\n<p style=\"display: inline !important;\" dir=\"ltr\"><strong>Simulations</strong>: We conducted extensive simulations studying the formation and evolution of filter bubbles. We implemented four recommender systems, including a random news recommender, a content-based recommender, a collaborative filtering recommender, and an oracle recommender. Rather than treating users as simply \"liberals\" or \"conservatives\", we drew on Pew surveys of political typology and simulated nine classes of users (e.g., solid liberals, disaffected Democrats, country first conservatives) with differing partisan preferences across 14 news topics. We observed that</p>\n<ul>\n<li>Users with more extreme preferences were shown less diverse content but had higher click-through rates than others.</li>\n<li>Filter bubbles created by content-based recommenders and collaborative filtering were markedly different. Content-based recommendations were susceptible to biases based on partisan language, leading to over-recommendation of linguistically polarized topics. Collaborative filtering recommenders, on the other hand, were susceptible to majority opinion, recommending the most popular topics regardless of a user's preferences.</li>\n<li>When users had divergent views across topics, recommenders tended to have a homogenization effect. For example, if a user was conservative on most issues but liberal on health care, they were shown more conservative health care articles than desired.</li>\n</ul>\n<p style=\"display: inline !important;\" dir=\"ltr\"><strong>User studies</strong>: We designed a user study, investigating what effect transparency and interaction interfaces have on filter bubbles. Based on the top recommendations, we developed a novel interface that presented the average political stance by topic and allowed the user to adjust the political stance of the recommendations by topic. We recruited 102 users through Amazon Turk who were randomly assigned to one of two groups: one with access to transparency and interaction interface (treatment) and the other without access (control). Both groups read and upvoted/downvoted a total of 30 articles. By analyzing over 3,000 user interactions, we made the following observations:</p>\n<ul>\n<li>Extremeness: Among users who initially view extreme, partisan articles, those in the treatment group were more likely to steer the system to less extreme articles. In contrast, among users who initially view less extreme articles, those in the treatment group were somewhat more likely to steer the system to extreme articles.</li>\n<li>Diversity: For both treatment and control groups, users steered the system toward less politically diverse news.&nbsp;</li>\n<li>User satisfaction: For both treatment and control groups, the rate of up-votes increased over time, particularly for users who initially had low up-vote ratios.</li>\n<li>User awareness: The transparency of the enhanced interface (1) raised user awareness of the lack of diversity in their recommended articles and (2) increased perceived understanding of the inner-workings of news recommendation systems.</li>\n</ul>\n<p><strong>Reducing cross-topic political homogenization</strong>: We observed that content-based news recommenders could result in a type of bias we term \"cross-topic political homogenization.\" This occurs when users have diverse political preferences by topic - e.g., users that prefer conservative articles on one topic but liberal articles on another. Because content-based recommenders learn text features that correlate with engagement, we find that they can have a homogenizing effect by recommending articles with the same political lean on both topics. We developed attention-based neural network models to reduce this homogenization by increasing attention on words that are topic-specific while decreasing attention on polarized, topic-general terms. We find that the proposed approach results in more accurate recommendations for simulated users with such diverse preferences. In our experiments using 45 topic pairs drawn from 900k news articles, we find that the proposed approach improves accuracy ~5% on the minority topic while still maintaining accuracy comparable to the baseline on the majority topic. These results provide evidence that recommendation systems can be designed to mitigate cross-topic homogenization.</p>\n<p><strong>Mentoring</strong>: Two PhD students were mentored through this project, both completing their PhD theses.</p>\n<ul>\n<li>Ping Liu. Understanding and combating filter bubbles in news recommender systems. (2022). Illinois Institute of Technology.&nbsp;</li>\n<li>Karthik Shivaram. Computational Models of User Engagement with Online News. (2023). Tulane University.&nbsp;</li>\n</ul><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/14/2023<br>\n\t\t\t\t\tModified by: Mustafa&nbsp;Bilgic</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n Algorithmic personalization of news and social media content aims to improve user experience; however, this filtering can have the unintended side effect of creating homogeneous \"filter bubbles,\" where users are over-exposed to ideas that conform with their preexisting beliefs. In this project, we collect a large news dataset, run large-scale simulations to study the formation and evolution of filter bubbles, conduct a user study investigating the effects of transparency and interaction interfaces on filter bubbles, and develop an algorithmic approach to prevent cross-topic political homogenization. Specific outcomes of this project are as follows:\n\n \n\nDataset: We identified 41 news sources from www.allsides.com, annotated with political stances ranging from very liberal to very conservative. We collected over 900K news articles from these sources. We annotated these articles (through a combination of manual and automated annotation) with one or more of 14 political topics (e.g., abortion, environment, etc.). This dataset serves as the basis for the outcomes below.\nSimulations: We conducted extensive simulations studying the formation and evolution of filter bubbles. We implemented four recommender systems, including a random news recommender, a content-based recommender, a collaborative filtering recommender, and an oracle recommender. Rather than treating users as simply \"liberals\" or \"conservatives\", we drew on Pew surveys of political typology and simulated nine classes of users (e.g., solid liberals, disaffected Democrats, country first conservatives) with differing partisan preferences across 14 news topics. We observed that\n\nUsers with more extreme preferences were shown less diverse content but had higher click-through rates than others.\nFilter bubbles created by content-based recommenders and collaborative filtering were markedly different. Content-based recommendations were susceptible to biases based on partisan language, leading to over-recommendation of linguistically polarized topics. Collaborative filtering recommenders, on the other hand, were susceptible to majority opinion, recommending the most popular topics regardless of a user's preferences.\nWhen users had divergent views across topics, recommenders tended to have a homogenization effect. For example, if a user was conservative on most issues but liberal on health care, they were shown more conservative health care articles than desired.\n\nUser studies: We designed a user study, investigating what effect transparency and interaction interfaces have on filter bubbles. Based on the top recommendations, we developed a novel interface that presented the average political stance by topic and allowed the user to adjust the political stance of the recommendations by topic. We recruited 102 users through Amazon Turk who were randomly assigned to one of two groups: one with access to transparency and interaction interface (treatment) and the other without access (control). Both groups read and upvoted/downvoted a total of 30 articles. By analyzing over 3,000 user interactions, we made the following observations:\n\nExtremeness: Among users who initially view extreme, partisan articles, those in the treatment group were more likely to steer the system to less extreme articles. In contrast, among users who initially view less extreme articles, those in the treatment group were somewhat more likely to steer the system to extreme articles.\nDiversity: For both treatment and control groups, users steered the system toward less politically diverse news. \nUser satisfaction: For both treatment and control groups, the rate of up-votes increased over time, particularly for users who initially had low up-vote ratios.\nUser awareness: The transparency of the enhanced interface (1) raised user awareness of the lack of diversity in their recommended articles and (2) increased perceived understanding of the inner-workings of news recommendation systems.\n\n\nReducing cross-topic political homogenization: We observed that content-based news recommenders could result in a type of bias we term \"cross-topic political homogenization.\" This occurs when users have diverse political preferences by topic - e.g., users that prefer conservative articles on one topic but liberal articles on another. Because content-based recommenders learn text features that correlate with engagement, we find that they can have a homogenizing effect by recommending articles with the same political lean on both topics. We developed attention-based neural network models to reduce this homogenization by increasing attention on words that are topic-specific while decreasing attention on polarized, topic-general terms. We find that the proposed approach results in more accurate recommendations for simulated users with such diverse preferences. In our experiments using 45 topic pairs drawn from 900k news articles, we find that the proposed approach improves accuracy ~5% on the minority topic while still maintaining accuracy comparable to the baseline on the majority topic. These results provide evidence that recommendation systems can be designed to mitigate cross-topic homogenization.\n\nMentoring: Two PhD students were mentored through this project, both completing their PhD theses.\n\nPing Liu. Understanding and combating filter bubbles in news recommender systems. (2022). Illinois Institute of Technology. \nKarthik Shivaram. Computational Models of User Engagement with Online News. (2023). Tulane University. \n\n\n\t\t\t\t\tLast Modified: 06/14/2023\n\n\t\t\t\t\tSubmitted by: Mustafa Bilgic"
 }
}