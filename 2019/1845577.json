{
 "awd_id": "1845577",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: QoS-aware Systems for Accelerated Datacenters",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2019-08-03",
 "awd_max_amd_letter_date": "2023-08-03",
 "awd_abstract_narration": "This project addresses the challenge of providing sufficient and efficient infrastructure to meet the increasing computational demands of Artificial Intelligence (AI) and Machine Learning (ML) applications. To support the ever-growing prevalence of ML and AI applications, data centers are incorporating specialized accelerator hardware for computation as recent work has shown that traditional CPU-based infrastructures are up to 100x less efficient than these accelerator-based designs. However, while the community has largely focused on hardware designs, currently there is very little research attention on designing system software to manage the performance and efficiency for hardware accelerated data centers. Current data center system software is mostly tailored to CPU platforms. There are significant and fundamental differences between CPUs and accelerators that impact the effectiveness of design decisions in the system software stack. This work rethinks and redesigns the system software stack for the emerging landscape of hardware acceleration in data centers.\r\n\t\r\nThe goal of this work is to redesign datacenter systems to support acceleration at scale to meet the future computational demand. The project designs system software to schedule and allocate resources among heterogeneous platforms composed of both general purpose processors and various accelerators, managing quality of service (QoS) and achieving high efficiency. To this end, the project focuses on three pillars of innovation. First, the project designs the cluster-level scheduler for heterogeneous accelerated infrastructures to precisely predict the performance interference and maximizes the utilization without violating QoS based on such prediction. Second, the project designs an application-level acceleration manager that accurately identifies the bottleneck service, estimates potential improvement using different acceleration strategies and dynamically (re)allocates the accelerator resources across service stages, resulting in significantly improved latency. Third, the project designs a node-level resource manager for accelerated platforms that manages unique sharing and reconfiguration behaviors on accelerators to ensure QoS and high throughput.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lingjia",
   "pi_last_name": "Tang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lingjia Tang",
   "pi_email_addr": "lingjia@umich.edu",
   "nsf_id": "000628990",
   "pi_start_date": "2019-08-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, 4609 Beyster Bldg.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 99541.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 102890.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 96064.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 99155.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 102350.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-268d6d0d-7fff-0b16-ca47-62939de20239\"> </span></p>\r\n<p dir=\"ltr\"><strong>Intellectual merit&nbsp;</strong></p>\r\n<p dir=\"ltr\"><span>This project aimed to address critical challenges in cross-stack system software design for accelerating AI computations in data centers and edge environments. The goals included enabling heterogeneous hardware platforms to work collaboratively, improving resource allocation, and achieving high efficiency and performance predictability in large-scale intelligent applications. Over the award&rsquo;s duration, we have achieved significant progress, advancing the state-of-the-art by both intelligent contributions as well as open-sourced systems and tooling for the industry and broader community.</span></p>\r\n<ol>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Design, develop and open source system to allocate resources in datacenters and between cloud and edge environments to improve AI workloads&rsquo; latency, throughput and energy efficiency.&nbsp;</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Design benchmarks and tools to facilitate research in emerging data center workloads and evaluate AI quality vs. cost/performance.&nbsp;</span></p>\r\n</li>\r\n</ol>\r\n<p dir=\"ltr\"><span>More specifically, here are the main outcomes of the proposal:</span></p>\r\n<p dir=\"ltr\"><span>1. System Design and Implementation:</span></p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Designed and developed a novel runtime system, </span><span>Jaseci</span><span>, to reduce the complexity of developing, optimizing, configuring, and deploying scale-out applications. Jaseci raises the level of abstraction by moving as much of the scale-out data management (database, logging etc), microservice componentization, AI model deployment and live update complexity into the runtime stack to be automated and optimized automatically. We use real-world AI applications to demonstrate Jaseci&rsquo;s benefit for application performance and developer productivity.</span></p>\r\n</li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Designed and developed the </span><span>Swarm Neurosplicing (SN)</span><span> runtime system architecture to enable collaborative AI inference on edge devices and cloud platforms. This system dynamically partitions neural network workloads across heterogeneous devices, optimizing inference latency and energy efficiency.</span></p>\r\n</li>\r\n</ul>\r\n<p>&nbsp;</p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Designed an end-to-end pipeline for multi-camera object detection, integrating GPUs, FPGAs, and TPUs to optimize power consumption, accuracy, and throughput.</span></p>\r\n</li>\r\n</ul>\r\n<p dir=\"ltr\"><span>2. Benchmarks and Evaluation Tools:</span></p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>D</span><span>esigned a novel cloud graphics rendering research infrastructure for VR and interactive 3D applications, </span><span>Pictor</span><span>. Pictor employs AI to mimic human interactions with complex 3D applications. It can also provide in-depth performance measurements for the complex software and hardware stack used for cloud 3D-graphics rendering. Performance analyses were conducted with these benchmarks to characterize 3D applications in the cloud and reveal new performance bottlenecks.&nbsp;</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Created the </span><span>SLaM Tool</span><span>, an open-source platform for evaluating Small Language Models (SLMs). This tool provides automated metrics for response quality, latency, and cost, facilitating large-scale experimentation and deployment.</span></p>\r\n</li>\r\n</ul>\r\n<p dir=\"ltr\"><span>3. Performance Improvements:</span></p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Jaseci</span><span> runtime resource allocator (JSOrg) achieves up to </span><span>2.64&times; latency and 2.59&times; throughput i</span><span>mprovement compared to the SOTA baseline, where developers statically config all remote microservices without dynamic reconfiguration capabilities.</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Evaluated SLMs against proprietary Large Language Models (LLMs), finding that SLMs deliver comparable response quality to OpenAI&rsquo;s GPT-4 with significantly lower costs </span><span>(5&times; to 29&times; cost reductions)</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Demonstrated up to </span><span>13.9&times; speedup in inference latency</span><span> and </span><span>4.8&times; improvement in battery life</span><span> for edge devices using </span><span>Swarm Neurosplicing.</span></p>\r\n</li>\r\n</ul>\r\n<p dir=\"ltr\">&nbsp;</p>\r\n<p dir=\"ltr\"><strong>Broader Impacts</strong></p>\r\n<p dir=\"ltr\"><span>1. Advancing System/AI Research:</span><span> This project has demonstrated the feasibility of intelligent resource allocation enabled by Jaseci to achieve better data center performance. In addition we present techniques to use collaborative intelligence to support sophisticated AI applications on edge devices. These systems have opened new avenues for AI accessibility and innovation.</span></p>\r\n<p dir=\"ltr\"><span>2. Cost Efficiency and Sustainability:</span><span> By reducing reliance on proprietary APIs and achieving substantial cost savings through self-hosted models, this work demonstrates the sustainable approaches for deploying AI models and provides necessary tooling for model section.</span></p>\r\n<p dir=\"ltr\"><span>3. Community Contributions:</span><span> The open-source release of tools like Jaseci and SLaM has empowered researchers and practitioners to evaluate and deploy AI models more effectively. Our systems have nabled real-world applications in industry, such as Myca.ai, showcasing the practical viability of self-hosted SLMs. Jaseci is also deployed in multiple companies including Ally and startup companies.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><strong>Publications:&nbsp;</strong></p>\r\n<p dir=\"ltr\"><span>1. &ldquo;Scaling Down to Scale Up: A Cost- Benefit Analysis of Replacing OpenAI&rsquo;s GPT-4 with Self-Hosted Open Source SLMs in Production&rdquo;. The IEEE International Symposium on Performance Analysis of Systems and Software, 2024</span></p>\r\n<p dir=\"ltr\"><span>2.&ldquo;The Jaseci Programming Paradigm and Runtime Stack: Building Scale-Out Production Applications Easy and Fast&rdquo;. The IEEE Computer Architecture Letters ( Volume: 22, Issue: 2, July-Dec. 2023)</span></p>\r\n<p dir=\"ltr\"><span>3.&nbsp; &ldquo;A Benchmarking Framework for Interactive 3-D Applications in the Cloud.&rdquo; In Proceedings of the 53rd IEEE/ACM International Symposium on Microarchitecture (MICRO), 2020</span></p>\r\n<div><span><br /></span></div>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/27/2025<br>\nModified by: Lingjia&nbsp;Tang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nIntellectual merit\r\n\n\nThis project aimed to address critical challenges in cross-stack system software design for accelerating AI computations in data centers and edge environments. The goals included enabling heterogeneous hardware platforms to work collaboratively, improving resource allocation, and achieving high efficiency and performance predictability in large-scale intelligent applications. Over the awards duration, we have achieved significant progress, advancing the state-of-the-art by both intelligent contributions as well as open-sourced systems and tooling for the industry and broader community.\r\n\r\n\r\n\n\nDesign, develop and open source system to allocate resources in datacenters and between cloud and edge environments to improve AI workloads latency, throughput and energy efficiency.\r\n\r\n\r\n\n\nDesign benchmarks and tools to facilitate research in emerging data center workloads and evaluate AI quality vs. cost/performance.\r\n\r\n\r\n\n\nMore specifically, here are the main outcomes of the proposal:\r\n\n\n1. System Design and Implementation:\r\n\r\n\r\n\n\nDesigned and developed a novel runtime system, Jaseci, to reduce the complexity of developing, optimizing, configuring, and deploying scale-out applications. Jaseci raises the level of abstraction by moving as much of the scale-out data management (database, logging etc), microservice componentization, AI model deployment and live update complexity into the runtime stack to be automated and optimized automatically. We use real-world AI applications to demonstrate Jasecis benefit for application performance and developer productivity.\r\n\r\n\r\n\n\n\r\n\r\n\r\n\n\nDesigned and developed the Swarm Neurosplicing (SN) runtime system architecture to enable collaborative AI inference on edge devices and cloud platforms. This system dynamically partitions neural network workloads across heterogeneous devices, optimizing inference latency and energy efficiency.\r\n\r\n\r\n\n\n\r\n\r\n\r\n\n\nDesigned an end-to-end pipeline for multi-camera object detection, integrating GPUs, FPGAs, and TPUs to optimize power consumption, accuracy, and throughput.\r\n\r\n\r\n\n\n2. Benchmarks and Evaluation Tools:\r\n\r\n\r\n\n\nDesigned a novel cloud graphics rendering research infrastructure for VR and interactive 3D applications, Pictor. Pictor employs AI to mimic human interactions with complex 3D applications. It can also provide in-depth performance measurements for the complex software and hardware stack used for cloud 3D-graphics rendering. Performance analyses were conducted with these benchmarks to characterize 3D applications in the cloud and reveal new performance bottlenecks.\r\n\r\n\r\n\n\nCreated the SLaM Tool, an open-source platform for evaluating Small Language Models (SLMs). This tool provides automated metrics for response quality, latency, and cost, facilitating large-scale experimentation and deployment.\r\n\r\n\r\n\n\n3. Performance Improvements:\r\n\r\n\r\n\n\nJaseci runtime resource allocator (JSOrg) achieves up to 2.64 latency and 2.59 throughput improvement compared to the SOTA baseline, where developers statically config all remote microservices without dynamic reconfiguration capabilities.\r\n\r\n\r\n\n\nEvaluated SLMs against proprietary Large Language Models (LLMs), finding that SLMs deliver comparable response quality to OpenAIs GPT-4 with significantly lower costs (5 to 29 cost reductions)\r\n\r\n\r\n\n\nDemonstrated up to 13.9 speedup in inference latency and 4.8 improvement in battery life for edge devices using Swarm Neurosplicing.\r\n\r\n\r\n\n\n\r\n\n\nBroader Impacts\r\n\n\n1. Advancing System/AI Research: This project has demonstrated the feasibility of intelligent resource allocation enabled by Jaseci to achieve better data center performance. In addition we present techniques to use collaborative intelligence to support sophisticated AI applications on edge devices. These systems have opened new avenues for AI accessibility and innovation.\r\n\n\n2. Cost Efficiency and Sustainability: By reducing reliance on proprietary APIs and achieving substantial cost savings through self-hosted models, this work demonstrates the sustainable approaches for deploying AI models and provides necessary tooling for model section.\r\n\n\n3. Community Contributions: The open-source release of tools like Jaseci and SLaM has empowered researchers and practitioners to evaluate and deploy AI models more effectively. Our systems have nabled real-world applications in industry, such as Myca.ai, showcasing the practical viability of self-hosted SLMs. Jaseci is also deployed in multiple companies including Ally and startup companies.\r\n\n\n\r\n\n\nPublications:\r\n\n\n1. Scaling Down to Scale Up: A Cost- Benefit Analysis of Replacing OpenAIs GPT-4 with Self-Hosted Open Source SLMs in Production. The IEEE International Symposium on Performance Analysis of Systems and Software, 2024\r\n\n\n2.The Jaseci Programming Paradigm and Runtime Stack: Building Scale-Out Production Applications Easy and Fast. The IEEE Computer Architecture Letters ( Volume: 22, Issue: 2, July-Dec. 2023)\r\n\n\n3. A Benchmarking Framework for Interactive 3-D Applications in the Cloud. In Proceedings of the 53rd IEEE/ACM International Symposium on Microarchitecture (MICRO), 2020\r\n\n\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 01/27/2025\n\n\t\t\t\t\tSubmitted by: LingjiaTang\n"
 }
}