{
 "awd_id": "1925960",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Towards Fast and Scalable Algorithms for Big Proteogenomics Data Analytics",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922625",
 "po_email": "jjli@nsf.gov",
 "po_sign_block_name": "Juan Li",
 "awd_eff_date": "2018-09-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 415950.0,
 "awd_amount": 415950.0,
 "awd_min_amd_letter_date": "2019-03-28",
 "awd_max_amd_letter_date": "2019-03-28",
 "awd_abstract_narration": "Proteogenomics studies require combination and integration of mass spectrometry data (MS) for proteomics and next generation sequencing (NGS) data for genomics.  This integration drastically increases the size of the data sets that need to be analyzed to make biological conclusions.  However, existing tools yield low accuracy and exhibit poor scalability for big proteogenomics data.  This CAREER grant is expected to lay a foundation for fast algorithmic and high performance computing solutions suitable for analyzing big proteogenomics data sets.  Design of accurate computational algorithms suitable for peta-scale data sets will be pursued and the software implementation will run on massively parallel supercomputers and graphical processing units.  The direction in this CAREER proposal is towards designing and building infrastructure, which would be useful for the broadest biological and ecological community.  A comprehensive interdisciplinary education will be executed for K12, undergraduate and graduate students to ensure that US retains its global leadership position in STEM fields.  This project thus serves the national interest, as stated by NSF's mission: to promote the progress of science and to advance the national health, prosperity and welfare.\r\n\r\nThe goal of the proposed CAREER grant is to design and develop algorithmic and high performance computing (HPC) foundations for practical sublinear and parallel algorithms for big proteogenomics data - especially for non-model organisms with previously unsequenced or partially sequenced genomes.  Integration of MS and NGS data sets required for proteogenomics studies exhibit enormous volume and velocity of data: NGS technologies such as Chip-Seq can generate tera-bytes of DNA/RNA data and mass spectrometers can generate millions of spectra (with thousand of peak per spectra).  The current systems for analyzing MS data are mainly driven by heuristic practices and do not scale well.  This CAREER proposal will explore a new class of reductive algorithms for analysis of MS data that can allow peptide deductions in sublinear time, compression algorithms that operate in sub-linear space, and denovo algorithms that operate on lossy reduced-form of the MS data.  Novel low-complexity sampling and reductive algorithms that can exploit the sparsity of MS data such as non-uniform FFT based convolution kernels can lead to superior similarity metrics not prone to spurious correlations.  The bottleneck in large system-biology studies is the low-scalability of coarse-grained parallel algorithms that do not exploit MS-specific data characteristics and lead to unbalanced loads due to non-uniform compute time required for peptide deductions.  This project aims to explore design and implementation of scalable algorithms for both NGS and MS data on multicore and GPU platforms using domain decomposition techniques based on spectral clustering, MS-specific hybrid load-balancing based on work-load estimate, and HPC dimensionality reduction strategies and novel out-of-core sketching & streaming fine-grained parallel algorithms.  These HPC solutions can enable previously impractical proteogenomics projects and allow biologists to perform computational experiments without needing expensive hardware.  All of the implemented algorithms will be made available as open-source code interfaced with Galaxy framework to ensure maximum impact in systems biology labs.  These designed techniques will then be integrated so that matching of spectra to RNA-Seq data can be accomplished without a reconstructed transcriptome.  The proposed tools aim to reveal new biological insight such as novel genes, proteins and PTM's and are crucial steps towards understanding the genomic, proteomic and evolutionary aspects of species in the tree of life.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fahad",
   "pi_last_name": "Saeed",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fahad Saeed",
   "pi_email_addr": "FSAEED@FIU.EDU",
   "nsf_id": "000602101",
   "pi_start_date": "2019-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Florida International University",
  "inst_street_address": "11200 SW 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "MIAMI",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3053482494",
  "inst_zip_code": "331992516",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "FL26",
  "org_lgl_bus_name": "FLORIDA INTERNATIONAL UNIVERSITY",
  "org_prnt_uei_num": "Q3KCVK5S9CP1",
  "org_uei_num": "Q3KCVK5S9CP1"
 },
 "perf_inst": {
  "perf_inst_name": "Florida International University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "331990001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "FL26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "793100",
   "pgm_ele_name": "Computational Biology"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7931",
   "pgm_ref_txt": "COMPUTATIONAL BIOLOGY"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 415948.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the proposed CAREER grant is to design and develop algorithmic and high performance computing (HPC) foundations for practical sublinear and parallel algorithms for big proteogenomics data - especially for non-model organisms with previously unsequenced or partially sequenced genomes. This CAREER grants enabled us to explore a new class of reductive algorithms for analysis of MS data that can allow peptide deductions in sublinear time, compression algorithms that operate in sub-linear space, and HPC algorithms that can operate on lossy reduced-form of the MS data.</p>\n<p>Following is the summary of our activities:</p>\n<p>1)&nbsp;&nbsp;&nbsp;&nbsp; We investigated and established that current proteogenomic tools are inadequate in scalability with increase size of the database, as well as with increasing number of species (Tariq, and Saeed, <em>IEEE Access</em> 2021).</p>\n<p>2)&nbsp;&nbsp;&nbsp;&nbsp; In order to design the building blocks needed for developing scalable HPC methods we designed three specific things: MaSS-Simulator (Awan &amp; Saeed, <em>PROTEOMICS</em> 2018), and Benchmarking data sets for proteomics (Awan &amp; Saeed 2021). Both these tools allowed us to generate, calibrate, and control the parameters that could be used for data simulation, and experimentations.</p>\n<p><em>3)&nbsp;&nbsp;&nbsp;&nbsp; </em>We then started building the computational blocks necessary for scalable computing. This include template-based strategy that can be used on CPU-GPU architectures (Awan &amp; Saeed, <em>Computers in Biology and Medicine, </em>2018), BFS algorithms that could run on CPU-GPU architectures (Artiles &amp; Saeed, IEEE IPDPS 2021), and algorithms that can run FFT-like computations on the CPU-GPU architectures (Artiles &amp; Saeed, IEEE BigData 2019). These building blocks enabled us to compute large-scale proteogenomics data analysis on a variety of homogenous and heterogeneous architectures. &nbsp;<em>&nbsp;</em></p>\n<p><em>4)&nbsp;&nbsp;&nbsp;&nbsp; </em>Two other strategies that we designed and developed were related to load-balancing of large-scale databases. To this end, we developed a compression method that would allow us to compress-and-compute the MS data without any decompression (Haseeb &amp; Saeed, IEEE BIBM 2019), and LBE algorithm (Haseeb &amp; Saeed, IEEE IPDPS 2019) which enable effective load-balancing based on the number of computations per unit database. We demonstrated that using both of these methods would enable massive reduction in the I/O for memory-distributed architectures and will result in load that is fairly balanced. <em></em></p>\n<p>5)&nbsp;&nbsp;&nbsp;&nbsp; We also developed a theoretical framework and verified with experimental results that for modern memory-distributed architectures, the amount of communication and I/O is a lot more than the computation costs, no matter what the scoring mechanism is used. This led to the development of a high-performance computing framework that used minimal communications costs to compute over large proteogenomics data sets in a reasonable time. We also demonstrated that our proposed method can give 10x speedups as compared to existing and established parallel computing methods.</p>\n<p>The outcome of the project was design and development of novel communication-avoidance parallel algorithms that were introduced and was a new paradigm for MS based omics. They allowed us to scale proteogenomic data analysis from MS based experiments. We demonstrated that using theoretical results and published results from existing HPC algorithms related to MS based omics data that the communication cost was the biggest cost when processing MS omics data sets. This has been a neglected factor in ALL existing HPC tools for MS based omics data analysis. This theoretical and empirical result opened up a new direction for research and excited the parallel computing community to work on these high impact problems. Using our HPC framework, we were able to show that the proposed techniques enabled 10x speedups as compared to any existing parallel computing framework. The speedups are more instrumental, when the data is large (i.e. for proteogenomics) since a serial or a sub-optimal parallel computing design results in significant paging of the memory. Therefore, the end result is that as compared to existing parallel computing technique which may need weeks of computation, we are able to process terabytes of data within hours on a memory-distributed supercomputer. We expect that our frameworks will be used for investigation of biological, and chemical samples from complex environmental and micrbiomes leading to advances in laboratory, and clinical treatment and drug discovery.&nbsp;</p>\n<p>Dr. Saeed's group has made fundamental advances in dealing with these large data sets for processing on homogenous and heterogeneous supercomputing architectures. One of the most interesting aspects of the research was compression of these large omics data sets and processing on these data sets without the need to decompress them.</p>\n<p>The NSF CAREER award has partially supported more than 6 PhD students, 2 MS and 4 undergraduate students, numerous research talks, and has resulted in more than 34 peer-reviewed publications. The software resulting from these novel high-performance computing is available on the PI lab webpage at: https://saeedlab.cs.fiu.edu/software/</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/03/2023<br>\n\t\t\t\t\tModified by: Fahad&nbsp;Saeed</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of the proposed CAREER grant is to design and develop algorithmic and high performance computing (HPC) foundations for practical sublinear and parallel algorithms for big proteogenomics data - especially for non-model organisms with previously unsequenced or partially sequenced genomes. This CAREER grants enabled us to explore a new class of reductive algorithms for analysis of MS data that can allow peptide deductions in sublinear time, compression algorithms that operate in sub-linear space, and HPC algorithms that can operate on lossy reduced-form of the MS data.\n\nFollowing is the summary of our activities:\n\n1)     We investigated and established that current proteogenomic tools are inadequate in scalability with increase size of the database, as well as with increasing number of species (Tariq, and Saeed, IEEE Access 2021).\n\n2)     In order to design the building blocks needed for developing scalable HPC methods we designed three specific things: MaSS-Simulator (Awan &amp; Saeed, PROTEOMICS 2018), and Benchmarking data sets for proteomics (Awan &amp; Saeed 2021). Both these tools allowed us to generate, calibrate, and control the parameters that could be used for data simulation, and experimentations.\n\n3)     We then started building the computational blocks necessary for scalable computing. This include template-based strategy that can be used on CPU-GPU architectures (Awan &amp; Saeed, Computers in Biology and Medicine, 2018), BFS algorithms that could run on CPU-GPU architectures (Artiles &amp; Saeed, IEEE IPDPS 2021), and algorithms that can run FFT-like computations on the CPU-GPU architectures (Artiles &amp; Saeed, IEEE BigData 2019). These building blocks enabled us to compute large-scale proteogenomics data analysis on a variety of homogenous and heterogeneous architectures.   \n\n4)     Two other strategies that we designed and developed were related to load-balancing of large-scale databases. To this end, we developed a compression method that would allow us to compress-and-compute the MS data without any decompression (Haseeb &amp; Saeed, IEEE BIBM 2019), and LBE algorithm (Haseeb &amp; Saeed, IEEE IPDPS 2019) which enable effective load-balancing based on the number of computations per unit database. We demonstrated that using both of these methods would enable massive reduction in the I/O for memory-distributed architectures and will result in load that is fairly balanced. \n\n5)     We also developed a theoretical framework and verified with experimental results that for modern memory-distributed architectures, the amount of communication and I/O is a lot more than the computation costs, no matter what the scoring mechanism is used. This led to the development of a high-performance computing framework that used minimal communications costs to compute over large proteogenomics data sets in a reasonable time. We also demonstrated that our proposed method can give 10x speedups as compared to existing and established parallel computing methods.\n\nThe outcome of the project was design and development of novel communication-avoidance parallel algorithms that were introduced and was a new paradigm for MS based omics. They allowed us to scale proteogenomic data analysis from MS based experiments. We demonstrated that using theoretical results and published results from existing HPC algorithms related to MS based omics data that the communication cost was the biggest cost when processing MS omics data sets. This has been a neglected factor in ALL existing HPC tools for MS based omics data analysis. This theoretical and empirical result opened up a new direction for research and excited the parallel computing community to work on these high impact problems. Using our HPC framework, we were able to show that the proposed techniques enabled 10x speedups as compared to any existing parallel computing framework. The speedups are more instrumental, when the data is large (i.e. for proteogenomics) since a serial or a sub-optimal parallel computing design results in significant paging of the memory. Therefore, the end result is that as compared to existing parallel computing technique which may need weeks of computation, we are able to process terabytes of data within hours on a memory-distributed supercomputer. We expect that our frameworks will be used for investigation of biological, and chemical samples from complex environmental and micrbiomes leading to advances in laboratory, and clinical treatment and drug discovery. \n\nDr. Saeed's group has made fundamental advances in dealing with these large data sets for processing on homogenous and heterogeneous supercomputing architectures. One of the most interesting aspects of the research was compression of these large omics data sets and processing on these data sets without the need to decompress them.\n\nThe NSF CAREER award has partially supported more than 6 PhD students, 2 MS and 4 undergraduate students, numerous research talks, and has resulted in more than 34 peer-reviewed publications. The software resulting from these novel high-performance computing is available on the PI lab webpage at: https://saeedlab.cs.fiu.edu/software/\n\n \n\n\t\t\t\t\tLast Modified: 10/03/2023\n\n\t\t\t\t\tSubmitted by: Fahad Saeed"
 }
}