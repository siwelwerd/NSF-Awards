{
 "awd_id": "1927564",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AI-DCL: EAGER: Human-in-the-Loop Fairness Optimization in Machine Learning with Minimax Loss and an Abstain Option",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frederick Kronz",
 "awd_eff_date": "2019-09-15",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 299995.0,
 "awd_amount": 299995.0,
 "awd_min_amd_letter_date": "2019-08-05",
 "awd_max_amd_letter_date": "2019-08-05",
 "awd_abstract_narration": "This project will implement machine learning algorithms that explores tradeoffs involving three factors: accuracy, fairness, and input data coverage. On the computational side, the project has the potential to bring about a paradigm shift for fair machine learning algorithms while improving uncertainty estimates for those algorithms. The project also includes the development of a human-in-the-loop optimization algorithm to enable humans to dynamically tune the three factors and thereby interact with the algorithm. On the sociological side, this work will provide a cross-cultural study of subject preferences when presented with quantifiable tradeoffs between the three specified factors; it will focus on behavioral differences between individuals in response to their interactive explorations of how the tradeoffs work. The results of this study will serve to complement prior related research that is more qualitative. The results of this study could have substantial impacts on policy making on machine learning algorithms. They could also serve to improve the public's in machine learning algorithms and enable more human-machine teams, which is important in the current era where machine learning algorithms have increasingly become black boxes while being more broadly deployed in many crucial real-life applications.\r\n\r\nThe goal of this project is to study the trade-off between accuracy, fairness, and data coverage in machine learning algorithms. The research team plans to develop novel hybrid human/machine-learning algorithms with an integrated, optimizable fairness component. The specific objectives are to develop algorithms that are designed to trade-off between three factors: (1) the traditional average error objective that pertains to utility, (2) a minimax error objective that minimize the maximal error occurring to any training example, which pertains to fairness, and (3) the coverage of the algorithm on the input distribution by providing an abstain option that the algorithm can utilize when it is not confident in giving a correct answer. The team will develop their algorithms using saddle point optimization approaches in zero-sum games. A human-in-the-loop optimization algorithm will be designed for humans to dynamically tune the three factors to facilitate interaction with the algorithm. The team will use a hybrid iterative approach to algorithm design and testing that is based in grounded theory, which is widely used in the human and social sciences. Humans will provide directions (more fairness) and specify the groups they want to cover, while the real-valued changes will be automatically computed. In order to better understand of trade-offs on utility, fairness and coverage from a sociological perspective, a broad cross-cultural evaluation will be performed with multiple social-cultural groups in the United States as well as an online platform to reach global users in China and Brazil.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fuxin",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fuxin Li",
   "pi_email_addr": "lif@eecs.oregonstate.edu",
   "nsf_id": "000637562",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shaozeng",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shaozeng Zhang",
   "pi_email_addr": "shaozeng.zhang@oregonstate.edu",
   "nsf_id": "000764334",
   "pi_start_date": "2019-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973318507",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 299995.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we are interested in how humans across different cultures would respond to the topic of individual fairness for AI (machine learning) algorithms. Especially, we believe that a human-in-the-loop approach, where humans can toggle the preference of the algorithm between accuracy, fairness and an abstain option (where the algorithm would refuse to make a prediction) would improve human understanding on the individual fairness question, and we are interested in studying human behavior before and after playing with such a system.</p>\n<p>&nbsp;</p>\n<p>Hence, we built a human-in-the-loop algorithm for individual fairness using the Stein Variational Gradient Descent (SVGD) algorithm to deal with uncertainty. This algorithm showed improvement in ensuring the individual fairness in the Adult dataset which is about predicting the income of an adult given different demographic indicators such as age, occupation, education, etc. During the course of the algorithm development, we also uncovered some of the difficulties in individual fairness tasks and improved our own understanding of the problem.</p>\n<p>&nbsp;</p>\n<p>We built an interactive user interface of the system and after several rounds of sandboxing and tweaking of the interface, we conducted the user testing and evaluation of the algorithm. We turned our project website that we had built earlier into a remote testing platform for test participants from the U.S. and China:<a href=\"https://blogs.oregonstate.edu/designingfairai/\"> https://blogs.oregonstate.edu/designingfairai/</a>, as shown in the screen capture below. Our user testing survey, also accessible on the project website, recorded the test participant&rsquo;s interactive use of our project algorithm. This survey focuses on the explicitness of the trade-off, the interpretability of our algorithms, coverage calibration (application domains) scenarios, and adjustable thresholds of fairness and efficiency. It also includes follow-up open questions, such as on their individual choosing of specific thresholds.</p>\n<p>&nbsp;</p>\n<p>After receiving the survey responses, we first did data processing and cleaning to make sure we use only valid data for analysis, which ended up with 143 valid responses.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>We briefly summarize the results of the survey and note the most important findings here. Other findings will be published with our paper later. Our survey starts with respondents&rsquo; general feedback on the project algorithm, the user interface (as shown in the picture) of the algorithm, and the achievement of the project goals. Most (78%) of the respondents <strong>agreed</strong> that the <strong>project achieved its overall goal </strong>that is to have algorithmic prediction automation opened up and thus allow AI users to directly intervene in algorithmic prediction and decision-making. Over half (69%) of them agreed that the design of this algorithm helps them better understand how automated prediction functions. Also over half (66%) felt that the understandability of the results were either good or excellent.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Our survey asked the respondents to assess the value and risk of AI application both before and after their use of the project algorithm. One critical question is whether applying AI/ML to assisting law enforcement would present more opportunities or concerns related to the risk of recidivism (likelihood of committing crimes again). Interestingly there was a <strong>significant</strong> <strong>increase in their sentiment that AI/ML had more opportunities than concerns</strong> after using the algorithm designed in this project. Closer analyses showed a few specific trends with relation to nationality, gender, and disciplinary training. There was a big shift (from 44.4% to <strong>72.9%</strong>) in respondents from China to higher agreement on more opportunities than concerns after using our algorithm. In comparison, a smaller but still clear shift (from <strong>50%</strong> to 57.3%) came from <strong>the US</strong>. A notable shift (from 39.1% to 62.7%) came from female respondents. The clear shift towards agreement on more opportunities than concerns also came from respondents professionally trained in computer science, engineering, humanities and social sciences, and business; however, respondents trained in natural sciences stayed consistent and those with interdisciplinary training moved more firmly towards disagreement. In comparison, <strong>there was little change in the perspective</strong> about the use of algorithms&rsquo; use in financial institutions to assist financial institutions to determine loan eligibility presents more opportunities than concerns.<strong>&nbsp;</strong></p>\n<p>&nbsp;</p>\n<p>Our survey also posed an algorithmically and ethically challenging real life example from the ADULT dataset in relation to individual fairness in evaluating or predicting the eligibility for financial loans. To address challenging individual cases in financial institutions in real life like the given example, most (73%) of respondents agreed that the algorithm should abstain from making a prediction and then invite human investigators to review and evaluate challenging cases in a traditional way. This response is almost the same from China and from the US.</p>\n<p>&nbsp;</p>\n<p>In short, our survey recorded demographically diverse potential user&rsquo;s interactive testing and evaluation of our project algorithm. We received largely positive feedback on our algorithm and interface, especially the opening up of the trade-off between automation efficiency, data coverage and fairness for users to intervene. The feedback and suggestions we received are also inspiring for our future research and design.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/28/2023<br>\nModified by: Fuxin&nbsp;Li</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we are interested in how humans across different cultures would respond to the topic of individual fairness for AI (machine learning) algorithms. Especially, we believe that a human-in-the-loop approach, where humans can toggle the preference of the algorithm between accuracy, fairness and an abstain option (where the algorithm would refuse to make a prediction) would improve human understanding on the individual fairness question, and we are interested in studying human behavior before and after playing with such a system.\n\n\n\n\n\nHence, we built a human-in-the-loop algorithm for individual fairness using the Stein Variational Gradient Descent (SVGD) algorithm to deal with uncertainty. This algorithm showed improvement in ensuring the individual fairness in the Adult dataset which is about predicting the income of an adult given different demographic indicators such as age, occupation, education, etc. During the course of the algorithm development, we also uncovered some of the difficulties in individual fairness tasks and improved our own understanding of the problem.\n\n\n\n\n\nWe built an interactive user interface of the system and after several rounds of sandboxing and tweaking of the interface, we conducted the user testing and evaluation of the algorithm. We turned our project website that we had built earlier into a remote testing platform for test participants from the U.S. and China: https://blogs.oregonstate.edu/designingfairai/, as shown in the screen capture below. Our user testing survey, also accessible on the project website, recorded the test participants interactive use of our project algorithm. This survey focuses on the explicitness of the trade-off, the interpretability of our algorithms, coverage calibration (application domains) scenarios, and adjustable thresholds of fairness and efficiency. It also includes follow-up open questions, such as on their individual choosing of specific thresholds.\n\n\n\n\n\nAfter receiving the survey responses, we first did data processing and cleaning to make sure we use only valid data for analysis, which ended up with 143 valid responses.\n\n\n\n\n\n\n\n\nWe briefly summarize the results of the survey and note the most important findings here. Other findings will be published with our paper later. Our survey starts with respondents general feedback on the project algorithm, the user interface (as shown in the picture) of the algorithm, and the achievement of the project goals. Most (78%) of the respondents agreed that the project achieved its overall goal that is to have algorithmic prediction automation opened up and thus allow AI users to directly intervene in algorithmic prediction and decision-making. Over half (69%) of them agreed that the design of this algorithm helps them better understand how automated prediction functions. Also over half (66%) felt that the understandability of the results were either good or excellent.\n\n\n\n\n\nOur survey asked the respondents to assess the value and risk of AI application both before and after their use of the project algorithm. One critical question is whether applying AI/ML to assisting law enforcement would present more opportunities or concerns related to the risk of recidivism (likelihood of committing crimes again). Interestingly there was a significant increase in their sentiment that AI/ML had more opportunities than concerns after using the algorithm designed in this project. Closer analyses showed a few specific trends with relation to nationality, gender, and disciplinary training. There was a big shift (from 44.4% to 72.9%) in respondents from China to higher agreement on more opportunities than concerns after using our algorithm. In comparison, a smaller but still clear shift (from 50% to 57.3%) came from the US. A notable shift (from 39.1% to 62.7%) came from female respondents. The clear shift towards agreement on more opportunities than concerns also came from respondents professionally trained in computer science, engineering, humanities and social sciences, and business; however, respondents trained in natural sciences stayed consistent and those with interdisciplinary training moved more firmly towards disagreement. In comparison, there was little change in the perspective about the use of algorithms use in financial institutions to assist financial institutions to determine loan eligibility presents more opportunities than concerns.\n\n\n\n\n\nOur survey also posed an algorithmically and ethically challenging real life example from the ADULT dataset in relation to individual fairness in evaluating or predicting the eligibility for financial loans. To address challenging individual cases in financial institutions in real life like the given example, most (73%) of respondents agreed that the algorithm should abstain from making a prediction and then invite human investigators to review and evaluate challenging cases in a traditional way. This response is almost the same from China and from the US.\n\n\n\n\n\nIn short, our survey recorded demographically diverse potential users interactive testing and evaluation of our project algorithm. We received largely positive feedback on our algorithm and interface, especially the opening up of the trade-off between automation efficiency, data coverage and fairness for users to intervene. The feedback and suggestions we received are also inspiring for our future research and design.\n\n\n\t\t\t\t\tLast Modified: 11/28/2023\n\n\t\t\t\t\tSubmitted by: FuxinLi\n"
 }
}