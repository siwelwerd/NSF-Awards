{
 "awd_id": "1901527",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: SMALL: Fast Prediction and Model Compression for Large-Scale Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2018-08-13",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 362846.0,
 "awd_amount": 362846.0,
 "awd_min_amd_letter_date": "2019-02-12",
 "awd_max_amd_letter_date": "2019-02-12",
 "awd_abstract_narration": "In order to handle large-scale problems, many algorithms have been proposed for improving the training speed of machine learning models. However, in many real world applications the bottleneck is at the prediction phase instead of the training phase due to the time and space complexity of prediction. Unlike the training phase that can run for several hours on multiple machines, the prediction phase usually runs on real-time systems; as a result, each prediction has to be done in a few seconds in order to provide immediate feedback to users. Furthermore, applications that run on mobile devices have even more strict constraints on memory capacity and computational resources. To address these issues, this research develops a new family of machine learning algorithms with faster prediction time and smaller model size. The outcome of this project creates a fundamental shift in the applicability of machine learning models to real-time online systems and on-device applications. Software packages and experimental platforms are made available to the public after being tested on applications. Besides the research objectives, the PI also pursues educational objectives including promoting undergraduate research, involving under-represented minorities in science and engineering, and developing undergraduate and graduate data science curriculums.\r\n\r\nThe goal of this project is to develop novel approaches for reducing prediction time and model size of machine learning algorithms. In particular, the project focuses on machine learning applications with large output space (matrix factorization, extreme multi-class/multi-label classification), and highly nonlinear models (kernel methods and deep neural networks). A series of approximation algorithms are studied, including tree-based algorithms, clustering approaches, and sub-linear time search algorithms. A unified framework is developed for these algorithms and the trade-off between accuracy and prediction time/model size is studied both in theory and in practice. The proposed algorithms are evaluated on a broad range of real world applications, including online web services and on-device applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cho-Jui",
   "pi_last_name": "Hsieh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Cho-Jui Hsieh",
   "pi_email_addr": "chohsieh@cs.ucla.edu",
   "nsf_id": "000711637",
   "pi_start_date": "2019-02-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 Wilshire Boulevard",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 362846.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-615a24f3-7fff-64eb-d00f-52d27a1e59fd\"> </span></p>\n<p dir=\"ltr\"><span>Despite achieving astonishing performance in many important applications, machine learning models typically consist of millions of parameters, leading to large memory requirements and slow inference speed, which limits their applications in real-world systems. For instance, an online search engine needs to react to each query in milliseconds, and many on-device models deployed on smartphones or embedded systems encounter even more strict constraints on memory and computing resources. This research project aims to design principled ways to improve the model size and inference speed of machine learning models without sacrificing their predictive performance.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The research conducted focused on some representative, concrete problems that arise in contemporary applications. In particular, the project has developed the following principled ways to improve the model size and inference speed: (a) A new family of fast inner product search algorithms have been developed to improve the prediction speed for large-scale recommendation and search models, (b) A family of data-aware low-rank approximation methods have been developed to compress neural networks with large weight matrices, such as recurrent neural networks and Transformers, (c) Novel techniques have been developed to stabilize existing neural architecture search algorithms to automatically design architectures that balance efficiency and accuracy, (d) An efficient Monte-Carlo Tree Search (MCTS) algorithm has been devised to improve the inference speed of reinforcement learning agents such as AlphaZero. In addition, the project also demonstrates the application of model compression in distributed training to reduce communication costs. As part of the project, state-of-the-art public domain software has been developed. The results from this funded project have been disseminated through publications in various venues, such as ICML, NeurIPS, ICLR, KDD, ICDM, SC, ICPP, which are leading conferences in machine learning and high-performance computing. In terms of education, the PI developed several machine learning and statistical computing courses for undergraduate and graduate students in both the Computer Science and Statistics departments. Several Ph.D. students obtained their degrees supported by funding from this project.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/11/2022<br>\n\t\t\t\t\tModified by: Cho-Jui&nbsp;Hsieh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nDespite achieving astonishing performance in many important applications, machine learning models typically consist of millions of parameters, leading to large memory requirements and slow inference speed, which limits their applications in real-world systems. For instance, an online search engine needs to react to each query in milliseconds, and many on-device models deployed on smartphones or embedded systems encounter even more strict constraints on memory and computing resources. This research project aims to design principled ways to improve the model size and inference speed of machine learning models without sacrificing their predictive performance. \nThe research conducted focused on some representative, concrete problems that arise in contemporary applications. In particular, the project has developed the following principled ways to improve the model size and inference speed: (a) A new family of fast inner product search algorithms have been developed to improve the prediction speed for large-scale recommendation and search models, (b) A family of data-aware low-rank approximation methods have been developed to compress neural networks with large weight matrices, such as recurrent neural networks and Transformers, (c) Novel techniques have been developed to stabilize existing neural architecture search algorithms to automatically design architectures that balance efficiency and accuracy, (d) An efficient Monte-Carlo Tree Search (MCTS) algorithm has been devised to improve the inference speed of reinforcement learning agents such as AlphaZero. In addition, the project also demonstrates the application of model compression in distributed training to reduce communication costs. As part of the project, state-of-the-art public domain software has been developed. The results from this funded project have been disseminated through publications in various venues, such as ICML, NeurIPS, ICLR, KDD, ICDM, SC, ICPP, which are leading conferences in machine learning and high-performance computing. In terms of education, the PI developed several machine learning and statistical computing courses for undergraduate and graduate students in both the Computer Science and Statistics departments. Several Ph.D. students obtained their degrees supported by funding from this project.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/11/2022\n\n\t\t\t\t\tSubmitted by: Cho-Jui Hsieh"
 }
}