{
 "awd_id": "1951729",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Small: Usable Interpretability",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2019-08-26",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 495592.0,
 "awd_amount": 495592.0,
 "awd_min_amd_letter_date": "2019-09-23",
 "awd_max_amd_letter_date": "2020-07-23",
 "awd_abstract_narration": "Deep neural network (DNN)-powered systems and services hold great promise to fundamentally transform the way people live, work and play. Yet, to fully unleash this potential, it is critical to improve their interpretability to make them more trustworthy and easy-to-use. The transformative nature of this project is to completely rethink how to define and implement the interpretation of DNNs and how to exploit this interpretability as a bridge to understand and control the DNN behaviors. The success of this project will not only improve the reliability, interactivity and operability of DNN-powered systems, but also promote more principled practice of building and using machine learning systems in general. The research products will be applicable to fields including machine learning, cyber-security and human-computer interaction.\r\n\r\nThis project aims to develop RIDDLE, a new interpretable deep learning framework that is reliable, because it deploys built-in defenses against adversarial manipulations; interactive, because it provides interfaces and mechanisms to perform in-depth, interactive analysis of DNN dynamics; and debuggable, because it employs interpretability as the lens for users to effectively control DNN behaviors. Along the three directions, the specific tasks of this project include: exploring the vulnerabilities of existing interpretation models to adversarial manipulations, uncovering their root causes, and developing practical defense mechanisms,  designing an expressive interpretation algebra framework to allow users to flexibly construct interactive analysis tools for a variety of DNNs and tasks, which circumvents the \"one-size-fits-all\" challenge, and building interpretation-based model debugging techniques, which allow users to effectively localize and fix model defects.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ting",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ting Wang",
   "pi_email_addr": "inbox.ting@gmail.com",
   "nsf_id": "000702128",
   "pi_start_date": "2019-09-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 330432.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 165160.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The state-of-the-art performance of deep neural networks (DNNs) is often achieved at the cost of their interpretability. This is a major drawback for domains where the interpretability of decisions is a critical prerequisite. While a plethora of interpretation methods have been proposed to help users understand the inner workings of DNNs, their interpretability is far from being practically useful. The goal of this project is to fundamentally improve the usability of interpretable deep learning system (IDLS) along 1) reliability - the interpretation should be robust against adversarial manipulations, 2) interactivity - the interpretation should account for the perception, understanding, and response of end-users, and 3) operability - the interpretation should serve as the lens for end-users to effectively understand and control DNN behaviors. Specifically, the project has produced the following intellectual products.</p>\n<p>Understanding of the fundamental vulnerabilities of state-of-the-art IDLSes -- We have found that the existing IDLSes are vulnerable to two types of manipulations. 1) Adversarial perturbation -- we have designed and evaluated attacks that can deceive both a DNN model and its interpretation model simultaneously and generate prediction and interpretation arbitrarily designated by the adversary. 2) Random noise -- we have found that the inherent random noises in the interpretation processes (e.g., caused by gradient computation and random sampling) can cause significant instability in the generated interpretations. To this end, we propose the Median Test for Feature Attribution, a novel approach to qualify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees.</p>\n<p>Theories and methods of enhancing the IDLS robustness against adversarial manipulations -- We have identified that one possible root cause of such vulnerabilities is the \"prediction-interpretation gap\": in existing IDLSes, the DNN model and its interpretation model are often misaligned and the interpretation model often only partially describes the DNN model's behaviors, resulting in the possibility for the adversary to manipulate both models simultaneously. Leveraging this finding, we have designed an adversarial training framework that can mitigate such vulnerabilities by minimizing the prediction-interpretation gap using carefully generated adversarial examples.</p>\n<p>Design and implementation of interactive interpretability for DNN models -- We have designed and evaluated an expressive algebra framework for interpreting DNN models. Its fundamental building blocks are a library of atomic, composable interpretation operators, including magnifier, navigator, differentiator, and integrator, which enable flexible construction of various interactive analysis tools.</p>\n<p>Technical innovation of troubleshooting DNN models through the lens of interpretation -- We have explored using interpretability as a novel defense mechanism against neural backdoor attacks where conventional defenses are inapplicable or ineffective. 1) For attacks embedded in the neural architectures, we propose to leverage architecture-level interpretability. By examining the given neural architecture in a data-free manner, we identify potential sub-structures that account for \"architectural shortcuts\", which may correspond to backdoors; 2) For attacks against self-supervised learning (SSL), we propose to leverage the interpretability of feature distribution. By examining the distribution of given (possibly poisoned) training data in the feature space, we identify samples that form a cluster much denser than the others, which may correspond to poisoning data. 3) For attacks against pre-trained language models (PLMs) as few-shot learners, we propose to leverage the interpretability of an incoming sample within the context of the few training samples. By comparing the representations of given samples under varying masking with respect to the training samples, we identify poisoned samples as ones with significant variations. All these interpretation-based defenses achieve effectiveness beyond conventional methods.</p>\n<p>To date, the project has produced 38 peer-reviewed publications published in top-tier security and privacy, machine learning, and human-computer interaction venues and has won one best paper award. 4 graduate and 2 undergraduate students have been trained in the research frontier of the security, privacy, and transparency challenges of machine learning techniques. 2 Ph.D. students have finished their dissertations partly based on their work in this project. The outcomes of this project have been incorporated into the course and seminar materials and disseminated through conference presentations, invited talks, and media coverage.</p><br>\n<p>\n Last Modified: 05/25/2024<br>\nModified by: Ting&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe state-of-the-art performance of deep neural networks (DNNs) is often achieved at the cost of their interpretability. This is a major drawback for domains where the interpretability of decisions is a critical prerequisite. While a plethora of interpretation methods have been proposed to help users understand the inner workings of DNNs, their interpretability is far from being practically useful. The goal of this project is to fundamentally improve the usability of interpretable deep learning system (IDLS) along 1) reliability - the interpretation should be robust against adversarial manipulations, 2) interactivity - the interpretation should account for the perception, understanding, and response of end-users, and 3) operability - the interpretation should serve as the lens for end-users to effectively understand and control DNN behaviors. Specifically, the project has produced the following intellectual products.\n\n\nUnderstanding of the fundamental vulnerabilities of state-of-the-art IDLSes -- We have found that the existing IDLSes are vulnerable to two types of manipulations. 1) Adversarial perturbation -- we have designed and evaluated attacks that can deceive both a DNN model and its interpretation model simultaneously and generate prediction and interpretation arbitrarily designated by the adversary. 2) Random noise -- we have found that the inherent random noises in the interpretation processes (e.g., caused by gradient computation and random sampling) can cause significant instability in the generated interpretations. To this end, we propose the Median Test for Feature Attribution, a novel approach to qualify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees.\n\n\nTheories and methods of enhancing the IDLS robustness against adversarial manipulations -- We have identified that one possible root cause of such vulnerabilities is the \"prediction-interpretation gap\": in existing IDLSes, the DNN model and its interpretation model are often misaligned and the interpretation model often only partially describes the DNN model's behaviors, resulting in the possibility for the adversary to manipulate both models simultaneously. Leveraging this finding, we have designed an adversarial training framework that can mitigate such vulnerabilities by minimizing the prediction-interpretation gap using carefully generated adversarial examples.\n\n\nDesign and implementation of interactive interpretability for DNN models -- We have designed and evaluated an expressive algebra framework for interpreting DNN models. Its fundamental building blocks are a library of atomic, composable interpretation operators, including magnifier, navigator, differentiator, and integrator, which enable flexible construction of various interactive analysis tools.\n\n\nTechnical innovation of troubleshooting DNN models through the lens of interpretation -- We have explored using interpretability as a novel defense mechanism against neural backdoor attacks where conventional defenses are inapplicable or ineffective. 1) For attacks embedded in the neural architectures, we propose to leverage architecture-level interpretability. By examining the given neural architecture in a data-free manner, we identify potential sub-structures that account for \"architectural shortcuts\", which may correspond to backdoors; 2) For attacks against self-supervised learning (SSL), we propose to leverage the interpretability of feature distribution. By examining the distribution of given (possibly poisoned) training data in the feature space, we identify samples that form a cluster much denser than the others, which may correspond to poisoning data. 3) For attacks against pre-trained language models (PLMs) as few-shot learners, we propose to leverage the interpretability of an incoming sample within the context of the few training samples. By comparing the representations of given samples under varying masking with respect to the training samples, we identify poisoned samples as ones with significant variations. All these interpretation-based defenses achieve effectiveness beyond conventional methods.\n\n\nTo date, the project has produced 38 peer-reviewed publications published in top-tier security and privacy, machine learning, and human-computer interaction venues and has won one best paper award. 4 graduate and 2 undergraduate students have been trained in the research frontier of the security, privacy, and transparency challenges of machine learning techniques. 2 Ph.D. students have finished their dissertations partly based on their work in this project. The outcomes of this project have been incorporated into the course and seminar materials and disseminated through conference presentations, invited talks, and media coverage.\t\t\t\t\tLast Modified: 05/25/2024\n\n\t\t\t\t\tSubmitted by: TingWang\n"
 }
}