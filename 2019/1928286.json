{
 "awd_id": "1928286",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FW-HTF-RM: Collaborative Research: Augmenting Social Media Content Moderation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2019-09-04",
 "awd_max_amd_letter_date": "2019-09-04",
 "awd_abstract_narration": "Around the world, users of social media platforms generate millions of comments, videos, and photos per day. Within this content is dangerous material such as child pornography, sex trafficking, and terrorist propaganda. Though platforms leverage algorithmic systems to facilitate detection and removal of problematic content, decisions about whether to remove content, whether it's as benign as an off-topic comment or as dangerous as self-harm or abuse videos, are often made by humans. Companies are hiring moderators by the thousands and tens of thousands work as volunteer moderators. This work involves economic, emotional, and often physical safety risks. With social media content moderation as the focus of work and the content moderators as the workers, this project facilitates the human-technology partnership by designing new technologies to augment moderator performance. The project will improve moderators' quality of life, augment their capabilities, and help society understand how moderation decisions are made and how to support the workers who help keep the internet open and enjoyable. These advances will enable moderation efforts to keep pace with user-generated content and ensure that problematic content does not overwhelm internet users. The project includes outreach and engagement activities with academic, industry, policy-makers, and the public that ensure the project's findings and tools support broad stakeholders impacted by user-generated content and its moderation.\r\n\r\nSpecifically, the project involves five main research objectives that will be met through qualitative, historical, experimental, and computational research approaches. First, the project will improve understanding of human-in-the-loop decision making practices and mental models of moderation by conducting interviews and observations with moderators across different content domains. Second, it will assess the socioeconomic impact of technology-augmented moderation through industry personnel interviews. Third, the project will test interventions to decrease the emotional toll on human moderators and optimize their performance through a series of experiments utilizing theories of stress alleviation. Fourth, the project will design, develop, and test a suite of cognitive assistance tools for live streaming moderators. These tools will focus on removing easy decisions and helping moderators dynamically manage their emotional and cognitive capabilities. Finally, the project will employ a historical perspective to analyze companies' content moderation policies to inform legal and platform policies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sarah",
   "pi_last_name": "Roberts",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Sarah T Roberts",
   "pi_email_addr": "sarah.roberts@ucla.edu",
   "nsf_id": "000780771",
   "pi_start_date": "2019-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 Wilshire Blvd, Suite 700",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "103Y00",
   "pgm_ele_name": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span><span>In the fall of 2021, during the midst of the COVID-19 pandemic, a research team, comprised of a lead PI, a staff researcher and three core graduate researchers, conducted semi-structured qualitative interviews with an in-house content moderation team at a mid-sized social platform, dubbed FanBase (a pseudonym). Using qualitative methods of semi-structured interviews designed to elicit in-depth and thoughtful responses, the research team asked nine open-ended questions intended to assess content moderators' mental models and work environment: computational and policy tools used; collaborative/collective knowledge-sharing practices; stamina and well-being; resources for dealing with mental and physical impacts of their work.</span></span></p>\n<p><span><span>The team's findings were significant in that they provided novel insight into the working conditions of content moderators. Specifically, we met with our interlocutors at a time of significant change: FanBase had experienced exponential growth during the onset of the COVID-19 pandemic. With the large-scale transition to remote work and an increased reliance on social media platforms for interaction, FanBase had experienced rapid growth and a need for scaling up their content moderation team while standardizing their practices and policies. As a result, we were able to observe radical cultural and organizational shifts across interviewed staff. Additionally, our interviews yielded important insights around the need for trauma-informed research practices for content moderation research as well as different forms of ongoing support for content moderation work. While there is ample research on individual harms experienced by content moderation workers in their day-to-day lives, there is little reserach on the&nbsp;<em>collective</em>&nbsp;trauma/harm experienced by workers when large-scale violent events are organized on their platforms. In the case of FanBase, we found that our interlocutors were deeply impacted by the fact that a violent, white-supremacist gathering, ultimately resulting in a homocide, was organized using their platform. Both formally and informally, this event and its aftermath informed the practices and interpersonal interactions of the Trust &amp; Safety team.&nbsp;<br /></span></span></p>\n<p><span>This work represents a novel entry into the field of content moderation studies, which endeavor to understand the way private firms undertake, enact and meet internal policy, legal and other demands from an operational perspective. These practices are typically opaque and misunderstood by regulators, legislators, advocates and the general public at large. The research undertaken through this grant support contributes new insights into how those tasked with social media policy implementation think about and approach their work, and how the work, in turn, influences the decisions and actions they take on behalf of their employers and the users that engage on their platforms.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/05/2023<br>\n\t\t\t\t\tModified by: Sarah&nbsp;T&nbsp;Roberts</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn the fall of 2021, during the midst of the COVID-19 pandemic, a research team, comprised of a lead PI, a staff researcher and three core graduate researchers, conducted semi-structured qualitative interviews with an in-house content moderation team at a mid-sized social platform, dubbed FanBase (a pseudonym). Using qualitative methods of semi-structured interviews designed to elicit in-depth and thoughtful responses, the research team asked nine open-ended questions intended to assess content moderators' mental models and work environment: computational and policy tools used; collaborative/collective knowledge-sharing practices; stamina and well-being; resources for dealing with mental and physical impacts of their work.\n\nThe team's findings were significant in that they provided novel insight into the working conditions of content moderators. Specifically, we met with our interlocutors at a time of significant change: FanBase had experienced exponential growth during the onset of the COVID-19 pandemic. With the large-scale transition to remote work and an increased reliance on social media platforms for interaction, FanBase had experienced rapid growth and a need for scaling up their content moderation team while standardizing their practices and policies. As a result, we were able to observe radical cultural and organizational shifts across interviewed staff. Additionally, our interviews yielded important insights around the need for trauma-informed research practices for content moderation research as well as different forms of ongoing support for content moderation work. While there is ample research on individual harms experienced by content moderation workers in their day-to-day lives, there is little reserach on the collective trauma/harm experienced by workers when large-scale violent events are organized on their platforms. In the case of FanBase, we found that our interlocutors were deeply impacted by the fact that a violent, white-supremacist gathering, ultimately resulting in a homocide, was organized using their platform. Both formally and informally, this event and its aftermath informed the practices and interpersonal interactions of the Trust &amp; Safety team. \n\n\nThis work represents a novel entry into the field of content moderation studies, which endeavor to understand the way private firms undertake, enact and meet internal policy, legal and other demands from an operational perspective. These practices are typically opaque and misunderstood by regulators, legislators, advocates and the general public at large. The research undertaken through this grant support contributes new insights into how those tasked with social media policy implementation think about and approach their work, and how the work, in turn, influences the decisions and actions they take on behalf of their employers and the users that engage on their platforms.\n\n\t\t\t\t\tLast Modified: 09/05/2023\n\n\t\t\t\t\tSubmitted by: Sarah T Roberts"
 }
}