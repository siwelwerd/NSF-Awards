{
 "awd_id": "1915099",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Impact of Molecular Biomarkers on Survival Outcomes: Signal Detection, Model Selection, and Statistical Inference in High-dimensional Settings",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 120000.0,
 "awd_amount": 120000.0,
 "awd_min_amd_letter_date": "2019-08-09",
 "awd_max_amd_letter_date": "2022-04-19",
 "awd_abstract_narration": "The goal of this project is to provide a theoretically justifiable and computationally feasible framework for analyzing large-scale data collected from epidemiology and other biomedical sciences, social science, marketing, environmental science, and econometrics. The proposed methods will provid a convenient means to identify the gene-specific impacts on survival and quantify the uncertainty of the estimates in high-dimensional settings. As a result, the methods will help detect patients' specific characteristics that make them respond differently to treatment or more susceptible to diseases, which is key to precision medicine. The PI further plans to disseminate the proposed research in education by redesigning graduate-level courses and training graduate students. \r\n\r\nThe project, through its three major aims, features a series of methods to address fundamental issues arising from high-dimensional survival data analysis. The first aim is to detect gene-specific impacts on cancer patients' survival and to provide an integrated framework of detecting individual genes' relevance to survival when genes have heterogeneous patterns of influence. The second aim focuses on sequentially selecting important predictors for predicting survival outcomes. This approach is different from existing forward regression approaches, which are not applicable to handle censored outcome data with high-dimensional covariates. The third aim is to develop methods to quantify the uncertainty of survival models with high-dimensional predictors by integrating model selection, estimation, and inference steps.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Hyokyoung",
   "pi_last_name": "Hong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hyokyoung Hong",
   "pi_email_addr": "hhong@msu.edu",
   "nsf_id": "000675072",
   "pi_start_date": "2019-08-09",
   "pi_end_date": "2022-04-19"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Haolei",
   "pi_last_name": "Weng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Haolei Weng",
   "pi_email_addr": "wenghaol@msu.edu",
   "nsf_id": "000812496",
   "pi_start_date": "2022-04-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "",
  "perf_city_name": "East Lansing",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488242600",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 55403.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 32514.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 32083.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">With the </span><span class=\"s2\">evolution of data acquisition technologies and storage facilities, large-scale datasets are routinely collected across various fields ranging from </span><span class=\"s3\">biomedical sciences, social science, and engineering to econometrics and finance. An important yet challenging task is to discover which variables in a large collection are meaningfully related to an outcome of interest, and to quantify their statistical associations. For instance, in biomedical studies, </span><span class=\"s4\">identifying and utilizing biomarkers, out of massive biological signals, that can perform as valuable prognostic tools for patients' survival is of vital importance. This project aimed to address this task by developing novel statistical theory and methods, particularly in the critical settings where </span><span class=\"s2\">the number of measured variables relative to sample size is large, the variables have </span><span class=\"s5\">heterogeneous patterns of influence</span><span class=\"s2\">, or the outcome data is censored.</span></p>\r\n<p class=\"p3\"><span class=\"s3\">The project considered several complex high-dimensional regression models to study different types of influence variables may have on the conditional distribution of the outcome. Based on regularization, marginal learning and sequential learning, a set of efficient variable selection methods were developed with provable guarantees that they do not miss any important variables. </span><span class=\"s7\">To provide assurance on the reproducibility of variable selection results in </span><span class=\"s3\">low signal-to-noise ratio scenarios, two popular variable selection frameworks were generalized to achieve controlled error rate of false selections in the presence of missing or censored data. The work also expanded upon a powerful nonparametric tool called trend filtering to optimally detect heterogeneous effects of variables on the mean and quantiles of the outcome. Finally, the project developed new statistical inference tools to rigorously quantify the uncertainty of estimated effects of variables on the censored survival outcomes.<span>&nbsp;</span></span></p>\r\n<p class=\"p3\"><span class=\"s3\">The effectiveness of new methods from the project were shown in extensive numerical experiments. Some of them have been also applied to real data in biological and social sciences. For instance, using the data from </span><span class=\"s7\">Interactive Diet and Activity Tracking in AARP Study to investigate the relationship between&nbsp;</span><span class=\"s1\">metabolomic profiles and ultra-processed food (UPF) intake, a set of potentially novel biomarkers associated with UPF intake were discovered to provide insights for future research; </span><span class=\"s2\">An </span><span class=\"s3\">international large-scale assessments data was analyzed to gain insights into the impact of non-cognitive factors on students' academic performance. In summary, the project developed a series of new statistical methods that are theoretically sound for learning large-scale data with complex structures, and contributed to general theories and methodologies in high-dimensional data analysis. The project also addressed </span><span class=\"s4\">an urgent call for training next generation statisticians in the field of high-dimensional data analytics by integrating the research into classroom teaching and providing thesis topics for three Ph.D. students.&nbsp;</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/29/2024<br>\nModified by: Haolei&nbsp;Weng</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nWith the evolution of data acquisition technologies and storage facilities, large-scale datasets are routinely collected across various fields ranging from biomedical sciences, social science, and engineering to econometrics and finance. An important yet challenging task is to discover which variables in a large collection are meaningfully related to an outcome of interest, and to quantify their statistical associations. For instance, in biomedical studies, identifying and utilizing biomarkers, out of massive biological signals, that can perform as valuable prognostic tools for patients' survival is of vital importance. This project aimed to address this task by developing novel statistical theory and methods, particularly in the critical settings where the number of measured variables relative to sample size is large, the variables have heterogeneous patterns of influence, or the outcome data is censored.\r\n\n\nThe project considered several complex high-dimensional regression models to study different types of influence variables may have on the conditional distribution of the outcome. Based on regularization, marginal learning and sequential learning, a set of efficient variable selection methods were developed with provable guarantees that they do not miss any important variables. To provide assurance on the reproducibility of variable selection results in low signal-to-noise ratio scenarios, two popular variable selection frameworks were generalized to achieve controlled error rate of false selections in the presence of missing or censored data. The work also expanded upon a powerful nonparametric tool called trend filtering to optimally detect heterogeneous effects of variables on the mean and quantiles of the outcome. Finally, the project developed new statistical inference tools to rigorously quantify the uncertainty of estimated effects of variables on the censored survival outcomes.\r\n\n\nThe effectiveness of new methods from the project were shown in extensive numerical experiments. Some of them have been also applied to real data in biological and social sciences. For instance, using the data from Interactive Diet and Activity Tracking in AARP Study to investigate the relationship betweenmetabolomic profiles and ultra-processed food (UPF) intake, a set of potentially novel biomarkers associated with UPF intake were discovered to provide insights for future research; An international large-scale assessments data was analyzed to gain insights into the impact of non-cognitive factors on students' academic performance. In summary, the project developed a series of new statistical methods that are theoretically sound for learning large-scale data with complex structures, and contributed to general theories and methodologies in high-dimensional data analysis. The project also addressed an urgent call for training next generation statisticians in the field of high-dimensional data analytics by integrating the research into classroom teaching and providing thesis topics for three Ph.D. students.\r\n\n\n\t\t\t\t\tLast Modified: 11/29/2024\n\n\t\t\t\t\tSubmitted by: HaoleiWeng\n"
 }
}