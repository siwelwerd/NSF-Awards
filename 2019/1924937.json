{
 "awd_id": "1924937",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Towards Scalable and Self-Aware Robotic Perception",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2019-08-19",
 "awd_max_amd_letter_date": "2019-08-19",
 "awd_abstract_narration": "Robot vision systems should be fast, to enhance the reaction times of robots to events in the visual world, capable of solving multiple vision problems simultaneously, and aware of their limitations. These properties are critical for robotic safety and collaboration. Safety is enhanced by faster reaction times (e.g. a car faster to detect obstacles has more room to stop before hitting them) and self-awareness (e.g., a robot should choose to stop to operate in situations that it deems too hard to be successful in). Collaboration is enhanced by scalability (which allows co-robots to solve more problems and thus behave more like human collaborators) and self-awareness (which simplifies the division of tasks between humans and robots, or teams of robots, with different skills). However, these properties have not been the focus of computer vision research, which has mostly addressed the design of networks that solve single tasks, usually requiring heavy computation and relatively low frame rates, and simply attempt to process all examples without any consideration for how difficult they are. This project addresses all these challenges, laying the foundation for a new generation of robotic perception systems that are more efficient, scalable, and self-aware. The research has applicability in areas of societal relevance, such as manufacturing, self-driving vehicles, intelligent systems, assisted living, homeland security, etc. Educationally, the project will provide exciting opportunities for both graduate and undergraduate research.\r\n\r\nThis project pursues a research agenda composed of several integrated contributions that advance the state of the art in deep learning for robotic vison. This includes 1) novel neural network quantization techniques that address the quantization of both network weights and activations, leading to deep learning models that can be fully implemented with binary operations, significantly enhancing the speed of all AI computations; 2) new families of networks that exploit extensive parameter sharing to achieve scalable inference for task ecologies, substantially increasing the number of networks that can be cached in a processor and, therefore, the number of vision problems that can be solved simultaneously by a robot; 3) new network architectures for self-aware deep learning, capable of assessing the difficulty of each example, predicting failures, and refusing to process examples that are too difficult, so as to mitigate the possibility of catastrophic errors.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nuno",
   "pi_last_name": "Vasconcelos",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Nuno M Vasconcelos",
   "pi_email_addr": "nuno@ece.ucsd.edu",
   "nsf_id": "000104017",
   "pi_start_date": "2019-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Dr",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930407",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 750000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>This project addressed the design of deep learning systems for computer vision and robotics. Many contributions resulted from the prject, in the areas of low-complexity networks, domain adaptation, long-tailed recognition, explainable AI, robust deep learning systems, multimodal deep learning systems, classification, object dection, and modeling of visual relationships.</span></p>\n<p>Image 1 illustrates a contribution of the project in the area of domain adaptation. Deep learning systems are known to be sensitive to domain shift, i.e. their perfornance degrades when deployed on a type of images (say cartoons)m different from that in which they were trained (say real images). While there has been substantial research to address this problem, most approaches consider only adaptation to a single new domain. The project introduced one of the first appraoches for the adaptation to multiple domains illustrated by the different types of images in the figure. In this way, an AI system trained to recognize real images of birds can generalize to different types of graphical renditions of these birds, from cartoons to hand paintings or clipart. This research has application in many domains of AI, e.g. smart cars that must generalize across countries or weather conditions and robots that must be able to operate in different types of environments.&nbsp;</p>\n<p>Image 2 illustrates a second contribution, which simultaneously addresses the problems of learning visual relations and long-tailed distributions. While much progress has been achieved in problems like object detection and recognition, AI systems must also learn to detect and recognize visual relations. In this work, we have address the problem of detecting subject-predicate-object relations in an image. In the example of the figure, a deep learning system identifies the relation robot-kicking-ball. In the literature, significant effort has been recently devoted to modeling visual relations.&nbsp;This has mostly addressed the design of neural network architectures, typically by adding parameters and increasing model complexity. However, visual relation learning is a long-tailed problem, due to the combinatorial nature of joint reasoning about groups of objects. Increasing model complexity is, in general, ill-suited for long-tailed problems due to their tendency to overfit. In this work, we explored an alternative hypothesis, denoted the Devil is in the Tails. Under this hypothesis, better performance is achieved by keeping the model simple but improving its ability to cope with long-tailed distributions.&nbsp;To test this hypothesis, we devised a new approach for training visual relationships models, which is inspired by state-of-the-art long-tailed recognition literature. This is based on an iterative decoupled training scheme, denoted Decoupled Training for Devil in the Tails (DT2), that captures the interplay between the long-tailed entity and predicate distributions of visual relations. Results show that, with an extremely simple architecture, DT2 significantly outperforms much more complex state-of-the-art methods on scene graph generation tasks. These are networks that produce a graph whose nodes are the objects in the scene and whose edges are their relationships.</p>\n<p>Image 3 illustrates a contribution of the project in the area of learning multimodal image representations.&nbsp; This work&nbsp;developed a self-supervised learning approach to learn simulataneous representations from video and audio. This means that the neural network does not have to be trained with labeled examples, which are expensive to collect. Instead, the network uses the common patterns between the two modalities to learn to identify different concepts. For example, that while a dog and a wolf look similar, they can be differentiated by their different sounds. Or that while two dogs can have very different appearance, they are related by the similar sounds that they make. In this way, the network can learn a space where different looking dogs are considered similar, while dogs and wolves are farther appart. The method uses contrastive learning for cross-modal discrimination of video from audio and vice-versa. We showed that optimizing for cross-modal discrimination, rather than withinmodal discrimination, is important to learn good representations from video and audio. With this simple but powerful insight, our method achieved highly competitive performance when finetuned on action recognition tasks.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/10/2023<br>\n\t\t\t\t\tModified by: Nuno&nbsp;M&nbsp;Vasconcelos</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336301512_drt--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336301512_drt--rgov-800width.jpg\" title=\"Fig 1: Multi-source Domain  Adaptation\"><img src=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336301512_drt--rgov-66x44.jpg\" alt=\"Fig 1: Multi-source Domain  Adaptation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A multi-source domain adaptation algorithm adapts a neural network trained with a certain type of images to several other types.</div>\n<div class=\"imageCredit\">Nuno Vasconcelos</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Nuno&nbsp;M&nbsp;Vasconcelos</div>\n<div class=\"imageTitle\">Fig 1: Multi-source Domain  Adaptation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336418338_dt2-acbs--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336418338_dt2-acbs--rgov-800width.jpg\" title=\"Fig 2: Learning of Visual Relations\"><img src=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336418338_dt2-acbs--rgov-66x44.jpg\" alt=\"Fig 2: Learning of Visual Relations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A system that learns visual relations, such as that the image depicts a \"robot-kicking- a ball\"</div>\n<div class=\"imageCredit\">Nuno Vasconcelos</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Nuno&nbsp;M&nbsp;Vasconcelos</div>\n<div class=\"imageTitle\">Fig 2: Learning of Visual Relations</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336582510_avid_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336582510_avid_teaser--rgov-800width.jpg\" title=\"Fig 3: Learning audio-visual representations\"><img src=\"/por/images/Reports/POR/2023/1924937/1924937_10635848_1673336582510_avid_teaser--rgov-66x44.jpg\" alt=\"Fig 3: Learning audio-visual representations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An algorithm to train deep learning systems without supervision, by identifying common patterns between audio and video</div>\n<div class=\"imageCredit\">Nuno Vasconcelos</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Nuno&nbsp;M&nbsp;Vasconcelos</div>\n<div class=\"imageTitle\">Fig 3: Learning audio-visual representations</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project addressed the design of deep learning systems for computer vision and robotics. Many contributions resulted from the prject, in the areas of low-complexity networks, domain adaptation, long-tailed recognition, explainable AI, robust deep learning systems, multimodal deep learning systems, classification, object dection, and modeling of visual relationships.\n\nImage 1 illustrates a contribution of the project in the area of domain adaptation. Deep learning systems are known to be sensitive to domain shift, i.e. their perfornance degrades when deployed on a type of images (say cartoons)m different from that in which they were trained (say real images). While there has been substantial research to address this problem, most approaches consider only adaptation to a single new domain. The project introduced one of the first appraoches for the adaptation to multiple domains illustrated by the different types of images in the figure. In this way, an AI system trained to recognize real images of birds can generalize to different types of graphical renditions of these birds, from cartoons to hand paintings or clipart. This research has application in many domains of AI, e.g. smart cars that must generalize across countries or weather conditions and robots that must be able to operate in different types of environments. \n\nImage 2 illustrates a second contribution, which simultaneously addresses the problems of learning visual relations and long-tailed distributions. While much progress has been achieved in problems like object detection and recognition, AI systems must also learn to detect and recognize visual relations. In this work, we have address the problem of detecting subject-predicate-object relations in an image. In the example of the figure, a deep learning system identifies the relation robot-kicking-ball. In the literature, significant effort has been recently devoted to modeling visual relations. This has mostly addressed the design of neural network architectures, typically by adding parameters and increasing model complexity. However, visual relation learning is a long-tailed problem, due to the combinatorial nature of joint reasoning about groups of objects. Increasing model complexity is, in general, ill-suited for long-tailed problems due to their tendency to overfit. In this work, we explored an alternative hypothesis, denoted the Devil is in the Tails. Under this hypothesis, better performance is achieved by keeping the model simple but improving its ability to cope with long-tailed distributions. To test this hypothesis, we devised a new approach for training visual relationships models, which is inspired by state-of-the-art long-tailed recognition literature. This is based on an iterative decoupled training scheme, denoted Decoupled Training for Devil in the Tails (DT2), that captures the interplay between the long-tailed entity and predicate distributions of visual relations. Results show that, with an extremely simple architecture, DT2 significantly outperforms much more complex state-of-the-art methods on scene graph generation tasks. These are networks that produce a graph whose nodes are the objects in the scene and whose edges are their relationships.\n\nImage 3 illustrates a contribution of the project in the area of learning multimodal image representations.  This work developed a self-supervised learning approach to learn simulataneous representations from video and audio. This means that the neural network does not have to be trained with labeled examples, which are expensive to collect. Instead, the network uses the common patterns between the two modalities to learn to identify different concepts. For example, that while a dog and a wolf look similar, they can be differentiated by their different sounds. Or that while two dogs can have very different appearance, they are related by the similar sounds that they make. In this way, the network can learn a space where different looking dogs are considered similar, while dogs and wolves are farther appart. The method uses contrastive learning for cross-modal discrimination of video from audio and vice-versa. We showed that optimizing for cross-modal discrimination, rather than withinmodal discrimination, is important to learn good representations from video and audio. With this simple but powerful insight, our method achieved highly competitive performance when finetuned on action recognition tasks. \n\n \n\n\t\t\t\t\tLast Modified: 01/10/2023\n\n\t\t\t\t\tSubmitted by: Nuno M Vasconcelos"
 }
}