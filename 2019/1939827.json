{
 "awd_id": "1939827",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Using Imperfect Predictions to Make Good Decisions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 301255.0,
 "awd_amount": 301255.0,
 "awd_min_amd_letter_date": "2019-07-30",
 "awd_max_amd_letter_date": "2020-05-06",
 "awd_abstract_narration": "As humans and other animals navigate the world they demonstrate remarkable flexibility in encountering unfamiliar systems, spaces and phenomena, learning to make predictions about how they will behave, and making good decisions based on those predictions. Crucial to this ability is the fact that one does not need to make perfectly accurate or fully detailed predictions to make good decisions. Though, due to our natural limitations, our predictions about the future are necessarily flawed, they are nevertheless sufficiently useful to make reasonable decisions. For artificial agents, in contrast, imperfect predictions often lead to catastrophic failures in decision making. Many existing approaches fundamentally assume that the agent will eventually learn to make perfect predictions and make perfect decisions, which is unreasonable in sufficiently rich, complex environments. This work considers the problem of developing artificial agents that are more aware of and more robust to their own limitations. Agents that can more robustly and flexibly learn from experience in truly complex environments have the potential to impact nearly any application in which decisions are made over time, for instance autonomous robots/vehicles, personal assistants, and medical/legal decision support. Furthermore, as the project will be undertaken at an undergraduate-only liberal arts college, undergraduate researchers will play an integral role in the work. The PI will also build on the strength of the liberal arts setting to enhance instruction of key discipline-specific research and writing skills throughout the Computer Science curriculum. Explicit development of these skills will not only improve students' preparation for a wide variety of career paths (including basic research) but is also aligned with best practices for broadening participation in the discipline. \r\n\r\nThis project studies model-based reinforcement learning (MBRL) under the assumption that the agent has fundamental limitations that prevent it from learning a perfect model or from producing optimal plans. The central hypothesis is that in this context the MBRL problem cannot be decomposed into separate model-learning and planning problems, each treating the other as an idealized black box. Rather the optimization process for each component must be aware of its role in the overall architecture and of the limitations of its partner. One key aim of the work is to derive novel measures of model quality that are more tightly related to the true objective of control performance than standard measures of one-step prediction accuracy adapted from supervised learning settings. Another is to investigate how model learning objectives/algorithms can be adapted to account for the limitations of the specific planner that will use the model. Further, control algorithms will be investigated that can make effective use of models of non-homogeneous quality by mediating between model-based and model-free knowledge. The ultimate goal is to integrate these principles into novel MBRL agents that are significantly more robust to limitations in the model class and/or planner and are able to succeed in environments that are too complex and high-dimensional to be modeled or solved exactly.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Erin",
   "pi_last_name": "Talvitie",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Erin J Talvitie",
   "pi_email_addr": "erik.talvitie@fandm.edu",
   "nsf_id": "000700376",
   "pi_start_date": "2019-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvey Mudd College",
  "inst_street_address": "301 PLATT BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "CLAREMONT",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9096218121",
  "inst_zip_code": "917115901",
  "inst_country_name": "United States",
  "cong_dist_code": "28",
  "st_cong_dist_code": "CA28",
  "org_lgl_bus_name": "HARVEY MUDD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "C76JKA5JY2B3"
 },
 "perf_inst": {
  "perf_inst_name": "Harvey Mudd College",
  "perf_str_addr": "301 Platt Boulevard 91711-5901",
  "perf_city_name": "Claremont",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "917115901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "28",
  "perf_st_cong_dist": "CA28",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 24963.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 91343.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 92092.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 92857.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In the reinforcement learning (RL) problem a decision-making agent receives a rewards that give the agent feedback on its behavior. The agent's goal is to learn to behavior that obtains the most reward. RL research is active in many application areas including autonomous vehicles, healthcare, and natural language processing. Advances in RL have the potential to impact nearly any application in which decisions are made over time, but there are barriers to the adoption of RL algorithms, which often require a great deal of experience and may take unsafe actions. Model-based RL (MBRL), in which the agent learns a predictive model of the environment and uses it to improve its decision-making (i.e. planning), may mitigate these concerns. Though they have the potential to require far less real experience, MBRL approaches are often brittle; even small flaws in the predictive model can cause catastrophic failures in decision-making. This project's aim was to enable MBRL algorithms that are robust to agent limitations and that can succeed in environments too complex to be modeled exactly.</p>\n<p><strong>New Theoretical Findings</strong></p>\n<p>The project studied the properties that make a model most useful for planning. Novel theoretical results emphasize the importance of multi-step accuracy, the accuracy of the model as it predicts outcomes multiple steps into the future, as opposed to the more typically considered one-step accuracy. Further, it was demonstrated that two models with the same one-step accuracy can have wildly differing degrees of multi-step accuracy; one such model may be useful for improving the agent's behavior and one may be catastrophically misleading. These theoretical results enabled the development of a novel MBRL algorithm focusing on improving the model's multi-step accuracy that offers theoretical performance guarantees and learns a model that is useful for planning despite its flaws (see Figure 1). A paper reporting these findings won the best paper award at&nbsp;<em>The Third International Conference on Reinforcement Learning and Decision Making</em>&nbsp;(RLDM 2017).</p>\n<p><strong>Selective Planning</strong></p>\n<p>The above emphasizes the importance of multi-step accuracy but, no matter how we measure accuracy, it is possible that the model will be insufficient to support planning. Furthermore, the model's accuracy level may differ over time and in different situations; there may be aspects of the environment that are difficult to predict and other aspects that are simple. For this reason, it is critical that the agent's learning process make use of the model only when it is reliable. The PI collaborated with colleagues from the University of Alberta to demonstrate that it is possible to measure quantities that are informative about the model's accuracy and to use this signal to selectively use the model when it is most reliable. The resulting algorithm can mitigate the chances of catastrophic failure due to model error (see Figure 2). In the future, techniques such as this one could be used to trade off between multiple models that each have different strengths and weaknesses.&nbsp;</p>\n<p><strong>Object-Based Models</strong></p>\n<p>In a step toward applying the insights described above to complex, high-dimensional environments, another aspect of the project focused on learning predictive models at the level of objects and their interactions/relationships rather than the more common focus on predicting low-level quantities, such as the color of pixels in an image. Object-based models have the potential to be more efficient to learn, more robust to model error, and more amenable to uncertainty estimation (necessary for selective planning). A novel approach to this problem, specifically focused on the needs of MBRL relating to efficiency and uncertainty estimation, is described in a paper currently under review.</p>\n<p><strong>Writing and Research in Liberal Arts Computer Science</strong></p>\n<p>The PI developed a sequence of assignments for an undergraduate introduction to AI course that scaffolds students' development of discipline-specific writing and research skills. Students work on open-ended problems and express their ideas and findings in a variety of genres including informal e-mails, technical memoranda, grant proposals, and scholarly reviews. The assignments culminate in a final project where students experience a small-scale version of the full basic research process, from project design to peer review and reporting findings. The emphasized skills are foundational to many career paths, including basic research. Further, research experiences are known to be important for broadening participation in the discipline. The assignments have been made publicly available for adoption by other instructors as a model assignment at Educational Advances in Artificial Intelligence</p>\n<p><strong>Other Impacts</strong></p>\n<p>The project has provided many research opportunities for undergraduate students. In all, 16 undergraduate students worked with the PI on this project and 6 of them so far have pursued related post-graduate studies. Several of these students are authors on a paper currently under review and several more are anticipated to be authors on work not yet submitted. The PI also collaborated with 4 graduate students at other institutions, offering additional opportunities for mentorship and professional development.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/27/2023<br>\n\t\t\t\t\tModified by: Erin&nbsp;J&nbsp;Talvitie</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1939827/1939827_10410737_1698382665454_shooter--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1939827/1939827_10410737_1698382665454_shooter--rgov-800width.jpg\" title=\"Multi-step Accuracy\"><img src=\"/por/images/Reports/POR/2023/1939827/1939827_10410737_1698382665454_shooter--rgov-66x44.jpg\" alt=\"Multi-step Accuracy\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(a) Top: a simple video game example. Middle: predictions using a standard approach. An initial error propagates. Bottom: predictions using our approach. It makes the same initial error, but recovers. (b) Planning performance. The standard approach fails. Our approach succeeds despite model flaws.</div>\n<div class=\"imageCredit\">Erin J. Talvitie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Erin&nbsp;J&nbsp;Talvitie</div>\n<div class=\"imageTitle\">Multi-step Accuracy</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1939827/1939827_10410737_1698382824753_selective--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1939827/1939827_10410737_1698382824753_selective--rgov-800width.jpg\" title=\"Selective Planning\"><img src=\"/por/images/Reports/POR/2023/1939827/1939827_10410737_1698382824753_selective--rgov-66x44.jpg\" alt=\"Selective Planning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(a) Planning with a sufficient model. Selective planning outperforms standard planning, because the model is inaccurate at first. (b) Planning with an insufficient model. Standard planning fails. Selective planning still performs well.</div>\n<div class=\"imageCredit\">Erin J. Talvitie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Erin&nbsp;J&nbsp;Talvitie</div>\n<div class=\"imageTitle\">Selective Planning</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn the reinforcement learning (RL) problem a decision-making agent receives a rewards that give the agent feedback on its behavior. The agent's goal is to learn to behavior that obtains the most reward. RL research is active in many application areas including autonomous vehicles, healthcare, and natural language processing. Advances in RL have the potential to impact nearly any application in which decisions are made over time, but there are barriers to the adoption of RL algorithms, which often require a great deal of experience and may take unsafe actions. Model-based RL (MBRL), in which the agent learns a predictive model of the environment and uses it to improve its decision-making (i.e. planning), may mitigate these concerns. Though they have the potential to require far less real experience, MBRL approaches are often brittle; even small flaws in the predictive model can cause catastrophic failures in decision-making. This project's aim was to enable MBRL algorithms that are robust to agent limitations and that can succeed in environments too complex to be modeled exactly.\n\nNew Theoretical Findings\n\nThe project studied the properties that make a model most useful for planning. Novel theoretical results emphasize the importance of multi-step accuracy, the accuracy of the model as it predicts outcomes multiple steps into the future, as opposed to the more typically considered one-step accuracy. Further, it was demonstrated that two models with the same one-step accuracy can have wildly differing degrees of multi-step accuracy; one such model may be useful for improving the agent's behavior and one may be catastrophically misleading. These theoretical results enabled the development of a novel MBRL algorithm focusing on improving the model's multi-step accuracy that offers theoretical performance guarantees and learns a model that is useful for planning despite its flaws (see Figure 1). A paper reporting these findings won the best paper award at The Third International Conference on Reinforcement Learning and Decision Making (RLDM 2017).\n\nSelective Planning\n\nThe above emphasizes the importance of multi-step accuracy but, no matter how we measure accuracy, it is possible that the model will be insufficient to support planning. Furthermore, the model's accuracy level may differ over time and in different situations; there may be aspects of the environment that are difficult to predict and other aspects that are simple. For this reason, it is critical that the agent's learning process make use of the model only when it is reliable. The PI collaborated with colleagues from the University of Alberta to demonstrate that it is possible to measure quantities that are informative about the model's accuracy and to use this signal to selectively use the model when it is most reliable. The resulting algorithm can mitigate the chances of catastrophic failure due to model error (see Figure 2). In the future, techniques such as this one could be used to trade off between multiple models that each have different strengths and weaknesses. \n\nObject-Based Models\n\nIn a step toward applying the insights described above to complex, high-dimensional environments, another aspect of the project focused on learning predictive models at the level of objects and their interactions/relationships rather than the more common focus on predicting low-level quantities, such as the color of pixels in an image. Object-based models have the potential to be more efficient to learn, more robust to model error, and more amenable to uncertainty estimation (necessary for selective planning). A novel approach to this problem, specifically focused on the needs of MBRL relating to efficiency and uncertainty estimation, is described in a paper currently under review.\n\nWriting and Research in Liberal Arts Computer Science\n\nThe PI developed a sequence of assignments for an undergraduate introduction to AI course that scaffolds students' development of discipline-specific writing and research skills. Students work on open-ended problems and express their ideas and findings in a variety of genres including informal e-mails, technical memoranda, grant proposals, and scholarly reviews. The assignments culminate in a final project where students experience a small-scale version of the full basic research process, from project design to peer review and reporting findings. The emphasized skills are foundational to many career paths, including basic research. Further, research experiences are known to be important for broadening participation in the discipline. The assignments have been made publicly available for adoption by other instructors as a model assignment at Educational Advances in Artificial Intelligence\n\nOther Impacts\n\nThe project has provided many research opportunities for undergraduate students. In all, 16 undergraduate students worked with the PI on this project and 6 of them so far have pursued related post-graduate studies. Several of these students are authors on a paper currently under review and several more are anticipated to be authors on work not yet submitted. The PI also collaborated with 4 graduate students at other institutions, offering additional opportunities for mentorship and professional development.\n\n\t\t\t\t\tLast Modified: 10/27/2023\n\n\t\t\t\t\tSubmitted by: Erin J Talvitie"
 }
}