{
 "awd_id": "1856636",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CNS Core: Medium: Collaborative Research: Cross Layer File Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 423297.0,
 "awd_amount": 431297.0,
 "awd_min_amd_letter_date": "2019-07-19",
 "awd_max_amd_letter_date": "2022-09-12",
 "awd_abstract_narration": "Computer hardware designers are developing new technologies for permanent storage of user data, such as non-volatile memories (NVM) and solid-state disks (SSD). Relative to the storage technologies commonly used just a decade ago, these offer unprecedented performance, but at a cost that is also much higher per megabyte of storage. This work uses software to bridge that gap: to build file storage systems that can seamlessly span multiple types of storage technology, from NVM to SSD to traditional disk storage, to reliably store user data at low cost and high performance.\r\n\r\nA key innovation is to change the interface between applications and the operating system to streamline file system updates.  Applications ask the kernel for permission to log changes to the file system in per-application NVM. These changes can be applied with performance close to the hardware limits of NVM, often under a microsecond per update. In the background, the kernel copies these changes back to its own data region, providing the illusion that all applications are working on the same storage system. The kernel then copies the data to SSD and disk to make room for additional updates to NVM.\r\n\r\nIndustry is rushing to deploy these new technologies because of the advantages they bring, but new software techniques are needed to bring those advantages to real users.  As desktops and cloud computing systems are the default platform for all types of computing used by billions of people worldwide, there is the potential for widespread benefit.  In addition, computer science instruction must adapt to address the challenges and solutions to these new technologies, to enable students to thrive in this new environment.  Proposed lecture and project materials will prepare students for this multi-layer storage and file system future.\r\n\r\nThe primary type of data to be produced will be software artifacts and measurements of these software artifacts.  During the project, software source code and data measurements will be stored using an industry-standard version control system and backed up on multiple servers in different areas of the University of Texas Austin computer science department building.  After publication, copies of the source code and data measurements will be moved, along with copies of all published papers, to a permanent repository at https://www.cs.utexas.edu/~simon/strata/ which will remain in place for at least ten years after completion of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Anderson",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas E Anderson",
   "pi_email_addr": "tom@cs.washington.edu",
   "nsf_id": "000196821",
   "pi_start_date": "2019-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4330 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 205575.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 114793.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 110929.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ee6a0bd8-7fff-2902-eed7-b443fba2104c\">\n<p dir=\"ltr\"><span>New hardware technologies for computer data storage in data centers, such as persistent memory, low latency solid state storage devices, and ultra-high capacity hard disks pose new challenges and opportunities to the file systems built on top of those devices. Combined with improvements in hardware support for low latency data center network transfers, replicating data persistently across servers can be nearly as fast as storing data locally, with stronger availability guarantees in the presence of failures. Traditional file systems were designed to manage only a single device at a time. Although it is possible to stitch together per-device file storage into a cross-device and cross-machine storage layer, this sacrifices performance and opportunities for cross-layer optimization.&nbsp; This project explores the design of file systems to take full advantage of low latency and high performance storage and network hardware, by managing storage seamlessly across multiple layers and across multiple servers.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>Intellectual merit:</span><span><br /></span><span><br /></span><span>Strata: This project proposed a novel restructuring of responsibilities between the system call library linked in with each application, and the kernel resident file system. Persistent memory devices are nearly as low latency as local server memory, making the cost of transferring control into the operating system on every file operation prohibitively expensive.&nbsp; Essential to changing this design is to provide correct behavior when files are shared between applications and to protect file metadata from corruption by buggy application code. Inside each application, Strata puts its file system updates into a temporary, compact per-application operation log stored in persistent memory. Repeated updates to the same files, a common pattern in many data center applications, can be efficiently coalesced, reducing load on slower solid state and hard disk storage.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>Assise: This project demonstrated the benefit of combining low latency persistent memory with low latency kernel bypass network hardware.&nbsp; Assise improves standard file system performance by orders of magnitude, by directly copying the persistent application log in Strata to remote servers.&nbsp; This provides high performance fault tolerance, without kernel involvement, block amplification, or unnecessary coherence traffic.&nbsp; Unlike standard file system replication strategies, Assise provides near-instantaneous application fail-over onto a cache replica that mirrors an application's local file system cache in the replica's persistent memory, while still providing strong correctness guarantees.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>Bento: This project shows how to reduce the friction of technology transfer for projects like Strata and Assise by supporting the drop-in, dynamic replacement of Linux kernel file systems with new functionality without needing to reboot the kernel and with minimal performance penalty relative to a kernel-native file system. Both Strata and Assise have both a user-level and kernel-level component. Bento allows the kernel portion of the file system to be debugged at user-level and then moved into the kernel once its functionality has been demonstrated.&nbsp; Bento introduces a new file system shim that allows for dynamic instantiation of new kernel file systems without needing to reboot either the kernel or applications, while also protecting the rest of the kernel from errors introduced by the file system.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>Broader impact:</span><span><br /></span><span><br /></span><span>Data centers are vital to the digital ecosystem, supporting a wide array of computing needs for individuals, businesses, and large organizations alike. Reducing the overhead of storing data, such as the techniques developed by this project, have the potential to significantly reduce operational costs and environmental impact of data centers. As is true for reducing highway congestion, reducing file system overhead allows all other resources, such as servers and networks, to be used more efficiently.&nbsp; It also enables applications to more freely use storage where appropriate, reducing the window of vulnerability for losing user data.&nbsp; Reducing overhead also allows data center operators to achieve more with existing infrastructure, delay upgrading existing equipment or adding new data centers, and lowering energy use for ongoing operations.</span><span><br /></span><span><br /></span><span>Furthermore, this project contributed to the academic and professional development of one Ph.D., two masters students, and two undergraduate students at the University of Washington, all of whom have graduated and taken positions in industry.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span></p>\n<br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/28/2024<br>\nModified by: Thomas&nbsp;E&nbsp;Anderson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nNew hardware technologies for computer data storage in data centers, such as persistent memory, low latency solid state storage devices, and ultra-high capacity hard disks pose new challenges and opportunities to the file systems built on top of those devices. Combined with improvements in hardware support for low latency data center network transfers, replicating data persistently across servers can be nearly as fast as storing data locally, with stronger availability guarantees in the presence of failures. Traditional file systems were designed to manage only a single device at a time. Although it is possible to stitch together per-device file storage into a cross-device and cross-machine storage layer, this sacrifices performance and opportunities for cross-layer optimization. This project explores the design of file systems to take full advantage of low latency and high performance storage and network hardware, by managing storage seamlessly across multiple layers and across multiple servers.\n\n\n\n\n\nIntellectual merit:\n\nStrata: This project proposed a novel restructuring of responsibilities between the system call library linked in with each application, and the kernel resident file system. Persistent memory devices are nearly as low latency as local server memory, making the cost of transferring control into the operating system on every file operation prohibitively expensive. Essential to changing this design is to provide correct behavior when files are shared between applications and to protect file metadata from corruption by buggy application code. Inside each application, Strata puts its file system updates into a temporary, compact per-application operation log stored in persistent memory. Repeated updates to the same files, a common pattern in many data center applications, can be efficiently coalesced, reducing load on slower solid state and hard disk storage.\n\n\n\n\n\nAssise: This project demonstrated the benefit of combining low latency persistent memory with low latency kernel bypass network hardware. Assise improves standard file system performance by orders of magnitude, by directly copying the persistent application log in Strata to remote servers. This provides high performance fault tolerance, without kernel involvement, block amplification, or unnecessary coherence traffic. Unlike standard file system replication strategies, Assise provides near-instantaneous application fail-over onto a cache replica that mirrors an application's local file system cache in the replica's persistent memory, while still providing strong correctness guarantees.\n\n\n\n\n\nBento: This project shows how to reduce the friction of technology transfer for projects like Strata and Assise by supporting the drop-in, dynamic replacement of Linux kernel file systems with new functionality without needing to reboot the kernel and with minimal performance penalty relative to a kernel-native file system. Both Strata and Assise have both a user-level and kernel-level component. Bento allows the kernel portion of the file system to be debugged at user-level and then moved into the kernel once its functionality has been demonstrated. Bento introduces a new file system shim that allows for dynamic instantiation of new kernel file systems without needing to reboot either the kernel or applications, while also protecting the rest of the kernel from errors introduced by the file system.\n\n\n\n\n\n\n\n\nBroader impact:\n\nData centers are vital to the digital ecosystem, supporting a wide array of computing needs for individuals, businesses, and large organizations alike. Reducing the overhead of storing data, such as the techniques developed by this project, have the potential to significantly reduce operational costs and environmental impact of data centers. As is true for reducing highway congestion, reducing file system overhead allows all other resources, such as servers and networks, to be used more efficiently. It also enables applications to more freely use storage where appropriate, reducing the window of vulnerability for losing user data. Reducing overhead also allows data center operators to achieve more with existing infrastructure, delay upgrading existing equipment or adding new data centers, and lowering energy use for ongoing operations.\n\nFurthermore, this project contributed to the academic and professional development of one Ph.D., two masters students, and two undergraduate students at the University of Washington, all of whom have graduated and taken positions in industry.\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 04/28/2024\n\n\t\t\t\t\tSubmitted by: ThomasEAnderson\n"
 }
}