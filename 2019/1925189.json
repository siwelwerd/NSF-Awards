{
 "awd_id": "1925189",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI:FND: Unifying standard physics-based control with learning-based perception and action to enable safe and agile object manipulation using unmanned aerial vehicles",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032927360",
 "po_email": "jenlin@nsf.gov",
 "po_sign_block_name": "Jenshan Lin",
 "awd_eff_date": "2019-09-15",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 749639.0,
 "awd_amount": 749639.0,
 "awd_min_amd_letter_date": "2019-08-27",
 "awd_max_amd_letter_date": "2019-08-27",
 "awd_abstract_narration": "Flying robots capable of object manipulation will enable new applications such as load pickup and delivery, infrastructure inspection and repair, agricultural crop management and harvesting. Currently though, aerial vehicles are limited in their agility and robustness when in close contact with their surroundings. More specifically, controlling aerial robots to interact with the natural environment requires complex models for inferring object dynamics in real-time through shape and appearance, dealing with contact and compliance, relying on complex perceptual cues such as occlusions or shadows, while at the same time ensuring safety and reliability. Currently, standard algorithms for robotic perception and control are not sufficient for such tasks. While machine learning techniques have proven powerful for vision-based perception and more recently for control in simple environments, current learning techniques are not directly suitable for agile autonomous vehicles where safety is critical and failed actions can be fatal for the robot and humans around it. To overcome these challenges, this project  proposes a framework that combines standard control methods with learning-based perception and action in an integrated framework equipped with formal high-confidence guarantees on performance. The proposed methodology aims to enable autonomous vehicles to accomplish tasks that are currently impossible or infeasible to achieve with standard methods. \r\n\r\nThe project will develop computational theory and algorithms that combine standard, i.e. physics and logic-based, control methods with learning-based control, implement a software framework and apply it to aerial manipulation tasks. More specifically, a fully differentiable framework will be developed that integrates components with known dynamics based on classical physical state representation and components that adapt to a given task through a learned implicit state representation that captures rich inertial and visual sensing. Then, a methodology for robust policy optimization with safety certificates will be developed based on high-fidelity stochastic models learned from robot data and then used to compute action policies in simulation using learned synthetic sensor models. The policies can be equipped with high-confidence formal bounds on performance and safety, which are validated and adapted in the real world. As a result, the robotic system can operate efficiently with guarantees on performance and safety. Finally, a fault-tolerant autonomy software framework will be implemented and the algorithms validated using three applications of aerial manipulation: object pick-up and transport in cluttered environments; remote sensor placement and infrastructure inspection; agricultural crop sampling and management. The proposed theory and methods are generally applicable to any robotic system operating in challenging environments, beyond aerial vehicles.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marin",
   "pi_last_name": "Kobilarov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marin Kobilarov",
   "pi_email_addr": "marin@jhu.edu",
   "nsf_id": "000629795",
   "pi_start_date": "2019-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N Charles Str",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182681",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "093E",
   "pgm_ref_txt": "System fab/packaging & assembly"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 749639.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-6ea151ca-7fff-36a1-a9d6-ffade80c05a3\"> </span></p>\r\n<p dir=\"ltr\"><span>The project developed computational methods that enable robotics systems to handle uncertainty and operate more reliably in natural environments. The techniques were implemented in generalizable software and demonstrated in hardware in the context of aerial manipulation. The specific objective was to enable aerial vehicles to grasp objects and to interact with their surrounding through contact motivated by applications such as infrastructure inspection and maintenance, agriculture (e.g. fruit-picking, sampling, pruning), and indoor \"remote hand\" for home assistance (e.g. finding and delivering a small object such as a phone or eyeglasses to a person with limited mobility, or fetching a small object from a kitchen shelf).&nbsp;</span></p>\r\n<p dir=\"ltr\">&nbsp;</p>\r\n<p dir=\"ltr\"><span>&nbsp;</span>Specifically, the methods developed accounted for: 1) uncertainty in a robot&rsquo;s dynamical model due to contact or change in payload, which if not addressed could render the system unstable; 2) uncertainty in the sensors and methods used for the estimation of the pose of the robot and objects of interest, required to achieve precise manipulation control. In addition, reinforcement learning based on an &ldquo;implicit world-model&rdquo; (i.e. learning a forward dynamics model of the environment in a latent encoding based on raw sensor and motion data) was employed to achieve non-prehensile manipulation without the need for complex contact models and precise geometric perceptual representation. The specific demonstrations of the developed techniques include:</p>\r\n<ul>\r\n<li>adaptation to changes in a platform&rsquo;s system parameters and environment to be able to continue to operate in the presence of unmodeled dynamics.</li>\r\n</ul>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>reliable prehensile manipulation from a floating base platform in real-</span>world complex environments with a grasping success rate of at least 90%.</p>\r\n</li>\r\n</ul>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>non-prehensile manipulation of objects from a floating base platform&nbsp;</span>with unknown contact dynamics through visual observation.</p>\r\n</li>\r\n</ul>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/15/2025<br>\nModified by: Marin&nbsp;Kobilarov</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744724905062_aerial_timelapse_clutter--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744724905062_aerial_timelapse_clutter--rgov-800width.jpg\" title=\"Example aerial manipulation of an object among clutter\"><img src=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744724905062_aerial_timelapse_clutter--rgov-66x44.jpg\" alt=\"Example aerial manipulation of an object among clutter\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Aerial time lapse of a cluttered environment experiment. Pick location\r\non the right and place location on the left.</div>\n<div class=\"imageCredit\">Cora Dimmig</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marin&nbsp;Kobilarov\n<div class=\"imageTitle\">Example aerial manipulation of an object among clutter</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744725106628_aerial_manipulator_real_vs_sim--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744725106628_aerial_manipulator_real_vs_sim--rgov-800width.png\" title=\"Aerial manipulator real and simulated vehicle.\"><img src=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744725106628_aerial_manipulator_real_vs_sim--rgov-66x44.png\" alt=\"Aerial manipulator real and simulated vehicle.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Autonomous aerial platform in flight grasping a target object. A precise simulated version was employed for validation and reinforcement learning purposes.</div>\n<div class=\"imageCredit\">Cora Dimmig</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marin&nbsp;Kobilarov\n<div class=\"imageTitle\">Aerial manipulator real and simulated vehicle.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744725212678_aerial_manipulator_scenes--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744725212678_aerial_manipulator_scenes--rgov-800width.png\" title=\"aerial manipulator experiments\"><img src=\"/por/images/Reports/POR/2025/1925189/1925189_10638273_1744725212678_aerial_manipulator_scenes--rgov-66x44.png\" alt=\"aerial manipulator experiments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example pick-and-place experiments for cans in clutter and single bottles.</div>\n<div class=\"imageCredit\">Cora Dimmig</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Marin&nbsp;Kobilarov\n<div class=\"imageTitle\">aerial manipulator experiments</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThe project developed computational methods that enable robotics systems to handle uncertainty and operate more reliably in natural environments. The techniques were implemented in generalizable software and demonstrated in hardware in the context of aerial manipulation. The specific objective was to enable aerial vehicles to grasp objects and to interact with their surrounding through contact motivated by applications such as infrastructure inspection and maintenance, agriculture (e.g. fruit-picking, sampling, pruning), and indoor \"remote hand\" for home assistance (e.g. finding and delivering a small object such as a phone or eyeglasses to a person with limited mobility, or fetching a small object from a kitchen shelf).\r\n\n\n\r\n\n\nSpecifically, the methods developed accounted for: 1) uncertainty in a robots dynamical model due to contact or change in payload, which if not addressed could render the system unstable; 2) uncertainty in the sensors and methods used for the estimation of the pose of the robot and objects of interest, required to achieve precise manipulation control. In addition, reinforcement learning based on an implicit world-model (i.e. learning a forward dynamics model of the environment in a latent encoding based on raw sensor and motion data) was employed to achieve non-prehensile manipulation without the need for complex contact models and precise geometric perceptual representation. The specific demonstrations of the developed techniques include:\r\n\r\nadaptation to changes in a platforms system parameters and environment to be able to continue to operate in the presence of unmodeled dynamics.\r\n\r\n\r\n\r\n\n\nreliable prehensile manipulation from a floating base platform in real-world complex environments with a grasping success rate of at least 90%.\r\n\r\n\r\n\r\n\r\n\n\nnon-prehensile manipulation of objects from a floating base platformwith unknown contact dynamics through visual observation.\r\n\r\n\r\n\n\n\t\t\t\t\tLast Modified: 04/15/2025\n\n\t\t\t\t\tSubmitted by: MarinKobilarov\n"
 }
}