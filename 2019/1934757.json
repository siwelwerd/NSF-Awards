{
 "awd_id": "1934757",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Advancing Science with Accelerated Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927382",
 "po_email": "vlukin@nsf.gov",
 "po_sign_block_name": "Vyacheslav (Slava) Lukin",
 "awd_eff_date": "2019-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 600311.0,
 "awd_amount": 600311.0,
 "awd_min_amd_letter_date": "2019-09-15",
 "awd_max_amd_letter_date": "2023-06-26",
 "awd_abstract_narration": "In the next generation of big science experiments, the demands for computing resources are expected to outstrip the capabilities of existing computing infrastructure. In light of this, a radical rethinking of the cyberinfrastructure is needed to contend with these developments. With the onset of deep learning, parallelized processing architectures have emerged as a solution. Combined with deep learning algorithms, parallelized processing architectures, in particular, Field Programmable Gate Arrays (FPGAs) have been shown to give large speedups in computing when compared with conventional CPUs. This project aims to bring machine learning based accelerated computing with FPGAs into the scientific community by targeting two big-data physics experiments: the Large Hadron Collider (LHC) and the Laser Interferometer Gravitational-wave  Observatory (LIGO). This project will push the frontiers of deep learning at scale, demonstrating the versatility and scalability of these methods to accelerate and enable new physics in the big data era. This project serves the national interest, as stated by NSF's mission, by promoting the progress of science. The PIs and their collaborators will build upon their recent work to design and exploit state-of-the-art neural network models for real-time data analytics, reducing overall computing latency. This new computing paradigm aims to significantly increase the processing capability at the LHC and LIGO, leading to an increased scientific output of these devices and,  potentially, foundational discoveries. The students to be mentored and trained in this research will interact closely with industry partners, creating new career opportunities, and strengthening synergies between academia and industry. In addition to sharing algorithms with the community through open source repositories, the team will continue to educate the community regarding credit and citation of scientific software.\r\n\r\nIn this project, the PIs will build upon their recent work developing high quality deep learning algorithms for real-time data analytics of time-series and image datasets using Field Programmable Gate Arrays (FPGAs) to accelerate low-latency inference of machine learning algorithms. The team will develop machine learning based acceleration tools focusing on FPGAs to be used within LIGO and the LHC experiments. The team's immediate goal is to take benchmark examples of LHC high level trigger processing and LIGO gravitational wave processing and construct demonstrators in each scenario. For this benchmark, they aim to design and implement an FPGA based accelerator that can perform low latency gravitational wave identification and LHC event reconstruction.  Additionally, the PIs aim to add the capability of graph based neural network accelerators for FPGAs. The open source tools to be developed as part of these activities will be readily shared with LIGO, LHC, and LSST. The project will create an advisory group, including members of large and small projects,  members of the neutrino physics, multi-messenger astronomy community, industry partners, computer scientists, and computational biologists. This project aims to bring together representatives of the different communities that will benefit from and can contribute to this work. The PIs will organize deep learning workshops and boot camps to train students and researchers on how to use and contribute to the framework, creating a wide network of contributors and developers across key science missions.  This project is part of the National Science Foundation's Harnessing the Data Revolution (HDR) Big Idea activity.\r\n\r\nThis project is part of the National Science Foundation's Harnessing the Data Revolution Big Idea activity.  The effort is jointly funded by the Office of Advanced Cyberinfrastructure.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Eliu",
   "pi_last_name": "Huerta Escudero",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eliu Huerta Escudero",
   "pi_email_addr": "elihu@uchicago.edu",
   "nsf_id": "000704886",
   "pi_start_date": "2019-09-15",
   "pi_end_date": "2021-05-06"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhizhen",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhizhen Zhao",
   "pi_email_addr": "zhizhenz@illinois.edu",
   "nsf_id": "000744305",
   "pi_start_date": "2021-05-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Volodymyr",
   "pi_last_name": "Kindratenko",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Volodymyr V Kindratenko",
   "pi_email_addr": "kindrtnk@illinois.edu",
   "nsf_id": "000316832",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Neubauer",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Mark S Neubauer",
   "pi_email_addr": "msn@illinois.edu",
   "nsf_id": "000509237",
   "pi_start_date": "2019-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Zhizhen",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhizhen Zhao",
   "pi_email_addr": "zhizhenz@illinois.edu",
   "nsf_id": "000744305",
   "pi_start_date": "2019-09-15",
   "pi_end_date": "2021-05-06"
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "Board of Trustees of the University of Illinois",
  "perf_str_addr": "506 S Wright St",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "099y00",
   "pgm_ele_name": "HDR-Harnessing the Data Revolu"
  },
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 288826.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 311485.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Real-time algorithms are critical in the Large Hadron Collider (LHC) experiments at CERN and gravitational wave experiments, including the Laser Interferometer Gravitational-Wave Observatory (LIGO). The challenge for gravitational waves is to quickly identify events and alert the rest of the community in a process known as multi-messenger astronomy; these have proven to be some of the most elucidating astrophysical events ever observed. At the LHC, the data rates are so enormous that we can only save one collision in 40,000 to reconstruct further. As a result, we rely on a real-time system that can only perform a cursory analysis to find interesting events. This analysis leaves critical events behind. By improving our knowledge of particle interactions in real-time, we can recover these events. Our goal in this project is to strengthen the real-time pipelines with better and faster algorithms utilizing deep learning. Faster algorithms lead to lower latency gravitational wave alerts and more LHC collisions analyzed.</p>\r\n<p>Deep learning algorithms have taken on rapid adoption across many scientific domains due to their incredible ability to provide solutions to very complicated tasks. A significant reason for their effectiveness originates from the rise of heterogeneous computation, which exploits the parallelism of certain processors, particularly GPUs and FPGAs, to perform deep learning calculations. In this project, we developed an end-to-end infrastructure to enable a deep learning computational pipeline for real-time gravitational wave data and high energy physics.&nbsp; Our pipeline can take deep learning algorithms and optimally deploy them into the data acquisition with minimal effort. Moreover, we created new algorithms for these pipelines that will significantly improve the quality of science, allowing for the potential of a discovery.</p>\r\n<p>We developed a comprehensive system for processing gravitational wave data with GPUs. This system can read data from detectors, denoise it, and classify gravitational wave signals, providing low-latency alerts.&nbsp; The team developed AI models that forecast binary neutron star and black hole-neutron star mergers in advanced LIGO noise. These models, tested with O2 data, can predict gravitational wave signals up to tens of seconds before the collision of binary components. The models have been made open-source through the Data Science and Learning Hub at Argonne National Laboratory.</p>\r\n<p>We applied Graph Neural Networks (GNNs) to track charged particle trajectories at the CERN Large Hadron Collider (LHC), addressing the challenge of high interaction densities in the upcoming HL-LHC phase. The team developed an automated translation tool, integrated with hls4ml, to convert GNNs into FPGA firmware for efficient particle tracking. This work aims to incorporate GNNs into trigger applications at the LHC to enhance real-time tracking.</p>\r\n<p>We<strong> </strong>enhanced an FPGA-based deep learning compiler to support large networks across multiple FPGAs, providing ultra-low latency inference. This system now supports various models, including Convolutional Neural Networks (CNNs) and Recursive Neural Networks (RNNs), significantly improving performance for batch inference.</p>\r\n<p>The research on both LHC and LIGO is guiding the development of a new computing model integrating heterogeneous computing. This model supports the deployment of complex algorithms involving machine learning models for tasks like real-time detection in neutrino experiments and collider detector triggering. The new system enables faster, low-latency inference, boosting scientific discovery opportunities in high-energy physics (HEP) and astrophysics. We enhanced computational systems and algorithms for real-time, low-latency processing of large-scale scientific data, particularly for gravitational wave detection and particle tracking in high-energy physics. It combines AI models, FPGA acceleration, and machine learning techniques to achieve performance improvements that enable next-generation scientific experiments.</p>\r\n<p>Our work is leading the way toward the integration of real-time AI&nbsp; for scientific experiments. Through this effort, we have developed new concepts in AI deployment on processors and created the infrastructure needed for fast adoption across scientific domains. This grant has trained six undergraduate students, five Ph.D students, supported a postdoc, organized two workshops, developed tutorials and new course materials, and helped cultivate a community to deploy low latency, high-throughput, fast machine learning.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2024<br>\nModified by: Zhizhen&nbsp;Zhao</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nReal-time algorithms are critical in the Large Hadron Collider (LHC) experiments at CERN and gravitational wave experiments, including the Laser Interferometer Gravitational-Wave Observatory (LIGO). The challenge for gravitational waves is to quickly identify events and alert the rest of the community in a process known as multi-messenger astronomy; these have proven to be some of the most elucidating astrophysical events ever observed. At the LHC, the data rates are so enormous that we can only save one collision in 40,000 to reconstruct further. As a result, we rely on a real-time system that can only perform a cursory analysis to find interesting events. This analysis leaves critical events behind. By improving our knowledge of particle interactions in real-time, we can recover these events. Our goal in this project is to strengthen the real-time pipelines with better and faster algorithms utilizing deep learning. Faster algorithms lead to lower latency gravitational wave alerts and more LHC collisions analyzed.\r\n\n\nDeep learning algorithms have taken on rapid adoption across many scientific domains due to their incredible ability to provide solutions to very complicated tasks. A significant reason for their effectiveness originates from the rise of heterogeneous computation, which exploits the parallelism of certain processors, particularly GPUs and FPGAs, to perform deep learning calculations. In this project, we developed an end-to-end infrastructure to enable a deep learning computational pipeline for real-time gravitational wave data and high energy physics. Our pipeline can take deep learning algorithms and optimally deploy them into the data acquisition with minimal effort. Moreover, we created new algorithms for these pipelines that will significantly improve the quality of science, allowing for the potential of a discovery.\r\n\n\nWe developed a comprehensive system for processing gravitational wave data with GPUs. This system can read data from detectors, denoise it, and classify gravitational wave signals, providing low-latency alerts. The team developed AI models that forecast binary neutron star and black hole-neutron star mergers in advanced LIGO noise. These models, tested with O2 data, can predict gravitational wave signals up to tens of seconds before the collision of binary components. The models have been made open-source through the Data Science and Learning Hub at Argonne National Laboratory.\r\n\n\nWe applied Graph Neural Networks (GNNs) to track charged particle trajectories at the CERN Large Hadron Collider (LHC), addressing the challenge of high interaction densities in the upcoming HL-LHC phase. The team developed an automated translation tool, integrated with hls4ml, to convert GNNs into FPGA firmware for efficient particle tracking. This work aims to incorporate GNNs into trigger applications at the LHC to enhance real-time tracking.\r\n\n\nWe enhanced an FPGA-based deep learning compiler to support large networks across multiple FPGAs, providing ultra-low latency inference. This system now supports various models, including Convolutional Neural Networks (CNNs) and Recursive Neural Networks (RNNs), significantly improving performance for batch inference.\r\n\n\nThe research on both LHC and LIGO is guiding the development of a new computing model integrating heterogeneous computing. This model supports the deployment of complex algorithms involving machine learning models for tasks like real-time detection in neutrino experiments and collider detector triggering. The new system enables faster, low-latency inference, boosting scientific discovery opportunities in high-energy physics (HEP) and astrophysics. We enhanced computational systems and algorithms for real-time, low-latency processing of large-scale scientific data, particularly for gravitational wave detection and particle tracking in high-energy physics. It combines AI models, FPGA acceleration, and machine learning techniques to achieve performance improvements that enable next-generation scientific experiments.\r\n\n\nOur work is leading the way toward the integration of real-time AI for scientific experiments. Through this effort, we have developed new concepts in AI deployment on processors and created the infrastructure needed for fast adoption across scientific domains. This grant has trained six undergraduate students, five Ph.D students, supported a postdoc, organized two workshops, developed tutorials and new course materials, and helped cultivate a community to deploy low latency, high-throughput, fast machine learning.\r\n\n\n\t\t\t\t\tLast Modified: 12/30/2024\n\n\t\t\t\t\tSubmitted by: ZhizhenZhao\n"
 }
}