{
 "awd_id": "1915801",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC: Early-Stage Interdisciplinary Collaboration: Modeling Memory Illusion for Predicting Trust in Online Information",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2019-06-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 316000.0,
 "awd_min_amd_letter_date": "2019-04-30",
 "awd_max_amd_letter_date": "2019-05-28",
 "awd_abstract_narration": "This project integrates advances in data science and key findings from psychological research to improve the prediction of trust in information on social media by modeling the psychological phenomenon known as the memory illusion. The memory illusion refers to memory errors that people make to remember information as an outcome of interpreting and making inferences from their past experience. This project will use social media data to examine the memory illusion with online information, and to understand how it is associated with people's trust in information on social media. Better understanding on the extent and impact of the memory illusion phenomenon using big data will inform machine-learning approaches to better measure trust in information with an additional human information-processing perspective, benefiting society by providing reliable online information, and increasing people's overall trust in information on social media.\r\n\r\nThis project pursues several research goals to advance the state-of-art of machine learning models to predict people's trust in information on social media. The first goal is to characterize the formation of associative inferences on Twitter information, and understand how it contributes to individuals' trust in tweets.  To advance this goal, the research will use big data and data-driven machine learning models. Based on the insights learned from big data, the second goal is to establish the causal relations between identified associative inferences and people's trust of social media information with laboratory and online user studies. The last goal is to model associative inferences into machine learning algorithms to improve the prediction of user trust in online information. The project will advance the state-of-the-art with regard to our understanding on people's trust in social media information in particular and human memory illusion in general. Through interdisciplinary socio-technical collaboration, the project will advance machine-learning models considering human information processing to improve the prediction of people's trust in information on social media, and improve understanding of human behavior using a big data approach to reveal relations among psychological phenomena on a scale that has not been possible with the smaller data sets collected in the laboratory. The interdisciplinary research using data science and psychological research will address theory-based research questions regarding the relationships of information veracity, trust, and information context. Students will participate in all phases of the research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aiping",
   "pi_last_name": "Xiong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aiping Xiong",
   "pi_email_addr": "axx29@psu.edu",
   "nsf_id": "000784636",
   "pi_start_date": "2019-04-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dongwon",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dongwon Lee",
   "pi_email_addr": "dongwon@psu.edu",
   "nsf_id": "000139387",
   "pi_start_date": "2019-04-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168026823",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "114Z",
   "pgm_ref_txt": "SaTC-CISE-SBE New Collabs"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 316000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project integrates key findings from psychological research and advances in data science to improve the prediction of trust in information on social media by modeling the psychological phenomenon known as the memory illusion. The memory illusion refers to memory errors that people make to remember information as an outcome of interpreting and making inferences from their past experiences.</p>\n<p>We also propose to use a data-science approach to understand the extent and impact of the memory illusion phenomenon in the wild to inform machine-learning models with an additional human information-processing perspective.<br /><br />For the overall project, we focus on the following research tasks:<br /><br />1. Establish the causal relations between identified associative inferences and people's trust of social media information with user studies;<br />2. Characterize the formation of associative inferences on Twitter information, and understand how it contributes to individuals' trust in tweets; and<br />3. Model associative inferences into machine learning algorithms to improve the prediction of user trust in online information.<br /><br /><br />In the 2021-2022 year, we focused on the following topics: <br /><br />1. Examine the effects of counterfactual explanation on correcting people's belief in fake news, as well as the most effective user comments<br />2. Investigate the effect of AI warnings on people's susceptibility to misinformation<br /><br />Major Activities:</p>\n<p><br />1. We propose to elucidate fact-checking predictions using counterfactual explanations to help people understand why a specific piece of news was fact-checked as fake. We frame counterfactual explanations for fact-checked fake news as a questions-answering (QA) task. We first generate questions from the false claim and retrieve potential answers from the evidence documents that are relevant. Then, we pick up the best QA pair by an entailment classifier. Finally, a counterfactual explanation is created using a matched QA pair, such that an opposite prediction will be made if the answer is removed (i.e., a piece of fake news becomes unsure or real). We conducted an online survey asking people to evaluate the explainability and quality of generated explanations using FEVER and a self-created Politifact-based dataset. The results suggest that our approach can achieve better performance compared to state-of-the-art methods.<br /><br /><br />2. Our previous studies on associative inference are based on human-subject experiments. To understand associative inference in the wild, we propose to collect fact-checked claims and cited sources from Snopes.com. The dataset includes 1200 fact-checked claims that are tagged with ``politics'' and published between Sep 28, 2020, and Dec 12, 2021. Our preliminary analysis showed that the real and fake claims revealed different associative inference patterns. For example, fewer sources were used to fact-check fake claims compared to real claims. Associative inferred fake news were based on pretty old resources that humans cannot recall the details.</p>\n<p>&nbsp;</p>\n<p>Publication:</p>\n<p>Dai, S-C, Hsu, Y-L, Xiong, A., &amp; Ku, L-W. (2022). Ask to know more: Generating counterfactual explanations for fake claims. In Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2022<br>\n\t\t\t\t\tModified by: Aiping&nbsp;Xiong</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project integrates key findings from psychological research and advances in data science to improve the prediction of trust in information on social media by modeling the psychological phenomenon known as the memory illusion. The memory illusion refers to memory errors that people make to remember information as an outcome of interpreting and making inferences from their past experiences.\n\nWe also propose to use a data-science approach to understand the extent and impact of the memory illusion phenomenon in the wild to inform machine-learning models with an additional human information-processing perspective.\n\nFor the overall project, we focus on the following research tasks:\n\n1. Establish the causal relations between identified associative inferences and people's trust of social media information with user studies;\n2. Characterize the formation of associative inferences on Twitter information, and understand how it contributes to individuals' trust in tweets; and\n3. Model associative inferences into machine learning algorithms to improve the prediction of user trust in online information.\n\n\nIn the 2021-2022 year, we focused on the following topics: \n\n1. Examine the effects of counterfactual explanation on correcting people's belief in fake news, as well as the most effective user comments\n2. Investigate the effect of AI warnings on people's susceptibility to misinformation\n\nMajor Activities:\n\n\n1. We propose to elucidate fact-checking predictions using counterfactual explanations to help people understand why a specific piece of news was fact-checked as fake. We frame counterfactual explanations for fact-checked fake news as a questions-answering (QA) task. We first generate questions from the false claim and retrieve potential answers from the evidence documents that are relevant. Then, we pick up the best QA pair by an entailment classifier. Finally, a counterfactual explanation is created using a matched QA pair, such that an opposite prediction will be made if the answer is removed (i.e., a piece of fake news becomes unsure or real). We conducted an online survey asking people to evaluate the explainability and quality of generated explanations using FEVER and a self-created Politifact-based dataset. The results suggest that our approach can achieve better performance compared to state-of-the-art methods.\n\n\n2. Our previous studies on associative inference are based on human-subject experiments. To understand associative inference in the wild, we propose to collect fact-checked claims and cited sources from Snopes.com. The dataset includes 1200 fact-checked claims that are tagged with ``politics'' and published between Sep 28, 2020, and Dec 12, 2021. Our preliminary analysis showed that the real and fake claims revealed different associative inference patterns. For example, fewer sources were used to fact-check fake claims compared to real claims. Associative inferred fake news were based on pretty old resources that humans cannot recall the details.\n\n \n\nPublication:\n\nDai, S-C, Hsu, Y-L, Xiong, A., &amp; Ku, L-W. (2022). Ask to know more: Generating counterfactual explanations for fake claims. In Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD).\n\n\t\t\t\t\tLast Modified: 09/01/2022\n\n\t\t\t\t\tSubmitted by: Aiping Xiong"
 }
}