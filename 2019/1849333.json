{
 "awd_id": "1849333",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "S&AS:FND:Viewer-Centric Spatial Reasoning and Learning for Safe Autonomous Navigation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2019-02-15",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 469997.0,
 "awd_amount": 469997.0,
 "awd_min_amd_letter_date": "2019-02-11",
 "awd_max_amd_letter_date": "2019-02-11",
 "awd_abstract_narration": "Autonomous navigation has emerged as one of contemporary society's most promising technological advances.  Robust, self-improving strategy for robot navigation will benefit several industries such as commercial and non-commercial transportation, large-scale infrastructure inspection, industrial warehousing, disaster response, and assistive robotics.  The main challenge to robust navigation lies in developing the ability to navigate unstructured, dynamic environments for which there may be insufficient data collected for training machine learning methods, and for which model-based reasoning is too complex. A purely learning-based strategy fails to have operational guarantees (i.e., collision avoidance is not guaranteed). The research proposes a mixed method solution whereby physics-based reasoning and machine learning work together to resolve the unstructured navigation problem. The combined approach will lead to a cognizant and reflective navigation pipeline whose performance improves with time. A central claim of this project is that the learning module will act as an efficient multi-hypothesis generator for potential navigation decisions, for which options can be processed, scored, and confirmed by the physics-based component.  The learning system will subsequently use these scores for online improvement. The net result will be mobile robots that are cognizant of their operation and adaptable to new information gained during task execution. \r\n\r\nThe research goal of this proposal is to derive a safe autonomous navigation framework for general settings through the use of a viewer-centric processing paradigm capable of leveraging learning and model driven methods to overcome the limitations of entirely object-centric approaches to navigation.  Appealing to Marr's framework for visual processing, the project investigates a viewer-centric approach to navigation.  By more tightly linking perceptual and planning representations through the viewer-centric approach, the new approach leverages measurements obtained during navigation to provide online assessment for improving performance and generating knowledge regarding navigation through unknown scenes. The project investigates the effect of a viewer-centric model representation for use in local planning, as well as the connection of such representations to reflective, experiential machine learning for improved performance that leverage the model-based planning subcomponent.  The research involves meeting the following objectives: 1) Confirming the robustness of a viewer-centric navigation framework combining model-based and deep learning-based approaches for safe navigation with cognizant and adaptive operation; 2) Demonstrating enhanced reasoning through scene-selective strategies that improve through experience; and 3) Extending the framework to dynamic scenes through learned models for the relative physics of motion, whereby moving objects are modeled in the viewer's frame of reference to detect dangerous relative motion profiles.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Patricio",
   "pi_last_name": "Vela",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Patricio A Vela",
   "pi_email_addr": "pvela@ece.gatech.edu",
   "nsf_id": "000495667",
   "pi_start_date": "2019-02-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "225 North Ave NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "039Y00",
   "pgm_ele_name": "S&AS - Smart & Autonomous Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "046Z",
   "pgm_ref_txt": "S&AS - Smart & Autonomous Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 469997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Though autonomous vehicles and robots have advanced significantly in the past decade, there is still uncertainty regarding the degree to which they may operate safely and in a collision-free manner. Current solutions have high computational requirements that may work for one particular application, but may not translate to other applications due to available compute hardware. This project explored an approach to navigation safety more aligned with cognitive models of perceptual processing. In these models, fully interpreted and reasoned 3D models of the environment are not necessary to make navigation decisions. Rather, partially processed information is often enough.&nbsp; By reproducing this conceptual framing using the idea of an egocentric collision map and using the notion of a navigation gap, which are open areas where it is safe to travel, the research project successfully derived a theoretical framework for proving locomotion safety of navigating robots. It was demonstrated to apply to ground robots or vehicles, to legged robots, and to aerial robots, while also demonstrating real-time computational performance, thereby showing a level of generalization not exhibited by prevailing strategies. Many of these methods aim to avoid collisions during the calculation of a navigation solution. However, the cognitive framework suggests that a better strategy is to first aim towards or attract to safe regions then while executing the aiming maneuver to avoid collision (e.g., after having already identified a navigation solution). Adjusting the ordering and prioritization of navigation constraints permits proving safety guarantees in a sequential manner.</p>\n<p>Additional studies examined the growing role of machine learning in robotics and found many of these solutions to be lacking.&nbsp; Furthermore, nothing could be proved regarding their operational outcomes.&nbsp; However, the navigation strategies derived during this project rely on setting various parameters or constants that impact their optimality or effectiveness in different environments. Applying reinforcement learning methods to dynamically update these parameters based on sensed scene information led to more consistent navigation outcomes across diverse environments. In essence, machine learning methods were shown to be capable of tuning or adjusting the parametric choices associated to navigation algorithms in an online manner so as to improve performance.&nbsp; The work identified a mechanism to gain from learnt navigation experiences while still leveraging the structured navigation framework with collision-free guarantees.</p>\n<p>Lastly, any autonomous vehicle or robot relies on sensory information to build out models of the local environment for collision avoidance.&nbsp; These models also assist with maneuver motion planning. However, the solutions employing traditional occupancy-based data structures and modeling approaches have high compute costs to assimilate streaming environment data.&nbsp; Furthermore, methods with safety guarantees rely on known models of the environment. The egocentric models derived during the project run in real-time across diverse compute platforms and permit on-the-fly generated environment models, thereby removing the main barrier to provable safety guarantees. These models employ machine learning methods with specialized optimization designs that have solution guarantees (in terms of their safety approximation and their ability to find a solution). In doing so, the derived product demonstrate how to modify machine learning techniques with poor to limited solution guarantees to one with strong solution guarantees, in a constructive manner. The resulting optimizations run in real-time and integrate well into existing navigation frameworks.</p>\n<p>Overall, the work has demonstrated that it is possible to use sensor derived information to build out and execute local, (guaranteed) collision-free motion plans in real-time. The project hypothesis that perception-based representations derived from cognitive visual-processing frameworks would provide favorable navigation solutions with strong guarantees was confirmed.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/04/2024<br>\nModified by: Patricio&nbsp;A&nbsp;Vela</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1849333/1849333_10591846_1722632004409_RobotDemonstrationStills--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1849333/1849333_10591846_1722632004409_RobotDemonstrationStills--rgov-800width.png\" title=\"Robot still frame of two test experiments with path overlays.\"><img src=\"/por/images/Reports/POR/2024/1849333/1849333_10591846_1722632004409_RobotDemonstrationStills--rgov-66x44.png\" alt=\"Robot still frame of two test experiments with path overlays.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Depiction of SaferGap navigation outcomes on a small mobile robot traversing two unknown environments.</div>\n<div class=\"imageCredit\">Patricio A. Vela, Shiyu Feng</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Patricio&nbsp;A&nbsp;Vela\n<div class=\"imageTitle\">Robot still frame of two test experiments with path overlays.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/1849333/1849333_10591846_1722631785115_GapMethods--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1849333/1849333_10591846_1722631785115_GapMethods--rgov-800width.png\" title=\"Gap methods for free-space determination\"><img src=\"/por/images/Reports/POR/2024/1849333/1849333_10591846_1722631785115_GapMethods--rgov-66x44.png\" alt=\"Gap methods for free-space determination\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Depicts several of the derived gap models for establishing a collision-free region for the robot to move within and permitting navigation between two objects.</div>\n<div class=\"imageCredit\">Patricio A. Vela, Shiyu Feng</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Patricio&nbsp;A&nbsp;Vela\n<div class=\"imageTitle\">Gap methods for free-space determination</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThough autonomous vehicles and robots have advanced significantly in the past decade, there is still uncertainty regarding the degree to which they may operate safely and in a collision-free manner. Current solutions have high computational requirements that may work for one particular application, but may not translate to other applications due to available compute hardware. This project explored an approach to navigation safety more aligned with cognitive models of perceptual processing. In these models, fully interpreted and reasoned 3D models of the environment are not necessary to make navigation decisions. Rather, partially processed information is often enough. By reproducing this conceptual framing using the idea of an egocentric collision map and using the notion of a navigation gap, which are open areas where it is safe to travel, the research project successfully derived a theoretical framework for proving locomotion safety of navigating robots. It was demonstrated to apply to ground robots or vehicles, to legged robots, and to aerial robots, while also demonstrating real-time computational performance, thereby showing a level of generalization not exhibited by prevailing strategies. Many of these methods aim to avoid collisions during the calculation of a navigation solution. However, the cognitive framework suggests that a better strategy is to first aim towards or attract to safe regions then while executing the aiming maneuver to avoid collision (e.g., after having already identified a navigation solution). Adjusting the ordering and prioritization of navigation constraints permits proving safety guarantees in a sequential manner.\n\n\nAdditional studies examined the growing role of machine learning in robotics and found many of these solutions to be lacking. Furthermore, nothing could be proved regarding their operational outcomes. However, the navigation strategies derived during this project rely on setting various parameters or constants that impact their optimality or effectiveness in different environments. Applying reinforcement learning methods to dynamically update these parameters based on sensed scene information led to more consistent navigation outcomes across diverse environments. In essence, machine learning methods were shown to be capable of tuning or adjusting the parametric choices associated to navigation algorithms in an online manner so as to improve performance. The work identified a mechanism to gain from learnt navigation experiences while still leveraging the structured navigation framework with collision-free guarantees.\n\n\nLastly, any autonomous vehicle or robot relies on sensory information to build out models of the local environment for collision avoidance. These models also assist with maneuver motion planning. However, the solutions employing traditional occupancy-based data structures and modeling approaches have high compute costs to assimilate streaming environment data. Furthermore, methods with safety guarantees rely on known models of the environment. The egocentric models derived during the project run in real-time across diverse compute platforms and permit on-the-fly generated environment models, thereby removing the main barrier to provable safety guarantees. These models employ machine learning methods with specialized optimization designs that have solution guarantees (in terms of their safety approximation and their ability to find a solution). In doing so, the derived product demonstrate how to modify machine learning techniques with poor to limited solution guarantees to one with strong solution guarantees, in a constructive manner. The resulting optimizations run in real-time and integrate well into existing navigation frameworks.\n\n\nOverall, the work has demonstrated that it is possible to use sensor derived information to build out and execute local, (guaranteed) collision-free motion plans in real-time. The project hypothesis that perception-based representations derived from cognitive visual-processing frameworks would provide favorable navigation solutions with strong guarantees was confirmed.\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 08/04/2024\n\n\t\t\t\t\tSubmitted by: PatricioAVela\n"
 }
}