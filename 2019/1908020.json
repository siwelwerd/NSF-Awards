{
 "awd_id": "1908020",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: SMALL: Clarifying Experimenter Bias by Identifying and Visualizing Experiment Bottlenecks",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jason Hallstrom",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 496893.0,
 "awd_amount": 496893.0,
 "awd_min_amd_letter_date": "2019-05-23",
 "awd_max_amd_letter_date": "2019-05-23",
 "awd_abstract_narration": "Running experiments to measure the performance of a system and comparing its performance to other works is a routine procedure of computer science research. However, an experimenter usually has a natural tendency to choose the experimental setting in favor of his/her own work.  This may introduce unfairness into the comparison. Instead of trying to eliminate such bias, which is difficult because human effort is unavoidable, this project tries to clarify such bias to readers by visualizing the experimental results so that a reader can understand the true intention of the experimenter.\r\n\r\nTo achieve this goal, this project observes that an experimenter usually introduces bias by tuning the bottleneck of his/her experiment. This project targets developing comprehensive, automatic, and efficient approaches to identify experiment bottlenecks and concise and informative approaches to present such bottlenecks to readers. To be concrete, this project proposes to use a wait-for graph and an interpretive cumulative distribution function (CDF) to identify and present bottlenecks related to throughput and latency respectively, which are the most widely used performance metrics.  It will further investigate how to scale these mechanisms to large and long-running systems.\r\n\r\nIf successful, this project has the potential for significant real-world impact, because first, performance analysis is a routine procedure in both industry and academia, and second, clarifying experimenter bias would allow our community to better evaluate a research work. This project will further provide materials to educate students in a variety of programs, including undergraduate and graduate operating systems courses in the Department of Computer Science and Engineering, new projects in the Department of Analytics, and new projects at the Ohio Hackathon (targeting undergraduate and high school students).\r\n\r\nAll versions of data, including program source code, papers, trace data, and other documentation will be maintained in a data repository at the Ohio State University using a versioning infrastructure, with periodic back up. The update of all data will be published at http://web.cse.ohio-state.edu/~yangwang/experimenter_bias.html. All data will be stored and maintained for at least five years after the completion of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yang Wang",
   "pi_email_addr": "wang.7564@osu.edu",
   "nsf_id": "000689394",
   "pi_start_date": "2019-05-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "395 Dreese Laboratories 2015 Nei",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101277",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 496893.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In both academia and industry, researchers often need to measure the performance of a system and compare the performance of different systems. This procedure, however, may easily introduce bias, since a researcher may choose experiment settings that are in favor of her system, sometimes unintentionally. Despite the community's long-lasting effort to build standardized benchmarks, this problem still persists since researchers can select among different benchmarks and further tune the parameters of the benchmark, the systems to be tested, the operating system, etc.</p>\n<p>This project tries to tackle this problem from multiple directions:</p>\n<p>1) It has carried out a study among both research prototypes and open-source software to understand the severity of this problem. The study does reveal that, in many cases, the improvement of one system over another may diminish or disappear under a different experiment setting.</p>\n<p>2) To clarify such bias, it proposes that a researcher should provide more details about the experiments in addition to the performance numbers. To be concrete, a researcher should analyze and report the bottleneck of her experiment, so that a reader can understand what the experiment really measures and whether it is meaningful to compare that to numbers reported by another paper. To facilitate this process, we have developed tools to identify and visualize throughput and latency bottlenecks.</p>\n<p>3) Another possible solution is to conduct experiments extensively to cover different settings, but this approach is often too expensive. To reduce cost, we study the feasibility of testing a subset of settings and use the result to predict the performance under other settings: if the prediction is accurate, we can gain the benefit of extensive evaluation with a lower cost. Our result is mixed: some systems have highly predictable performance and thus can benefit from this approach significantly; others are less predictable and require a large number of samples. Based on the results, we propose that our community should emphasize \"predictability\" in addition to \"reproducibility\",&nbsp; since high predictability is helpful to limit bias during experiments and unpredictability often indicates implementation problems in the system.</p>\n<p>We have disseminated results through publications in major system and databasse conferences (VLDB, CIDR, Eurosys, etc) and have published all source code and raw data on github. The project has helped the PI to develop materials for teaching concurrency control related topics in graduate-level courses and for the OSU's exploreCSR event, which targets promoting research experience among undergraduate students, particularly underrepresnted students. This project has contributed to the professional development of seven Ph.D. students (three of them have graduated) and four undergraduate students (all have decided to pursue a graduate degree in CS). Notably, two of our papers are primarily carried out by undergraduate students, and one undergraduate student has won the undergraduate research award in the department due to his contribution to this project.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/03/2023<br>\n\t\t\t\t\tModified by: Yang&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn both academia and industry, researchers often need to measure the performance of a system and compare the performance of different systems. This procedure, however, may easily introduce bias, since a researcher may choose experiment settings that are in favor of her system, sometimes unintentionally. Despite the community's long-lasting effort to build standardized benchmarks, this problem still persists since researchers can select among different benchmarks and further tune the parameters of the benchmark, the systems to be tested, the operating system, etc.\n\nThis project tries to tackle this problem from multiple directions:\n\n1) It has carried out a study among both research prototypes and open-source software to understand the severity of this problem. The study does reveal that, in many cases, the improvement of one system over another may diminish or disappear under a different experiment setting.\n\n2) To clarify such bias, it proposes that a researcher should provide more details about the experiments in addition to the performance numbers. To be concrete, a researcher should analyze and report the bottleneck of her experiment, so that a reader can understand what the experiment really measures and whether it is meaningful to compare that to numbers reported by another paper. To facilitate this process, we have developed tools to identify and visualize throughput and latency bottlenecks.\n\n3) Another possible solution is to conduct experiments extensively to cover different settings, but this approach is often too expensive. To reduce cost, we study the feasibility of testing a subset of settings and use the result to predict the performance under other settings: if the prediction is accurate, we can gain the benefit of extensive evaluation with a lower cost. Our result is mixed: some systems have highly predictable performance and thus can benefit from this approach significantly; others are less predictable and require a large number of samples. Based on the results, we propose that our community should emphasize \"predictability\" in addition to \"reproducibility\",  since high predictability is helpful to limit bias during experiments and unpredictability often indicates implementation problems in the system.\n\nWe have disseminated results through publications in major system and databasse conferences (VLDB, CIDR, Eurosys, etc) and have published all source code and raw data on github. The project has helped the PI to develop materials for teaching concurrency control related topics in graduate-level courses and for the OSU's exploreCSR event, which targets promoting research experience among undergraduate students, particularly underrepresnted students. This project has contributed to the professional development of seven Ph.D. students (three of them have graduated) and four undergraduate students (all have decided to pursue a graduate degree in CS). Notably, two of our papers are primarily carried out by undergraduate students, and one undergraduate student has won the undergraduate research award in the department due to his contribution to this project.\n\n \n\n\t\t\t\t\tLast Modified: 08/03/2023\n\n\t\t\t\t\tSubmitted by: Yang Wang"
 }
}