{
 "awd_id": "1941355",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Enhanced Pain Recognition Through Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei Ding",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2022-04-30",
 "tot_intn_awd_amt": 129300.0,
 "awd_amount": 129300.0,
 "awd_min_amd_letter_date": "2019-08-16",
 "awd_max_amd_letter_date": "2022-02-28",
 "awd_abstract_narration": "Pain is a distressing feeling often caused by intense or damaging stimuli and commonly defined as an unpleasant sensory and emotional experience associated with actual or potential tissue damage. In clinical medicine, pain is often regarded as a symptom of an underlying condition. Since pain is a complex and subjective phenomenon, objectively defining pain has been difficult. This project is designed to develop machine learning and computer algorithms for enhanced and objective pain recognition based on unique behavioral and neuronal activity patterns recorded in a novel animal pain model. The investigators will then apply and test the algorithmic tool to two local and systemic inflammatory conditions that are often associated with severe pain. Successful completion of this project will potentially lead to development of innovative pain assessment tools for both mechanistic exploration and future management of pain.\r\n\r\nTechnically, the project has two specific aims. The first one is to establish machine learning-based pain recognition algorithms built on unique behavioral and central neuronal activity patterns induced by pain. The investigative team has established a novel animal model of neuropathic pain (trigeminal neuralgia) that produces unique behavioral (e.g., orbital tightening and face grimace) and central neuronal activity patterns as documented by video recording and in vivo two-photon imaging, respectively. This model has generated wealth of high-quality data sets directly related to spontaneous pain and offers a rich source as training materials for machine learning using convolutional neural network, which will enable the team to develop computer algorithms for enhanced pain recognition. The second aim is to examine the pain recognition algorithms developed in Aim 1 by investigating its performance in animal models with clinical relevance. Specifically, the team will employ ankle joint arthritis and bacterial sepsis as models for local and systemic inflammatory conditions, respectively. Using conventional methods, spontaneous pain behaviors have not been reliably captured in these models. The team plans to prove that pain recognition algorithms based on convolutional neural network established in Aim 1 are powerful and versatile to reveal novel patterns indicative of spontaneous pain, despite vastly different underlying mechanisms of pain. The results will be highly valuable to inform future studies aiming at optimizing treatment regimens and improving long term outcomes of the diseases.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wei",
   "pi_last_name": "Chao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wei Chao",
   "pi_email_addr": "wchao@som.umaryland.edu",
   "nsf_id": "000805365",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Hu",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Peter F Hu",
   "pi_email_addr": "phu@som.umaryland.edu",
   "nsf_id": "000805605",
   "pi_start_date": "2019-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland at Baltimore",
  "inst_street_address": "220 ARCH ST OFC LEVEL2",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4107063559",
  "inst_zip_code": "212011531",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, BALTIMORE",
  "org_prnt_uei_num": "",
  "org_uei_num": "Z9CRZKD42ZT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland at Baltimore",
  "perf_str_addr": "660 W. Redwood Street",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212011541",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 129300.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Project Outcome Report (FINAL) &ndash; NSF Award# 1941355</p>\n<p>Project Title: EAGER: Enhanced Pain Recognition Through Machine Learning</p>\n<p>Recipient Organization: University of Maryland at Baltimore</p>\n<p>Project/Grant Period: January 1, 2020 &ndash; April 30, 2022</p>\n<p>PI: Wei Chao, Co-PI: Peter Hu</p>\n<p>&nbsp;</p>\n<p>Assessing pain intensity often relies on self-rating, which is subjective, may not be reliable, and requires a reasonable level of cognitive function of patients. Alternatively, one can recognize pain by observing subject&rsquo;s behavior, which may require human expertise and the evaluation may vary among different experts. It would be useful to develop pain evaluation tools that perceive pain level by observing patients&rsquo; behavior in an automatic way that does not rely on human decision. The aim of the study is to objectively and quantitatively analyze pain-related behavioral changes in preclinical models of neuropathic pain using computer vision, which consists of a set of algorithms that mimic the human vision system and allow computers to process and infer visual information.</p>\n<p>Two sets of behavior observation have been recorded in mouse models of trigeminal neuralgia (a prototypical neuropathic pain with orofacial pain and toothache): <span style=\"text-decoration: underline;\">a) food preference</span> and <span style=\"text-decoration: underline;\">b) mating behavior.</span> In the food preference study, a dish of soft food and a dish of solid food were both provided in the same mouse cage. Individual mouse of certain type of trigeminal neuralgia was observed in the cage after overnight starvation to observe their eating behavior. For the mating behavior study, an injured male mouse and a normal female mouse were place in a cage and observed up to 15 minutes at different time points post nerve injury. Computer vision techniques were then used to extract time series data of mouse movements in the videos. In the food preference study, by calculating the distance between the mouse body center and the centers of solid vs. soft food dish, we estimated the time that the mouse spent near each of the food plates. Within an observation period, we calculated the spatial distribution of a mouse and quantify its proximity to soft vs. solid food. From the heatmap, we could visually sense which food &ndash; solid vs. soft dish that the mouse stayed with (and preferred). For mating behavior study, to accurately track mice movement and distinguish male and female mice, we added different color dots to the mouse heads and distinguished male vs. female mice by their color marks. Similarly, by calculating the distance between two mice, we estimated if mice of opposite sex were actively chasing each other. To quantify the chasing movement and mating behavior of these mice, several features of computer modeling were created. With the detected locations of mice in each frame, we calculated the average moving speed and distance between two mice in each second for each mouse. From two consecutive seconds, we calculated cosine similarity to represent the mouse moving direction. Based on these features, we also created the features as total body area of two mice, entropy of each mouse moving direction, speed entropy of male mouse, speed entropy of female mouse and speed correlation. With those derived variables, we used a subset of clips to train a machine learning algorithm, which classifies each second as mice being mating or not. We also tested the learned model using the remaining clips.</p>\n<p>Using computer vision technique, we detected mouse movement and quantify their behavior patterns automatically. For the food preference experiments, we observed that mice with trigeminal neuralgia preferred soft food, compared with healthy control mice. For the mating behavior experiments, we found that the healthy control mice have more mating time and mating episode (detected by machine learning algorithms) than mice with trigeminal neuralgia.</p>\n<p>Using preclinical model of pain, this project demonstrated the power of computer vision with machine learning for automated behavior recognition. Eventually, quantifiable behavior patterns could be used for reliable and objective pain assessment.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/15/2022<br>\n\t\t\t\t\tModified by: Wei&nbsp;Chao</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1941355/1941355_10634943_1643569146845_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1941355/1941355_10634943_1643569146845_Figure1--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2022/1941355/1941355_10634943_1643569146845_Figure1--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1.Tracking mouse movement by color detection. (a) Recognize male and female mice. (b) Detect two mice body.</div>\n<div class=\"imageCredit\">W. Chao and P. Hu</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Chao</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1941355/1941355_10634943_1643569456467_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1941355/1941355_10634943_1643569456467_Figure2--rgov-800width.jpg\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2022/1941355/1941355_10634943_1643569456467_Figure2--rgov-66x44.jpg\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2. An example features plot of one video. 1st axes (top): distance of two mice; 2nd: speed of female mouse; 3rd: speed of male mouse; 4th and 5th: are speed correlation of different moving window; 6th: speed entropy of female mouse; 7th: peed entropy of male mouse; 8th: similarity of directio</div>\n<div class=\"imageCredit\">W. Chao</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Wei&nbsp;Chao</div>\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nProject Outcome Report (FINAL) &ndash; NSF Award# 1941355\n\nProject Title: EAGER: Enhanced Pain Recognition Through Machine Learning\n\nRecipient Organization: University of Maryland at Baltimore\n\nProject/Grant Period: January 1, 2020 &ndash; April 30, 2022\n\nPI: Wei Chao, Co-PI: Peter Hu\n\n \n\nAssessing pain intensity often relies on self-rating, which is subjective, may not be reliable, and requires a reasonable level of cognitive function of patients. Alternatively, one can recognize pain by observing subject\u2019s behavior, which may require human expertise and the evaluation may vary among different experts. It would be useful to develop pain evaluation tools that perceive pain level by observing patients\u2019 behavior in an automatic way that does not rely on human decision. The aim of the study is to objectively and quantitatively analyze pain-related behavioral changes in preclinical models of neuropathic pain using computer vision, which consists of a set of algorithms that mimic the human vision system and allow computers to process and infer visual information.\n\nTwo sets of behavior observation have been recorded in mouse models of trigeminal neuralgia (a prototypical neuropathic pain with orofacial pain and toothache): a) food preference and b) mating behavior. In the food preference study, a dish of soft food and a dish of solid food were both provided in the same mouse cage. Individual mouse of certain type of trigeminal neuralgia was observed in the cage after overnight starvation to observe their eating behavior. For the mating behavior study, an injured male mouse and a normal female mouse were place in a cage and observed up to 15 minutes at different time points post nerve injury. Computer vision techniques were then used to extract time series data of mouse movements in the videos. In the food preference study, by calculating the distance between the mouse body center and the centers of solid vs. soft food dish, we estimated the time that the mouse spent near each of the food plates. Within an observation period, we calculated the spatial distribution of a mouse and quantify its proximity to soft vs. solid food. From the heatmap, we could visually sense which food &ndash; solid vs. soft dish that the mouse stayed with (and preferred). For mating behavior study, to accurately track mice movement and distinguish male and female mice, we added different color dots to the mouse heads and distinguished male vs. female mice by their color marks. Similarly, by calculating the distance between two mice, we estimated if mice of opposite sex were actively chasing each other. To quantify the chasing movement and mating behavior of these mice, several features of computer modeling were created. With the detected locations of mice in each frame, we calculated the average moving speed and distance between two mice in each second for each mouse. From two consecutive seconds, we calculated cosine similarity to represent the mouse moving direction. Based on these features, we also created the features as total body area of two mice, entropy of each mouse moving direction, speed entropy of male mouse, speed entropy of female mouse and speed correlation. With those derived variables, we used a subset of clips to train a machine learning algorithm, which classifies each second as mice being mating or not. We also tested the learned model using the remaining clips.\n\nUsing computer vision technique, we detected mouse movement and quantify their behavior patterns automatically. For the food preference experiments, we observed that mice with trigeminal neuralgia preferred soft food, compared with healthy control mice. For the mating behavior experiments, we found that the healthy control mice have more mating time and mating episode (detected by machine learning algorithms) than mice with trigeminal neuralgia.\n\nUsing preclinical model of pain, this project demonstrated the power of computer vision with machine learning for automated behavior recognition. Eventually, quantifiable behavior patterns could be used for reliable and objective pain assessment.\n\n \n\n\t\t\t\t\tLast Modified: 08/15/2022\n\n\t\t\t\t\tSubmitted by: Wei Chao"
 }
}