{
 "awd_id": "1909930",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Improving Mobile Device Input for Users who are Blind or Low Vision",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 274000.0,
 "awd_amount": 290000.0,
 "awd_min_amd_letter_date": "2019-08-26",
 "awd_max_amd_letter_date": "2020-04-10",
 "awd_abstract_narration": "Smartphones are an essential part of everyday life. But for people with visual impairments, basic tasks like composing text messages or browsing the web can be prohibitively slow and difficult. The goal of this project is to develop accessible text entry methods that will enable people with visual impairments to enter text at rates comparable to sighted people. This project will design new algorithms and feedback methods for today's standard text entry approaches of tapping on individual keys, gesturing across keys, or dictating via speech. The project aims to help users avoid errors by enabling more accurate input via audio and tactile feedback, help users find errors by providing audio and visual annotation of uncertain portions of the text, and help users correct errors by combining the probabilistic information from the original input, the correction, and approximate information about an error's location. Improving text entry methods for people who are blind or have low vision will enable them to use their mobile devices more effectively for work and leisure. Thus, this project represents an important step to achieving equity for people with visual impairments. \r\n\r\nThis project will contribute novel interface designs to the accessibility and human-computer interaction literature. It will advance the state-of-the-art in mobile device accessibility by, first, studying text entry accessibility for low vision in addition to blind people. Next, the researchers will study and develop accessible gesture typing input methods.  Finally, the project will develop accessible speech input methods.  This project will produce design guidelines, feedback methods, input techniques, recognition algorithms, user study results, and software prototypes that will guide improvements to research and commercial input systems for users who are blind or low-vision. Further, the project's work on the error correction and revision process will improve the usability and performance of touchscreen and speech input methods for everyone.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shiri",
   "pi_last_name": "Azenkot",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shiri Azenkot",
   "pi_email_addr": "shiri.azenkot@cornell.edu",
   "nsf_id": "000690878",
   "pi_start_date": "2019-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell Tech",
  "perf_str_addr": "2 West Loop Rd",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100441501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 274000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-b08bbb4a-7fff-ebf4-03f5-ebc081539f6b\"> </span></p>\r\n<p dir=\"ltr\"><span>This Collaborative Research on Improving Mobile Device Input for Users who are Blind or Low Vision focused on enabling equal access to mobile devices for people with visual impairments (both blind and low vision). Built-in services allow smartphone users with visual impairments to access their devices, but tasks like text entry remain challenging. This project pursued avenues to make text interactions faster and more accurate for people with low vision, by improving algorithms and interfaces employed on standard smartphones. Specific research goals were to help users avoid errors when inputting text, find errors in existing text, and correct users in text. The project used a combination of lab studies and longitudinal smartphone use logging studies.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>This project contributed novel interface designs to the accessibility and human-computer interaction literature. It advanced the state-of-the-art in mobile device accessibility by:&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>(1) studying text entry accessibility for low vision in addition to blind people,&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>(2) studying and developing accessible gesture typing input methods, and&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>(3) studying and developing accessible speech input methods.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>This project produced design guidelines, feedback methods, input techniques, recognition algorithms, user study results, and software prototypes that will guide improvements to research and commercial input systems for users who are blind or low-vision. Further, this project created new algorithms that improve the correction and revision process in any recognition-based input method. This project&rsquo;s most important impact is pushing the envelope of techniques and knowledge about how to make the ubiquitous mobile device more accessible and useful to people with visual impairments.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Early efforts </span><span>developed a prototype allowing users to enter text by silently speaking a word, using the mobile device's camera to detect lip movements. A challenge emerged as visually impaired users had difficulty keeping their mouths within the camera frame, which led to the realization that there were no existing systems or design guidelines to assist visually impaired people in using frontal cameras. This led to a qualitative interview study confirming the need for alternatives to effectively use the frontal camera.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Over the course of the project, large language model agents, as well as smart speakers and voice assistants, became increasingly ubiquitous. Thus, the team investigated the possibility of inputting text into a mobile device using a virtual assistant. We aimed to emulate the way in which people dictate letters, notes, etc to a (human) assistant. Instead of pressing a &ldquo;start dictation&rdquo; button, speaking, and then using the touchscreen keys to edit the output text, a user could simply converse with a virtual assistant, asking them to input certain words or phrases and make edits with natural language. We studied different relationship models of virtual assistants, such as &ldquo;friend,&rdquo; &ldquo;butler,&rdquo; &ldquo;expert,&rdquo; and &ldquo;caregiver,&rdquo; to better understand the relationship between BLV individuals and conversational assistants, ultimately observing a tension between AI adaptation and screen reader customization.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>This work was foundational to the future of (accessible) human-computer interaction. LLMs will become evermore powerful and we will be able to leverage them to make human-computer tasks much more natural. Our work was among the first that explored how LLMs can replace current accessible computer tools, such as screen readers, which are notoriously difficult and slow to use. Using LLMs to complete human-computer will be much more natural, efficient, and equitable.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/11/2025<br>\nModified by: Shiri&nbsp;Azenkot</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732116172_Speaking_with_my_screen_reader_02--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732116172_Speaking_with_my_screen_reader_02--rgov-800width.png\" title=\"Speaking With My Screen Reader: Participant Ratings of Conversation Agents\"><img src=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732116172_Speaking_with_my_screen_reader_02--rgov-66x44.png\" alt=\"Speaking With My Screen Reader: Participant Ratings of Conversation Agents\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Average Likert scale responses for each role, regarding user control, representation of on-screen content, personalization, respectfulness, helpfulness, and necessity of customization. Butler had the highest scores on average, but felt less personalized to the user compared to Caregiver.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Speaking With My Screen Reader: Participant Ratings of Conversation Agents</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732066146_Speaking_with_my_screen_reader_01--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732066146_Speaking_with_my_screen_reader_01--rgov-800width.png\" title=\"Speaking With My Screen Reader: Describing Conversation Agents\"><img src=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732066146_Speaking_with_my_screen_reader_01--rgov-66x44.png\" alt=\"Speaking With My Screen Reader: Describing Conversation Agents\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Aspects of CANVAS for Audio Fictions: Tone-of-Voice Dimensions, Web Activity, and Contexts. For each role (friend, butler, expert, and caregiver), researchers defined humor, style, manner, mood, web activity, and device/environment.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Speaking With My Screen Reader: Describing Conversation Agents</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732000209_Understanding_How_People_with_Visual_Impairments_Take_Selfies_01b--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732000209_Understanding_How_People_with_Visual_Impairments_Take_Selfies_01b--rgov-800width.png\" title=\"Understanding How People with Visual Impairments Take Selfies: Experiences and Challenges\"><img src=\"/por/images/Reports/POR/2025/1909930/1909930_10637754_1741732000209_Understanding_How_People_with_Visual_Impairments_Take_Selfies_01b--rgov-66x44.png\" alt=\"Understanding How People with Visual Impairments Take Selfies: Experiences and Challenges\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Four blind or low-vision participants hold their phones up in various ways to take a selfie, including one- and two-handed and at different distances. Their faces are blurred for privacy.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Understanding How People with Visual Impairments Take Selfies: Experiences and Challenges</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThis Collaborative Research on Improving Mobile Device Input for Users who are Blind or Low Vision focused on enabling equal access to mobile devices for people with visual impairments (both blind and low vision). Built-in services allow smartphone users with visual impairments to access their devices, but tasks like text entry remain challenging. This project pursued avenues to make text interactions faster and more accurate for people with low vision, by improving algorithms and interfaces employed on standard smartphones. Specific research goals were to help users avoid errors when inputting text, find errors in existing text, and correct users in text. The project used a combination of lab studies and longitudinal smartphone use logging studies.\r\n\n\n\r\n\n\nThis project contributed novel interface designs to the accessibility and human-computer interaction literature. It advanced the state-of-the-art in mobile device accessibility by:\r\n\n\n(1) studying text entry accessibility for low vision in addition to blind people,\r\n\n\n(2) studying and developing accessible gesture typing input methods, and\r\n\n\n(3) studying and developing accessible speech input methods.\r\n\n\nThis project produced design guidelines, feedback methods, input techniques, recognition algorithms, user study results, and software prototypes that will guide improvements to research and commercial input systems for users who are blind or low-vision. Further, this project created new algorithms that improve the correction and revision process in any recognition-based input method. This projects most important impact is pushing the envelope of techniques and knowledge about how to make the ubiquitous mobile device more accessible and useful to people with visual impairments.\r\n\n\n\r\n\n\nEarly efforts developed a prototype allowing users to enter text by silently speaking a word, using the mobile device's camera to detect lip movements. A challenge emerged as visually impaired users had difficulty keeping their mouths within the camera frame, which led to the realization that there were no existing systems or design guidelines to assist visually impaired people in using frontal cameras. This led to a qualitative interview study confirming the need for alternatives to effectively use the frontal camera.\r\n\n\n\r\n\n\nOver the course of the project, large language model agents, as well as smart speakers and voice assistants, became increasingly ubiquitous. Thus, the team investigated the possibility of inputting text into a mobile device using a virtual assistant. We aimed to emulate the way in which people dictate letters, notes, etc to a (human) assistant. Instead of pressing a start dictation button, speaking, and then using the touchscreen keys to edit the output text, a user could simply converse with a virtual assistant, asking them to input certain words or phrases and make edits with natural language. We studied different relationship models of virtual assistants, such as friend, butler, expert, and caregiver, to better understand the relationship between BLV individuals and conversational assistants, ultimately observing a tension between AI adaptation and screen reader customization.\r\n\n\n\r\n\n\nThis work was foundational to the future of (accessible) human-computer interaction. LLMs will become evermore powerful and we will be able to leverage them to make human-computer tasks much more natural. Our work was among the first that explored how LLMs can replace current accessible computer tools, such as screen readers, which are notoriously difficult and slow to use. Using LLMs to complete human-computer will be much more natural, efficient, and equitable.\r\n\n\n\t\t\t\t\tLast Modified: 03/11/2025\n\n\t\t\t\t\tSubmitted by: ShiriAzenkot\n"
 }
}