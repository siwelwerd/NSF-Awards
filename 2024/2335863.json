{
 "awd_id": "2335863",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CCSS: Unobtrusive Epidermal Sensors and Deep Gaussian Process Networks for Silent Speech Recognition from Articulatory Muscle Activities",
 "cfda_num": "47.041",
 "org_code": "07010007",
 "po_phone": "7032925394",
 "po_email": "rnash@nsf.gov",
 "po_sign_block_name": "Richard Nash",
 "awd_eff_date": "2024-08-01",
 "awd_exp_date": "2027-07-31",
 "tot_intn_awd_amt": 463227.0,
 "awd_amount": 463227.0,
 "awd_min_amd_letter_date": "2024-07-30",
 "awd_max_amd_letter_date": "2024-07-30",
 "awd_abstract_narration": "Silent speech interfaces are systems that do not rely on voiced speech signals for speech communications. Such interfaces have long been pursued to help people with voice disorders caused by diseases, laryngectomy, accidents, vocal abuse, or aging. Existing silent speech systems face several challenges, including bulkiness, obtrusiveness, immobility, and poor robustness against interferences. The lack of a truly efficient and unobtrusive system has precluded its continuous use and wide-scale deployment. This project aims to develop an efficient, skin-friendly, mechanically/visually unobtrusive, and personalized silent speech interface. Skin-attachable sensors are designed to track muscle activities induced by speech. Novel machine learning methods are developed for converting the sensed signals to spoken speech and for customized silent speech recognition. The developed hardware-software platform will impact broad fields, such as rehabilitation, healthcare, motion tracking, robot control, and human-machine interactions. For example, the developed system can help restore spoken communication for people with voice disorders. The system provides a more natural input method than touch and gesture to interact with smart machines, such as cellphones, computers, prosthetics, robots, and virtual/augmented reality. The system is particularly useful when private information delivery is needed, a quiet environment is desired, or voice-based speech signals are compromised by underwater or noisy environments. In addition, emerging techniques from this project will be adopted to develop education modules, demonstrations, or lab components to enrich classroom teaching, capstone design projects, and K-12 outreach activities.\r\n\r\nThe development of the proposed customizable and robust platform that captures and interprets speech-relevant muscle activities for silent speech recognition is carried out through three specific research tasks. First, unobtrusive electromyogram sensing electrodes for tracking speech-relevant muscle activities will be designed and characterized. Second, machine learning algorithms based on deep Gaussian process networks will be explored to decode sensing signals for speech recognition. Finally, machine learning methods will be developed to assist personalization and optimization of the sensing array. This project will yield new knowledge about the design of biopotential sensors with good sensing capability while addressing the issues of user-friendliness. The developed machine learning methodology is general and can easily be extended to other domains beyond speech recognition. The optimization of sensor layouts for each user will allow for customized systems that accommodate the unique muscle movement patterns of each user and maximize speech recognition accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shanshan",
   "pi_last_name": "Yao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shanshan Yao",
   "pi_email_addr": "shanshan.yao@stonybrook.edu",
   "nsf_id": "000830547",
   "pi_start_date": "2024-07-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Petar",
   "pi_last_name": "Djuric",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Petar M Djuric",
   "pi_email_addr": "petar.djuric@stonybrook.edu",
   "nsf_id": "000110452",
   "pi_start_date": "2024-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "perf_city_name": "STONY BROOK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "104E",
   "pgm_ref_txt": "MEMS/NEMS"
  },
  {
   "pgm_ref_code": "8028",
   "pgm_ref_txt": "Sensor Technology"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 463227.0
  }
 ],
 "por": null
}