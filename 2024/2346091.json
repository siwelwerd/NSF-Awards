{
 "awd_id": "2346091",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Enabling Intelligent Cameras in Internet-of-Things via a Holistic Platform, Algorithm, and Hardware Co-design",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032922303",
 "po_email": "eabed@nsf.gov",
 "po_sign_block_name": "Eyad Abed",
 "awd_eff_date": "2023-10-01",
 "awd_exp_date": "2024-10-31",
 "tot_intn_awd_amt": 272326.0,
 "awd_amount": 138986.0,
 "awd_min_amd_letter_date": "2023-10-27",
 "awd_max_amd_letter_date": "2023-10-27",
 "awd_abstract_narration": "There has been a tremendous demand for bringing Deep Neural Network (DNN) powered functionality into Internet of Thing (IoT) devices to enable ubiquitous intelligent \"IoT cameras\". However, state-of-the-art DNNs have a prohibitive energy cost, making them impractical to be deployed in resource-constrained IoT platforms. This project will develop a novel energy-efficient DNN framework, via a systematic integration of platform, hardware, and algorithm co-design innovations. Despite a growing interest in energy-efficient DNNs, existing techniques lack a systematic optimization across the full stack of design abstraction, from systems through algorithms to hardware implementation. The proposed research advocates an innovative, holistic effort towards energy-efficient and adaptive DNN-powered \"IoT cameras\" by jointly optimizing the platform-, hardware-, and algorithm-level co-design efforts. On the system level, we will address how to automatically generate and adapt DNN models and implementation, to meet a variety of \"IoT devices\" application-specific performance needs and device-specific resource constraints. On the hardware level, we will leverage the observed high sparsity in DNN activations for energy-efficient hardware implementations of both DNN training and inference by using low-cost zero predictors and hence bypass unnecessary computations. On the algorithm level, we will develop innovative factorized sparsity regularization in DNN training as well as efficient, controllable adaptive inference mechanisms, fully complementing and closely integrating with our hardware innovations.           \r\n \r\nThe proposed research will advance the scientific domain of each level, from system and algorithm, to hardware and a holistic, systematic cross-level methodology for designing energy-efficient intelligent systems. Progress on this project will enable ubiquitous DNN-powered intelligent functions in a significantly increased number of resource-constrained daily-life devices, across numerous camera-based Internet-of-Things (IoT) applications such as traffic monitoring, self-driving and smart cars, personal digital assistants, surveillance and security, and augmented reality. As camera-based IoT devices penetrate all walks of life, by enabling DNN-powered intelligence to be pervasive in these devices, the proposed research can have a tremendous impact on global societies and economies.   The research will be integrated with education on energy efficient deep learning.  Educational activities include curriculum development, undergraduate research, and outreach to K-12 students.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yingyan",
   "pi_last_name": "Lin",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Yingyan C Lin",
   "pi_email_addr": "celine.lin@gatech.edu",
   "nsf_id": "000758624",
   "pi_start_date": "2023-10-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "926 DALNEY ST NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "30332",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8888",
   "pgm_ref_txt": "LEARNING & INTELLIGENT SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 122986.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-5b6c41a1-7fff-fc24-e825-d7ae3952be99\"> </span></p>\r\n<p dir=\"ltr\"><span>This project set out to tackle the challenge of enabling deep neural network (DNN) functionality on resource-constrained &ldquo;IoT cameras&rdquo; in a manner that is both energy-efficient and adaptable to varying workloads. To achieve this, we pursued three principal goals: (1) developing a system-level framework that automatically adapts DNNs to various devices and applications, (2) designing hardware accelerators that leverage the inherent sparsity of DNNs and address dynamic inference requirements, and (3) devising algorithmic optimizations&mdash;such as novel training mechanisms and regularizations&mdash;to fully exploit the specialized hardware. By tightly integrating these three levels of co-design, we demonstrated that real-time intelligent vision, and even partial training, can run effectively on ultra-compact, low-power platforms without compromising accuracy.</span><span><br /></span><span><br /></span><span>For </span><strong>Goal 1 (System-Level Adaptation)</strong><span>, we explored how to automatically generate and schedule DNN models for diverse devices and constraints. A key outcome was an analytical performance predictor, </span><em>DNN-Chip Predictor (ICASSP 2020)</em><span>, which efficiently explores the design space of DNN-to-accelerator mappings to identify optimal layer-wise dataflows that minimize energy and latency. Building on these predictive tools, we integrated advanced lensless imaging and in-camera computing in </span><em>SACoD (ICCV 2021)</em><span> to develop an &ldquo;intelligent camera&rdquo; pipeline that integrates sensor, optics, and CNN-based processing into a unified framework. The culmination of this work was </span><em>i-FlatCam (VLSI 2022)</em><span>, an ultra-compact lensless camera chip capable of real-time inference (&gt;200 FPS), which won first place in a major conference demonstration session. These achievements demonstrate that automatically generated, hardware-aware dataflows can efficiently run advanced computer vision tasks while adhering to stringent form-factor and power constraints.</span><span><br /></span><span><br /></span><span>Turning to </span><strong>Goal 2 (Hardware-Level Efficiency)</strong><span>, our efforts centered on designing domain-specific accelerators that exploit sparsity, dynamic precision, and distributed workloads. We introduced a variety of specialized architectures. One example, </span><em>2-in-1 Accelerator (MICRO 2021)</em><span>, was proposed to support random precision switching to gain adversarial robustness while retaining high throughput. Such a dynamic precision design was also verified to enhance the overall efficiency of DNNs on multiple devices in </span><em>InstantNet (DAC 2021)</em><span>. Another accelerator handles dynamic fractional bitwidth assignments&mdash;useful for input-adaptive or &ldquo;on-the-fly&rdquo; energy savings. We also advanced a multi-pronged hardware approach for Vision Transformers in </span><em>ViTCoD (HPCA 2023)</em><span>, which polarizes attention maps, adds on-chip encoder/decoder blocks, and leverages a two-branch microarchitecture to efficiently handle both dense and sparse computations. Finally, in work on 3D reconstruction from 2D images captured by IoT devices, we designed </span><em>Fusion3D (MICRO 2024)</em><span>, a 3D reconstruction and rendering accelerator that unifies all stages&mdash;sampling, feature interpolation, and rendering&mdash;into a single pipeline. This work demonstrated that specialized hardware blocks can be co-designed with novel algorithms to achieve significant gains in throughput, energy efficiency, and real-time responsiveness.</span><span><br /></span><span><br /></span><span>For </span><strong>Goal 3 (Algorithm-Level Optimizations)</strong><span>, we explored ways to reinforce hardware-driven design choices at the algorithmic stage. These include early-bird lottery-ticket techniques in </span><em>GEBT (ICLR 2020)</em><span> that prune graph neural networks without sacrificing performance, robust lottery-ticket approaches in </span><em>Robust Tickets (DAC 2023)</em><span> that transfer better than standard subnetworks, and quantization strategies in </span><em>Contrastive Quant (DAC 2022)</em><span> that enhance certain types of self-supervised or contrastive learning. In the context of large language models (LLMs), we proposed </span><em>Linearized-LLM (ICML 2024)</em><span>, a technique that harnesses linear attention alongside speculative decoding to resolve the quadratic complexity that typically arises in autoregressive transformers. These algorithmic improvements align with, and even exploit, hardware constraints: for instance, if an accelerator supports dynamic precision, an input-aware quantization technique can drastically reduce inference costs with negligible accuracy impact.</span><span><br /></span><span><br /></span><span>Across these three goals, our co-design approach has yielded multiple end-to-end results that unify hardware and algorithms. In the lensless camera domain, </span><em>i-FlatCam</em><span> stands out as an industry-ready solution for low-power, real-time vision tasks such as VR/AR eye tracking and embedded robotics. Equally noteworthy, </span><em>Fusion3D</em><span> demonstrated that even computationally intensive tasks like Neural Radiance Fields (NeRF) training and inference can be condensed into a single- or multi-chip design, enabling instant scene reconstruction (within two seconds) and real-time rendering (&ge;30 FPS) under standard USB bandwidth constraints.</span><span><br /></span><span><br /></span><span>Beyond technical achievements, this project has produced peer-reviewed publications and oral or poster presentations in top venues while also making its corresponding codebase publicly available&mdash;providing a valuable resource for the research community. Additionally, it has created research opportunities for undergraduate and graduate students across multiple disciplines.</span><span><br /></span><span><br /></span><span>In summary, this project successfully achieves its overarching objective of enabling &ldquo;IoT cameras&rdquo; that deliver advanced DNN functionality within stringent resource constraints. Rather than treating hardware and algorithms as separate silos, our results demonstrate that co-designing hardware, system software, and models in tandem significantly expands the possibilities for edge AI. The final chip prototypes and published frameworks not only serve as proof of concept but also as a roadmap for future efforts aimed at embedding powerful intelligence into ultra-constrained &ldquo;IoT cameras&rdquo; systems.</span></p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/01/2025<br>\nModified by: Yingyan&nbsp;Celine&nbsp;Lin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThis project set out to tackle the challenge of enabling deep neural network (DNN) functionality on resource-constrained IoT cameras in a manner that is both energy-efficient and adaptable to varying workloads. To achieve this, we pursued three principal goals: (1) developing a system-level framework that automatically adapts DNNs to various devices and applications, (2) designing hardware accelerators that leverage the inherent sparsity of DNNs and address dynamic inference requirements, and (3) devising algorithmic optimizationssuch as novel training mechanisms and regularizationsto fully exploit the specialized hardware. By tightly integrating these three levels of co-design, we demonstrated that real-time intelligent vision, and even partial training, can run effectively on ultra-compact, low-power platforms without compromising accuracy.\n\nFor Goal 1 (System-Level Adaptation), we explored how to automatically generate and schedule DNN models for diverse devices and constraints. A key outcome was an analytical performance predictor, DNN-Chip Predictor (ICASSP 2020), which efficiently explores the design space of DNN-to-accelerator mappings to identify optimal layer-wise dataflows that minimize energy and latency. Building on these predictive tools, we integrated advanced lensless imaging and in-camera computing in SACoD (ICCV 2021) to develop an intelligent camera pipeline that integrates sensor, optics, and CNN-based processing into a unified framework. The culmination of this work was i-FlatCam (VLSI 2022), an ultra-compact lensless camera chip capable of real-time inference (200 FPS), which won first place in a major conference demonstration session. These achievements demonstrate that automatically generated, hardware-aware dataflows can efficiently run advanced computer vision tasks while adhering to stringent form-factor and power constraints.\n\nTurning to Goal 2 (Hardware-Level Efficiency), our efforts centered on designing domain-specific accelerators that exploit sparsity, dynamic precision, and distributed workloads. We introduced a variety of specialized architectures. One example, 2-in-1 Accelerator (MICRO 2021), was proposed to support random precision switching to gain adversarial robustness while retaining high throughput. Such a dynamic precision design was also verified to enhance the overall efficiency of DNNs on multiple devices in InstantNet (DAC 2021). Another accelerator handles dynamic fractional bitwidth assignmentsuseful for input-adaptive or on-the-fly energy savings. We also advanced a multi-pronged hardware approach for Vision Transformers in ViTCoD (HPCA 2023), which polarizes attention maps, adds on-chip encoder/decoder blocks, and leverages a two-branch microarchitecture to efficiently handle both dense and sparse computations. Finally, in work on 3D reconstruction from 2D images captured by IoT devices, we designed Fusion3D (MICRO 2024), a 3D reconstruction and rendering accelerator that unifies all stagessampling, feature interpolation, and renderinginto a single pipeline. This work demonstrated that specialized hardware blocks can be co-designed with novel algorithms to achieve significant gains in throughput, energy efficiency, and real-time responsiveness.\n\nFor Goal 3 (Algorithm-Level Optimizations), we explored ways to reinforce hardware-driven design choices at the algorithmic stage. These include early-bird lottery-ticket techniques in GEBT (ICLR 2020) that prune graph neural networks without sacrificing performance, robust lottery-ticket approaches in Robust Tickets (DAC 2023) that transfer better than standard subnetworks, and quantization strategies in Contrastive Quant (DAC 2022) that enhance certain types of self-supervised or contrastive learning. In the context of large language models (LLMs), we proposed Linearized-LLM (ICML 2024), a technique that harnesses linear attention alongside speculative decoding to resolve the quadratic complexity that typically arises in autoregressive transformers. These algorithmic improvements align with, and even exploit, hardware constraints: for instance, if an accelerator supports dynamic precision, an input-aware quantization technique can drastically reduce inference costs with negligible accuracy impact.\n\nAcross these three goals, our co-design approach has yielded multiple end-to-end results that unify hardware and algorithms. In the lensless camera domain, i-FlatCam stands out as an industry-ready solution for low-power, real-time vision tasks such as VR/AR eye tracking and embedded robotics. Equally noteworthy, Fusion3D demonstrated that even computationally intensive tasks like Neural Radiance Fields (NeRF) training and inference can be condensed into a single- or multi-chip design, enabling instant scene reconstruction (within two seconds) and real-time rendering (30 FPS) under standard USB bandwidth constraints.\n\nBeyond technical achievements, this project has produced peer-reviewed publications and oral or poster presentations in top venues while also making its corresponding codebase publicly availableproviding a valuable resource for the research community. Additionally, it has created research opportunities for undergraduate and graduate students across multiple disciplines.\n\nIn summary, this project successfully achieves its overarching objective of enabling IoT cameras that deliver advanced DNN functionality within stringent resource constraints. Rather than treating hardware and algorithms as separate silos, our results demonstrate that co-designing hardware, system software, and models in tandem significantly expands the possibilities for edge AI. The final chip prototypes and published frameworks not only serve as proof of concept but also as a roadmap for future efforts aimed at embedding powerful intelligence into ultra-constrained IoT cameras systems.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 03/01/2025\n\n\t\t\t\t\tSubmitted by: YingyanCelineLin\n"
 }
}