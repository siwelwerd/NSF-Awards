{
 "awd_id": "2421874",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Task DCL: Navigation in Crowds for Persons with Blindness/Low Vision: A Virtual Environment to Explore Multimodal Sensory Processing",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032927557",
 "po_email": "amedinab@nsf.gov",
 "po_sign_block_name": "Alexandra Medina-Borja",
 "awd_eff_date": "2024-09-01",
 "awd_exp_date": "2026-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2024-08-05",
 "awd_max_amd_letter_date": "2024-08-05",
 "awd_abstract_narration": "This EArly-concept Grant for Exploratory Research (EAGER) grant will leverage cutting-edge technology, including virtual reality and wearable devices, to study how individuals with visual impairment navigate through crowds. Most individuals with visual impairment live in urban areas where job opportunities and healthcare services are more accessible. Navigating dynamic city environments, particularly crowded spaces such as subway stations and bustling streets, is challenging for individuals with visual impairment who rely on auditory and tactile cues for navigation. These challenges are exacerbated in emergency situations by the need for swift and accurate responses that become essential for physical safety. By creating simulated urban environments enriched with advanced auditory and haptic cues in virtual reality, the project aims to elucidate sensorimotor interactions and cognitive processes underpinning the navigation of persons with visual impairment through crowded spaces. The insights gained from this research will inform the development of wearable technologies that enhance the safety and independence of individuals with visual impairment, thereby promoting their health, prosperity, and welfare.\r\n\r\nThis research project fills crucial knowledge gaps in understanding how persons with visual impairment navigate dynamic urban environments. By studying their cognition and behavior in immersive virtual reality settings, the project will provide insights for developing better assistive technologies and enterprise resilience strategies. Lived experiences of patients with glaucoma will inform the design of the multisensory tasks to reflect real-world challenges faced by persons with impairment. In these realistic virtual reality environments, participants will interact with synthetic actors who form a virtual crowd, and experience various urban scenarios enriched with visual, tactile, and auditory cues. The project will emphasize physics-based integration of these cues within the virtual reality environment to create immersive experiences mirroring real-world challenges. All three sensory cues will play a vital role in guiding navigation decisions and interactions through virtual crowds. Data collected in experiments will be explored within a novel network-inference approach to identify the cognitive and behavioral processes each person undergoes to make effective and efficient navigation decisions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maurizio",
   "pi_last_name": "Porfiri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maurizio Porfiri",
   "pi_email_addr": "mporfiri@nyu.edu",
   "nsf_id": "000240360",
   "pi_start_date": "2024-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John Ross",
   "pi_last_name": "Rizzo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John Ross Rizzo",
   "pi_email_addr": "JohnRoss.Rizzo@nyumc.org",
   "nsf_id": "000734919",
   "pi_start_date": "2024-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "70 WASHINGTON SQ S",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "058Y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  },
  {
   "pgm_ele_code": "164200",
   "pgm_ele_name": "Special Initiatives"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "010E",
   "pgm_ref_txt": "DISABILITY RES & HOMECARE TECH"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": null
}