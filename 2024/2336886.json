{
 "awd_id": "2336886",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Latency-controlled Reduction of Data Center Expenses for Handling Bursty ML Inference Requests",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2024-01-01",
 "awd_exp_date": "2026-12-31",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2023-12-26",
 "awd_max_amd_letter_date": "2023-12-26",
 "awd_abstract_narration": "Data centers must often over-provision the GPUs used for online machine learning (ML) inference because inference requests can arrive in a large number as bursts. Such over-provisioning results in unnecessarily high capital expenses (CapEx) for data centers. This project aims to design a holistic management framework that handles bursty ML inference requests with latency guarantees and minimized CapEx. First, the proposed framework will co-locate ML inference and training workloads efficiently on the same GPUs, with latency guarantees, for improving GPU utilization. As a result, GPUs can be used mostly for inference when a request burst comes, and then mostly for training afterwards, which can reduce the number of GPUs needed, and thus the data center\u2019s CapEx. Second, a novel task scheduling algorithm will be designed to consolidate negatively correlated ML tasks onto the same GPUs for further reducing CapEx. Third, data center power/cooling CapEx will be reduced as well, by exploiting existing energy storage devices already equipped in most data centers to supply additional energy during a burst, in order to avoid expensive power facility upgrades. Finally, all the software, hardware, and power/cooling facilities will be coordinated as a holistic framework to efficiently manage a data center as one massive warehouse-scale computer.\r\n\r\nAs ML cloud services are becoming increasingly popular, the fast-growing business demands for computing are driving data centers to run their GPUs and servers with higher performance and lower costs. The success of this project would positively impact data center design by allowing data centers to safely handle ML inference bursts, with desirable latency guarantees and minimized CapEx. Currently, in order to deal with bursty ML workloads at an increasing scale with latency guarantees, a data center must often 1) over-provision their inference GPUs and keep them separated from GPUs used for training in order to minimize the risk of resource competition that might jeopardize inference latency; and 2) upgrade the corresponding power/cooling facilities for hosting the increasing number of GPUs and servers to prepare for occasional worst-case scenarios. Such capital investment increases can impose a serious burden on ML/AI companies that run their own data centers. For start-ups and smaller ML/AI companies that rely on the cloud, the higher data center CapEx can lead to higher cloud service bills. The proposed framework can provide latency-controlled CapEx reduction for data centers, thus allowing those booming ML/AI companies to have a better chance of success. Further, this project could benefit other workloads that also perform intensive GPU computing, such as high-performance computing, big data analytics, and cloud virtual reality. The results of the project will be transitioned into new and existing undergraduate and graduate courses at The Ohio State University. Students from groups underrepresented in computing will be actively recruited in this project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaorui",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaorui Wang",
   "pi_email_addr": "xwang@ece.osu.edu",
   "nsf_id": "000344273",
   "pi_start_date": "2023-12-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1960 KENNY RD",
  "perf_city_name": "COLUMBUS",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": null
}