{
 "awd_id": "2410717",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Quantization and Compression for Neural Networks: Theory and Algorithms",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927212",
 "po_email": "jmead@nsf.gov",
 "po_sign_block_name": "Jodi Mead",
 "awd_eff_date": "2024-08-01",
 "awd_exp_date": "2027-07-31",
 "tot_intn_awd_amt": 299889.0,
 "awd_amount": 299889.0,
 "awd_min_amd_letter_date": "2024-06-03",
 "awd_max_amd_letter_date": "2024-06-03",
 "awd_abstract_narration": "The research project focuses on making advanced AI technologies, like the neural networks used in language models and for image generation or analysis, more efficient and environmentally friendly. Today, these technologies require significant computational power, which makes them both costly and energy-intensive. This project aims at developing algorithms for replacing such networks with functionally equivalent ones that require fewer computational resources, without losing their effectiveness. The resulting reduction in computational complexity will also help enable the use of AI in real-time applications and on devices with limited resources, further expanding this critical technology's reach. Thus, the expected scientific outcome is the development of robust, efficient, practical algorithms that are also backed by rigorous theoretical guarantees. Moreover, we anticipate that the theoretical tools we develop in order to analyze these algorithms will find broader use in other application areas. The project also emphasizes the importance of education and academic community involvement. By integrating research findings into university courses and involving students from potentially diverse backgrounds, the project will help prepare the next generation of mathematicians and engineers. Additionally, it aims to share breakthroughs with broader communities through journal publications, workshops, and conferences, while also connecting students with real-world industry applications.\r\n\r\nAs previously stated, the project aims to address the challenge of compressing large neural networks, which are pivotal in modern AI applications but are resource-intensive. Thus, the research will focus on developing algorithms that reduce the computational demands of these networks by minimizing the memory and power needed without compromising their performance. Our approach involves three main strategies, quantization, pruning, and low-rank approximation. Among these, quantization transforms neural network parameters into formats that require fewer bits, thus reducing memory usage and computational intensity. Meanwhile, pruning selectively removes less important parameters from the network to streamline computations. Finally, low-rank approximation replaces large matrices representing the weights in the network with products of smaller matrices, in a way that retains essential information while requiring less memory and computation.  We will develop algorithmic approaches for quantization, pruning, and low-rank approximation that are underpinned by rigorous mathematical theories to ensure the reliability and effectiveness of the compressed models. In our analysis, we will utilize stochastic process theory, geometric functional analysis, discrete geometry, discrepancy theory, optimization theory, compressed sensing, and dimensionality reduction, to name a few. These diverse areas will help us establish a solid theoretical foundation for our algorithms consisting of lower bounds on best-possible theoretical error guarantees as well as upper bounds on the errors resulting from our algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rayan",
   "pi_last_name": "Saab",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rayan Saab",
   "pi_email_addr": "rsaab@ucsd.edu",
   "nsf_id": "000653192",
   "pi_start_date": "2024-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DRIVE",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 299889.0
  }
 ],
 "por": null
}