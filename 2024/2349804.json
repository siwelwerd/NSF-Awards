{
 "awd_id": "2349804",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Conference: Workshop on Advanced Automated Systems, Contestability, and the Law",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2023-11-01",
 "awd_exp_date": "2024-10-31",
 "tot_intn_awd_amt": 29601.0,
 "awd_amount": 29601.0,
 "awd_min_amd_letter_date": "2023-10-27",
 "awd_max_amd_letter_date": "2023-10-27",
 "awd_abstract_narration": "This award is in support of an interdisciplinary workshop to be held in the winter of 2023-2024 (current plan is January, 2024). The workshop is organized collaboratively by PIs at Tufts University and Columbia University The workshop's primary objective is to inform the executive branch and U.S. government agencies about new technological advances and related concerns and risks related to the legal contestability of artificial intelligence used in creating government software-enabled processes, regulations, and legal proceedings. \r\n\r\nOver two days, the workshop features speakers and panels, all of whom are leading researchers in the field of artificial intelligence and the law.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Bellovin",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Steven M Bellovin",
   "pi_email_addr": "smb@cs.columbia.edu",
   "nsf_id": "000164204",
   "pi_start_date": "2023-10-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "202 LOW LIBRARY 535 W 116 ST MC 4309,",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "10027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 29601.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>So-called explainable AI---artificial intelligence systems that can give a rationale for their answers---are very much a research area. That said, the increasing use of AI by government agencies, including the Social Security Administration, means that some form of accountability is necessary.</span></p>\r\n<p><span>We focused instead on \"contestable\" AI: AI systems that emit enough information to let someone affected challenge the outcome. We held a workshop on this; participants include researchers in AI, government people, and (quite crucially) people whose lives have been or are likely to be affected by AI systems. Their input was crucial: what do <em>they</em>&nbsp;need to dispute a decision. Following the workshop, the organizers produced a report.</span></p>\r\n<p><span>Our report recommended the following:</span></p>\r\n<ul>\r\n<li><span>Adequate notice be given when a government system requiring contestability is being developed and used.</span></li>\r\n<li><span>Notice to the public must be adequate to allow for system challenges to the system, before large numbers of people are affected.</span></li>\r\n<li><span>Notices to individuals must be comprehensible: how was the decision made, and what is necessary to contest it?</span></li>\r\n<li><span>Contestability must be part of the system design from the beginning.</span></li>\r\n<li><span>Designers should always consider not deploying the system if they can't incorporate contestability</span></li>\r\n<li><span>Design consultations should include operators, end users, decision makers, and (very important and often overlooked) decision subjects.</span></li>\r\n<li><span>Stakeholders who will be affected by the system must be involved from the beginning.</span></li>\r\n<li><span>Contestability features must be stress-tested before the system goes live.</span></li>\r\n<li><span>Contestability should be accessible to and usable by people with different backgrounds.</span></li>\r\n<li><span>Reproducibility of outcomes is crucial.</span></li>\r\n<li><span>The automated system's decisions must follow the law&mdash;progammers may not ignore difficult-to-implement provisions.</span></li>\r\n<li><span>Additional research on design of such systems would be helpful.</span></li>\r\n</ul>\r\n<p>Following this, we held an instructional workshop for students from Tufts University and Spelman College, a historically Black women's school. The goal of this effort was to educate the students about the problem, the surrounding legal background, and how are recommendations addressed the issue.</p><br>\n<p>\n Last Modified: 02/18/2025<br>\nModified by: Steven&nbsp;M&nbsp;Bellovin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nSo-called explainable AI---artificial intelligence systems that can give a rationale for their answers---are very much a research area. That said, the increasing use of AI by government agencies, including the Social Security Administration, means that some form of accountability is necessary.\r\n\n\nWe focused instead on \"contestable\" AI: AI systems that emit enough information to let someone affected challenge the outcome. We held a workshop on this; participants include researchers in AI, government people, and (quite crucially) people whose lives have been or are likely to be affected by AI systems. Their input was crucial: what do theyneed to dispute a decision. Following the workshop, the organizers produced a report.\r\n\n\nOur report recommended the following:\r\n\r\nAdequate notice be given when a government system requiring contestability is being developed and used.\r\nNotice to the public must be adequate to allow for system challenges to the system, before large numbers of people are affected.\r\nNotices to individuals must be comprehensible: how was the decision made, and what is necessary to contest it?\r\nContestability must be part of the system design from the beginning.\r\nDesigners should always consider not deploying the system if they can't incorporate contestability\r\nDesign consultations should include operators, end users, decision makers, and (very important and often overlooked) decision subjects.\r\nStakeholders who will be affected by the system must be involved from the beginning.\r\nContestability features must be stress-tested before the system goes live.\r\nContestability should be accessible to and usable by people with different backgrounds.\r\nReproducibility of outcomes is crucial.\r\nThe automated system's decisions must follow the lawprogammers may not ignore difficult-to-implement provisions.\r\nAdditional research on design of such systems would be helpful.\r\n\r\n\n\nFollowing this, we held an instructional workshop for students from Tufts University and Spelman College, a historically Black women's school. The goal of this effort was to educate the students about the problem, the surrounding legal background, and how are recommendations addressed the issue.\t\t\t\t\tLast Modified: 02/18/2025\n\n\t\t\t\t\tSubmitted by: StevenMBellovin\n"
 }
}