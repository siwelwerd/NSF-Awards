{
 "awd_id": "2418946",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "The Cost of AI: A Comparative Study of Machine Learning Training Methods",
 "cfda_num": "47.070, 47.075, 47.084",
 "org_code": "04010000",
 "po_phone": "7032924420",
 "po_email": "cbethel@nsf.gov",
 "po_sign_block_name": "Cindy Bethel",
 "awd_eff_date": "2024-08-15",
 "awd_exp_date": "2028-07-31",
 "tot_intn_awd_amt": 399634.0,
 "awd_amount": 399634.0,
 "awd_min_amd_letter_date": "2024-08-15",
 "awd_max_amd_letter_date": "2024-08-15",
 "awd_abstract_narration": "In recent years, Artificial Intelligence (AI) research has made rapid advances that led to numerous real-world applications. While some researchers explore fairness and bias in AI systems, few address how researchers navigate conflicting ethical issues in how data is trained to create these AI systems. This study will identify and articulate the ethical questions regarding Machine Learning (ML) training methods, emphasizing environmental cost, labor practices, financial cost, and data quality trade-offs when choosing ML training methods in research settings. These research findings will contribute a model for AI researchers to weigh data training methods for responsible AI research. Focusing on these ethical and financial considerations in a research setting will provide tools to evaluate research approaches that serve our nation best and will contribute to training students to consider these trade-offs as they move into industry roles. \r\n\r\nThis study focuses on large language models (LLMs), such as ChatGPT, Llama, and Claude. While many approaches to AI and Natural Language Processing (NLP) rely on supervised ML through training data, LLMs employ pre-training of large-scale models on vast amounts of data collected from the internet. To enable LLMs to grasp the intricacies of language and align their outputs to match human preferences, they undergo pre-training on extensive datasets and fine-tuning on labels generated by digital piecework workers. Our study centers on the production of this fine-tuning data. NLP research is a fast-growing field, with over 5,000 papers published in 2021 and over a 50% increase in research production between 2017 and 2021. LLMs offer an excellent case study because various methods are used to train them - broadly categorized into supervised, unsupervised, and reinforcement learning approaches. These methods are often used in combination, and the choice depends on the specific goals of the model and the available data. The study consists of three primary objectives:  1) identify current practices among researchers of large-scale data sets to train LLMs by conducting interviews and surveys; 2) examine the trade-offs from multiple types of data training methods using comparative studies of (a) LLMs to generate training datasets with or without a human-in-the-loop, (b) digital piecework or paid crowd work (such as Mechanical Turk), and (c) dedicated data workers employed by research groups; and 3) develop ethical models and resources that communicate how to weigh data training methods based on cost, time, quality, diversity, environmental impact, and labor practices for responsible AI training.\r\n\r\nThis project is funded through the ER2 program by the Directorate for Social, Behavioral and Economic Sciences.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Elizabeth",
   "pi_last_name": "DiSalvo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elizabeth DiSalvo",
   "pi_email_addr": "edisalvo3@gatech.edu",
   "nsf_id": "000636469",
   "pi_start_date": "2024-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Ritter",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Ritter",
   "pi_email_addr": "alan.ritter@cc.gatech.edu",
   "nsf_id": "000655595",
   "pi_start_date": "2024-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Wei",
   "pi_last_name": "Xu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wei Xu",
   "pi_email_addr": "wei.xu@cc.gatech.edu",
   "nsf_id": "000726735",
   "pi_start_date": "2024-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "129Y00",
   "pgm_ele_name": "ER2-Ethical & Responsible Res"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 399634.0
  }
 ],
 "por": null
}