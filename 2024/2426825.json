{
 "awd_id": "2426825",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Conference: Modeling Randomness in Neural Network Training: Mathematical, Statistical, and Numerical Guarantees",
 "cfda_num": "47.049, 47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2024-06-01",
 "awd_exp_date": "2025-05-31",
 "tot_intn_awd_amt": 39733.0,
 "awd_amount": 39733.0,
 "awd_min_amd_letter_date": "2024-05-24",
 "awd_max_amd_letter_date": "2024-05-24",
 "awd_abstract_narration": "Neural networks are at the heart of modern machine learning and artificial intelligence (ML/AI) systems. The rapid development of these technologies has led to rapid adoption across a variety of domains, particularly in speech processing, computer vision, and natural language processing. At the same time, the theoretical underpinnings of these statistical models are not yet fully understood. The question of how and why neural networks \u201cwork\" can be approached from a variety of mathematical perspectives. One of the most promising mathematical tools for analysis of neural networks is random matrix theory, a field that has recently reached mathematical maturity and whose relevance and applicability to modeling, understanding, and characterizing a vast array of science and technology problems keeps growing every day. From principle component analysis and random growth processes to particle interactions and community detection in large networks, random matrices are now used to investigate and explain high-dimensional phenomena like concentration (the so- called \"blessing of dimensionality\" as opposed to the \"curse of dimensionality\"). Recent results in universality allow for usage of more complex, non-Gaussian models, sometimes even allowing for limited dependencies. This prompts the question: what can probability theory in general---and random matrix theory (RMT) in particular---tell us about neural networks, modern machine learning, and AI? Such fundamental insights could lead to novel approaches that simplify or improve the efficiency of neural network design and training.\r\n\r\nThe DIMACS Center at Rutgers University will hold the Workshop on Modeling Randomness in Neural Network Training: Mathematical, Statistical, and Numerical Guarantees at Rutgers University on June 5\u20137, 2024. This is a multidisciplinary workshop to create bridges between the different mathematical and computational communities by bringing together researchers with a diverse set of perspectives on neural networks. Random matrix theory can be used to understand different phenomena in neural network training. The workshop will center around the following themes: understanding matrix-valued random processes that arise during neural network training, modeling/measuring uncertainty and designing estimators for training processes, and applications to these designs within optimization algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anand",
   "pi_last_name": "Sarwate",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Anand D Sarwate",
   "pi_email_addr": "anand.sarwate@rutgers.edu",
   "nsf_id": "000608994",
   "pi_start_date": "2024-05-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ioana",
   "pi_last_name": "Dumitriu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ioana Dumitriu",
   "pi_email_addr": "idumitriu@ucsd.edu",
   "nsf_id": "000071417",
   "pi_start_date": "2024-05-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tony",
   "pi_last_name": "Chiang",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Tony Y Chiang",
   "pi_email_addr": "tc@alum.mit.edu",
   "nsf_id": "000575652",
   "pi_start_date": "2024-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "3 RUTGERS PLZ",
  "perf_city_name": "NEW BRUNSWICK",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "089018559",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126300",
   "pgm_ele_name": "PROBABILITY"
  },
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 39733.0
  }
 ],
 "por": null
}