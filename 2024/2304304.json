{
 "awd_id": "2304304",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I: A Mixed-Computation Neural Network Acceleration Stack for Edge Inference",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2023-12-15",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 274915.0,
 "awd_amount": 274915.0,
 "awd_min_amd_letter_date": "2023-12-14",
 "awd_max_amd_letter_date": "2023-12-14",
 "awd_abstract_narration": "The broader/commercial impact of this Small Business Innovation Research (SBIR) Phase I project is to improve the sustainability of artificial intelligence by reducing carbon emissions for training neural networks and performing inference at the edge. Additionally, the technology will spawn new applications and use cases for edge inference (including personal health, advanced data analytics, and informed decision-making), resulting in significant improvements in people's lives and well-being. The commercial potential is substantial (i.e., tens of billions of dollars annually), as are the potential economic benefits to US high-technology industries.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project sets out to develop a mixed-computation neural network acceleration stack utilizing optimally designed and provisioned hardware resources. This acceleration stack empowers a heterogeneous hardware realization of a neural network inference engine whereby computations required in various network layers may be done by using different number systems and different precision levels. The acceleration stack can thus achieve very high inference speed and energy efficiency while maintaining the inference accuracy compared to a homogeneous hardware realization of the network using 16-bit floating point computations. To support the design, optimization, and runtime efficiency of this edge inference accelerator, a full suite of software and design automation tools comprising a distiller for neural network architecture optimization and training, a logic synthesizer for generating optimized gate-level realization of very large and complex Boolean and multi-valued logic functions, a compiler for generating and scheduling control-flow and data path instructions that are executed on the target fabric, and a runtime system for orchestrating data movement will also be provided. The resulting edge inference accelerator will be deployable on resource-constrained, energy-limited, and cost-sensitive edge devices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mahdi",
   "pi_last_name": "Nazemi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mahdi Nazemi",
   "pi_email_addr": "mnazemi@usc.edu",
   "nsf_id": "000953744",
   "pi_start_date": "2023-12-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "NOUVAI INC.",
  "inst_street_address": "800 N CRESCENT HEIGHTS BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3106257196",
  "inst_zip_code": "900466902",
  "inst_country_name": "United States",
  "cong_dist_code": "30",
  "st_cong_dist_code": "CA30",
  "org_lgl_bus_name": "NOUVAI INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "QNDYZ7NR49S4"
 },
 "perf_inst": {
  "perf_inst_name": "Nouvai Inc",
  "perf_str_addr": null,
  "perf_city_name": "Marina Del Rey",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "902926606",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 274915.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Nouvai Inc. is leading an innovative SBIR project that seeks to create a cutting-edge mixed-computation neural network acceleration stack, strategically leveraging meticulously designed hardware resources. This powerful stack facilitates a heterogeneous approach to implementing a neural network inference engine, enabling diverse computations across various network layers to utilize flexible number systems and varying precision levels. Consequently, this acceleration stack demonstrates considerably enhanced inference speed and energy efficiency, all while preserving accuracy, in comparison to a traditional homogeneous hardware approach that depends solely on 16-bit floating-point calculations. <br />To optimize the design, runtime efficiency, and overall performance of Nouvai&rsquo;s edge inference accelerator, we have developed a robust suite of software and design automation tools. This comprehensive toolkit features a distiller for refining both neural network architecture and training processes, a neural architecture search (NAS) engine, and a model transformer. Collectively, these elements facilitate a precise allocation of number systems and precision levels, while implementing transformations that restore accuracy following extensive pruning and quantization. Furthermore, we have created a specialized compiler designed for generating and scheduling control-flow and data path instructions tailored to the target hardware, complemented by a runtime system that manages data movement efficiently.<br />Our findings reveal that the integrated hardware/software solution, termed Maxel (Mixed-Computational Acceleration Stack), achieves a marked reduction in end-to-end inference latency across a range of neural networks, including convolutional neural networks and transformer models, all with an output accuracy degradation of less than 1%. Nouvai has effectively produced an FPGA-based mixed-computation hardware platform, which incorporates a systolic array composed of multiply-and-accumulate (MAC) processing elements, a MAC vector processor designed for aggregating partial results, and a lookup table-based implementation for nonlinear functions and data format conversion. <br />In addition, Nouvai has established a fully cohesive software ecosystem that facilitates distillation, mixed-computation assignments, compilation, high-level synthesis, and operational runtime control. This enables the realization of exceptionally energy-efficient and ultra-low latency versions of progressively trained and optimized neural network models on the mixed-computation hardware platform. With these pioneering developments, Nouvai is poised to make a substantial impact on the forefront of inference task execution across a wide variety of neural network architectures, particularly in domains such as vision processing and large language models.<br />Nouvai's R&amp;D efforts have optimized and efficiently mapped neural network models, especially transformer architectures, which serve as the underlying models for large language models (LLMs) and chatbots. Efficient inference using LLMs in turn revolutionizes various fields by enhancing speed and responsiveness. In healthcare, they aid in analyzing patient records for quicker decision-making and personalized care. In education, LLMs facilitate real-time tutoring and personalized learning experiences, while law firms use them for efficient document review and research. LLMs improve accessibility for individuals with disabilities and assist in environmental research by processing large datasets. In human resources, they streamline processes and enhance engagement. Their cloud-based implementations optimize resources, and techniques like pruning improve computational efficiency. LLMs also enhance information retrieval and summarization, support real-time translation, and personalize educational experiences. By democratizing access to knowledge, they foster economic growth and innovation, while stimulating creativity across artistic disciplines.</p><br>\n<p>\n Last Modified: 02/21/2025<br>\nModified by: Mahdi&nbsp; &nbsp;Nazemi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nNouvai Inc. is leading an innovative SBIR project that seeks to create a cutting-edge mixed-computation neural network acceleration stack, strategically leveraging meticulously designed hardware resources. This powerful stack facilitates a heterogeneous approach to implementing a neural network inference engine, enabling diverse computations across various network layers to utilize flexible number systems and varying precision levels. Consequently, this acceleration stack demonstrates considerably enhanced inference speed and energy efficiency, all while preserving accuracy, in comparison to a traditional homogeneous hardware approach that depends solely on 16-bit floating-point calculations. \nTo optimize the design, runtime efficiency, and overall performance of Nouvais edge inference accelerator, we have developed a robust suite of software and design automation tools. This comprehensive toolkit features a distiller for refining both neural network architecture and training processes, a neural architecture search (NAS) engine, and a model transformer. Collectively, these elements facilitate a precise allocation of number systems and precision levels, while implementing transformations that restore accuracy following extensive pruning and quantization. Furthermore, we have created a specialized compiler designed for generating and scheduling control-flow and data path instructions tailored to the target hardware, complemented by a runtime system that manages data movement efficiently.\nOur findings reveal that the integrated hardware/software solution, termed Maxel (Mixed-Computational Acceleration Stack), achieves a marked reduction in end-to-end inference latency across a range of neural networks, including convolutional neural networks and transformer models, all with an output accuracy degradation of less than 1%. Nouvai has effectively produced an FPGA-based mixed-computation hardware platform, which incorporates a systolic array composed of multiply-and-accumulate (MAC) processing elements, a MAC vector processor designed for aggregating partial results, and a lookup table-based implementation for nonlinear functions and data format conversion. \nIn addition, Nouvai has established a fully cohesive software ecosystem that facilitates distillation, mixed-computation assignments, compilation, high-level synthesis, and operational runtime control. This enables the realization of exceptionally energy-efficient and ultra-low latency versions of progressively trained and optimized neural network models on the mixed-computation hardware platform. With these pioneering developments, Nouvai is poised to make a substantial impact on the forefront of inference task execution across a wide variety of neural network architectures, particularly in domains such as vision processing and large language models.\nNouvai's R&D efforts have optimized and efficiently mapped neural network models, especially transformer architectures, which serve as the underlying models for large language models (LLMs) and chatbots. Efficient inference using LLMs in turn revolutionizes various fields by enhancing speed and responsiveness. In healthcare, they aid in analyzing patient records for quicker decision-making and personalized care. In education, LLMs facilitate real-time tutoring and personalized learning experiences, while law firms use them for efficient document review and research. LLMs improve accessibility for individuals with disabilities and assist in environmental research by processing large datasets. In human resources, they streamline processes and enhance engagement. Their cloud-based implementations optimize resources, and techniques like pruning improve computational efficiency. LLMs also enhance information retrieval and summarization, support real-time translation, and personalize educational experiences. By democratizing access to knowledge, they foster economic growth and innovation, while stimulating creativity across artistic disciplines.\t\t\t\t\tLast Modified: 02/21/2025\n\n\t\t\t\t\tSubmitted by: Mahdi Nazemi\n"
 }
}