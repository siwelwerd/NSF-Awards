{
 "awd_id": "2402876",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: HCC: Medium: Enhancing Communication and Interaction for Individuals with Severe Disabilities: A Novel Interface Leveraging Multiple Information Sources",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924420",
 "po_email": "cbethel@nsf.gov",
 "po_sign_block_name": "Cindy Bethel",
 "awd_eff_date": "2025-01-01",
 "awd_exp_date": "2028-12-31",
 "tot_intn_awd_amt": 456853.0,
 "awd_amount": 456853.0,
 "awd_min_amd_letter_date": "2024-09-06",
 "awd_max_amd_letter_date": "2024-09-06",
 "awd_abstract_narration": "This project builds and tests new technology to help people with severe speech and motor impairments better control computers and use them to communicate. Such impairments make it very hard to use voice, touch, keyboard, and other common ways of interacting with computers. One alternative that can help is \"single-switch\" interaction, where systems allow people to use the muscle control they do have to simulate clicking a single button. For instance, Stephen Hawking famously composed lectures and interview responses with his computer by twitching his cheek; other interfaces use blinking, or puffing air into a special sensor. However, these single-switch interaction methods, and the interfaces built based on them, are much slower and more error-prone than voice, touch, or keyboard interfaces. In this project, the research team collaborates with people with severe disabilities to develop technology to make it faster, easier, and more accurate for this set of people to communicate and use computers. The key general idea is to use additional information about a person's context such as where they are looking, what they have done recently, and other clues to their intentions to help computers guess, and suggest, complicated communication goals based on simple single-switch interactions.\r\n\r\nTo meet these goals, this project leverages machine learning and a novel user interface to synthesize four currently under-utilized information sources. First, users are likely to be looking at a target when they select it; unlike methods that use eye gaze as a pointer, such passive information requires no conscious eye effort from the user and may provide information even when a user's gaze is not precise enough for selection by itself. Second, modern large language models can often anticipate what a user is interested in writing and can likely learn to leverage contextual clues (e.g., who the user is writing to or topics suggested by a communication partner) with only a small number of training examples. Third, patterns of behavior across users and time can offer clues; for example, similar users may provide information about a new user, or a user might exhibit different behaviors at different times of the day. Fourth, sequences of noisy input actions can jointly inform a desired user action such as writing a word. The project combines these four sources of information with a user's single-switch input to improve speed and accuracy of text entry and computer interaction tasks. This project's innovations are designed and tested with users with disabilities in an extensive, multi-pronged set of participatory design activities and experiments. Participatory design with this set of users is particularly challenging due to their communication speed. This project therefore investigates a novel combination of participatory design methods in which stakeholders provide feedback via an asynchronous messaging platform, in co-design interviews, and via surveys designed to be easily completed with single-switch interaction techniques.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Keith",
   "pi_last_name": "Vertanen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Keith Vertanen",
   "pi_email_addr": "vertanen@mtu.edu",
   "nsf_id": "000620484",
   "pi_start_date": "2024-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan Technological University",
  "inst_street_address": "1400 TOWNSEND DR",
  "inst_street_address_2": "",
  "inst_city_name": "HOUGHTON",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "9064871885",
  "inst_zip_code": "499311200",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MI01",
  "org_lgl_bus_name": "MICHIGAN TECHNOLOGICAL UNIVERSITY",
  "org_prnt_uei_num": "GKMSN3DA6P91",
  "org_uei_num": "GKMSN3DA6P91"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan Technological University",
  "perf_str_addr": "1400 TOWNSEND DR",
  "perf_city_name": "HOUGHTON",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "499311200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 456853.0
  }
 ],
 "por": null
}