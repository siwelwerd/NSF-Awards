{
 "awd_id": "2413265",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Theory for the Practice of Deep Learning: Insights into Autoencoder, LLM Fine-Tuning, and Transfer Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032925307",
 "po_email": "tmaiti@nsf.gov",
 "po_sign_block_name": "Tapabrata Maiti",
 "awd_eff_date": "2024-08-01",
 "awd_exp_date": "2027-07-31",
 "tot_intn_awd_amt": 299756.0,
 "awd_amount": 299756.0,
 "awd_min_amd_letter_date": "2024-07-24",
 "awd_max_amd_letter_date": "2024-07-24",
 "awd_abstract_narration": "AI entered a new and accelerated phase with the public rollout in Nov. 2022, of the generative AI large language model (LLM) ChatGPT, which is a transformer deep learning (DL) model with 1.5 billion parameters trained on 570GB of data. The potential impact of ChatGPT and other chatbots in scientific research, teaching, medicine, government, business, and society at large is enormous. Currently, the dominant empirical paradigm for solving tasks using deep learning is to first pretrain massive models in an unsupervised manner on large data corpora and then fine-tune them on specific tasks of interest. For example, the standard practice for pretraining modern LLMs like ChatGPT is to train models to predict the next token on datasets scraped from the internet, which allows the model to learn meaningful and general representations about language. Although models learn extensively about language during the pre-training phase, they are typically not immediately useful for tasks of interest, and transfer learning must be applied by fine-tuning on a specific downstream task, such as coding, math, chat-botting, etc. There are many important questions about this fine-tuning process, such as how the hyperparameters should be set and when different algorithms can be expected to generalize well. In this project, a theoretical study will be taken to address different aspects of finetuning and transfer learning with the aim of producing practically relevant guidance for improving efficiency and generalization performance for these settings. This project includes financial support and mentorship for graduate students.\r\n \r\nConcretely, the proposed research will focus on the following two directions: developing methods that help choose the learning rate and rank in a near-optimal manner for a popular finetuning method known as Low Rank Adapters (LoRA). Insights from the large width scaling theory of neural networks will be used to guide how to select hyperparameters appropriately. Successful methods have the potential to greatly reduce the computing cost of hyperparameter tuning when finetuning large models. The second thrust involves studying transfer learning in the context of over-parametrized linear regression. The setting in over-parametrized linear regression is rich enough to provide conceptual insights into modern deep learning yet simplified enough for a rigorous mathematical study of generalization performance. Building upon previous works analyzing in-distribution generalization performance in over-parametrized linear regression and using similar random matrix theoretic tools, extensions will be made to the out-of-distribution transfer learning setting. By rigorously characterizing how transfer learning affects generalization, intuition will be provided for practitioners seeking to predict how various shifts will affect the performance of their models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bin",
   "pi_last_name": "Yu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bin Yu",
   "pi_email_addr": "binyu@stat.berkeley.edu",
   "nsf_id": "000465148",
   "pi_start_date": "2024-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "1608 4TH ST STE 201",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947101749",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "1269",
   "pgm_ref_txt": "STATISTICS"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 299756.0
  }
 ],
 "por": null
}