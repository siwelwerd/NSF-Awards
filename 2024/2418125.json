{
 "awd_id": "2418125",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Conference: New horizons in language science: large language models, language structure, and the neural basis of language",
 "cfda_num": "47.070, 47.075",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2024-03-01",
 "awd_exp_date": "2025-02-28",
 "tot_intn_awd_amt": 49923.0,
 "awd_amount": 49923.0,
 "awd_min_amd_letter_date": "2024-02-27",
 "awd_max_amd_letter_date": "2024-02-27",
 "awd_abstract_narration": "Workshop Title: New Horizons in Language Science: Large Language Models, Language Structure, and Language Processing in the Human Mind and Brain. \r\n\r\nThis workshop is dedicated to interdisciplinary connections between today\u2019s large language models, language structure, and language processing in the human mind and brain. We will convene a group of experts spanning computer science, linguistics, cognitive science, and neuroscience to discuss recent progress in deep learning based natural language technology, which have captured not only scientific but also general commercial and public attention under the name \"Large Language Models\u201d (LLMs), and the implications of this progress for our scientific understanding of language structure and the basis of language acquisition and processing in the human mind and brain. The workshop will foster collaboration and encourage innovative research directions, with the intent of producing a white paper to inform future research and policymaking.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Roger",
   "pi_last_name": "Levy",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Roger P Levy",
   "pi_email_addr": "rplevy@mit.edu",
   "nsf_id": "000508659",
   "pi_start_date": "2024-02-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127Y00",
   "pgm_ele_name": "Sci of Lrng & Augmented Intel"
  },
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 49923.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Large Language Models (LLMs) are among the most transformative products of modern artificial intelligence. They are remarkably successful as technological tools, and pose great opportunities, but also challenges, for the scientific study of natural language in the human mind and brain. This workshop presented talks, commentary, and discussion dedicated to the following three themes:<br /><br />1. What insights do large language models provide for the study of human language?<br />2. What insights does the study of human language provide for large language model development?<br />3. What key future scientific opportunities lie at the interface between the study of human language and large language model development?<br /><br />The workshop was dedicated to interdisciplinary connections between today&rsquo;s large language models, language structure, and language processing in the human mind and brain. The structure of language, and the mental and neural basis of how it is learned, understood, and produced, have been perennial central questions in linguistics, computer science, cognitive science, and neuroscience. Historically, it has been a major challenge to develop implemented computational models that can generate and process language in anything approaching a human-like manner.<br /><br />In recent years, however, this situation has been transformed by the impressive success of modern deep-learning technology: relatively simple artificial neural network architectures, when coupled with large-scale natural language corpora and computational software and hardware for training massive models with billions to hundreds of billions of parameters, learn to generate complex text of remarkable fluency and even seem to exhibit numerous \"emergent\" behaviors such as the ability to rhyme, metaphorical language use, and certain types of common-sense reasoning. Contemporary large language models (LLMs) achieve these successes even though&ndash;or perhaps, because&ndash;their internal representations are high-dimensional numeric embedding vectors that superficially seem to be very unlike the symbolic, hierarchical grammatical representations traditionally used to describe linguistic structure. Despite this apparent difference, LLMs' context-based word predictions reflect complex aspects of linguistic structure and correlate with human behavioral responses, tree-structured grammatical representations of sentences can be decoded with surprising accuracy from LLMs' embeddings, and those embeddings can even be used to predict high-dimensional brain responses during real-time language comprehension.<br /><br />But language in LLMs is also very different from language in humans. LLMs' training data is not grounded in extra-linguistic sensory or social context; their inductive biases do not always reflect common features found across languages of the world; their interpretive strategies can be fooled by superficial features of linguistic inputs; their patterns in ambiguity management differ from humans; and their common-sense reasoning patterns are often unreliable and inconsistent. In some cases, symbolic approaches can still yield superior performance on their own or in tandem with LLMs. Overall, while LLMs constitute remarkable technological advances, there are strong reasons to believe that they offer far from a complete picture of language development and processing in the human mind and brain.<br /><br />Inspired by this state of affairs, this workshop offered interdisciplinary talks from 12 speakers and three moderated discussions among nine total commentators, spanning the fields of machine learning &amp; natural language processing, linguistics, neuroscience, and cognitive science. Thousands of online participants listened in over the workshop's two-day period. The website from the workshop hosts these talks and commentaries, which can be used in the future for education and research. Overall, the workshop advanced the development of its numerous constituent disciplines through presentations and discussions regarding the state of the art in all these disciplines and how Large Language Models have led to new insights and helped clarify key questions for future research.</p><br>\n<p>\n Last Modified: 04/11/2025<br>\nModified by: Roger&nbsp;P&nbsp;Levy</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nLarge Language Models (LLMs) are among the most transformative products of modern artificial intelligence. They are remarkably successful as technological tools, and pose great opportunities, but also challenges, for the scientific study of natural language in the human mind and brain. This workshop presented talks, commentary, and discussion dedicated to the following three themes:\n\n1. What insights do large language models provide for the study of human language?\n2. What insights does the study of human language provide for large language model development?\n3. What key future scientific opportunities lie at the interface between the study of human language and large language model development?\n\nThe workshop was dedicated to interdisciplinary connections between todays large language models, language structure, and language processing in the human mind and brain. The structure of language, and the mental and neural basis of how it is learned, understood, and produced, have been perennial central questions in linguistics, computer science, cognitive science, and neuroscience. Historically, it has been a major challenge to develop implemented computational models that can generate and process language in anything approaching a human-like manner.\n\nIn recent years, however, this situation has been transformed by the impressive success of modern deep-learning technology: relatively simple artificial neural network architectures, when coupled with large-scale natural language corpora and computational software and hardware for training massive models with billions to hundreds of billions of parameters, learn to generate complex text of remarkable fluency and even seem to exhibit numerous \"emergent\" behaviors such as the ability to rhyme, metaphorical language use, and certain types of common-sense reasoning. Contemporary large language models (LLMs) achieve these successes even thoughor perhaps, becausetheir internal representations are high-dimensional numeric embedding vectors that superficially seem to be very unlike the symbolic, hierarchical grammatical representations traditionally used to describe linguistic structure. Despite this apparent difference, LLMs' context-based word predictions reflect complex aspects of linguistic structure and correlate with human behavioral responses, tree-structured grammatical representations of sentences can be decoded with surprising accuracy from LLMs' embeddings, and those embeddings can even be used to predict high-dimensional brain responses during real-time language comprehension.\n\nBut language in LLMs is also very different from language in humans. LLMs' training data is not grounded in extra-linguistic sensory or social context; their inductive biases do not always reflect common features found across languages of the world; their interpretive strategies can be fooled by superficial features of linguistic inputs; their patterns in ambiguity management differ from humans; and their common-sense reasoning patterns are often unreliable and inconsistent. In some cases, symbolic approaches can still yield superior performance on their own or in tandem with LLMs. Overall, while LLMs constitute remarkable technological advances, there are strong reasons to believe that they offer far from a complete picture of language development and processing in the human mind and brain.\n\nInspired by this state of affairs, this workshop offered interdisciplinary talks from 12 speakers and three moderated discussions among nine total commentators, spanning the fields of machine learning & natural language processing, linguistics, neuroscience, and cognitive science. Thousands of online participants listened in over the workshop's two-day period. The website from the workshop hosts these talks and commentaries, which can be used in the future for education and research. Overall, the workshop advanced the development of its numerous constituent disciplines through presentations and discussions regarding the state of the art in all these disciplines and how Large Language Models have led to new insights and helped clarify key questions for future research.\t\t\t\t\tLast Modified: 04/11/2025\n\n\t\t\t\t\tSubmitted by: RogerPLevy\n"
 }
}