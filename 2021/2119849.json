{
 "awd_id": "2119849",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Measuring and Reducing Algorithmic Discrimination with Quasi-Experimental Data",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927269",
 "po_email": "ceavey@nsf.gov",
 "po_sign_block_name": "Cheryl Eavey",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2021-08-06",
 "awd_max_amd_letter_date": "2021-08-06",
 "awd_abstract_narration": "This research project will develop new tools to measure and reduce algorithmic discrimination in several high-stakes settings. Algorithms guide an increasingly large number of decisions. Alongside this rise is a concern that algorithmic decision-making will entrench or worsen discrimination against legally protected groups. However, quantifying algorithmic discrimination is often hampered by a selection challenge: an individual's qualification for a decision, which is often used to define discrimination, is typically only available for the group of individuals who were selected for treatment by an existing human or algorithmic decision-maker. This project will overcome this fundamental selection challenge by developing new tools to measure algorithmic discrimination. The project also will develop alternative algorithms that minimize or reduce discrimination. The researchers will apply these tools in multiple high-stakes settings, including pretrial detention, employment screening, medical testing, and child welfare investigations. The research is of considerable policy interest given the rapid adoption of algorithms in a variety of settings. The investigators are committed to increasing diversity in the economics research community by recruiting, training, and mentoring women, under-represented minorities, and first-generation college students as undergraduate research assistants and predoctoral fellows. Code produced by this project will be made publicly available.\r\n\r\nThis research project will develop tools to measure algorithmic discrimination. The project also will develop alternative non-discriminatory algorithms when qualification is unobserved for a subset of individuals. For example, in the employment context, whether an individual would be hired after an interview is not observed for applicants screened out before the interview is held. The investigators will show that this selection challenge can be overcome with knowledge of average qualification rates across different groups. Further, these average qualification rates can be estimated by utilizing random assignment of decision-makers to individuals. This insight can be used not only to measure algorithmic discrimination, but to develop alternative algorithms that reduce or eliminate discrimination. The project will consider several extensions. The investigators will utilize experimentation to measure algorithmic discrimination and improve accuracy. The interaction between algorithms and human decision-making also will be explored, as human discretion remains important in most real-world settings. The results of this research will have implications for more accurately quantifying the trade-offs between algorithmic transparency, accuracy, and fairness.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Will",
   "pi_last_name": "Dobbie",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Will S Dobbie",
   "pi_email_addr": "will_dobbie@hks.harvard.edu",
   "nsf_id": "000833501",
   "pi_start_date": "2021-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Hull",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peter Hull",
   "pi_email_addr": "peter_hull@brown.edu",
   "nsf_id": "000807008",
   "pi_start_date": "2021-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Arnold",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Arnold",
   "pi_email_addr": "daarnold@ucsd.edu",
   "nsf_id": "000846670",
   "pi_start_date": "2021-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "1033 Massachusetts Ave",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021385369",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132000",
   "pgm_ele_name": "Economics"
  },
  {
   "pgm_ele_code": "133300",
   "pgm_ele_name": "Methodology, Measuremt & Stats"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the project was to develop new quasi-experimental tools to understand algorithmic discrimination and build non-discriminatory algorithms when the outcome of interest is only selectively observed. With the support of NSF, we have completed two projects on this topic. In the first project, we study how to measure algorithmic discrimination in settings in which the target of interest is selectively observed. In the second, we show how to build algorithms that are non-discriminatory by construction, using insights from our first project.</p>\n<p>In &ldquo;Measuring Racial Discrimination in Algorithms&rdquo; (published at the <em>AEA Papers &amp; Proceedings</em>), we show how discrimination in algorithmic predictions can be measured in the context of pretrial bail decisions&mdash;extending methods we developed in Arnold, Dobbie, and Hull (2022) to measure discrimination in judicial decisions. We define algorithmic discrimination as differences in pretrial risk scores conditional on the defendant&rsquo;s objective potential for pretrial misconduct. In pretrial detention, misconduct is only selectively observed in the population as some defendants are detained and therefore never have an opportunity to engage in pretrial misconduct. This complicates the measurement of algorithmic discrimination in practice.&nbsp; We show how this selection problem is solved by estimating four race-specific parameters: the average pretrial misconduct potential in the population of white and Black defendants, along with the race-specific covariances of misconduct potential and algorithmic recommendations. We illustrate our approach using data from the NYC pretrial system, showing a large disparity in recommended release rates between equally risky white and Black defendants even when the algorithm does not directly use defendant race.</p>\n<p>Motivated by this finding of significant algorithmic discrimination in commonly used predictive algorithms, in &ldquo;Building Non-Discriminatory Algorithms in Selected Data&rdquo; (forthcoming in the <em>American Economic Review: Insights</em>), we develop new quasi-experimental tools to understand algorithmic discrimination and build non-discriminatory algorithms when the outcome of interest is only selectively observed. We focus our analysis on linear prediction models, which include the most common algorithms in the pretrial setting and elsewhere. We first show how algorithmic discrimination arises in such models from disparities in the algorithmic inputs among individuals with the same outcome of interest. For example, consider a scenario where Black defendants have more prior criminal convictions than white defendants with the same objective misconduct potential due to racial bias in past policing and prosecutorial decisions. Algorithmic predictions that put positive weight on this input would then, all else equal, generate systematically higher risk scores for Black defendants than white defendants with the same misconduct potential. We propose a new graphical tool to assess the net effect of such conditional disparities on algorithmic discrimination.</p>\n<p>This framework yields a conceptually simple way to build non-discriminatory algorithms: use an initial pre-processing step to purge conditional disparities in the inputs, thereby eliminating algorithmic discrimination at its source. The key empirical challenge in applying this framework is that the outcome of interest is only selectively observed in most high-stakes settings. Recall, for example, that in the pretrial context misconduct outcomes are observed only among past defendants who are released before trial and not among defendants who are detained. We next show how this challenge can be overcome with experimental or quasi-experimental variation. Specifically, we show that the challenge reduces to estimating a small set of moments capturing the mean of the selectively observed outcome and its correlation with race and the non-race algorithmic inputs. We then show how these key moments can be estimated by &ldquo;selection-correcting&rdquo; the observed misconduct potential mean and correlations using exogenous shocks to the selection mechanism, such as from a randomized experiment or the quasi-random assignment of individuals to decision-makers with varying treatment rates.</p>\n<p>We apply these new tools in the New York City pretrial system, using the quasi-random assignment of bail judges with varying release rates to address the selective observability of pretrial misconduct outcomes. We find that a conventional algorithm generates predicted risk scores that are 2.5 percentage points (8%) higher for Black defendants than white defendants conditional on true misconduct potential. Our non-discriminatory algorithm purges the conditional input disparities, thereby eliminating the risk score disparity between white and Black defendants with the same misconduct potential. By correcting for the selective observability of misconduct outcomes, our non-discriminatory algorithm also generates more accurate risk score predictions&mdash;reducing mean squared error by 12.1% compared to the conventional model. Our pre-processing adjustment thus offers a rare &ldquo;free lunch&rdquo; in this setting, improving both fairness and accuracy compared to conventional models.</p>\n<p>The tools we developed in these projects contribute to a broader understanding of algorithmic discrimination and the trade-off between fairness and accuracy in other high-stakes settings. Our work also has important policy implications, potentially helping to shape decisions on the adoption of non-discriminatory algorithms in other high-stakes settings where algorithmic predictions can have significant and lasting impacts on affected individuals.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/25/2024<br>\nModified by: Will&nbsp;S&nbsp;Dobbie</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of the project was to develop new quasi-experimental tools to understand algorithmic discrimination and build non-discriminatory algorithms when the outcome of interest is only selectively observed. With the support of NSF, we have completed two projects on this topic. In the first project, we study how to measure algorithmic discrimination in settings in which the target of interest is selectively observed. In the second, we show how to build algorithms that are non-discriminatory by construction, using insights from our first project.\n\n\nIn Measuring Racial Discrimination in Algorithms (published at the AEA Papers & Proceedings), we show how discrimination in algorithmic predictions can be measured in the context of pretrial bail decisionsextending methods we developed in Arnold, Dobbie, and Hull (2022) to measure discrimination in judicial decisions. We define algorithmic discrimination as differences in pretrial risk scores conditional on the defendants objective potential for pretrial misconduct. In pretrial detention, misconduct is only selectively observed in the population as some defendants are detained and therefore never have an opportunity to engage in pretrial misconduct. This complicates the measurement of algorithmic discrimination in practice. We show how this selection problem is solved by estimating four race-specific parameters: the average pretrial misconduct potential in the population of white and Black defendants, along with the race-specific covariances of misconduct potential and algorithmic recommendations. We illustrate our approach using data from the NYC pretrial system, showing a large disparity in recommended release rates between equally risky white and Black defendants even when the algorithm does not directly use defendant race.\n\n\nMotivated by this finding of significant algorithmic discrimination in commonly used predictive algorithms, in Building Non-Discriminatory Algorithms in Selected Data (forthcoming in the American Economic Review: Insights), we develop new quasi-experimental tools to understand algorithmic discrimination and build non-discriminatory algorithms when the outcome of interest is only selectively observed. We focus our analysis on linear prediction models, which include the most common algorithms in the pretrial setting and elsewhere. We first show how algorithmic discrimination arises in such models from disparities in the algorithmic inputs among individuals with the same outcome of interest. For example, consider a scenario where Black defendants have more prior criminal convictions than white defendants with the same objective misconduct potential due to racial bias in past policing and prosecutorial decisions. Algorithmic predictions that put positive weight on this input would then, all else equal, generate systematically higher risk scores for Black defendants than white defendants with the same misconduct potential. We propose a new graphical tool to assess the net effect of such conditional disparities on algorithmic discrimination.\n\n\nThis framework yields a conceptually simple way to build non-discriminatory algorithms: use an initial pre-processing step to purge conditional disparities in the inputs, thereby eliminating algorithmic discrimination at its source. The key empirical challenge in applying this framework is that the outcome of interest is only selectively observed in most high-stakes settings. Recall, for example, that in the pretrial context misconduct outcomes are observed only among past defendants who are released before trial and not among defendants who are detained. We next show how this challenge can be overcome with experimental or quasi-experimental variation. Specifically, we show that the challenge reduces to estimating a small set of moments capturing the mean of the selectively observed outcome and its correlation with race and the non-race algorithmic inputs. We then show how these key moments can be estimated by selection-correcting the observed misconduct potential mean and correlations using exogenous shocks to the selection mechanism, such as from a randomized experiment or the quasi-random assignment of individuals to decision-makers with varying treatment rates.\n\n\nWe apply these new tools in the New York City pretrial system, using the quasi-random assignment of bail judges with varying release rates to address the selective observability of pretrial misconduct outcomes. We find that a conventional algorithm generates predicted risk scores that are 2.5 percentage points (8%) higher for Black defendants than white defendants conditional on true misconduct potential. Our non-discriminatory algorithm purges the conditional input disparities, thereby eliminating the risk score disparity between white and Black defendants with the same misconduct potential. By correcting for the selective observability of misconduct outcomes, our non-discriminatory algorithm also generates more accurate risk score predictionsreducing mean squared error by 12.1% compared to the conventional model. Our pre-processing adjustment thus offers a rare free lunch in this setting, improving both fairness and accuracy compared to conventional models.\n\n\nThe tools we developed in these projects contribute to a broader understanding of algorithmic discrimination and the trade-off between fairness and accuracy in other high-stakes settings. Our work also has important policy implications, potentially helping to shape decisions on the adoption of non-discriminatory algorithms in other high-stakes settings where algorithmic predictions can have significant and lasting impacts on affected individuals.\n\n\n\t\t\t\t\tLast Modified: 10/25/2024\n\n\t\t\t\t\tSubmitted by: WillSDobbie\n"
 }
}