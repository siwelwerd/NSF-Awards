{
 "awd_id": "2125692",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Advancing Fairness in AI with Human-Algorithm Collaborations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Steven Breckler",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 565013.0,
 "awd_amount": 498076.0,
 "awd_min_amd_letter_date": "2021-03-25",
 "awd_max_amd_letter_date": "2022-03-22",
 "awd_abstract_narration": "Artificial intelligence (AI) systems are increasingly used to assist humans in making high-stakes decisions, such as online information curation, resume screening, mortgage lending, police surveillance, public resource allocation, and pretrial detention. While the hope is that the use of algorithms will improve societal outcomes and economic efficiency, concerns have been raised that algorithmic systems might inherit human biases from historical data, perpetuate discrimination against already vulnerable populations, and generally fail to embody a given community's important values. Recent work on algorithmic fairness has characterized the manner in which unfairness can arise at different steps along the development pipeline, produced dozens of quantitative notions of fairness, and provided methods for enforcing these notions. However, there is a significant gap between the over-simplified algorithmic objectives and the complications of real-world decision-making contexts. This project aims to close the gap by explicitly accounting for the context-specific fairness principles of actual stakeholders, their acceptable fairness-utility trade-offs, and the cognitive strengths and limitations of human decision-makers throughout the development and deployment of the algorithmic system. \r\n\r\nTo meet these goals, this project enables close human-algorithm collaborations that combine innovative machine learning methods with approaches from human-computer interaction (HCI) for eliciting feedback and preferences from human experts and stakeholders. There are three main research activities that naturally correspond to three stages of a human-in-the-loop AI system. First, the project will develop novel fairness elicitation mechanisms that will allow stakeholders to effectively express their perceptions on fairness. To go beyond the traditional approach of statistical group fairness, the investigators will formulate new fairness measures for individual fairness based on elicited feedback. Secondly, the project will develop algorithms and mechanisms to manage the trade-offs between the new fairness measures developed in the first step, and multiple existing fairness and accuracy measures. Finally, the project will develop algorithms to detect and mitigate human operators' biases, and methods that rely on human feedback to correct and de-bias existing models during the deployment of the AI system.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhiwei Steven",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhiwei Steven Wu",
   "pi_email_addr": "zstevenwu@cmu.edu",
   "nsf_id": "000792262",
   "pi_start_date": "2021-03-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue WQED Building",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 482074.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research project has advanced the research and education on algorithmic fairness. The outcomes can be organized into three themes.</p>\n<p>1. Fairness Elicitation</p>\n<p>Researchers developed a new framework for fairness elicitation that allows stakeholders to contribute their perspectives directly, making machine learning models more context-specific in their understanding of fairness. By learning models that respect stakeholders' fairness constraints while maintaining accuracy, the research bridges theoretical fairness constructs and practical needs.</p>\n<p>Key achievements included integrating tailored fairness definitions with existing statistical metrics, leading to publications in <em>FORC 2021</em>&nbsp;and <em>NeurIPS 2020</em>. The latter introduced an algorithm for real-time learning models that respect individual fairness constraints. Here, an auditor oversees decisions to identify violations, enabling the system to adapt and maintain accuracy. This novel approach marks a significant shift toward adaptive, sensitive AI.</p>\n<p><br />2. Managing Algorithmic Trade-offs</p>\n<p>The project also addressed the challenge of balancing fairness and accuracy. By developing methods to visualize these trade-offs, designers and users can better understand which algorithms align with their specific goals. This approach was demonstrated in a large-scale recidivism prediction experiment, outlined in <em>DIS 2020</em>, and showed how visualization tools facilitate decision-making in high-stakes scenarios.</p>\n<p>Another major contribution is the \"Value Card\" toolkit, introducing Fairness, Accountability, Transparency, and Ethics (FATE) principles into computer science education. By helping students grasp the impacts of machine learning models, the toolkit prepares a new generation to manage algorithmic trade-offs in model deployment. Published in <em>ACM FAccT 2021</em>, it has already shown effectiveness in improving students&rsquo; understanding.</p>\n<p>3. Investigating Algorithmic Bias</p>\n<p>Investigating bias in real-world systems is another crucial outcome of the project. The research evaluated the Allegheny Family Screening Tool (AFST), an AI system used in child welfare, through in-depth interviews with workers. This process revealed concerns about biases and the need to better align AI recommendations with diverse values. Research findings from this work appeared in<em>&nbsp;FAccT 2022, CHI 2022, and DIS 2022</em>, highlighting the importance of developing tools sensitive to social contexts.</p>\n<p>In a separate study, researchers explored biases in the gig economy by engaging policy experts and conducting workshops with gig workers. This study identified critical issues of bias that affect worker conditions and well-being. The findings emphasized the importance of transparent data-sharing platforms that can empower gig workers to address discrimination, helping to inform future policy and a fairer gig economy.</p>\n<p>Furthermore, the project tackled predictive models in fields like medicine and criminal justice. It introduced methods to reduce biases caused by outcome measurement errors and selection biases from historical data. This unbiased risk minimization method, validated with healthcare and employment data, improves model reliability while correcting for these errors. The resulting paper in <em>FAccT 2023</em>&nbsp;won a Best Paper Award.</p>\n<p><br />Conclusion</p>\n<p>Overall, this project has advanced fairness in AI through technical frameworks, empirical studies, and stakeholder engagement. It has paved the way for machine learning systems to better meet ethical and social needs. By integrating personalized fairness definitions, managing algorithmic trade-offs, and addressing real-world biases, the project provides comprehensive solutions that ensure fairness in practice.&nbsp;</p><br>\n<p>\n Last Modified: 05/07/2024<br>\nModified by: Zhiwei Steven&nbsp;Wu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe research project has advanced the research and education on algorithmic fairness. The outcomes can be organized into three themes.\n\n\n1. Fairness Elicitation\n\n\nResearchers developed a new framework for fairness elicitation that allows stakeholders to contribute their perspectives directly, making machine learning models more context-specific in their understanding of fairness. By learning models that respect stakeholders' fairness constraints while maintaining accuracy, the research bridges theoretical fairness constructs and practical needs.\n\n\nKey achievements included integrating tailored fairness definitions with existing statistical metrics, leading to publications in FORC 2021and NeurIPS 2020. The latter introduced an algorithm for real-time learning models that respect individual fairness constraints. Here, an auditor oversees decisions to identify violations, enabling the system to adapt and maintain accuracy. This novel approach marks a significant shift toward adaptive, sensitive AI.\n\n\n\n2. Managing Algorithmic Trade-offs\n\n\nThe project also addressed the challenge of balancing fairness and accuracy. By developing methods to visualize these trade-offs, designers and users can better understand which algorithms align with their specific goals. This approach was demonstrated in a large-scale recidivism prediction experiment, outlined in DIS 2020, and showed how visualization tools facilitate decision-making in high-stakes scenarios.\n\n\nAnother major contribution is the \"Value Card\" toolkit, introducing Fairness, Accountability, Transparency, and Ethics (FATE) principles into computer science education. By helping students grasp the impacts of machine learning models, the toolkit prepares a new generation to manage algorithmic trade-offs in model deployment. Published in ACM FAccT 2021, it has already shown effectiveness in improving students understanding.\n\n\n3. Investigating Algorithmic Bias\n\n\nInvestigating bias in real-world systems is another crucial outcome of the project. The research evaluated the Allegheny Family Screening Tool (AFST), an AI system used in child welfare, through in-depth interviews with workers. This process revealed concerns about biases and the need to better align AI recommendations with diverse values. Research findings from this work appeared inFAccT 2022, CHI 2022, and DIS 2022, highlighting the importance of developing tools sensitive to social contexts.\n\n\nIn a separate study, researchers explored biases in the gig economy by engaging policy experts and conducting workshops with gig workers. This study identified critical issues of bias that affect worker conditions and well-being. The findings emphasized the importance of transparent data-sharing platforms that can empower gig workers to address discrimination, helping to inform future policy and a fairer gig economy.\n\n\nFurthermore, the project tackled predictive models in fields like medicine and criminal justice. It introduced methods to reduce biases caused by outcome measurement errors and selection biases from historical data. This unbiased risk minimization method, validated with healthcare and employment data, improves model reliability while correcting for these errors. The resulting paper in FAccT 2023won a Best Paper Award.\n\n\n\nConclusion\n\n\nOverall, this project has advanced fairness in AI through technical frameworks, empirical studies, and stakeholder engagement. It has paved the way for machine learning systems to better meet ethical and social needs. By integrating personalized fairness definitions, managing algorithmic trade-offs, and addressing real-world biases, the project provides comprehensive solutions that ensure fairness in practice.\t\t\t\t\tLast Modified: 05/07/2024\n\n\t\t\t\t\tSubmitted by: Zhiwei StevenWu\n"
 }
}