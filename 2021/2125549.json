{
 "awd_id": "2125549",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SCC-PG: Closed-loop Intervention to Promote a Supportive and Interactive Environment around Children",
 "cfda_num": "47.070, 47.076",
 "org_code": "05050000",
 "po_phone": "7032928637",
 "po_email": "llyons@nsf.gov",
 "po_sign_block_name": "Leilah Lyons",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 177000.0,
 "awd_min_amd_letter_date": "2021-08-25",
 "awd_max_amd_letter_date": "2023-07-18",
 "awd_abstract_narration": "For both parents and educators, monitoring and adjusting their behaviors to ensure that children develop appropriate prosocial and learning behaviors is a complex balance between nurturance and limit setting. When these interactions are strained, negative or coercive cycles may emerge that delay appropriate development and exacerbate existing impairment. To disrupt the development of coercive cycles, adults must have the ability to accurately assess the quality of their interactions with children and integrate this information into personal change. Approaches to measuring these types of interactions will inform what we know about the mechanisms of child social, emotional, and learning development in STEM learning settings, and enable the creation of adaptive interventions for those moments when support is most needed. This project envisions a closed-loop intervention framework to promote a supportive and interactive environment around children. Smart wearables will sense interaction and responses between the children and their parents or educators, using embedded machine learning technology to recognize supportive behaviors. The perceived behaviors will be sent to a cloud server where adaptive interaction strategies will be identified from either online psychological consultation or artificial intelligence. These interaction strategies will then be provided to the parents and educators in the form of guidance cues to promote a supportive STEM learning environment around the children.\r\n\r\nThis planning project aims to understand the barriers and critical problems in the implementation of smart technology and psychological strategies to support adult-child interactions in STEM learning settings. The work will proceed by convening key stakeholders (parent organizations, formal educational institutions, and informal educational institutions) in a series of iterative discussions to produce a set of adult-child behavioral targets that are essential to children\u2019s development of social, emotional, and learning skills. Further discussions will then identify mechanisms to enhance these behaviors, and reduce competing, less effective approaches. Qualitative thematic analysis of the discussions will be used to capture these behaviors and mechanisms. Then technologies will be developed to measure, provide feedback on, and improve these behaviors. These devices will be piloted with adult-child dyads. Audiovisual data collected by the devices will be human coded as well as processed by algorithms to vet the technological capacity of the devices to detect and respond to targeted behaviors. A series of debriefing interviews and surveys with adult-child dyads will be used to determine the feasibility, acceptability, and utility of the devices. The collected preliminary data will support the forming of critical technological and social science research questions that co-inform one another: questions about the social engagement between adults and children will drive the technical research, and what can be discovered via the technological research will open up new questions that can be posed about social engagement between children and adults. Adult-child interactions are key social factors that integrate to produce student social, emotional, and academic outcomes. Within our informal educational communities, our formal educational communities, and our familial communities it is essential to find the best mechanisms for measuring, providing feedback, and improving these interactions. This work thus seeks to advance a new approach to, and evidence-based understanding of, the development of STEM learning. This Smart and Connected Communities project is also supported by the Advancing Informal STEM Learning program, which seeks to (a) advance new approaches to and evidence-based understanding of the design and development of STEM learning in informal environments; (b) provide multiple pathways for broadening access to and engagement in STEM learning experiences; (c) advance innovative research on and assessment of STEM learning in informal environments; and (d) engage the public of all ages in learning STEM in informal environments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ou",
   "pi_last_name": "Bai",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ou Bai",
   "pi_email_addr": "obai@fiu.edu",
   "nsf_id": "000507662",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kellina",
   "pi_last_name": "Lupas",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Kellina K Lupas",
   "pi_email_addr": "kpyle@fiu.edu",
   "nsf_id": "000851142",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Pelham",
   "pi_mid_init": "E",
   "pi_sufx_name": "Jr",
   "pi_full_name": "William E Pelham",
   "pi_email_addr": "wpelham@fiu.edu",
   "nsf_id": "000850818",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Florida International University",
  "inst_street_address": "11200 SW 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "MIAMI",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3053482494",
  "inst_zip_code": "331992516",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "FL26",
  "org_lgl_bus_name": "FLORIDA INTERNATIONAL UNIVERSITY",
  "org_prnt_uei_num": "Q3KCVK5S9CP1",
  "org_uei_num": "Q3KCVK5S9CP1"
 },
 "perf_inst": {
  "perf_inst_name": "Florida International University",
  "perf_str_addr": "11200 SW 8TH ST",
  "perf_city_name": "Miami",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "331990001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "FL26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725900",
   "pgm_ele_name": "AISL"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "042Z",
   "pgm_ref_txt": "S&CC: Smart and Connected Communities"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "04002324DB",
   "fund_name": "NSF STEM Education",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0421",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002122DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 150000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 27000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>For both parents and educators, monitoring and adjusting their behaviors to ensure that children develop appropriate prosocial and learning behaviors is a complex balance between nurturance and limit setting. This project envisions a closed-loop intervention framework to promote a supportive and interactive environment around children. Smart wearables will sense interaction and responses between the children and their parents or educators, using embedded machine learning technology to recognize supportive behaviors. The perceived behaviors will be sent to a cloud server where adaptive interaction strategies will be identified from either online psychological consultation or artificial intelligence. These interaction strategies will then be provided to the parents and educators in the form of guidance cues to promote a supportive STEM learning environment around the children.</p>\n<p>In the designed closed-loop intervention framework, customized dyadic interaction cameras are employed to capture real-time video and audio data during these interactions. The audio recordings are automatically transcribed into text data using an auto-transcription service. Subsequently, DPICS (Dyadic Parent-Child Interaction Coding System) codes are assigned to each utterance using a DPICS machine learning (ML) model. Additionally, the dyad&rsquo;s emotional states are determined from the video and voice/tone data using fine-tuned ML models for facial expression and tone-based emotion analysis. Monitoring emotional states during dyadic interactions is vital for effective communication, relationship building, conflict resolution, adaptation to changing circumstances, understanding cultural nuances, and promoting overall well-being. This is crucial for teachers and parents to collaborate effectively with children, aiming for positive outcomes. Lastly, the DPICS codes and emotional states of the dyad are integrated for quality analysis of the dyadic interactions. The preliminary results of the ML model development from this pilot fund are:</p>\n<ul>\n<li>Speaker Identification to Differentiate Teacher/Parent&rsquo;s Talk from Child&rsquo;s Talk: We tested dyadic interaction data with a total of 4,123 utterances. For the utterances longer than 0.8s without a third person involvement, we observed that 99.3% of them (3,172 utterances including 2,362 parent&rsquo;s utterances and 810 child&rsquo;s utterances) were correctly labeled as speaker A or B by the Assembly AI. Further, we studied six 1-minute-long parent-child interaction data, we found that 97.0% of utterances were correctly identified as either parent or child by the developed automated system.</li>\n<li>Automated and Accurate DPICS Coding for Dyadic Interactions: For the first experiment, we achieved 71.0% accuracy from five-fold cross-validation when employing the Roberta embedding method. When adding four-family data to train the model, the accuracy achieved 75.5% on the left one-family data. This result is comparable to a previous ML-based DPICS model for eight DPICS codes of 78.3% (our model can achieve 79.8% for eight DPICS codes). We found that when providing more data, the performance can be further improved, i.e., 71.0% vs. 75.5%. Moreover, the higher performance using a pre-trained model, i.e., Roberta, suggests that pre-trained models may better address the complex natural language modeling. </li>\n<li>Automated and Accurate Facial Emotion Recognition during Teacher/Parent-ChildInteractions: We have investigated the model accuracy and speed/computational load from several facial emotion recognition models We first tested the model performance on the public benchmark dataset of CK+ and FER2013, achieving an accuracy of 100% and 92.5% respectively. Moreover, we investigated multiple labels from the seven emotions (e.g., Happy/Surprise, Sad/Angry), and the ML model results were treated as correct answers if they matched one of the multiple labels by manual annotation. The average accuracy for three families (three parents and three children) is 88.1%; despite the emotion recognition for one parent and one child is low about 65%, the rest are all above 94%. </li>\n<li>Automated and accurate emotion recognition from dyadic interaction voices/tones: After evaluating state-of-art tone-based emotion models, we determined the SOTAemotion model, i.e., TIM-Net, which provided the highest accuracy over six public data sets. We further evaluated the same three families used in facial emotion recognition. we removed periods with a third person as it is difficult to understand the effects from the third person to the dyads. Then, we segmented the data into utterances for tone emotion recognition including 949, 1,387, and 419 utterances from three families of a total of 2,755 utterances. We achieved an overall accuracy of 84.0% over seven emotions on parents&rsquo; utterances while an overall accuracy of 67.8% on children's utterances. We found that one reason for lower performance in children is that the children had more variable and abundant tones in a spoiled manner. Another possible reason is that the emotion model was trained from adults only, but the child&rsquo;s frequency range is different than that of adults which makes the model less robust.</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/04/2024<br>\nModified by: Ou&nbsp;Bai</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nFor both parents and educators, monitoring and adjusting their behaviors to ensure that children develop appropriate prosocial and learning behaviors is a complex balance between nurturance and limit setting. This project envisions a closed-loop intervention framework to promote a supportive and interactive environment around children. Smart wearables will sense interaction and responses between the children and their parents or educators, using embedded machine learning technology to recognize supportive behaviors. The perceived behaviors will be sent to a cloud server where adaptive interaction strategies will be identified from either online psychological consultation or artificial intelligence. These interaction strategies will then be provided to the parents and educators in the form of guidance cues to promote a supportive STEM learning environment around the children.\n\n\nIn the designed closed-loop intervention framework, customized dyadic interaction cameras are employed to capture real-time video and audio data during these interactions. The audio recordings are automatically transcribed into text data using an auto-transcription service. Subsequently, DPICS (Dyadic Parent-Child Interaction Coding System) codes are assigned to each utterance using a DPICS machine learning (ML) model. Additionally, the dyads emotional states are determined from the video and voice/tone data using fine-tuned ML models for facial expression and tone-based emotion analysis. Monitoring emotional states during dyadic interactions is vital for effective communication, relationship building, conflict resolution, adaptation to changing circumstances, understanding cultural nuances, and promoting overall well-being. This is crucial for teachers and parents to collaborate effectively with children, aiming for positive outcomes. Lastly, the DPICS codes and emotional states of the dyad are integrated for quality analysis of the dyadic interactions. The preliminary results of the ML model development from this pilot fund are:\n\nSpeaker Identification to Differentiate Teacher/Parents Talk from Childs Talk: We tested dyadic interaction data with a total of 4,123 utterances. For the utterances longer than 0.8s without a third person involvement, we observed that 99.3% of them (3,172 utterances including 2,362 parents utterances and 810 childs utterances) were correctly labeled as speaker A or B by the Assembly AI. Further, we studied six 1-minute-long parent-child interaction data, we found that 97.0% of utterances were correctly identified as either parent or child by the developed automated system.\nAutomated and Accurate DPICS Coding for Dyadic Interactions: For the first experiment, we achieved 71.0% accuracy from five-fold cross-validation when employing the Roberta embedding method. When adding four-family data to train the model, the accuracy achieved 75.5% on the left one-family data. This result is comparable to a previous ML-based DPICS model for eight DPICS codes of 78.3% (our model can achieve 79.8% for eight DPICS codes). We found that when providing more data, the performance can be further improved, i.e., 71.0% vs. 75.5%. Moreover, the higher performance using a pre-trained model, i.e., Roberta, suggests that pre-trained models may better address the complex natural language modeling. \nAutomated and Accurate Facial Emotion Recognition during Teacher/Parent-ChildInteractions: We have investigated the model accuracy and speed/computational load from several facial emotion recognition models We first tested the model performance on the public benchmark dataset of CK+ and FER2013, achieving an accuracy of 100% and 92.5% respectively. Moreover, we investigated multiple labels from the seven emotions (e.g., Happy/Surprise, Sad/Angry), and the ML model results were treated as correct answers if they matched one of the multiple labels by manual annotation. The average accuracy for three families (three parents and three children) is 88.1%; despite the emotion recognition for one parent and one child is low about 65%, the rest are all above 94%. \nAutomated and accurate emotion recognition from dyadic interaction voices/tones: After evaluating state-of-art tone-based emotion models, we determined the SOTAemotion model, i.e., TIM-Net, which provided the highest accuracy over six public data sets. We further evaluated the same three families used in facial emotion recognition. we removed periods with a third person as it is difficult to understand the effects from the third person to the dyads. Then, we segmented the data into utterances for tone emotion recognition including 949, 1,387, and 419 utterances from three families of a total of 2,755 utterances. We achieved an overall accuracy of 84.0% over seven emotions on parents utterances while an overall accuracy of 67.8% on children's utterances. We found that one reason for lower performance in children is that the children had more variable and abundant tones in a spoiled manner. Another possible reason is that the emotion model was trained from adults only, but the childs frequency range is different than that of adults which makes the model less robust.\n\n\n\n\t\t\t\t\tLast Modified: 05/04/2024\n\n\t\t\t\t\tSubmitted by: OuBai\n"
 }
}