{
 "awd_id": "2127918",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Enabling Interpretable AI via Bayesian Deep Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2025-09-30",
 "tot_intn_awd_amt": 499926.0,
 "awd_amount": 499926.0,
 "awd_min_amd_letter_date": "2021-08-25",
 "awd_max_amd_letter_date": "2022-08-09",
 "awd_abstract_narration": "Interpretability is one of the fundamental obstacles on the adoption and deployment of deep-learning-based AI systems across various fields such as healthcare, e-commerce, transportation, earth science, and manufacturing. An ideal interpretable model should be able to interpret its prediction using human-understandable concepts (e.g., \u201ccolor\u201d and \u201cshape\u201d), conform to conditional dependencies in the real world (e.g., whether a customer's purchase is due to a discount), and handle uncertainty in data (e.g., how certain the model is about the rainfall tomorrow). Unfortunately, deep learning as a connectionist approach does not natively support these desiderata. The goal of this project is to develop a general interpreter framework for deep learning models. Interpreters under this framework can be plugged into a deep learning model and interpret its predictions using a graph of human-understandable concepts, without sacrificing the model\u2019s performance. Methods developed in this project will be applied in health monitoring to interpret models\u2019 reasoning on patient status, and in recommender systems to interpret models\u2019 recommended items for users.\r\n\r\nThis project will develop two sets of methods based on Bayesian deep learning: (1) \u201cBayesian deep interpreters\u201d that interpret deep learning models with graphical models describing the conditional dependencies leading to current predictions. (2) \u201cBayesian deep controllers\u201d that control deep learning models' predictions by manipulating specific random variables in the graphical models attached to the controlled models. Development of such novel methods will build intellectual and formal connection between deep learning and probabilistic graphical models, two major machine learning paradigms that have long been seen as incompatible. It will advance the state of the art on machine learning and AI by: (1) formulating a new Bayesian deep learning framework to unify deep learning and graphical models, the synergy of which will significantly improve deep learning interpretability, (2) under such a principled framework, designing concrete methods that are plug-and-play and therefore do not sacrifice the deep learning models' performance (e.g., accuracy),  (3) investigating what theoretical guarantees the developed methods provide and therefore laying foundations for future work by the team and the community, (4) analyzing the trade-off between accuracy, interpretability, and controllability and providing design guidance for interpretable AI systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hao",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hao Wang",
   "pi_email_addr": "hw488@cs.rutgers.edu",
   "nsf_id": "000819788",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yongfeng",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yongfeng Zhang",
   "pi_email_addr": "yongfeng.zhang@rutgers.edu",
   "nsf_id": "000763753",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "33 Knightsbridge Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088543925",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 360350.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 139576.0
  }
 ],
 "por": null
}