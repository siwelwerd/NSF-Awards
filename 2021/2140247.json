{
 "awd_id": "2140247",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Distributed Heterogeneous Data Analytics via Federated Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2021-08-25",
 "awd_max_amd_letter_date": "2021-08-25",
 "awd_abstract_narration": "With the growth of devices in the Internet of Things (IoT), a huge amount of data are generated at the network edge. This provides valuable resources for learning insightful information and enabling intelligent applications such as, self-driving, video analytics, anomaly detection, etc. Federated learning (FL) is a promising technique that enables a large number of clients orchestrated by a central server to collaboratively learn a machine learning model without sharing data. However, the data owned by different devices are typically not independent and identically distributed (non-IID) due to different user preferences and usage patterns. Conventional FL methods fail to generalize well for most clients. In addition to data heterogeneity, system heterogeneity; that is, where clients have different computation and communication capabilities, is another critical challenge for FL development.  Because the central server does not perform the aggregation until receiving all the clients\u2019 updates, system heterogeneity significantly slows down the model training if the clients are randomly selected to participate in the training. The goal of this research is to develop a unified FL framework for addressing both data and system heterogeneity at the same time.\r\n\r\nThis project will pave the foundations for properly handling data and system heterogeneity in FL with three integrated components: 1) unveiling essential reasons of performance degradation in FL with non-IID data; 2) exploring comprehensive principles to guide the client composition for FL with non-IID data; and 3) developing a unified FL method for addressing both data and system heterogeneity simultaneously, including a client utility function and a reinforcement learning based client composition method. This project will develop and train undergraduate and graduate researchers with comprehensive experience for developing FL systems, including recruiting minority and under-represented students. The outcome of this project will be incorporated in both new and existing undergraduate and graduate courses at Duke University.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiran",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiran Chen",
   "pi_email_addr": "yiran.chen@duke.edu",
   "nsf_id": "000575362",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St. Suite 710",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we designed heterogeneity-aware federated learning (FL) methods to enable efficient data analytics, considering both data and system heterogeneity. This approach aimed to provide a better accuracy-speed tradeoff for FL training. The research comprised two tasks. The first focused on data heterogeneity, aiming to reduce the inconsistency between the local objective of each client and the global objective. The second task considered system heterogeneity, leading to the design of a unified FL framework that addresses both data and system heterogeneity simultaneously.</p>\n<p>In the first task, we developed FedMask &ndash; a communication and computation-efficient FL framework. By applying FedMask, each device can learn a personalized and structured sparse Deep Neural Network (DNN), which runs efficiently on devices. To achieve our goal, each device learns a sparse binary mask (i.e., 1 bit per network parameter) while keeping the parameters of each local model unchanged; only these binary masks are communicated between the server and the devices. Instead of learning a shared global model in classic FL, each device obtains a personalized and structured sparse model composed by applying the learned binary mask to the fixed parameters of the local model. Our experiments showed that FedMask, compared with status quo approaches, improved inference accuracy by 28.47%, and reduced communication and computation costs by 34.48X and 2.44X, respectively. FedMask also achieved a 1.56X inference speedup and reduced energy consumption by 1.78X.</p>\n<p>Knowledge Distillation (KD) is a highly effective approach for deploying large-scale pre-trained language models in low-latency environments. It transfers the knowledge from large-scale models to smaller student models. Traditional KD approaches utilize soft labels and intermediate activations generated by the teacher to transfer knowledge solely to the student model parameters. In our work, ReAugKD, we demonstrated that providing non-parametric memory access in the form of a knowledge base containing the teacher&rsquo;s soft labels and predictions can further enhance the student model's capacity and improve generalization. To enable efficient retrieval from the knowledge base by the student, we proposed a new Retrieval-augmented KD framework with a loss function that aligns the relational knowledge in the teacher and student embedding spaces. Our experiments show that this retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark, achieving a score of 81.76.</p>\n<p>In the second task, we observed that due to the often-limited communication bandwidth of edge devices, most existing FL methods randomly select only a subset of devices to participate in training in each communication round. This approach, compared to engaging all available clients, can lead to significant performance degradation on non-IID (independent and identically distributed) data. We identified that the essential cause of this degradation is the class imbalance in the grouped data from randomly selected clients. Based on this insight, we designed an efficient heterogeneity-aware client sampling mechanism, namely Federated Class-balanced Sampling (Fed-CBS), which effectively reduces class imbalance in the grouped dataset from intentionally selected clients. We first proposed a privacy-preserving measure of class imbalance. Utilizing this measure, we developed a computation-efficient client sampling strategy, ensuring that the actively selected clients generate a more class-balanced grouped dataset with theoretical guarantees. Experimental results show that Fed-CBS outperforms existing approaches in terms of test accuracy and convergence rate, while achieving comparable or even better performance than the ideal scenario where all available clients participate in FL training.</p>\n<p>This two-year research project supported two graduate students, resulting in the publication of eight conference papers in total. The students completed summer internships with Apple, Meta, and Vellum. One student received his Ph.D. during the project period and subsequently joined Meta as a PostDoc.</p>\n<p>Additionally, the project produced 18 invited seminars, talks, and panels to disseminate its research outcomes.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/03/2024<br>\nModified by: Yiran&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we designed heterogeneity-aware federated learning (FL) methods to enable efficient data analytics, considering both data and system heterogeneity. This approach aimed to provide a better accuracy-speed tradeoff for FL training. The research comprised two tasks. The first focused on data heterogeneity, aiming to reduce the inconsistency between the local objective of each client and the global objective. The second task considered system heterogeneity, leading to the design of a unified FL framework that addresses both data and system heterogeneity simultaneously.\n\n\nIn the first task, we developed FedMask  a communication and computation-efficient FL framework. By applying FedMask, each device can learn a personalized and structured sparse Deep Neural Network (DNN), which runs efficiently on devices. To achieve our goal, each device learns a sparse binary mask (i.e., 1 bit per network parameter) while keeping the parameters of each local model unchanged; only these binary masks are communicated between the server and the devices. Instead of learning a shared global model in classic FL, each device obtains a personalized and structured sparse model composed by applying the learned binary mask to the fixed parameters of the local model. Our experiments showed that FedMask, compared with status quo approaches, improved inference accuracy by 28.47%, and reduced communication and computation costs by 34.48X and 2.44X, respectively. FedMask also achieved a 1.56X inference speedup and reduced energy consumption by 1.78X.\n\n\nKnowledge Distillation (KD) is a highly effective approach for deploying large-scale pre-trained language models in low-latency environments. It transfers the knowledge from large-scale models to smaller student models. Traditional KD approaches utilize soft labels and intermediate activations generated by the teacher to transfer knowledge solely to the student model parameters. In our work, ReAugKD, we demonstrated that providing non-parametric memory access in the form of a knowledge base containing the teachers soft labels and predictions can further enhance the student model's capacity and improve generalization. To enable efficient retrieval from the knowledge base by the student, we proposed a new Retrieval-augmented KD framework with a loss function that aligns the relational knowledge in the teacher and student embedding spaces. Our experiments show that this retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark, achieving a score of 81.76.\n\n\nIn the second task, we observed that due to the often-limited communication bandwidth of edge devices, most existing FL methods randomly select only a subset of devices to participate in training in each communication round. This approach, compared to engaging all available clients, can lead to significant performance degradation on non-IID (independent and identically distributed) data. We identified that the essential cause of this degradation is the class imbalance in the grouped data from randomly selected clients. Based on this insight, we designed an efficient heterogeneity-aware client sampling mechanism, namely Federated Class-balanced Sampling (Fed-CBS), which effectively reduces class imbalance in the grouped dataset from intentionally selected clients. We first proposed a privacy-preserving measure of class imbalance. Utilizing this measure, we developed a computation-efficient client sampling strategy, ensuring that the actively selected clients generate a more class-balanced grouped dataset with theoretical guarantees. Experimental results show that Fed-CBS outperforms existing approaches in terms of test accuracy and convergence rate, while achieving comparable or even better performance than the ideal scenario where all available clients participate in FL training.\n\n\nThis two-year research project supported two graduate students, resulting in the publication of eight conference papers in total. The students completed summer internships with Apple, Meta, and Vellum. One student received his Ph.D. during the project period and subsequently joined Meta as a PostDoc.\n\n\nAdditionally, the project produced 18 invited seminars, talks, and panels to disseminate its research outcomes.\n\n\n\t\t\t\t\tLast Modified: 01/03/2024\n\n\t\t\t\t\tSubmitted by: YiranChen\n"
 }
}