{
 "awd_id": "2132887",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Mutually Assistive Robotics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2022-01-01",
 "awd_exp_date": "2025-08-31",
 "tot_intn_awd_amt": 1499499.0,
 "awd_amount": 1515499.0,
 "awd_min_amd_letter_date": "2021-08-25",
 "awd_max_amd_letter_date": "2023-07-24",
 "awd_abstract_narration": "This project will advance the state-of-the-art for how robots that render assistance to users with disabilities interact with and learn from those users.  Most current work takes a deficit-based approach to disability, in which the robot is assumed to be the more competent partner in the interaction and the user with disabilities provides high-level goals to a system that primarily helps them with mundane tasks of daily living.  This approach is deficient in two significant ways: first, it removes agency and control from users, directly counteracting the psychological benefits of assistive technology and potentially replacing a loss of independence to caregivers with a loss of independence to robots; and second, it fails to address tasks that improve quality of life, such as artistic expression or grooming, where the specific sequence of actions, the manner in which those actions are carried out, and control over those actions is the goal itself.  This research takes a strengths-based approach to assistive robotics, developing new methods that allow the robot and user to freely assist each other to complete tasks, and evaluating those methods in activities that improve people's quality of life and where users' autonomy and control over both the goal and manner of completing a task are important.  Project outcomes will include new methods for robot learning that empower people with disabilities to collaboratively design, control, and influence robot behavior while engaging in pleasurable hobbies, controlling their own appearance, and generally engaging in creative interaction with the world.  These methods will help to ensure that the next generation of assistive robotics support the quality of life and joyous self-expression for people with disabilities, as well as their daily chores This should significantly improve the lives of the substantial number of Americans of all ages who live with physical disabilities.  Additional broad impact will derive from algorithms for human-robot interaction that significantly advance not only that field but also inform future work in interactive reinforcement learning, learning for robotics, and intelligent assistive technologies.\r\n\r\nLeveraging the team's expertise in assistive technology, human-robot interaction, augmented reality, and human-robot interaction, project goals will be achieved through three technological innovations. First, algorithms to enhance initial model learning with mutual assistance from robot to human and human to robot at multiple levels of abstraction, from direct control to language. Second, new methods for giving users usable mental models of the robot, such as selecting and displaying information through augmented reality to empower users to understand robot perception and decision-making and improve their ability to influence robot behavior. Finally, new interactive learning algorithms that enable users to exploit feedback after initial learning and ensure that users can influence the manner in which tasks are conducted as well as task goals. These algorithms will be united in a three-layer architecture for assistive robotics that explicitly supports assistance from both robot to human and human to robot at each level.  At the lowest level is a data-driven mapping from sensory state to movement. At the middle level, those motions are named as atomic actions such as reaching, pouring, or grasping, and grouped based on parameters such as target objects or features of the motion.  At the highest level, actions are represented symbolically with pre- and post-conditions and combined into multi-step plans that achieve user-specified goals while being modified online by the user.  In addition to supporting both robot-to-human and human-to-robot assistance at all levels, this architecture will also allow for the flow of information between levels, especially in robot-to-human assistance.   The work will be validated with the help of expert user-collaborators with disabilities, as well as in larger-scale studies that validate foundational technological developments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Elaine",
   "pi_last_name": "Short",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Elaine S Short",
   "pi_email_addr": "elaine.short@tufts.edu",
   "nsf_id": "000808909",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Rogers",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chris Rogers",
   "pi_email_addr": "Crogers@tufts.edu",
   "nsf_id": "000440364",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthias",
   "pi_last_name": "Scheutz",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Matthias J Scheutz",
   "pi_email_addr": "matthias.scheutz@tufts.edu",
   "nsf_id": "000289027",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jivko",
   "pi_last_name": "Sinapov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jivko Sinapov",
   "pi_email_addr": "Jivko.Sinapov@tufts.edu",
   "nsf_id": "000766011",
   "pi_start_date": "2021-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Tufts University",
  "inst_street_address": "80 GEORGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "MEDFORD",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6176273696",
  "inst_zip_code": "021555519",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "TRUSTEES OF TUFTS COLLEGE",
  "org_prnt_uei_num": "WL9FLBRVPJJ7",
  "org_uei_num": "WL9FLBRVPJJ7"
 },
 "perf_inst": {
  "perf_inst_name": "Tufts University - School of Engineering",
  "perf_str_addr": "200 Boston Ave.",
  "perf_city_name": "Medford",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021555528",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 1499499.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": null
}