{
 "awd_id": "2128439",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "SBIR Phase II: Three Dimensional Monocular Thermal Ranging Camera",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928323",
 "po_email": "bschrag@nsf.gov",
 "po_sign_block_name": "Benaiah Schrag",
 "awd_eff_date": "2021-09-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 998319.0,
 "awd_amount": 1498177.0,
 "awd_min_amd_letter_date": "2021-09-10",
 "awd_max_amd_letter_date": "2023-11-02",
 "awd_abstract_narration": "This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2). \r\n\r\nThe broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project is to improve the safe operation of Advanced Driver-Assistance Systems (ADAS) equipped vehicles and robotic mobility machinery while helping overcome the sensing challenges that have delayed autonomous vehicle (AV) adoption and acceptance. Degraded Visual Environments (DVE) such as solar glare, darkness, smoke, and dust impact the safety of vehicle occupants, as well as those external to the vehicle, especially pedestrians and cyclists. The new sensor modality of the 3D monocular thermal technology developed as part of this project may improve the safe operation of on-road and off-road vehicles alike.  The system seeks to detect and locate pedestrians, cyclists and objects in all conditions, improving vehicle automated braking, obstacle avoidance, localization, and navigation. Beyond improving safety, the new 3D monocular thermal modality devise will improve the utility and commercial value of vehicles and robotic machines by extending the operating range to include the most challenging environmental conditions.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase II project will develop a new sensor modality that addresses the shortcomings of camera, radar and LiDAR sensors used in current automotive and robotic mobility applications. This project seeks to develop the simple, yet powerful innovation of thermal monocular ranging where a single thermal camera produces 2D video detail concurrent with a range map or point cloud of much higher spatio-temporal density than contemporary sensors. Drawing inspiration from monocular convolutional neural network (CNN) ranging work recently published in the visible domain, this project creates a new field of monocular 3D thermal imaging. Unlike visible-based CNN systems that rely on color information and reflected light, this project will construct from first principles new thermal networks and ground truth sets to train the CNN. The extreme price pressures of the automotive market can only be met through adoption of a new thermal detector technology. This project further develops new thin-film detector technology and the detector to readout interconnect application process with potential breakthroughs in thermal camera performance, utility, and affordability.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eugene",
   "pi_last_name": "Petilli",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Eugene M Petilli",
   "pi_email_addr": "genep@owlai.us",
   "nsf_id": "000808708",
   "pi_start_date": "2021-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "OWL AUTONOMOUS IMAGING INC",
  "inst_street_address": "562 WILLOWBROOK OFFICE PARK",
  "inst_street_address_2": "",
  "inst_city_name": "FAIRPORT",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5857218168",
  "inst_zip_code": "144504202",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "OWL AUTONOMOUS IMAGING INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "KN5CDCJWVCV7"
 },
 "perf_inst": {
  "perf_inst_name": "OWL AUTONOMOUS IMAGING INC",
  "perf_str_addr": "562 Willowbrook Office Park",
  "perf_city_name": "Fairport",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "144504202",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "165E",
   "pgm_ref_txt": "SBIR Phase IIB"
  },
  {
   "pgm_ref_code": "8990",
   "pgm_ref_txt": "Optics and Photonics"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01AB2324DB",
   "fund_name": "R&RA DRSA DEFC AAB",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "1V21",
   "app_name": "R&RA ARP Act DEFC V",
   "app_symb_id": "040100",
   "fund_code": "010V2122DB",
   "fund_name": "R&RA ARP Act DEFC V",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 998319.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 499858.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<p><span>&nbsp;</span><strong>NSF IIB Progress Report&nbsp;</strong></p>\n<p><strong>Executive Summary </strong>- Owl AI&lsquo;s focus over this first development period can be broken down into three main areas: (1) 3D Classification/Segmentation, (2) Shutterless Operation, and (3) Application Testing and Benchmarking.&nbsp;</p>\n<p><strong>3D Classification/Segmentation&nbsp;</strong></p>\n<p>We have begun working on a new Object Classification Neural Network that takes RGB and Thermal images for input. This was made possible by algorithms that align these images across modalities. We have also been adding more classes to our network, expanding the list of objects that our system can identify. We are having some of our data hand-labelled which will greatly improve our classification precision. We have also begun design of our next generation neural network which will combine classification and ranging into a single network.&nbsp;</p>\n<p><strong>Shutterless Operation&nbsp;</strong></p>\n<p>Calibration of LWIR cameras becomes invalid if ambient temperature changes. This necessitates a mechanical shutter, which acts as a uniform &ldquo;black body&rdquo;, for field calibration. Interrupting video stream for calibration is a major safety issue for automotive, as is the reliability of mechanical devices like shutters. IP protection will be pursued for this disruptive market feature.&nbsp;</p>\n<p><strong>Application Testing and Benchmarking&nbsp;</strong></p>\n<p>We have collected a new dataset that includes LiDAR data. We can use that dataset to train our ranging network and measure its performance against the LiDAR ground truth.&nbsp;</p>\n<p><strong>Technical Progress&nbsp;</strong></p>\n<p><strong>3D Classification/Segmentation&nbsp;</strong></p>\n<p>We have made significant progress toward our goals for Neural Networks and Software. These improvements have come from changes to our Software as well as improvements to our training datasets.&nbsp;</p>\n<p>We publish Detection3DArray messages to the ROS network for each object that our networks classify in each frame. This provides our customers with a 3D point cloud for each object which provide improved scene awareness for ADAS and AV systems. The vehicle&rsquo;s perception stack will receive information from Owl&rsquo;s Detection3DArray and fuse that data from other sensors to create a digital representation of the driving environment. By publishing only the point cloud data for Objects of Interest (OoI) and not the entire scene, we compress the amount of data that needs to be transmitted and processed by the perception software. The 3D data payload coming from our network is presented in X,Y,Z coordinates in units of meters which makes fusion with data from other sensors very easy for vehicle integration.&nbsp;</p>\n<p>We have worked steadily to increase the number of classes of objects that our Neural Network can identify. After observing that our Network sometimes incorrectly classifies traffic lights as pedestrians (they have similar height to width ratio and thermal response), we added a distinct class for traffic lights to our network. In order to provide training examples for our networks to learn the large animal class, we outfitted a life-sized deer model with heaters to emulate the body heat of a deer. We will also be collecting datasets with real live deer. Our next addition will be bicycles and motorcycles.&nbsp;</p>\n<p><strong>Shutterless Operation&nbsp;</strong></p>\n<p>At present, this effort is unable to fully realize the methods as we are awaiting focal plane arrays (FPA) from our partner.&nbsp;</p>\n<ol>\n<li>TEC (Thermo-Electric Cooler) temperature stabilization to extend calibration operating range &ndash; The development of the PID controller is on hold pending completion of a new camera board. The PID controller monitors the FPA substrate temperature and adjusts the TEC to maintain the proper temperature. Once complete and into fabrication and assembly, the engineer will begin developing the code necessary to implement a PID controller in an on-board microcontroller. The effort also includes tuning the PID controller.&nbsp;</li>\n</ol><ol>\n<li>Develop new calibration software which leverages the environmental isolation and FPA temperature sensors to eliminate the mechanical shutter &ndash; The development of a calibration algorithm has begun but once again, until a focal plane array is available, it is difficult to assess performance of the algorithm.&nbsp;</li>\n<li>Prototype design and fabrication &ndash; As mentioned in item #4 above, the components are available and awaiting final assembly and integration with the focal plane array.&nbsp;</li>\n</ol>\n<p>&nbsp;</p>\n<p><strong>Application Testing and Benchmarking&nbsp;</strong></p>\n<p>We have continued our partnership with VSI Labs and worked with them in August to collect an on-road dataset. VSI Labs has a test vehicle outfitted with other sensing technologies like LiDAR, RADAR, high-accuracy GPS, and high-quality RGB visible cameras. To use that dataset, we first had a calibrate and rectify our Thermal camera images to their RGB camera and LiDAR. We have used the LiDAR as Ground Truth distance to train our Monocular Depth Estimation network (we have not had access to LiDAR data before) and the results are promising. More work needs to be done but we believe that the LiDAR Ground Truth dataset will produce a better ranging network than previous methods.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/09/2024<br>\nModified by: Eugene&nbsp;M&nbsp;Petilli</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nNSF IIB Progress Report\n\n\nExecutive Summary - Owl AIs focus over this first development period can be broken down into three main areas: (1) 3D Classification/Segmentation, (2) Shutterless Operation, and (3) Application Testing and Benchmarking.\n\n\n3D Classification/Segmentation\n\n\nWe have begun working on a new Object Classification Neural Network that takes RGB and Thermal images for input. This was made possible by algorithms that align these images across modalities. We have also been adding more classes to our network, expanding the list of objects that our system can identify. We are having some of our data hand-labelled which will greatly improve our classification precision. We have also begun design of our next generation neural network which will combine classification and ranging into a single network.\n\n\nShutterless Operation\n\n\nCalibration of LWIR cameras becomes invalid if ambient temperature changes. This necessitates a mechanical shutter, which acts as a uniform black body, for field calibration. Interrupting video stream for calibration is a major safety issue for automotive, as is the reliability of mechanical devices like shutters. IP protection will be pursued for this disruptive market feature.\n\n\nApplication Testing and Benchmarking\n\n\nWe have collected a new dataset that includes LiDAR data. We can use that dataset to train our ranging network and measure its performance against the LiDAR ground truth.\n\n\nTechnical Progress\n\n\n3D Classification/Segmentation\n\n\nWe have made significant progress toward our goals for Neural Networks and Software. These improvements have come from changes to our Software as well as improvements to our training datasets.\n\n\nWe publish Detection3DArray messages to the ROS network for each object that our networks classify in each frame. This provides our customers with a 3D point cloud for each object which provide improved scene awareness for ADAS and AV systems. The vehicles perception stack will receive information from Owls Detection3DArray and fuse that data from other sensors to create a digital representation of the driving environment. By publishing only the point cloud data for Objects of Interest (OoI) and not the entire scene, we compress the amount of data that needs to be transmitted and processed by the perception software. The 3D data payload coming from our network is presented in X,Y,Z coordinates in units of meters which makes fusion with data from other sensors very easy for vehicle integration.\n\n\nWe have worked steadily to increase the number of classes of objects that our Neural Network can identify. After observing that our Network sometimes incorrectly classifies traffic lights as pedestrians (they have similar height to width ratio and thermal response), we added a distinct class for traffic lights to our network. In order to provide training examples for our networks to learn the large animal class, we outfitted a life-sized deer model with heaters to emulate the body heat of a deer. We will also be collecting datasets with real live deer. Our next addition will be bicycles and motorcycles.\n\n\nShutterless Operation\n\n\nAt present, this effort is unable to fully realize the methods as we are awaiting focal plane arrays (FPA) from our partner.\n\nTEC (Thermo-Electric Cooler) temperature stabilization to extend calibration operating range  The development of the PID controller is on hold pending completion of a new camera board. The PID controller monitors the FPA substrate temperature and adjusts the TEC to maintain the proper temperature. Once complete and into fabrication and assembly, the engineer will begin developing the code necessary to implement a PID controller in an on-board microcontroller. The effort also includes tuning the PID controller.\n\nDevelop new calibration software which leverages the environmental isolation and FPA temperature sensors to eliminate the mechanical shutter  The development of a calibration algorithm has begun but once again, until a focal plane array is available, it is difficult to assess performance of the algorithm.\nPrototype design and fabrication  As mentioned in item #4 above, the components are available and awaiting final assembly and integration with the focal plane array.\n\n\n\n\n\n\nApplication Testing and Benchmarking\n\n\nWe have continued our partnership with VSI Labs and worked with them in August to collect an on-road dataset. VSI Labs has a test vehicle outfitted with other sensing technologies like LiDAR, RADAR, high-accuracy GPS, and high-quality RGB visible cameras. To use that dataset, we first had a calibrate and rectify our Thermal camera images to their RGB camera and LiDAR. We have used the LiDAR as Ground Truth distance to train our Monocular Depth Estimation network (we have not had access to LiDAR data before) and the results are promising. More work needs to be done but we believe that the LiDAR Ground Truth dataset will produce a better ranging network than previous methods.\n\n\n\t\t\t\t\tLast Modified: 04/09/2024\n\n\t\t\t\t\tSubmitted by: EugeneMPetilli\n"
 }
}