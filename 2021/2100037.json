{
 "awd_id": "2100037",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: Enhancing Continuous Integration Testing for the Open-Source Ecosystem",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 328425.0,
 "awd_amount": 328425.0,
 "awd_min_amd_letter_date": "2020-11-04",
 "awd_max_amd_letter_date": "2021-07-12",
 "awd_abstract_narration": "Continuous integration (CI) is an important software development activity that aims to improve software development by automating software compilation and regression testing.  Recent studies report that CI helps developers deploy faster and reduce development cost. Given these success stories, CI has attracted rapidly increasing interest and adoption, e.g., Travis CI, the currently most popular CI service, is used by over 300,000 GitHub projects.  Despite the success of CI, developers report they would like to see improvements in CI. First, they want to faster obtain regression test results.  Second, they want better handling of so-called flaky tests, which are regression tests that can non-deterministically pass or fail, and whose failures negatively affect developer's productivity.  Third, developers report that CI builds do not provide sufficient debugging assistance for reasoning about failed regression tests.  While regression testing has been studied for over three decades, it has not been studied in the context of CI until recently.\r\n\r\nTo substantially improve regression testing in CI, the PIs propose to develop novel techniques and tools that address three important challenges: (1) test selection to speed up regression testing and the development cycle, (2) test reliability to mitigate the problems that flaky tests introduce, and (3) debugging assistance to ease the effort of diagnosing and fixing the true and flaky regression test failures. The PIs plan to develop techniques and tools based on a mix of static and dynamic program analyses, leveraging not only information from two project revisions (as traditional in regression testing) but also from all historical build and testing information available in CI testing. The PIs plan to embody their techniques in a tool-set and evaluate them extensively on open-source projects and in industrial collaborations. The broader impacts of enhancing continuous integration testing are to allow software developers to faster build higher quality software, which can benefit our modern society that greatly depends on software.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonathan",
   "pi_last_name": "Bell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jonathan Bell",
   "pi_email_addr": "j.bell@northeastern.edu",
   "nsf_id": "000730377",
   "pi_start_date": "2020-11-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 120108.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 101970.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 106347.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Regression testing is the primary approach that developers rely on to ensure that software is released without bugs. With the proliferation of cloud computing, software engineers have increasingly relied on a practice called \"continuous integration\" that leverages on-demand computing utilities to execute hundreds of tests in parallel, transforming what might have once been a three-day test suite into a one-hour test suite. Whereas these test suites may have previously been run on a weekly or monthly basis, they now are run as frequently as on every single change. Engineers benefit from faster feedback, identifying bugs and performance regressions sooner and optimizing productivity, reducing development costs overall. But, CI is also a relatively new process, and it brings many challenges.</p>\n<p>This collaborative project has investigated the challenges and opportunities in CI, developing new approaches and tools to 1) speed up the testing and development cycle, 2) mitigate the problems of \"flaky\" tests\" that randomly fail even when there is no bug in the code, and 3) help developers to debug failing tests. This outcomes report focuses on the findings specific to this PI, primarily centered in the area of flaky tests. PI Marinov (under award number 1763788) and PI Zhang (under award number 2141474) report in greater detail on outcomes under the other thrusts of this project.</p>\n<p>Flaky tests undermine efficiency with CI, because developers cannot easily determine when a test failure is due to their recent changes or due to flakiness. We have created several novel approaches and implemented them in tools to help developers answer the key question: \"How can we quickly determine if a test failure is flaky, or a true failure we should investigate?\" Answering this question allows developers to focus their limited time on fixing real bugs or developing new features --- not on tracing down false alarms. In order to establish a core body of knowledge in this area, we used rigorous empirical methods to answer foundational questions such as: \"What kinds of code changes cause tests to become flaky?\" and: \"Are flaky tests flaky starting from when they are first written, or do they become flaky later on in development?\" Knowing answers to these questions informs other research in flaky tests, allowing researchers to design new interventions to prevent flakiness from being introduced to a codebase. To provide a rich dataset of flaky tests, we ran the test suites of open-source projects tens of thousands of times, identifying tests that could both pass and fail without any changes to the tests or the code. Using this dataset, we proposed a set of features that could be used as input to a classifier to predict which tests are likely to be flaky. This dataset has become impactful in the field, adopted widely by other researchers. These outcomes have been disseminated through articles published in prestigious academic venues and through open-source software releases.</p>\n<p>This project supported the training of multiple doctoral, masters and undergraduate students, including several who are members of groups historically underrepresented in computer science. It has led to the creation of curricular materials used to teach continuous integration in undergraduate and graduate courses, further disseminating the knowledge. Research conducted as part of this project has also informed how software testing is taught and evaluated in courses at multiple institutions, supporting the overall project goals of increasing software quality.</p><br>\n<p>\n Last Modified: 12/06/2023<br>\nModified by: Jonathan&nbsp;Bell</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRegression testing is the primary approach that developers rely on to ensure that software is released without bugs. With the proliferation of cloud computing, software engineers have increasingly relied on a practice called \"continuous integration\" that leverages on-demand computing utilities to execute hundreds of tests in parallel, transforming what might have once been a three-day test suite into a one-hour test suite. Whereas these test suites may have previously been run on a weekly or monthly basis, they now are run as frequently as on every single change. Engineers benefit from faster feedback, identifying bugs and performance regressions sooner and optimizing productivity, reducing development costs overall. But, CI is also a relatively new process, and it brings many challenges.\n\n\nThis collaborative project has investigated the challenges and opportunities in CI, developing new approaches and tools to 1) speed up the testing and development cycle, 2) mitigate the problems of \"flaky\" tests\" that randomly fail even when there is no bug in the code, and 3) help developers to debug failing tests. This outcomes report focuses on the findings specific to this PI, primarily centered in the area of flaky tests. PI Marinov (under award number 1763788) and PI Zhang (under award number 2141474) report in greater detail on outcomes under the other thrusts of this project.\n\n\nFlaky tests undermine efficiency with CI, because developers cannot easily determine when a test failure is due to their recent changes or due to flakiness. We have created several novel approaches and implemented them in tools to help developers answer the key question: \"How can we quickly determine if a test failure is flaky, or a true failure we should investigate?\" Answering this question allows developers to focus their limited time on fixing real bugs or developing new features --- not on tracing down false alarms. In order to establish a core body of knowledge in this area, we used rigorous empirical methods to answer foundational questions such as: \"What kinds of code changes cause tests to become flaky?\" and: \"Are flaky tests flaky starting from when they are first written, or do they become flaky later on in development?\" Knowing answers to these questions informs other research in flaky tests, allowing researchers to design new interventions to prevent flakiness from being introduced to a codebase. To provide a rich dataset of flaky tests, we ran the test suites of open-source projects tens of thousands of times, identifying tests that could both pass and fail without any changes to the tests or the code. Using this dataset, we proposed a set of features that could be used as input to a classifier to predict which tests are likely to be flaky. This dataset has become impactful in the field, adopted widely by other researchers. These outcomes have been disseminated through articles published in prestigious academic venues and through open-source software releases.\n\n\nThis project supported the training of multiple doctoral, masters and undergraduate students, including several who are members of groups historically underrepresented in computer science. It has led to the creation of curricular materials used to teach continuous integration in undergraduate and graduate courses, further disseminating the knowledge. Research conducted as part of this project has also informed how software testing is taught and evaluated in courses at multiple institutions, supporting the overall project goals of increasing software quality.\t\t\t\t\tLast Modified: 12/06/2023\n\n\t\t\t\t\tSubmitted by: JonathanBell\n"
 }
}