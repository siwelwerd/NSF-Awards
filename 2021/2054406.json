{
 "awd_id": "2054406",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Integrating Human and Machine Learning for Enabling Co-Adaptive Body-Machine Interfaces",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922633",
 "po_email": "aleoness@nsf.gov",
 "po_sign_block_name": "Alex Leonessa",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 714227.0,
 "awd_amount": 714227.0,
 "awd_min_amd_letter_date": "2021-07-26",
 "awd_max_amd_letter_date": "2021-07-26",
 "awd_abstract_narration": "This project for the Mind, Machine, and Motor Nexus (M3X) program will advance understanding of how people learn new neuromotor skills, and subsequently apply that understanding to the creation of innovative wearable device controllers called body-machine interfaces (BoMIs). Individuals with neuromuscular impairment -- perhaps due to stroke or spinal cord injury -- may have difficulty carrying out the activities of everyday life. This project explores novel interfaces through which an individual can use their body's residual mobility to issue commands to assistive devices such as computer cursors, wheelchairs, or robotic arms. The project has three main research goals. The first is to improve existing methods for translating small body movements into controller commands for assistive devices. The second is to model the process by which the human user learns over time to use the body-machine interface. The third is to apply the obtained model of the learning process to enable the body-machine interface to adjust to the evolving characteristics of the human user. An interface that does not adapt to changes in its user may significantly degrade in performance over time. On the other hand, an interface whose properties instantly change with every small shift in user behavior will be difficult to control. The ultimate outcome of this project will be human-machine interfaces based on body movement that consider the user and the interface as two components of an integrated system in which each component continually learns from and adapts to the other. The results of the project will lead to assistive devices that more affordable, and provide more versatile control and ease of use. The underlying principles of co-adaptation to be identified through this work are also relevant to rehabilitation from disease or injury, as well as to increasing the capabilities of human-operated robotic systems.\r\n\r\nRecent work has shown that linear methods such as principal component analysis (PCA) may be effectively used in a body-machine interface (BoMI) to map elements from a higher dimensional feature space of body movements onto a lower dimensional space of device commands. In this project, the features that provide input to the BoMI are generated by multiple inertial measurement units (IMUs) worn by the user; the IMUs report their current orientation in an inertial reference space. The output from the BoMI are commands used to control a sequence of representative devices, specifically a computer cursor, a simulated wheelchair, an actual wheelchair, and a simulated manipulator arm. The three technical goals of the project are as follows: 1) Compare the performance of a linear map based on PCA to a nonlinear map based on an autoencoder network (AEN) for providing input features to the BoMI that translates residual mobility space features into device commands. The AEN is capable of representing a richer variety of features than PCA, but it remains to be shown, for example, whether human users can make effective use of that variety. 2) Obtain a computable representation of the process by which humans learn neuromotor skills. This representation will be based on the premise that humans simultaneously learn both a forward and inverse map of the relationship between neuromotor signals and the resulting physical outcomes. Once learned, the forward map predicts the outcomes that will result from a certain set of signals, while the inverse map is used to generate the signals that correspond to a given desired physical outcome. As a person learns mastery of a neuromotor skill, the forward and inverse maps become more accurate predictors of actual behavior, and the degree of learning can be monitored through estimates of these maps. 3) Incorporate a co-adaptation algorithm into the BoMI for maintaining performance as the user's mastery of the BoMI changes. In most current approaches to human-machine interfaces, the interface is fixed following an initial calibration stage, and the user must learn to control that interface configuration. In this project, the learning representation of objective (2) will be used to monitor and periodically update the BoMI map parameters. The implementation of this objective is aided by parallels between the human learning model and the AEN training method, which automatically generates a decoder network that captures the inverse map between desired device commands and the corresponding residual mobility features needed to produce them.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ferdinando",
   "pi_last_name": "Mussa-Ivaldi",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Ferdinando A Mussa-Ivaldi",
   "pi_email_addr": "sandro@northwestern.edu",
   "nsf_id": "000106336",
   "pi_start_date": "2021-07-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sara",
   "pi_last_name": "Solla",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Sara A Solla",
   "pi_email_addr": "solla@northwestern.edu",
   "nsf_id": "000428940",
   "pi_start_date": "2021-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rehabilitation Institute of Chicago",
  "inst_street_address": "355 E ERIE ST",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3122385195",
  "inst_zip_code": "606113167",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "REHABILITATION INSTITUTE OF CHICAGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "XAJWT43U55A3"
 },
 "perf_inst": {
  "perf_inst_name": "Rehabilitation Institute of Chicago",
  "perf_str_addr": "355 East Erie St",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606113167",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "058Y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "070E",
   "pgm_ref_txt": "INTEG OF HUMAN & COGNITIVE"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 714227.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project&rsquo;s goals develop human-machine programming that endowed the sensor interfaces with user-centered adaptation, called <em>collaborative reciprocal learning</em>. This was shown to shift the burden of learning from the user to the machine, by attending to the user's evolving motor abilities. We pursued three objectives: 1) To evaluate a new nonlinear dimensionality reduction method for external device control using autoencoder networks (AENs), 2) model the human and machine co-learning dynamic process based on goals and errors, and 3) develop and test coadaptive machine learning rules for updating this interface, quantifying the advantages of this collaborative learning. We recognized that this ill-posed inverse mathematical problem can be approached in the most convenient manner, and the results can be generalized to the many areas that might benefit.</p>\r\n<p>The key result was a <em>body-machine interface</em>&nbsp;(BoMI), adapting its interface only the range of a user&rsquo;s specific movement ability. A strategy to control a robot took high-dimensional body movements sensor readings (the many joint angles), then passed them through decoders that producing a lower-dimensional signal for control. We used techniques such as principal component analysis (PCA) or autoencoders (AE) that could in principle implement simultaneous and continuous control. However, this task is mathematically difficult to learn because the relations are hard to perceive, and the small errors can propagate backwards through this problem and make it inaccurate.</p>\r\n<p>One project developed a controller from a high-dimensional (17D) hand sensor to low-dimensional (2D) space of signals with 4 unimpaired participants. Participants were able to significantly decrease the practice time to be able to match a target gesture with a virtual. Three of four participants improved their path. An alternative BoMI mapped 8D upper arm kinematic signals into the 4D joint angles of a virtual robot. The technique distributed variance uniformly across the output space and showed that 12 of 12 subjects managed operational skills. Moreover, retaining on another day produced more continuous control. The approach makes it applicable for therapy, tailored to the specific abilities of patients.</p>\r\n<p>We then explored the opposite situation -- facilitating learning low- to high-dimensional control. A 17D virtual hand was guided by a 2D signal from a mouse and myoelectric interfaces. We found myoelectric interfaces were just as capable as a mouse, and that the implicit 2D training was not inferior to training directly on the high dimensional hand. The most beneficial was feedback containing explicit connections between the low and high dimensions.</p>\r\n<p>Next, we explored pairs, or <em>dyads</em>, where the user&rsquo;s mapping (internal model) of the body and device can change across learning a new skill. Shared practice of a new skill by pairs of na&iuml;ve participants that controlled a cursor presented on two monitors, each involving 8D upper arm kinematics mapped to the 2D positions of two cursors. Participants practiced reaching tasks training as individuals, socially, and back to individual. Shared cursor used a common map. No difference performance was observed after social practice, but the partner who contributed less adopted a new model, indicating that in such dyadic synergic learning, the teacher influences the learner.</p>\r\n<p>Next, we explored intermediate visual feedback as people learned to control a novel system with cascaded redundant mappings between finger movements and 2D cursor movement, controlled by 18 hand signals. The final position of the cursor was determined by a 3D virtual kinematic chain governed by angles, resulting in a18 to 3 to 2 mapping. Only half of the subjects could see this intermediate 3D kinematic chain along with the cursor, and these learned slightly faster and used less redundant (null space) motions.</p>\r\n<p>These techniques demonstrate how to facilitate this very general problem of co-adaptation in human-machine control of devices that also involve mapping dimensions.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/03/2025<br>\nModified by: Ferdinando&nbsp;A&nbsp;Mussa-Ivaldi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis projects goals develop human-machine programming that endowed the sensor interfaces with user-centered adaptation, called collaborative reciprocal learning. This was shown to shift the burden of learning from the user to the machine, by attending to the user's evolving motor abilities. We pursued three objectives: 1) To evaluate a new nonlinear dimensionality reduction method for external device control using autoencoder networks (AENs), 2) model the human and machine co-learning dynamic process based on goals and errors, and 3) develop and test coadaptive machine learning rules for updating this interface, quantifying the advantages of this collaborative learning. We recognized that this ill-posed inverse mathematical problem can be approached in the most convenient manner, and the results can be generalized to the many areas that might benefit.\r\n\n\nThe key result was a body-machine interface(BoMI), adapting its interface only the range of a users specific movement ability. A strategy to control a robot took high-dimensional body movements sensor readings (the many joint angles), then passed them through decoders that producing a lower-dimensional signal for control. We used techniques such as principal component analysis (PCA) or autoencoders (AE) that could in principle implement simultaneous and continuous control. However, this task is mathematically difficult to learn because the relations are hard to perceive, and the small errors can propagate backwards through this problem and make it inaccurate.\r\n\n\nOne project developed a controller from a high-dimensional (17D) hand sensor to low-dimensional (2D) space of signals with 4 unimpaired participants. Participants were able to significantly decrease the practice time to be able to match a target gesture with a virtual. Three of four participants improved their path. An alternative BoMI mapped 8D upper arm kinematic signals into the 4D joint angles of a virtual robot. The technique distributed variance uniformly across the output space and showed that 12 of 12 subjects managed operational skills. Moreover, retaining on another day produced more continuous control. The approach makes it applicable for therapy, tailored to the specific abilities of patients.\r\n\n\nWe then explored the opposite situation -- facilitating learning low- to high-dimensional control. A 17D virtual hand was guided by a 2D signal from a mouse and myoelectric interfaces. We found myoelectric interfaces were just as capable as a mouse, and that the implicit 2D training was not inferior to training directly on the high dimensional hand. The most beneficial was feedback containing explicit connections between the low and high dimensions.\r\n\n\nNext, we explored pairs, or dyads, where the users mapping (internal model) of the body and device can change across learning a new skill. Shared practice of a new skill by pairs of nave participants that controlled a cursor presented on two monitors, each involving 8D upper arm kinematics mapped to the 2D positions of two cursors. Participants practiced reaching tasks training as individuals, socially, and back to individual. Shared cursor used a common map. No difference performance was observed after social practice, but the partner who contributed less adopted a new model, indicating that in such dyadic synergic learning, the teacher influences the learner.\r\n\n\nNext, we explored intermediate visual feedback as people learned to control a novel system with cascaded redundant mappings between finger movements and 2D cursor movement, controlled by 18 hand signals. The final position of the cursor was determined by a 3D virtual kinematic chain governed by angles, resulting in a18 to 3 to 2 mapping. Only half of the subjects could see this intermediate 3D kinematic chain along with the cursor, and these learned slightly faster and used less redundant (null space) motions.\r\n\n\nThese techniques demonstrate how to facilitate this very general problem of co-adaptation in human-machine control of devices that also involve mapping dimensions.\r\n\n\n\t\t\t\t\tLast Modified: 02/03/2025\n\n\t\t\t\t\tSubmitted by: FerdinandoAMussa-Ivaldi\n"
 }
}