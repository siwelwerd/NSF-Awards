{
 "awd_id": "2120333",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CCRI:NEW: Research Infrastructure for Real-Time Computer Vision and Decision Making via Mobile Robots",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 229565.0,
 "awd_amount": 229565.0,
 "awd_min_amd_letter_date": "2021-08-10",
 "awd_max_amd_letter_date": "2021-08-10",
 "awd_abstract_narration": "This project will create a research infrastructure for computer vision and real-time control of autonomous mobile robots (both aerial and ground). The infrastructure includes four integrated components: (1) A Purdue laboratory decorated as miniature cities. (2) Simulators that reflect the physical laboratory. (3) Programmable aerial robots with the same interface as the simulators. (4) Sample solutions for research on artificial intelligence, computer vision, and robot control for evaluation and comparison. This infrastructure will be available to the research community in multiple ways: (1) Users can evaluate their solutions with the simulators in a safe virtual environment. (2) Users can upload their control programs and this team will launch the robots inside Purdue's laboratory. Users can observe the robots remotely using the high-speed cameras already deployed in the laboratory. (3) Users can bring their own robots to the laboratory and conduct experiments. (4) This project will create competitions for researchers to demonstrate their solutions using autonomous mobile robots in simulated emergency and rescue scenarios. The competitions will use miniature buildings and people for the robots to recognize and count objects (such as number of people, vehicles, and houses), assess situations (such as the number of collapsed bridges), while avoiding obstacles.\r\n\r\nThis infrastructure will be available for investigating a wide range of research topics, including (1) real-time computer vision and control. The decorated laboratory will allow researchers to evaluate their solutions for real-time vision and control methods using active computer vision, navigation, and semantic segmentation in a three-dimensional environment. (2) simulation of robot fleets. Users can evaluate and improve their methods in a safe virtual environment before deployment. (3) This infrastructure will integrate virtual and physical environments so that solutions running in the simulators can be ported directly to the physical robots for experiments. (4) collision avoidance, multi-robot coordination, emergency response, computer security, and efficient machine learning on embedded systems. (5) agriculture, city planning, emergency response, and inspection of civil structures. This project will build STEM talents because autonomous robots and visual data are naturally appealing to the general public. With the simulators, students at all levels can participate without the cost of purchasing physical robots. This research infrastructure will reduce the barriers to innovations. This infrastructure will also encourage innovations in machine learning that are efficient in energy and can be ported to resource constrained embedded systems such as aerial robots. The project will engage a broader audience including K-12 students as well because of the many applications described above.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiran",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiran Chen",
   "pi_email_addr": "yiran.chen@duke.edu",
   "nsf_id": "000575362",
   "pi_start_date": "2021-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St, Suite 710",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 229565.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project created a research infrastructure for computer vision and real-time control of autonomous mobile robots (both aerial and ground). This infrastructure includes simulators that reflect the physical laboratory and programmable aerial robots with the same interface as the simulators.</p>\r\n<p>The team at Duke University was responsible for using Neural Architecture Search (NAS) to obtain good families of models for hardware adaptations. We first studied NAS through the following two approaches: (1) breaking down the hierarchy of deep learning models into Directed Acyclic Graphs (DAGs) and harnessing the topological properties in graphs to improve the efficiency of search; (2) extending the scope of NAS to various applications, such as low-power human pose estimation, human detection, and efficient 3D point-cloud segmentation. The results showed good promise in current NAS methods. We also studied NAS methods in recommender systems to discover efficient models for real-world applications.</p>\r\n<p>For searching efficient neural architectures, the developed DukeNet Model provides a hardware-aware model for low-power image recognition. We made hardware adaptations to tailor searched models for resource-constrained DSPs with &le; 7ms maximum latency. Under INT8 inference, our model achieves a good accuracy-latency trade-off on the ImageNet test dataset. The ScaleNAS leaderboard provides an extension of our NAS methods on human pose estimation benchmarks. ScaleNAS proposes a one-shot approach to learn scale-aware representations in efficient human pose estimations.</p>\r\n<p>We also employed NAS to develop versatile models that address complex problems and demands. Building on graph heuristics, we leveraged architectural heterogeneity and dense connectivity to drive our discovery process. In the realm of recommender systems, we advanced both model quality and efficiency through end-to-end differentiable NAS, combining heterogeneous building blocks to craft architectures for large-scale recommenders. Our distributed system-based approach enables searches on terabyte-scale datasets within just 2 hours, demonstrating a scalable solution for crafting architectures that meet real-world demands. For Out-of-Distribution (OoD) detection, we focused on ensuring the safety of model deployment in real-world scenarios by exploring better dense connectivity using our NAS methodologies. By utilizing a predictor-based search framework and proposing an evolving distillation algorithm to improve our search strategy, we achieved state-of-the-art performance in OoD evaluation.</p>\r\n<p>The project trained 4 Ph.D. students and published 3 conference papers.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/31/2025<br>\nModified by: Yiran&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project created a research infrastructure for computer vision and real-time control of autonomous mobile robots (both aerial and ground). This infrastructure includes simulators that reflect the physical laboratory and programmable aerial robots with the same interface as the simulators.\r\n\n\nThe team at Duke University was responsible for using Neural Architecture Search (NAS) to obtain good families of models for hardware adaptations. We first studied NAS through the following two approaches: (1) breaking down the hierarchy of deep learning models into Directed Acyclic Graphs (DAGs) and harnessing the topological properties in graphs to improve the efficiency of search; (2) extending the scope of NAS to various applications, such as low-power human pose estimation, human detection, and efficient 3D point-cloud segmentation. The results showed good promise in current NAS methods. We also studied NAS methods in recommender systems to discover efficient models for real-world applications.\r\n\n\nFor searching efficient neural architectures, the developed DukeNet Model provides a hardware-aware model for low-power image recognition. We made hardware adaptations to tailor searched models for resource-constrained DSPs with  7ms maximum latency. Under INT8 inference, our model achieves a good accuracy-latency trade-off on the ImageNet test dataset. The ScaleNAS leaderboard provides an extension of our NAS methods on human pose estimation benchmarks. ScaleNAS proposes a one-shot approach to learn scale-aware representations in efficient human pose estimations.\r\n\n\nWe also employed NAS to develop versatile models that address complex problems and demands. Building on graph heuristics, we leveraged architectural heterogeneity and dense connectivity to drive our discovery process. In the realm of recommender systems, we advanced both model quality and efficiency through end-to-end differentiable NAS, combining heterogeneous building blocks to craft architectures for large-scale recommenders. Our distributed system-based approach enables searches on terabyte-scale datasets within just 2 hours, demonstrating a scalable solution for crafting architectures that meet real-world demands. For Out-of-Distribution (OoD) detection, we focused on ensuring the safety of model deployment in real-world scenarios by exploring better dense connectivity using our NAS methodologies. By utilizing a predictor-based search framework and proposing an evolving distillation algorithm to improve our search strategy, we achieved state-of-the-art performance in OoD evaluation.\r\n\n\nThe project trained 4 Ph.D. students and published 3 conference papers.\r\n\n\n\t\t\t\t\tLast Modified: 01/31/2025\n\n\t\t\t\t\tSubmitted by: YiranChen\n"
 }
}