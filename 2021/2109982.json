{
 "awd_id": "2109982",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: Small: Collaborative: Content-Based Viewport Prediction Framework for Live Virtual Reality Streaming",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922935",
 "po_email": "dmedhi@nsf.gov",
 "po_sign_block_name": "Deepankar Medhi",
 "awd_eff_date": "2021-01-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 109982.0,
 "awd_amount": 125982.0,
 "awd_min_amd_letter_date": "2021-02-01",
 "awd_max_amd_letter_date": "2021-02-01",
 "awd_abstract_narration": "Virtual reality (VR) video streaming has been gaining popularity recently with the rapid adoption of mobile head mounted display (HMD) devices in the consumer video market. As the cost for the immersive experience drops, VR video streaming introduces new bandwidth and performance challenges, especially in live streaming, due to the delivery of 360-degree views. This project develops a new content-based viewport prediction framework to improve the bandwidth and performance in live VR streaming, which predicts the user's viewport through a fusion of tracking the moving objects in the video, extracting the video semantics, and modeling the user's viewport of interest.\r\n\r\nThis project consists of three research thrusts. First, it develops a content-based viewport prediction framework for live VR streaming by tracking the motions and semantics of the objects. Second, it employs hardware and software techniques to facilitate real-time execution and scale the viewport prediction mechanism to a large number of users. Third, it develops evaluation frameworks to verify the functionality, performance, and scalability of the approach. The project uniquely considers the correlation between video content and user behavior, which leverages the deterministic nature of the former to conquer the randomness of the latter.\r\n\r\nWith the rapidly increasing popularity of VR systems in domain-specific immersive environments, the project will benefit several VR-related fields of studies with significant bandwidth savings and performance improvements, such as VR-based live broadcast, healthcare, and scientific visualization. Moreover, the interdisciplinary nature of the project will enhance the education and recruitment of underrepresented minorities in several science, technology, engineering, and mathematics (STEM) fields.\r\n\r\nThe project repository will be stored on a publicly accessible server (https://github.com/hwsel). All the project data will be maintained for at least five years following the end of the grant period.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yan",
   "pi_last_name": "Yan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yan Yan",
   "pi_email_addr": "yyan55@uic.edu",
   "nsf_id": "000784712",
   "pi_start_date": "2021-02-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Illinois Institute of Technology",
  "inst_street_address": "10 W 35TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125673035",
  "inst_zip_code": "606163717",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "ILLINOIS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E2NDENMDUEG8"
 },
 "perf_inst": {
  "perf_inst_name": "Illinois Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606163717",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 109982.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-864a7976-7fff-4ae9-c849-8bb3539c1536\"> </span></p>\n<p dir=\"ltr\"><span>This project developed a new content-based viewport prediction framework to reduce the bandwidth consumption when streaming live virtual reality (VR) videos from the content server to the end-user head mounted displays. During the course of the project, the PIs have accomplished three major research outcomes: (1) a content-based viewport prediction framework for live VR video streaming considering the motion and semantics of the objects in the video; (2) hardware/software techniques to enhance the speed and scalability of the proposed framework; and (3) an evaluation framework to verify the functionality, performance, and scalability of the proposed viewport prediction framework. Along with the research outcomes, the project has generated the following major products that address the intellectual merit and broader impacts of the conducted work.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Products that address intellectual merit:&nbsp;</span></p>\n<p dir=\"ltr\"><span>(1) Publications: The project has generated research publications in top conferences/journals of multimedia, virtual reality, mobile computing, computer vision, machine learning signal processing, and computer architecture/hardware.</span></p>\n<p dir=\"ltr\"><span>(2) Software releases: The project has generated several open source software releases for the viewport prediction framework, available online at </span><a href=\"https://github.com/hwsel/\"><span>https://github.com/hwsel/</span></a></p>\n<p dir=\"ltr\"><span>(3) Dataset: The project has generated a 360-degree human activity video dataset, available online at </span><a href=\"https://egok360.github.io/\"><span>https://egok360.github.io/</span></a></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Products that address broader impacts:&nbsp;</span></p>\n<p dir=\"ltr\"><span>(1) Education materials: The content and outcomes of the project have been incorporated as education materials in several undergraduate/graduate courses offered by the PIs in their specific institutions, such as Introduction to Machine Learning, Advanced Image Processing and Computer Vision, Deep Learning Embedded Systems, Advanced Topics on Computer Vision and Multimedia, and Advances in Deep Learning. The courses and education materials benefitted a large group of undergraduate and graduate students in their programs of studies and future career development.</span></p>\n<p dir=\"ltr\"><span>(2) Student mentoring: The project has supported the PIs mentoring multiple graduate and undergraduate students in their specific institutions. The students have worked closely with the PIs in the planned research activities, with regularly scheduled research meetings and mentoring sessions. The students have learned the required knowledge and expertise in multimedia systems/signal processing, deep learning, and computer vision.</span></p>\n<p dir=\"ltr\"><span>(3) Presentations and media coverages: The project has resulted in invited presentations at conferences, research institutions, and industry, as well as coverages at various media channels. The presentations and media coverages serve as important means of research dissemination to the society for potential technology transfer.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/30/2023<br>\n\t\t\t\t\tModified by: Yan&nbsp;Yan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project developed a new content-based viewport prediction framework to reduce the bandwidth consumption when streaming live virtual reality (VR) videos from the content server to the end-user head mounted displays. During the course of the project, the PIs have accomplished three major research outcomes: (1) a content-based viewport prediction framework for live VR video streaming considering the motion and semantics of the objects in the video; (2) hardware/software techniques to enhance the speed and scalability of the proposed framework; and (3) an evaluation framework to verify the functionality, performance, and scalability of the proposed viewport prediction framework. Along with the research outcomes, the project has generated the following major products that address the intellectual merit and broader impacts of the conducted work.\n\n \nProducts that address intellectual merit: \n(1) Publications: The project has generated research publications in top conferences/journals of multimedia, virtual reality, mobile computing, computer vision, machine learning signal processing, and computer architecture/hardware.\n(2) Software releases: The project has generated several open source software releases for the viewport prediction framework, available online at https://github.com/hwsel/\n(3) Dataset: The project has generated a 360-degree human activity video dataset, available online at https://egok360.github.io/\n\n \nProducts that address broader impacts: \n(1) Education materials: The content and outcomes of the project have been incorporated as education materials in several undergraduate/graduate courses offered by the PIs in their specific institutions, such as Introduction to Machine Learning, Advanced Image Processing and Computer Vision, Deep Learning Embedded Systems, Advanced Topics on Computer Vision and Multimedia, and Advances in Deep Learning. The courses and education materials benefitted a large group of undergraduate and graduate students in their programs of studies and future career development.\n(2) Student mentoring: The project has supported the PIs mentoring multiple graduate and undergraduate students in their specific institutions. The students have worked closely with the PIs in the planned research activities, with regularly scheduled research meetings and mentoring sessions. The students have learned the required knowledge and expertise in multimedia systems/signal processing, deep learning, and computer vision.\n(3) Presentations and media coverages: The project has resulted in invited presentations at conferences, research institutions, and industry, as well as coverages at various media channels. The presentations and media coverages serve as important means of research dissemination to the society for potential technology transfer.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 08/30/2023\n\n\t\t\t\t\tSubmitted by: Yan Yan"
 }
}