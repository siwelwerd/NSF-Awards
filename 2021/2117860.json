{
 "awd_id": "2117860",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research in DRMS:  Connecting Artificial Intelligence Literacy and Human-AI Decision Making Outcomes in Organizational Hiring",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927263",
 "po_email": "roconnor@nsf.gov",
 "po_sign_block_name": "Robert O'Connor",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 27963.0,
 "awd_amount": 27963.0,
 "awd_min_amd_letter_date": "2021-07-27",
 "awd_max_amd_letter_date": "2021-07-27",
 "awd_abstract_narration": "This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).\r\n\r\nOrganizations are increasingly incorporating artificial intelligence (AI)-based technologies into decision-making processes. For example, hiring teams may use AI-based tools that analyze application data, such as resumes and interview recordings, to provide hiring recommendations. AI-based algorithms can process large amounts of data and generate recommendations that inform decisions that used to be made solely by human decision-makers. Despite the widespread use of AI-based technologies, everyday non-expert users of these technologies may not have sufficient knowledge about AI, or AI literacy, to make fair decisions using AI-based recommendations. This Doctoral Dissertation Research Improvement Grant (DDRIG) examines whether receiving recommendations from an AI-based source impacts outcomes when decision-makers vary in their AI literacy. This project has two primary purposes: 1) to develop a measure for people's AI literacy within the context of decision-making, and 2) to test this measure in a hiring scenario experiment. This research supports NSF\u2019s mission to promote the progress of science by contributing tools that support further research on human-AI interaction, a subject becoming more relevant as organizations continue to introduce new AI-based technologies into the workplace. This work helps people, regardless of technical background, become better informed and thoughtful human-AI decision-makers. The findings from this research advance a validated measure for AI literacy that researchers, individuals, and organizations can use to measure individuals' general understanding of AI-based technologies and identify potential gaps in knowledge that can impact how they use AI-based information to make decisions. This research also informs the development of educational resources that help job seekers navigate the AI-based hiring process.\r\n\r\nThe research entails two phases. Phase I involves a scale development effort that uses past research and pilot interview data to develop and test scale items for an AI literacy measure. To determine whether people's AI literacy plays a significant role in how people use AI-based information to make hiring recommendations, phase II involves a hiring scenario experiment. In the experiment, participants evaluate a mock job application and use input from a secondary evaluation to decide whether the applicant should move forward in the hiring process, and participants' AI literacy (as measured by the scale developed in phase I) is used as a control variable. The results from this investigation may help various stakeholders better understand how people's understanding of AI influences decision-making outcomes. The findings have the potential to contribute to work on AI training and education at large. Furthermore, the experiment is the first application of the AI literacy measure that catalyzes future research exploring the relationship between AI literacy and human-AI decision-making.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Keri",
   "pi_last_name": "Stephens",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Keri K Stephens",
   "pi_email_addr": "keristephens@austin.utexas.edu",
   "nsf_id": "000537592",
   "pi_start_date": "2021-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anastazja",
   "pi_last_name": "Harris",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Anastazja G Harris",
   "pi_email_addr": "harrisana@austin.utexas.edu",
   "nsf_id": "000846085",
   "pi_start_date": "2021-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Austin",
  "perf_str_addr": "2504-A Whitis Ave., Stop A1105",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121075",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132100",
   "pgm_ele_name": "Decision, Risk & Mgmt Sci"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "1V21",
   "app_name": "R&RA ARP Act DEFC V",
   "app_symb_id": "040100",
   "fund_code": "010V2122DB",
   "fund_name": "R&RA ARP Act DEFC V",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 27963.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Artificial-intelligence infused technologies continue to be incorporated into various workplace tools. Beyond conducting technical activities, algorithms are being increasingly used to provide recommendations to human decision makers. An example of this type of technology includes tools that evaluate job applicants? video interview responses and provide feedback. Though this technology is growing, there is still a lot unknown about how people actually use the results from AI-based tools to inform decisions. Additionally, there is more to know about people?s understanding about AI technology and potential biases they might hold due to their level of AI literacy. The goal of this project was to develop a measure for AI literacy and test whether it has a relationship to how people interpret and use recommendations from an AI-based tool evaluating job interview responses.</p>\n<p>&nbsp;</p>\n<p>The intellectual merit of this project includes developing a deeper understanding of the different perspectives people have towards AI-based tools when they are used to inform decisions during the hiring process. Beyond people?s technical understanding of how algorithms work, this project provides additional insight into how people make sense of AI?s role in decision-making. Using insights from both qualitative and quantitative approaches, the scale development process resulted in creating measures that highlight people?s common perspectives on how AI is used to inform decisions. The validated measure for AI literacy and machine bias includes three factors: openness to AI, perceived value of AI, and AI qualities. Furthermore, this research adds to the growing research on human-AI interaction and how people?s understanding of AI and potential bias can impact opinions on AI?s role in decision-making.</p>\n<p>&nbsp;</p>\n<p>The broader impacts of this study include providing opportunities for future research to expand on measures of AI literacy and bias. Additionally, this project provided multiple training and professional development opportunities for undergraduate students from underrepresented backgrounds. Also, by collaborating with university career services, this project opened the door for research through career services, which provides another avenue through which students and practitioners can be exposed to research. Lastly, by creating educational materials based on our findings, educators can teach others about how to navigate a human-AI hiring process.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/05/2023<br>\n\t\t\t\t\tModified by: Keri&nbsp;K&nbsp;Stephens</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nArtificial-intelligence infused technologies continue to be incorporated into various workplace tools. Beyond conducting technical activities, algorithms are being increasingly used to provide recommendations to human decision makers. An example of this type of technology includes tools that evaluate job applicants? video interview responses and provide feedback. Though this technology is growing, there is still a lot unknown about how people actually use the results from AI-based tools to inform decisions. Additionally, there is more to know about people?s understanding about AI technology and potential biases they might hold due to their level of AI literacy. The goal of this project was to develop a measure for AI literacy and test whether it has a relationship to how people interpret and use recommendations from an AI-based tool evaluating job interview responses.\n\n \n\nThe intellectual merit of this project includes developing a deeper understanding of the different perspectives people have towards AI-based tools when they are used to inform decisions during the hiring process. Beyond people?s technical understanding of how algorithms work, this project provides additional insight into how people make sense of AI?s role in decision-making. Using insights from both qualitative and quantitative approaches, the scale development process resulted in creating measures that highlight people?s common perspectives on how AI is used to inform decisions. The validated measure for AI literacy and machine bias includes three factors: openness to AI, perceived value of AI, and AI qualities. Furthermore, this research adds to the growing research on human-AI interaction and how people?s understanding of AI and potential bias can impact opinions on AI?s role in decision-making.\n\n \n\nThe broader impacts of this study include providing opportunities for future research to expand on measures of AI literacy and bias. Additionally, this project provided multiple training and professional development opportunities for undergraduate students from underrepresented backgrounds. Also, by collaborating with university career services, this project opened the door for research through career services, which provides another avenue through which students and practitioners can be exposed to research. Lastly, by creating educational materials based on our findings, educators can teach others about how to navigate a human-AI hiring process.\n\n\t\t\t\t\tLast Modified: 08/05/2023\n\n\t\t\t\t\tSubmitted by: Keri K Stephens"
 }
}