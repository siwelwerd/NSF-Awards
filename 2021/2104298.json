{
 "awd_id": "2104298",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Enabling Real-Time AI on End Devices through Compression-Compilation Co-Design",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 254700.0,
 "awd_amount": 254700.0,
 "awd_min_amd_letter_date": "2021-06-30",
 "awd_max_amd_letter_date": "2021-06-30",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project is in the array of new opportunities it creates for expanding uses of machine intelligence. By providing an efficient way to transform deep learning models to best fit the constraints on end devices and real-time applications, this project will shorten the time to market for artificial intelligence applications by orders of magnitude, and hence significantly accelerate the development and deployment of intelligent software in health, commerce, financial, defense, social networks, and many other areas.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project aims to address the important barriers for efficient development of real-time Artificial Intelligence applications on end devices (smartphones, drones, etc.). It does this through a breakthrough technology, compression-compilation co-design. Compression and compilation are the two key steps in fitting a deep learning model on a hardware for efficient execution. Model compression reduces the size of deep learning models; compilation generates executable codes from a given deep learning model. The principle of compression-compilation co-design is to design the two components for AI in a hand-in-hand manner. The technology uses a novel approach to synergize a set of novel model compression methods with compression-aware code compilation techniques. The result is a technology that achieves several-fold higher artificial intelligence model compression rates over the state of the art, several times faster speed, better energy efficiency, and satisfying accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xipeng",
   "pi_last_name": "Shen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xipeng Shen",
   "pi_email_addr": "xshen5@ncsu.edu",
   "nsf_id": "000180293",
   "pi_start_date": "2021-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "COCOPIE LLC",
  "inst_street_address": "2479 E BAYSHORE RD",
  "inst_street_address_2": "STE 280A",
  "inst_city_name": "PALO ALTO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "7579680210",
  "inst_zip_code": "943033228",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "COCOPIE INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "KLBYDFCNP2F3"
 },
 "perf_inst": {
  "perf_inst_name": "COCOPIE LLC",
  "perf_str_addr": "15 MARGARET RD UNIT 2",
  "perf_city_name": "NEWTON",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024611610",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 254700.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Through this project, the team have created the first fully automatic end-to-end tool for DNN model-code cooptimization, named CoCoPIE XGen. CoCoPIE XGen is&nbsp;<span>the first AI acceleration toolchain that&nbsp;</span><em>co-optimizes</em><span>&nbsp;the models and code of deep neural network (DNN), making AI models&nbsp;many times smaller and faster for use on devices while maintaining accuracy. It builds the first major milestone for clearing the long-standing obstacles to AI deployment on end devices.&nbsp;</span></p>\n<p><span>Now that DNN technology in the cloud has shown its potential, the market demand to deploy AI models directly on end devices has become increasingly strong in recent years. Service providers and device makers want to run AI models on end devices to provide better experiences to their customers and save operations costs. Sometimes, it is mandatory to deploy AI on end devices when it comes to data privacy that avoids data transmission to the cloud or data locality that avoids service failure due to cloud connection. It is, however, challenging to deploy AI on end devices. The current important AI models are mostly designed for cloud deployment, with billions of parameters and operations, relying on the cloud servers' high computation power to run them. Running the models on end devices requires times of reduction in both the model size and the model execution time. Existing tools have isolated solutions that either optimize the AI model size or the execution speed, leading to unsatisfactory results and leaving the AI capabilities largely locked out on devices.</span></p>\n<p>XGen solves the challenge through its proprietary co-design technology in the AI model's&nbsp;<em>compression, compilation, and runtime</em>. The technology compresses AI models in high ratio while still maintaining the model representation friendly for highly efficient code generation. It meanwhile tailors the generated code to best map and schedule the compressed model to the hardware.</p>\n<p>With CoCoPIE XGen, an input AI model can turn into a directly deployable code with large speedups, much smaller size, and significant power savings while keeping the model accuracy to users' satisfaction. CoCoPIE XGen is a low-code tool. Users just need to input their AI requirements through its intuitive user interface; XGen then automatically conducts the model and code co-optimization and generates the desirable code ready to be integrated into end applications. In the first release, it supports the integration with applications of Android and iOS. Future releases will support more target platforms and systems. CoCoPIE XGen also offers full controllability so that users can fully customize the optimizations as they desire.</p>\n<p><span>The superior optimization capability of CoCoPIE XGen&nbsp;</span><em>makes ubiquitous AI deployment possible&nbsp;</em><span>on off-the-shelf devices without relying on dedicated hardware support.&nbsp;<span>By removing the key roadblock to AI on end devices, CoCoPIE XGen allows businesses to capitalize on the huge market of device AI that had been beyond their reach.</span></span></p>\n<p>The team is currently working with their global partners to put CoCoPIE XGen in use to meet various commercial demands.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2022<br>\n\t\t\t\t\tModified by: Xipeng&nbsp;Shen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThrough this project, the team have created the first fully automatic end-to-end tool for DNN model-code cooptimization, named CoCoPIE XGen. CoCoPIE XGen is the first AI acceleration toolchain that co-optimizes the models and code of deep neural network (DNN), making AI models many times smaller and faster for use on devices while maintaining accuracy. It builds the first major milestone for clearing the long-standing obstacles to AI deployment on end devices. \n\nNow that DNN technology in the cloud has shown its potential, the market demand to deploy AI models directly on end devices has become increasingly strong in recent years. Service providers and device makers want to run AI models on end devices to provide better experiences to their customers and save operations costs. Sometimes, it is mandatory to deploy AI on end devices when it comes to data privacy that avoids data transmission to the cloud or data locality that avoids service failure due to cloud connection. It is, however, challenging to deploy AI on end devices. The current important AI models are mostly designed for cloud deployment, with billions of parameters and operations, relying on the cloud servers' high computation power to run them. Running the models on end devices requires times of reduction in both the model size and the model execution time. Existing tools have isolated solutions that either optimize the AI model size or the execution speed, leading to unsatisfactory results and leaving the AI capabilities largely locked out on devices.\n\nXGen solves the challenge through its proprietary co-design technology in the AI model's compression, compilation, and runtime. The technology compresses AI models in high ratio while still maintaining the model representation friendly for highly efficient code generation. It meanwhile tailors the generated code to best map and schedule the compressed model to the hardware.\n\nWith CoCoPIE XGen, an input AI model can turn into a directly deployable code with large speedups, much smaller size, and significant power savings while keeping the model accuracy to users' satisfaction. CoCoPIE XGen is a low-code tool. Users just need to input their AI requirements through its intuitive user interface; XGen then automatically conducts the model and code co-optimization and generates the desirable code ready to be integrated into end applications. In the first release, it supports the integration with applications of Android and iOS. Future releases will support more target platforms and systems. CoCoPIE XGen also offers full controllability so that users can fully customize the optimizations as they desire.\n\nThe superior optimization capability of CoCoPIE XGen makes ubiquitous AI deployment possible on off-the-shelf devices without relying on dedicated hardware support. By removing the key roadblock to AI on end devices, CoCoPIE XGen allows businesses to capitalize on the huge market of device AI that had been beyond their reach.\n\nThe team is currently working with their global partners to put CoCoPIE XGen in use to meet various commercial demands.\n\n \n\n\t\t\t\t\tLast Modified: 09/01/2022\n\n\t\t\t\t\tSubmitted by: Xipeng Shen"
 }
}