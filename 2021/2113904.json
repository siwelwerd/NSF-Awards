{
 "awd_id": "2113904",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CCSS: Learning to Optimize: From New Algorithms to New Theory",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032924568",
 "po_email": "hdai@nsf.gov",
 "po_sign_block_name": "Huaiyu Dai",
 "awd_eff_date": "2021-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 220000.0,
 "awd_amount": 220000.0,
 "awd_min_amd_letter_date": "2021-08-05",
 "awd_max_amd_letter_date": "2021-08-05",
 "awd_abstract_narration": "Solving machine learning (ML) problems requires efficient and scalable optimization algorithms. State-of-the-art general purpose algorithms often need to compute a large number of iterations and hence have limited applicability to real-time applications. To circumvent this shortcoming, learning to optimize (L2O) methods aim to learn a shorter (i.e., faster) optimization path over a task distribution at meta-training, based on the tasks\u2019 common structures and a more global view of their geometries, and then apply the learned optimizer to new similar tasks at meta-testing. Despite extensive empirical success, the existing L2O methods perform well mainly on optimization tasks with similar structures, but likely perform poorly on out-of-distribution tasks. Furthermore, there has been little theory understanding the convergence and generalization of L2O algorithms. Thus, the proposed program will design novel L2O approaches, so that the trained optimizer can generalize to a broad range of practical tasks, particularly out-of-distribution tasks, and will have guaranteed convergence and generalization performance in L2O training and testing.\r\n\r\nSpecifically, the proposed program will design new L2O approaches with both generalizability to out-of-distribution tasks and safeguarded feature for guaranteed worst-case convergence, will develop a theoretical framework for analyzing the convergence rate for L2O meta-training, and will provide comprehensive characterization of the generalization performance for L2O meta-testing. The new algorithms and theory will be evaluated over applications of on-device model adaptation in internet-of-things (IoT) systems, sparse recovery for images and wireless signals, and algorithmic adaptation in reconfiguration of communication systems. The project is anticipated to significantly mature the field of L2O, and provide training opportunities for a diverse group of students at the new intersection of optimization, machine learning, signal processing, and data science.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhangyang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhangyang Wang",
   "pi_email_addr": "atlaswang@utexas.edu",
   "nsf_id": "000746175",
   "pi_start_date": "2021-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "2501 Speedway, C0803",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121684",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "153E",
   "pgm_ref_txt": "Wireless comm & sig processing"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 220000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The proposed program aimed to innovate Learning to Optimize (L2O) techniques, making them adaptable to a wide range of practical tasks, including those beyond their training distribution. Additionally, we sought to establish a robust theoretical framework to guarantee the convergence of L2O meta-training and to characterize its generalization capabilities during meta-testing. Over the life of this award, we achieved significant milestones that contribute to both the intellectual merit and broader impacts of our work.</p>\n<p><strong>Year 1: Pioneering Uncertainty and Generalization in L2O</strong> In the first year, we infused Bayesian uncertainty into L2O, marking the first attempt to recognize and quantify the uncertainty of optimization algorithms. This breakthrough provided a new dimension in understanding and improving L2O's reliability. Collaborating with PI Liang of OSU, we analyzed existing hand-crafted optimizers, identifying core metrics that enhance generalization. This led to the innovative &ldquo;learn to generalize&rdquo; design, enabling L2O to meta-train optimizers with enhanced generalization abilities, crucial for optimizing new and diverse tasks.</p>\n<p><strong>Year 2: Extending L2O's Theoretical and Practical Horizons</strong> Building on our initial success, the second year focused on the generalization theory of L2O. In collaboration with OSU, we addressed two major generalization challenges: optimizer generalization and optimizee generalization. Our research established a theoretical connection between local entropy and Hessian, proposing flatness-aware regularizers to enhance L2O&rsquo;s ability to generalize. Additionally, in partnership with JP Morgan AI Research, we applied L2O to differential games, creating a stable and efficient solver framework. We also published the first comprehensive L2O benchmark, providing a valuable resource for the research community and promoting reproducible research through our Open-L2O platform.</p>\n<p><strong>Year 3: Practical Innovations and Theoretical Advancements</strong> Our third year saw the development of the Gradient Low-Rank Projection (GaLore) strategy, significantly reducing memory usage in optimizer states and enabling the training of large language models (LLMs) on consumer-grade GPUs. This practical innovation demonstrated GaLore's capability to support efficient full-parameter learning without compromising performance. We also introduced an 8-bit version of GaLore, further enhancing memory efficiency. Theoretically, we explored the training of one-hidden-layer overparameterized ReLU networks in the neural tangent kernel (NTK) regime, achieving sparse activation and fast convergence. Our findings included improved convergence rates and generalization bounds, offering new insights into the optimization landscape of neural networks.</p>\n<p><strong>Intellectual Merit:</strong> This project advanced the field of optimization in machine learning by introducing novel methods and theoretical insights. By integrating Bayesian uncertainty and developing flatness-aware regularizers, we enhanced the robustness and generalization capabilities of L2O. Our work on differential games and the extensive benchmarking of L2O approaches provided practical solutions and valuable resources for the research community. The GaLore strategy&rsquo;s impact on LLM training demonstrated the feasibility of efficient optimization on consumer hardware, pushing the boundaries of what is achievable in resource-constrained environments.</p>\n<p><strong>Broader Impacts:</strong> The broader impacts of this project are substantial. The practical innovations in memory-efficient optimization techniques can significantly reduce the computational resources required for training large models, making advanced AI technologies more accessible and sustainable. Our open-source contributions, such as the Open-L2O benchmark, foster transparency, collaboration, and further advancements in the field. By addressing fundamental theoretical challenges, our work also lays the groundwork for future research in optimization and machine learning, potentially benefiting a wide range of applications, from scientific research to industry practices.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/05/2024<br>\nModified by: Zhangyang&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe proposed program aimed to innovate Learning to Optimize (L2O) techniques, making them adaptable to a wide range of practical tasks, including those beyond their training distribution. Additionally, we sought to establish a robust theoretical framework to guarantee the convergence of L2O meta-training and to characterize its generalization capabilities during meta-testing. Over the life of this award, we achieved significant milestones that contribute to both the intellectual merit and broader impacts of our work.\n\n\nYear 1: Pioneering Uncertainty and Generalization in L2O In the first year, we infused Bayesian uncertainty into L2O, marking the first attempt to recognize and quantify the uncertainty of optimization algorithms. This breakthrough provided a new dimension in understanding and improving L2O's reliability. Collaborating with PI Liang of OSU, we analyzed existing hand-crafted optimizers, identifying core metrics that enhance generalization. This led to the innovative learn to generalize design, enabling L2O to meta-train optimizers with enhanced generalization abilities, crucial for optimizing new and diverse tasks.\n\n\nYear 2: Extending L2O's Theoretical and Practical Horizons Building on our initial success, the second year focused on the generalization theory of L2O. In collaboration with OSU, we addressed two major generalization challenges: optimizer generalization and optimizee generalization. Our research established a theoretical connection between local entropy and Hessian, proposing flatness-aware regularizers to enhance L2Os ability to generalize. Additionally, in partnership with JP Morgan AI Research, we applied L2O to differential games, creating a stable and efficient solver framework. We also published the first comprehensive L2O benchmark, providing a valuable resource for the research community and promoting reproducible research through our Open-L2O platform.\n\n\nYear 3: Practical Innovations and Theoretical Advancements Our third year saw the development of the Gradient Low-Rank Projection (GaLore) strategy, significantly reducing memory usage in optimizer states and enabling the training of large language models (LLMs) on consumer-grade GPUs. This practical innovation demonstrated GaLore's capability to support efficient full-parameter learning without compromising performance. We also introduced an 8-bit version of GaLore, further enhancing memory efficiency. Theoretically, we explored the training of one-hidden-layer overparameterized ReLU networks in the neural tangent kernel (NTK) regime, achieving sparse activation and fast convergence. Our findings included improved convergence rates and generalization bounds, offering new insights into the optimization landscape of neural networks.\n\n\nIntellectual Merit: This project advanced the field of optimization in machine learning by introducing novel methods and theoretical insights. By integrating Bayesian uncertainty and developing flatness-aware regularizers, we enhanced the robustness and generalization capabilities of L2O. Our work on differential games and the extensive benchmarking of L2O approaches provided practical solutions and valuable resources for the research community. The GaLore strategys impact on LLM training demonstrated the feasibility of efficient optimization on consumer hardware, pushing the boundaries of what is achievable in resource-constrained environments.\n\n\nBroader Impacts: The broader impacts of this project are substantial. The practical innovations in memory-efficient optimization techniques can significantly reduce the computational resources required for training large models, making advanced AI technologies more accessible and sustainable. Our open-source contributions, such as the Open-L2O benchmark, foster transparency, collaboration, and further advancements in the field. By addressing fundamental theoretical challenges, our work also lays the groundwork for future research in optimization and machine learning, potentially benefiting a wide range of applications, from scientific research to industry practices.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 08/05/2024\n\n\t\t\t\t\tSubmitted by: ZhangyangWang\n"
 }
}