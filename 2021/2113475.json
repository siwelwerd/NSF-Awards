{
 "awd_id": "2113475",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Efficient Bayesian Global Optimization with Applications to Deep Learning and Computer Experiments",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 199999.0,
 "awd_amount": 199999.0,
 "awd_min_amd_letter_date": "2021-06-30",
 "awd_max_amd_letter_date": "2023-07-16",
 "awd_abstract_narration": "The primary objective of this research is to develop global optimization methods which will dramatically enhance the optimization efficiency in the studies of complex scientific problems. The research findings will significantly accelerate discoveries in numerous scientific disciplines involving artificial intelligence and numerical simulations such as mechanical engineering, energy, automated transportation, aerospace engineering, environmental science, and materials science. Integrated into the research is an education plan that emphasizes interdisciplinary training for a broad body of students and increasing participation from underrepresented groups. The PIs will recruit female students and undergraduate students from underrepresented groups and actively involve them in this research. Research findings will be disseminated at conferences. Furthermore, research findings will also be integrated into PIs\u2019 courses to have optimization and data analysis training for graduate and undergraduate students.  \r\n\r\nThis research focuses on Bayesian global optimization which refers to active learning strategies developed by stochastic process priors for the optimization of expensive \"black box\" functions. Motivated by the challenges emerged from global optimization in deep learning and computer experiments, two innovative Bayesian active learning methods will be developed which are applicable to problems with conditionally dependent inputs and non-Gaussian stochastic outputs. The first method will address an important but unresolved issue arising from the optimization of stochastic outputs in classification problems. The novelty lies in an expected improvement criterion developed based on a generalized Gaussian process which leads to a tractable objective function with an intuitive interpretation. The asymptotic convergence properties will be developed rigorously under the continuum-armed-bandit settings. The second method is based on a new correlation function for a branching and nested structure, which occurs commonly in practice. Sufficient conditions on the validity of the new correlation functions is expected to be derived and a new class of optimal initial designs will be systematically constructed. The innovative idea of automatic-tuning in deep learning by a rigorous Bayesian global optimization will shed light on new methodologies for the optimization of \"black box\" functions and inspire new research ideas in machine learning, optimization, and spatial statistics. Beyond the applications to computer vision and optimal controls in robotics, the design, modeling, and optimization strategies can open new avenues for studying complex optimization problems with expensive unknown functions and energize both theoretical and applied research.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ying",
   "pi_last_name": "Hung",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ying Hung",
   "pi_email_addr": "yhung@stat.rutgers.edu",
   "nsf_id": "000516687",
   "pi_start_date": "2021-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "110 Frelinghuysen Rd",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 74119.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 76203.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 49677.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Studies in science and engineering often require efficient statistical surrogate models and global optimization tools. Modern applications of global optimization and surrogate modeling, particularly in deep learning and computer experiments, not only create unprecedented opportunities for scientific discoveries, but they also present unique statistical challenges. These challenges arise from unconventional inputs and data sparsity in complex problems. This proposal aims to address these challenges through efficient Bayesian approaches, leveraging new kernel functions for Gaussian process (GP) models that effectively capture the functional structure of the input variables. We also introduce a novel procedure that, given sparse data generated from a nonlinear dynamical system, can characterize specific local and/or global dynamic behavior with rigorous probability guarantees. This procedure integrates surrogate modeling and topology to significantly reduce the amount of data required to describe the underlying global dynamics of robot controllers. Furthermore, we propose a new GP model for efficient tuning of deep learning parameters. This new model focuses on two types of parameters that commonly occur in deep learning settings but have been overlooked in the literature. Sufficient conditions are rigorously derived to guarantee the validity of the proposed model, and the asymptotic convergence of the proposed optimization framework is rigorously studied. Based on the new GP model, higher prediction accuracy and better optimization efficiency are observed in a series of synthetic simulations and real data applications of neural networks. The proposed methods are widely applicable, aiming to enhance the efficiency of deep learning algorithms and provide deeper insights into the underlying dynamics of engineering and medical applications. Under the support of this grant, research papers are written and submitted to journals and peer-reviewed conferences for publication.&nbsp;The research findings are disseminated at conferences and departmental seminars. Three Ph.D. students are partially supported by this grant, including a female student who graduated in 2023. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/22/2024<br>\nModified by: Ying&nbsp;Hung</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nStudies in science and engineering often require efficient statistical surrogate models and global optimization tools. Modern applications of global optimization and surrogate modeling, particularly in deep learning and computer experiments, not only create unprecedented opportunities for scientific discoveries, but they also present unique statistical challenges. These challenges arise from unconventional inputs and data sparsity in complex problems. This proposal aims to address these challenges through efficient Bayesian approaches, leveraging new kernel functions for Gaussian process (GP) models that effectively capture the functional structure of the input variables. We also introduce a novel procedure that, given sparse data generated from a nonlinear dynamical system, can characterize specific local and/or global dynamic behavior with rigorous probability guarantees. This procedure integrates surrogate modeling and topology to significantly reduce the amount of data required to describe the underlying global dynamics of robot controllers. Furthermore, we propose a new GP model for efficient tuning of deep learning parameters. This new model focuses on two types of parameters that commonly occur in deep learning settings but have been overlooked in the literature. Sufficient conditions are rigorously derived to guarantee the validity of the proposed model, and the asymptotic convergence of the proposed optimization framework is rigorously studied. Based on the new GP model, higher prediction accuracy and better optimization efficiency are observed in a series of synthetic simulations and real data applications of neural networks. The proposed methods are widely applicable, aiming to enhance the efficiency of deep learning algorithms and provide deeper insights into the underlying dynamics of engineering and medical applications. Under the support of this grant, research papers are written and submitted to journals and peer-reviewed conferences for publication.The research findings are disseminated at conferences and departmental seminars. Three Ph.D. students are partially supported by this grant, including a female student who graduated in 2023. \n\n\n\t\t\t\t\tLast Modified: 10/22/2024\n\n\t\t\t\t\tSubmitted by: YingHung\n"
 }
}