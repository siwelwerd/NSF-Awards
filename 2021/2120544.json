{
 "awd_id": "2120544",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SaTC: CORE: Small: Foundations for the Next Generation of Private Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2021-07-16",
 "awd_max_amd_letter_date": "2021-07-16",
 "awd_abstract_narration": "Recent advances in large-scale machine learning (ML) promise a range of benefits to society, but also introduce new risks.  One major risk is a loss of privacy for the individuals whose data powers the machine learning algorithms. There are now convincing demonstrations that algorithms for machine learning can reveal sensitive information about individuals in their training data by memorizing specific strings of sensitive text such as bank account numbers or through membership-inference attacks. In the recent years, a framework called differential privacy---a mathematically principled, quantitative notion of what it means for an algorithm to ensure privacy for the individuals who contribute training data---has led to significant progress towards privacy in machine learning. This progress offers a proof-of-concept that we can hope to enjoy some of the benefits of using machine learning on sensitive data, while measuring and limiting breaches of confidentiality.  This project will investigate and begin to make some of the fundamental advances that are necessary to make differentially private ML a viable technology.  The focus will be on laying the groundwork for differentially private ML for entire systems, rather than for standalone tasks, which have been the focus of prior work.  This project team comprising researchers with a broad range of expertise in ML, algorithms, systems, and cybersecurity, has planned a set of education tasks: public-facing set of course materials on differentially private machine learning and statistics and and an undergraduate-level textbook on differential privacy.\r\n\r\nThis project includes three technical thrusts that will lay the groundwork for future efforts to build private ML systems.  The first thrust will be to improve the foundational algorithms that enable differentially private ML on high-dimensional data.  The second thrust will be to build a bridge between algorithms for standalone ML tasks and algorithms for systems-level workloads of ML tasks, by developing differentially private algorithms for training many personalized models, which is a paradigmatic workload in ML.  The final thrust will consist of empirical work on auditing differentially private ML methods to understand how the real-world privacy costs compare to those predicted by the theory of differential privacy when these algorithms are used as part of realistic workloads, such as models that are continually updated with new data. This privacy auditing will also facilitate detecting unwanted memorization of training data in machine learning, and also provide more quantitative approaches to auditing differentially private algorithms based on membership-inference and data poisoning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Roxana",
   "pi_last_name": "Geambasu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Roxana Geambasu",
   "pi_email_addr": "roxana@cs.columbia.edu",
   "nsf_id": "000602293",
   "pi_start_date": "2021-07-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "500 W 120th St",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276623",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-cb51b33e-7fff-b73b-60a7-0fb485d89521\">\n<p dir=\"ltr\"><span>Recent advances in large-scale machine learning (ML) promise a range of benefits to society, but also risks.&nbsp; One major risk is a loss of privacy for the individuals whose data powers these advances. There are now convincing demonstrations that algorithms for machine learning can reveal sensitive information about individuals in their training data.&nbsp; In the past five years a framework called differential privacy---a mathematically principled, quantitative notion of what it means for an algorithm to ensure privacy for the individuals who contribute training data---has led to significant progress towards privacy in ML. It offers a proof-of-concept that we can hope to enjoy some of the benefits of ML on sensitive data while measuring and limiting breaches of confidentiality.&nbsp; This project investigated and began to make the fundamental advances that are necessary to make differentially private ML a viable technology.</span></p>\n<p dir=\"ltr\"><span>The main goal of this project was to develop differentially private ML technology at the level of entire systems, rather than standalone tasks, which was the focus of prior work.&nbsp; The project made contributions on three separate thrusts.&nbsp; The first thrust improved the foundational algorithms that enable differentially private ML on high-dimensional data.&nbsp; The second thrust bridged between algorithms for standalone ML tasks and algorithms for systems-level workloads of ML tasks, by developing differentially private algorithms for training many personalized models.&nbsp; The final thrust developed empirical methods for auditing differentially private ML to understand the real-world privacy costs when these algorithms are used as part of realistic workloads, such as models that are continually updated with new data.</span></p>\n<div><span><br /></span></div>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/25/2024<br>\nModified by: Roxana&nbsp;Geambasu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nRecent advances in large-scale machine learning (ML) promise a range of benefits to society, but also risks. One major risk is a loss of privacy for the individuals whose data powers these advances. There are now convincing demonstrations that algorithms for machine learning can reveal sensitive information about individuals in their training data. In the past five years a framework called differential privacy---a mathematically principled, quantitative notion of what it means for an algorithm to ensure privacy for the individuals who contribute training data---has led to significant progress towards privacy in ML. It offers a proof-of-concept that we can hope to enjoy some of the benefits of ML on sensitive data while measuring and limiting breaches of confidentiality. This project investigated and began to make the fundamental advances that are necessary to make differentially private ML a viable technology.\n\n\nThe main goal of this project was to develop differentially private ML technology at the level of entire systems, rather than standalone tasks, which was the focus of prior work. The project made contributions on three separate thrusts. The first thrust improved the foundational algorithms that enable differentially private ML on high-dimensional data. The second thrust bridged between algorithms for standalone ML tasks and algorithms for systems-level workloads of ML tasks, by developing differentially private algorithms for training many personalized models. The final thrust developed empirical methods for auditing differentially private ML to understand the real-world privacy costs when these algorithms are used as part of realistic workloads, such as models that are continually updated with new data.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 02/25/2024\n\n\t\t\t\t\tSubmitted by: RoxanaGeambasu\n"
 }
}