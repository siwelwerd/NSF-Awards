{
 "awd_id": "2101936",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: Fair Machine Learning with Restricted Access to Sensitive Personal Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 174998.0,
 "awd_amount": 174998.0,
 "awd_min_amd_letter_date": "2021-01-07",
 "awd_max_amd_letter_date": "2021-01-07",
 "awd_abstract_narration": "Machine learning is increasingly applied to assist consequential decision makings, typically by learning a model to automatically score people's potential and prioritizing advantaged decisions on those receiving higher scores. While this enables more efficient and evidence-based decision makings, recent studies show that many model scorings are biased against minority people and can result in negative societal impacts. This has triggered intensive research interests in developing fair machine learning techniques that can mitigate demographic bias in model scoring. The problem is, existing developments are running into a conflict with data privacy regulations. To be specific, most fair learning techniques require free access to one's sensitive demographic data, but the latter is increasingly restricted to use for protecting one's privacy. There are ongoing debates on whether it is permissible or necessary to use sensitive demographic data in fair machine learning, but so far no consensus has been reached due to the lack of scientific investigations. This project aims to fill this gap, not only for establishing a fundamental relation between fairness and privacy but also for broadening the deployment and impact of fair learning techniques in real-world applications; the project will also have an important educational impact via the involvement of underrepresented students in computer science research and the creation of a new curriculum on ethical machine learning to train the next-generation ethics-aware data scientists.\r\n\r\nThis project will develop novel fair machine learning techniques with restricted access to sensitive demographic data (SDD). Three scenarios will be formulated and solved: where SDD is not accessible, where SDD can be accessed with cost, and where SDD can be accessed through a private third party. To tackle these scenarios, this project will integrate fairness objectives with a variety of sophisticated learning techniques including transfer learning, active learning, distributed learning and private learning. The project will also investigate a fundamental relation between fairness and privacy in machine learning, that is, how much fairness can be achieved in a model's scoring if one has to protect certain privacy of the sensitive demographic data when learning the model. The developed solutions will be presented in conference and journal venues, and the project website will provide access to the results, with references to the codes for the developed and evaluated algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chao",
   "pi_last_name": "Lan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chao Lan",
   "pi_email_addr": "clan@ou.edu",
   "nsf_id": "000763773",
   "pi_start_date": "2021-01-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Oklahoma Norman Campus",
  "inst_street_address": "660 PARRINGTON OVAL RM 301",
  "inst_street_address_2": "",
  "inst_city_name": "NORMAN",
  "inst_state_code": "OK",
  "inst_state_name": "Oklahoma",
  "inst_phone_num": "4053254757",
  "inst_zip_code": "730193003",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OK04",
  "org_lgl_bus_name": "UNIVERSITY OF OKLAHOMA",
  "org_prnt_uei_num": "",
  "org_uei_num": "EVTSTTLCEWS5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Oklahoma Norman Campus",
  "perf_str_addr": "",
  "perf_city_name": "Norman",
  "perf_st_code": "OK",
  "perf_st_name": "Oklahoma",
  "perf_zip_code": "730199705",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OK04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 174998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-04623899-7fff-13ec-0d06-0fcdbc31b6b8\"> </span></p>\n<p><span id=\"docs-internal-guid-da515137-7fff-886d-7cf2-0c1d332f6aa7\"> </span></p>\n<p><span id=\"docs-internal-guid-3b19910a-7fff-0ad2-9319-b83e8c471337\"> </span></p>\n<p><span id=\"docs-internal-guid-730df6e2-7fff-e63b-b7ef-c35b96946f2a\"> </span></p>\n<p><span id=\"docs-internal-guid-6b71c7c1-7fff-1ff1-0a89-5cb8bc64c520\"> </span></p>\n<p><span id=\"docs-internal-guid-5cabff60-7fff-ce84-52ea-98780b4df2d5\"> </span></p>\n<p><span id=\"docs-internal-guid-8d1c4cc5-7fff-2e42-b850-6b50efe525da\"> </span></p>\n<p dir=\"ltr\"><span>This project addresses a conflict between fairness and privacy in machine learning, that is, how to learn fair models with restricted access to the sensitive personal attribute data that are often protected by privacy regulations.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We first design two novel and practical algorithmic solutions. One is a two-party learning framework where a modeling center has no sensitive data but can learn fair models by assembling random models that are privately identified as fair by a trusted second party which holds the sensitive data, and our experimental results show this solution maintains both model accuracy and fairness achieved by regular fairness-aware learners with no data privacy protection. The other solution is an active learning framework where a modeling center has limited sensitive data to train fair models but can strategically query a few more for training instances that are potentially unfairly treated, and our experimental results show this strategy improves model fairness more efficiently than random query while maintaining its improvement rate on model accuracy.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We then develop theoretical foundations for the two solutions under more general settings. Motivated by the two-party learning solution, we design the first model agnostic randomized learning framework which can efficiently learn a provably accurate model given any hypothesis class by optimally assembling a set of random models drawn from the class through merely solving a linear problem. Motivated by the active learning solution, we design the first active learning strategy for the metric-fair learner which labels potentially unfairly treated instances and can provably achieve an O(log1/e) sample complexity for attaining a fairness budget e which is better than SOTA O(1/e^2).&nbsp;</span></p>\n<p dir=\"ltr\"><span>Finally, we investigate the robustness of privacy protection against inference attack for the two-party learning solution. Our experimental results show an adversary with sufficient knowledge at the modeling center can infer sensitive data at the trusted party by solving an integer programming problem, but such inference can be prevented by randomizing the process of fair model identification. Our theoretical studies suggest the designed randomized learning framework, by assembling sufficient random models, can preserve differential privacy of training data without sacrificing model accuracy. This suggests randomized learning is a promising technique for ethical ML designs.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/22/2022<br>\n\t\t\t\t\tModified by: Chao&nbsp;Lan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\n \n\n \n\n \n\n \n\n \n\n \nThis project addresses a conflict between fairness and privacy in machine learning, that is, how to learn fair models with restricted access to the sensitive personal attribute data that are often protected by privacy regulations. \nWe first design two novel and practical algorithmic solutions. One is a two-party learning framework where a modeling center has no sensitive data but can learn fair models by assembling random models that are privately identified as fair by a trusted second party which holds the sensitive data, and our experimental results show this solution maintains both model accuracy and fairness achieved by regular fairness-aware learners with no data privacy protection. The other solution is an active learning framework where a modeling center has limited sensitive data to train fair models but can strategically query a few more for training instances that are potentially unfairly treated, and our experimental results show this strategy improves model fairness more efficiently than random query while maintaining its improvement rate on model accuracy. \nWe then develop theoretical foundations for the two solutions under more general settings. Motivated by the two-party learning solution, we design the first model agnostic randomized learning framework which can efficiently learn a provably accurate model given any hypothesis class by optimally assembling a set of random models drawn from the class through merely solving a linear problem. Motivated by the active learning solution, we design the first active learning strategy for the metric-fair learner which labels potentially unfairly treated instances and can provably achieve an O(log1/e) sample complexity for attaining a fairness budget e which is better than SOTA O(1/e^2). \nFinally, we investigate the robustness of privacy protection against inference attack for the two-party learning solution. Our experimental results show an adversary with sufficient knowledge at the modeling center can infer sensitive data at the trusted party by solving an integer programming problem, but such inference can be prevented by randomizing the process of fair model identification. Our theoretical studies suggest the designed randomized learning framework, by assembling sufficient random models, can preserve differential privacy of training data without sacrificing model accuracy. This suggests randomized learning is a promising technique for ethical ML designs. \n\n \n\n\t\t\t\t\tLast Modified: 12/22/2022\n\n\t\t\t\t\tSubmitted by: Chao Lan"
 }
}