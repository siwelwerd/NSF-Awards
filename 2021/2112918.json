{
 "awd_id": "2112918",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Overparameterization, Global Convergence of the Expectation-Maximization Algorithm, and Beyond",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yulia Gel",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 370000.0,
 "awd_amount": 370000.0,
 "awd_min_amd_letter_date": "2021-06-14",
 "awd_max_amd_letter_date": "2021-06-14",
 "awd_abstract_narration": "The expectation-maximization (EM) algorithm is among the most popular algorithms for statistical inference. Despite a wide range of successful applications in both statistics and machine learning, there is little finite-sample theoretical analysis explaining the effectiveness of EM and its variants. Recently, there have been some encouraging successes on the global convergence guarantee of the EM algorithm, but often under unrealistic and impractical assumptions.  The PI will integrate the recent success of overparametrization in deep learning with EM to overcome the aforementioned limitations. The research presented in this project will significantly advance the celebrated algorithms in statistics and machine learning including EM, mean-field variational inference, and Gibbs sampling by providing guarantees of global convergence and statistical optimalities.  The research will help address the non-convex optimization challenges for a range of important and classical statistical models and shed light on the recent successes of deep learning. The wide range of applications of EM, mean-field variational inference, and Gibbs sampling and the importance of clustering ensure that the progress we make towards our objectives will have a great impact on the broad scientific community which includes neuroscience and medicine. Research results from this project will be disseminated through research articles, workshops, and seminar series to researchers in other disciplines. The project will integrate research and education by teaching monograph courses and organizing workshops and seminars to support graduate students and postdocs, particularly women, underrepresented minorities, domestic students, and young researchers, to work on this topic.\r\n\r\nThe PI will develop methods for obtaining global convergence under possibly the weakest assumptions for a general class of latent variable models\u2019 estimation with an unknown number of clusters. The PI will address the following questions: 1) can we show that the overparameterized EM converges globally to the true parameters without any separation condition and any knowledge of the number of clusters and cluster sizes under a certain distance (such as Wasserstein)? 2) how fast does the algorithm converge? 3) what are the parameter estimation and clustering error rates and how do they compare to the optimal statistical accuracy? and 4) if not optimal statistically, can we achieve the optimality by adding a second stage EM initialized by the output of the overparameterized EM? There are three aims to develop a comprehensive theory to analyze the overparameterized EM and go beyond: 1) studying the global convergence of overparameterized EM for Gaussian Mixtures for both parameter estimation and latent cluster recovery and statistical optimality of the two-stage EM, 2) extending the two-stage EM to its variants including two-stage mean-field variational inference and Gibbs sampling and considering a unified analysis for a class of overparameterized algorithms, and 3) extending the analysis for Gaussian mixtures to general location mixture models and Stochastic Block Models and possibly a unified framework of latent variable models. In addition, the PI will work closely with the Yale Child Study Center and Yale Therapeutic Radiology Department to explore the appropriate EM algorithm and its variants for neuroscience, autism spectrum disorder, and cancer risk stratification.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Huibin",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Huibin Zhou",
   "pi_email_addr": "huibin.zhou@yale.edu",
   "nsf_id": "000148898",
   "pi_start_date": "2021-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "10 Hillhouse",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208290",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 370000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><dd>\r\n<div class=\"tinyMCEContent\">\r\n<p>We significantly advanced the understanding of Spectral Clustering.&nbsp;We consider two arbitrary matrices where one is a leave-one-column-out sub- matrix of the other one and establish a novel perturbation upper bound for the distance between two corresponding singular subspaces. It is well-suited for mixture models and results in a sharper and finer statistical analysis than classical perturbation bounds such as Wedin&rsquo;s Theorem. Powered by this leave- one-out perturbation theory, we provide a deterministic entrywise analysis for the performance of the spectral clustering under mixture models. Our analysis leads to an explicit exponential error rate for the clustering of sub-Gaussian mixture models.</p>\r\n<p>High-dimensional multi-reference alignment is motivated by the important Cryo-EM problem.&nbsp;We study the continuous multi-reference alignment model of estimating a periodic function on the circle from noisy and circularly-rotated observations. Motivated by analogous high-dimensional problems that arise in cryo-electron microscopy, we establish minimax rates for estimating generic signals that are explicit in the dimension.</p>\r\n<p>High-dimensional Gaussian mixture is a classical statistical model.&nbsp;We study the optimal rate of estimation in a finite Gaussian location mixture model in high dimensions without separation conditions. Both the theoretical and methodological development rely on a careful application of the method of moments. Central to our results is the observation that the information geometry of finite Gaussian mixtures is characterized by the moment tensors of the mixing distribution, whose low-rank structure can be exploited to obtain a sharp local entropy bound.</p>\r\n<p>The recent, impressive advances in algorithmic generation of high-fidelity image, audio, and video are largely due to great successes in score-based diffusion models. A key implementing step is score matching, that is, the estimation of the score function of the forward diffusion process from training data. As shown in earlier literature, the total variation distance between the law of a sample generated from the trained diffusion model and the ground truth distribution can be controlled by the score matching risk. Despite the widespread use of score-based diffusion models, basic theoretical questions concerning exact optimal statistical rates for score estimation and its application to density estimation remain open. We establish the sharp minimax rate of score estimation for smooth, compactly supported densities.&nbsp;</p>\r\n<p>&nbsp;</p>\r\n</div>\r\n</dd><br>\n<p>\n Last Modified: 01/12/2025<br>\nModified by: Huibin&nbsp;Zhou</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\r\n\r\n\n\nWe significantly advanced the understanding of Spectral Clustering.We consider two arbitrary matrices where one is a leave-one-column-out sub- matrix of the other one and establish a novel perturbation upper bound for the distance between two corresponding singular subspaces. It is well-suited for mixture models and results in a sharper and finer statistical analysis than classical perturbation bounds such as Wedins Theorem. Powered by this leave- one-out perturbation theory, we provide a deterministic entrywise analysis for the performance of the spectral clustering under mixture models. Our analysis leads to an explicit exponential error rate for the clustering of sub-Gaussian mixture models.\r\n\n\nHigh-dimensional multi-reference alignment is motivated by the important Cryo-EM problem.We study the continuous multi-reference alignment model of estimating a periodic function on the circle from noisy and circularly-rotated observations. Motivated by analogous high-dimensional problems that arise in cryo-electron microscopy, we establish minimax rates for estimating generic signals that are explicit in the dimension.\r\n\n\nHigh-dimensional Gaussian mixture is a classical statistical model.We study the optimal rate of estimation in a finite Gaussian location mixture model in high dimensions without separation conditions. Both the theoretical and methodological development rely on a careful application of the method of moments. Central to our results is the observation that the information geometry of finite Gaussian mixtures is characterized by the moment tensors of the mixing distribution, whose low-rank structure can be exploited to obtain a sharp local entropy bound.\r\n\n\nThe recent, impressive advances in algorithmic generation of high-fidelity image, audio, and video are largely due to great successes in score-based diffusion models. A key implementing step is score matching, that is, the estimation of the score function of the forward diffusion process from training data. As shown in earlier literature, the total variation distance between the law of a sample generated from the trained diffusion model and the ground truth distribution can be controlled by the score matching risk. Despite the widespread use of score-based diffusion models, basic theoretical questions concerning exact optimal statistical rates for score estimation and its application to density estimation remain open. We establish the sharp minimax rate of score estimation for smooth, compactly supported densities.\r\n\n\n\r\n\r\n\t\t\t\t\tLast Modified: 01/12/2025\n\n\t\t\t\t\tSubmitted by: HuibinZhou\n"
 }
}