{
 "awd_id": "2123605",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: NSC-FO: Active vision during natural behavior: More than meets the eye?",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 162118.0,
 "awd_amount": 162118.0,
 "awd_min_amd_letter_date": "2021-08-17",
 "awd_max_amd_letter_date": "2021-08-17",
 "awd_abstract_narration": "Vision is a process by which the image falling on the eyes is processed by specialized neurons within visual brain areas. Neurons in the early stages of visual processing convey information about small bits of the visual scene, like pixel-detectors in a camera. For example, a neuron in visual cortex might respond best to a small white bar at a particular location in visual space. Should this example neuron respond differently when the white bar is part of an object that we have seen before, or one that we are moving towards? Psychology might suggest so, but for almost 60 years, most scientists studying the neural basis of visual perception have implicitly assumed that responses of neurons in visual cortex depend only on the visual image falling on the eyes. It is increasingly clear that neurons in the visual cortex do indeed care about behavioral context \u2013 as well as the state of the brain itself. These external, internal, and contextual factors influence how neurons process the visual scene. Exactly how much these \u201cnon-visual\u201d factors influence visual cortical neurons remains a significant open question that this project aims to address.\r\n\r\nThe experiments will record from neurons in the visual cortex of ferrets as they freely explore a naturalistic environment. Using position and eye-tracking cameras, the project will both recreate a movie of what the ferret saw within the environment, and track other observable variables related to behavior. The movie will then be replayed to the ferret while it is anesthetized, thus directly measuring any differences in neuronal responses to the same visual stimulation in these two very different contexts. Analysis will compare the physiological quality and statistical properties of neuronal responses across naturalistic and anesthetized conditions to quantify the contribution of natural context to neuronal responses. Results will relate the differences in the freely moving context to specific sources, like motor actions such as eye and head movements, familiarity with specific visual features, and their behavioral relevance. Experiments will inform models for how these sources influence neuronal activity, setting the stage for understanding the function of non-retinal inputs for sensory perception. The project will provide a foundation for long-term studies of natural vision.\r\nThis project is funded by Integrative Strategies for Understanding Neural and Cognitive Systems (NCS), a multidisciplinary program jointly supported by the Directorates for Biology (BIO), Computer and Information Science and Engineering (CISE), Education and Human Resources (EHR), Engineering (ENG), and Social, Behavioral, and Economic Sciences (SBE).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Farran",
   "pi_last_name": "Briggs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Farran Briggs",
   "pi_email_addr": "farran_briggs@urmc.rochester.edu",
   "nsf_id": "000819048",
   "pi_start_date": "2021-08-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ralf",
   "pi_last_name": "Haefner",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Ralf M Haefner",
   "pi_email_addr": "rhaefne2@ur.rochester.edu",
   "nsf_id": "000691462",
   "pi_start_date": "2021-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "601 Elmwood Ave, Box 603",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146420001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "862400",
   "pgm_ele_name": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8551",
   "pgm_ref_txt": "IntgStrat Undst Neurl&Cogn Sys"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 162118.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Our technical proof-of-concept proposal establishes a platform for studying active sensory perception in naturalistic conditions within the laboratory, extending the boundaries of what is currently possible in sensory systems neuroscience. Our experimental paradigm leverages simulated visual reality with mobile neurophysiological recording technology. Our ambitious new research agenda is to describe holistic (sensory and non-sensory) cortical responses during active perception, challenging traditional paradigms of sensory neuronal responses, upon which broader future goals will build. Our multidisciplinary research bridged neurophysiological and computational methodologies and advances knowledge and educational opportunities through data sharing.</span></p>\n<p>The first specific objective of the project (Aim 1) was to develop a strategy to synchronously collect 4 data streams (V1 neurons, eye position, head position, and video of animals&rsquo; view). This has been accomplished and data collection is currently underway. A second objective under Aim 1 was to establish an analysis pipeline for generate movie data that integrate eye and head position as well as ferrets&rsquo; views of the environment. This has also been accomplished and will be refined as more data is collected and following model simulation results.</p>\n<p><strong>&nbsp;</strong></p>\n<p>The second specific objective (Aim 2) was to model V1 neuronal activity using AFMs. To date, code has been developed and refined using existing data from other collaborations, and we will begin to apply to our newly collected, integrated data within the next two months.<strong>&nbsp;</strong></p>\n<p>Key outcomes of this project were successful implantation of multielectrode arrays, recording of neuronal activity in freely moving animals, successful head and eye tracking in freely moving animals, and combination of all of these components to collect complete datasets with all component data included so that these data can be modeled. The main technological and data acquisition outcomes have been achieved and data analyses and modeling will continue to be refined over the next 1 year.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/10/2023<br>\n\t\t\t\t\tModified by: Farran&nbsp;Briggs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur technical proof-of-concept proposal establishes a platform for studying active sensory perception in naturalistic conditions within the laboratory, extending the boundaries of what is currently possible in sensory systems neuroscience. Our experimental paradigm leverages simulated visual reality with mobile neurophysiological recording technology. Our ambitious new research agenda is to describe holistic (sensory and non-sensory) cortical responses during active perception, challenging traditional paradigms of sensory neuronal responses, upon which broader future goals will build. Our multidisciplinary research bridged neurophysiological and computational methodologies and advances knowledge and educational opportunities through data sharing.\n\nThe first specific objective of the project (Aim 1) was to develop a strategy to synchronously collect 4 data streams (V1 neurons, eye position, head position, and video of animals\u2019 view). This has been accomplished and data collection is currently underway. A second objective under Aim 1 was to establish an analysis pipeline for generate movie data that integrate eye and head position as well as ferrets\u2019 views of the environment. This has also been accomplished and will be refined as more data is collected and following model simulation results.\n\n \n\nThe second specific objective (Aim 2) was to model V1 neuronal activity using AFMs. To date, code has been developed and refined using existing data from other collaborations, and we will begin to apply to our newly collected, integrated data within the next two months. \n\nKey outcomes of this project were successful implantation of multielectrode arrays, recording of neuronal activity in freely moving animals, successful head and eye tracking in freely moving animals, and combination of all of these components to collect complete datasets with all component data included so that these data can be modeled. The main technological and data acquisition outcomes have been achieved and data analyses and modeling will continue to be refined over the next 1 year.\n\n \n\n\t\t\t\t\tLast Modified: 10/10/2023\n\n\t\t\t\t\tSubmitted by: Farran Briggs"
 }
}