{
 "awd_id": "2104880",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CNS: NeTS: Adaptive Cache Dimensioning in Cloud CDNs: Foundations and Practice",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Darleen Fisher",
 "awd_eff_date": "2021-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2021-05-04",
 "awd_max_amd_letter_date": "2021-05-04",
 "awd_abstract_narration": "The current Internet infrastructure provides a wide range of services such as music and movies delivery, messaging, video-conferencing and software download. The data transmitted across the Internet corresponding to these services is dubbed as \u201ccontent\u201d. As an increasing number of users desire such services over the Internet, firms called content providers are engaged in developing systems that ensure that the demanded services are available at high quality of experience, i.e., with minimal delay.  This is achieved by the process of \u201cservice placement\u201d, which places replicas of popular services near end users at small servers called caches, coupled with additional copies at larger servers deeper in the Internet.  If the content requested by a user is available at a cache, it is promptly delivered.  Otherwise, the request must be forwarded to servers that are further, hence increasing delay. However, provisioning these small and large servers is expensive. Under the cloud computing paradigm, cloud providers make server resources available for rent and allow dynamic sizing of caches, referred to as cache dimensioning.  This implies that costs for the content provider may be significantly reduced.  This project develops methodologies on cache dimensioning for handling different types of services.   A significant challenge lies in the fact that popularity of services changes with time, and hence learning, dimensioning and service placement must happen continually.  The solution approach is via machine learning, and the project contributes to the fundamentals of learning from a sequence of samples over time, entitled online learning.  The project also includes the development of educational materials on networking, distributed systems and machine learning.\r\n \r\n\r\nThis project considers the cache dimensioning problem in cloud content distribution networks (CDNs), where the objective is to decide how much storage to place at each location in the network.  This project addresses key issues essential to developing theoretical foundations, practical online algorithms and low-complexity implementation for providing adaptive cache dimensioning differentiated services in cloud CDNs. This requires the conjunction of several mathematical tools to analyze online algorithms, leading to systems development to make the algorithms a reality. This project develops a social welfare maximization-based framework for providing adaptive cache dimensioning differentiated service in cloud CDNs. The project is organized into three interdependent thrusts. The first thrust focuses on a Time-to-Live (TTL) approximation analogy-based analysis to decouple the behaviors of different contents by means of dynamically adapting the timer values to maximize the social welfare. The second thrust focuses on online optimization-based analysis by leveraging online learning to design new online reactive algorithms that are aware of non-stationary popularity and traffic variations.  The third thrust focuses on implementation and evaluation on public cloud infrastructure.  An immediate impact of this project is to help design next-generation cloud CDNs leading to greater enterprise productivity and user satisfaction.  The impact is enhanced by specific minority inclusion activities, an education plan focusing on caching and machine learning, as well as outreach in the form of summer camps for high school students. At the same time the project develops fundamental theories that pertain to the area of machine learning, specifically to online learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jian",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jian Li",
   "pi_email_addr": "jian.li.3@stonybrook.edu",
   "nsf_id": "000814632",
   "pi_start_date": "2021-05-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Binghamton",
  "inst_street_address": "4400 VESTAL PKWY E",
  "inst_street_address_2": "",
  "inst_city_name": "BINGHAMTON",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6077776136",
  "inst_zip_code": "13902",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "L9ZDVULCHCV3",
  "org_uei_num": "NQMVAAQUFU53"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Binghamton",
  "perf_str_addr": "4400 Vestal Parkway East",
  "perf_city_name": "Binghamton",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "139026000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">The proliferation of video-related applications has made content distribution networks (CDNs) a critical component of Internet infrastructure.&nbsp;However, the entry cost of traditional CDNs is usually high.&nbsp;With the advent of cloud computing, the content service provided by CDNs has been offered as managed platforms with a novel pay-as-you-go model. This project considers the cache dimensioning problem in such cloud CDNs, which is to decide how much storage to place at each location in the network. Concurrent with the explosion of cache dimensioning service in cloud CDNs, there has been a dramatic increase in the diversity in service expectations and this advocates the need for content delivery infrastructures with service differentiation among different applications and content classes. This project addresses key issues essential to harvesting the potential of providing cache dimensioning differentiated services by developing theoretical foundations, practical online algorithms and low-complexity implementation. The project outcomes are summarized as follows:&nbsp;</p>\n<p class=\"p1\">- We studied the dynamic cache dimensioning problem, where the objective is to decide how much storage to place in the cache to minimize the total costs with respect to the storage and content delivery latency. Since there is a natural timescale separation between cache dimensioning and content caching, where the former is a much slower operation than the latter, we formulated the problem as a two-timescale Markov decision process (MDP). This MDP turns out to be a restless bandit problem, which is computationally intractable. First,&nbsp;we relaxed the MDP and studied the corresponding fluid dynamics, from which we designed the Whittle index policy for the original MDP. We explicitly derived Whittle indices of each content and proved that our fluid Whittle index policy is asymptotically optimal. Due to the time-varying nature of cloud CDNs, system parameters such as content request and delivery rates are typically unknown. We further proposed a reinforcement learning (RL) solution. Different from classical RL algorithms that often suffer from curse of dimensionality, we proposed fW-UCB that not only leverages the approach of optimism-in-the-face-of-uncertainty to balance exploration and exploitation, but more importantly, it learns to leverage the near-optimal index policy for making decisions. We showed that fW-UCB achieves a sub-linear regret with a low-complexity. Finally, we provided a Redis-based implementation via Amazon ElastiCache Service to cache dimensioning to evaluate our policies using real traces. This work appeared in IEEE/ACM Transactions on Networking 2023 and presented at IEEE INFOCOM 2022. Our code is available on GitHub:&nbsp;<a href=\"https://git.io/JyShk\">https://git.io/JyShk</a></p>\n<p class=\"p1\">- We further explored the potential of providing cache dimensioning differentiated services, especially for latency-sensitive applications. Existing caching literature often assumes that&nbsp;user-perceived latency upon a cache hit is negligible. Some recent efforts start linking this to the potential performance degradation when minimizing end-user latency in the presence of delayed hits, which occur in high-throughput systems when multiple requests to the same content occur before the content is fetched from the remote server. Despite some recent efforts, there remains a major gap between this observation and the goal of efficient online latency-aware caching algorithm design. To close this gap, we developed LA-Cache, a timer-based mechanism to account for delayed hits for contents with variable sizes and fetching latencies. This provides a theoretical basis for the understanding and design of latency-aware caching. Specifically, each content is associated with a timer indicating the fetching latency between the cache and remote server. Upon a cache miss, all requests (i.e., delayed hits) arriving at the cache during a certain time period dictated by its timer suffer a corresponding latency before these requests are truly served. This approach explicitly characterizes the expected average latency of a caching system in the presence of delayed hits. This further enables us to derive a simple ranking function to quickly prioritize contents to minimizing latency.&nbsp;We implemented a LA-Cache prototype within Apache Traffic Server. The latency achieved by our implementations agrees closely with theoretical predictions of our model. Our experimental results using production traces show that LA-Cache reduces latencies by 5%-15% compared to state-of-the-art methods depending on backend RTTs. This work appeared in USENIX ATC 2022. Our code is available on GitHub:&nbsp;<a href=\"https://github.com/GYan58/la-cache\">https://github.com/GYan58/la-cache</a></p>\n<p class=\"p2\">- In summary, this project proposed new online cache dimensioning algorithms and our index-based cache dimensioning policy is of independent interests. The project also contributed to time-to-live (TTL) caching, which we leveraged to design latency-aware caching policy. Our algorithms are lightweight and prototyped with testbed implementation using production systems, hence, can be leveraged by CDN providers to improve system efficiency and quality of experience of customers.&nbsp;</p>\n<p class=\"p2\">- The PI integrated related topics into undergraduate education. The PI initiated the DeepRacer Edge Cloud via capstone design project to bring machine learning into reality. Specifically, senior students leveraged cloud computing resources from the NSF CloudBank to demonstrate DeepRacers with RL.&nbsp;The PI also actively participated in STEM education, e.g., the STEM WEEK and summer camp at Binghamton University.</p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/12/2023<br>\n\t\t\t\t\tModified by: Jian&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The proliferation of video-related applications has made content distribution networks (CDNs) a critical component of Internet infrastructure. However, the entry cost of traditional CDNs is usually high. With the advent of cloud computing, the content service provided by CDNs has been offered as managed platforms with a novel pay-as-you-go model. This project considers the cache dimensioning problem in such cloud CDNs, which is to decide how much storage to place at each location in the network. Concurrent with the explosion of cache dimensioning service in cloud CDNs, there has been a dramatic increase in the diversity in service expectations and this advocates the need for content delivery infrastructures with service differentiation among different applications and content classes. This project addresses key issues essential to harvesting the potential of providing cache dimensioning differentiated services by developing theoretical foundations, practical online algorithms and low-complexity implementation. The project outcomes are summarized as follows: \n- We studied the dynamic cache dimensioning problem, where the objective is to decide how much storage to place in the cache to minimize the total costs with respect to the storage and content delivery latency. Since there is a natural timescale separation between cache dimensioning and content caching, where the former is a much slower operation than the latter, we formulated the problem as a two-timescale Markov decision process (MDP). This MDP turns out to be a restless bandit problem, which is computationally intractable. First, we relaxed the MDP and studied the corresponding fluid dynamics, from which we designed the Whittle index policy for the original MDP. We explicitly derived Whittle indices of each content and proved that our fluid Whittle index policy is asymptotically optimal. Due to the time-varying nature of cloud CDNs, system parameters such as content request and delivery rates are typically unknown. We further proposed a reinforcement learning (RL) solution. Different from classical RL algorithms that often suffer from curse of dimensionality, we proposed fW-UCB that not only leverages the approach of optimism-in-the-face-of-uncertainty to balance exploration and exploitation, but more importantly, it learns to leverage the near-optimal index policy for making decisions. We showed that fW-UCB achieves a sub-linear regret with a low-complexity. Finally, we provided a Redis-based implementation via Amazon ElastiCache Service to cache dimensioning to evaluate our policies using real traces. This work appeared in IEEE/ACM Transactions on Networking 2023 and presented at IEEE INFOCOM 2022. Our code is available on GitHub: https://git.io/JyShk\n- We further explored the potential of providing cache dimensioning differentiated services, especially for latency-sensitive applications. Existing caching literature often assumes that user-perceived latency upon a cache hit is negligible. Some recent efforts start linking this to the potential performance degradation when minimizing end-user latency in the presence of delayed hits, which occur in high-throughput systems when multiple requests to the same content occur before the content is fetched from the remote server. Despite some recent efforts, there remains a major gap between this observation and the goal of efficient online latency-aware caching algorithm design. To close this gap, we developed LA-Cache, a timer-based mechanism to account for delayed hits for contents with variable sizes and fetching latencies. This provides a theoretical basis for the understanding and design of latency-aware caching. Specifically, each content is associated with a timer indicating the fetching latency between the cache and remote server. Upon a cache miss, all requests (i.e., delayed hits) arriving at the cache during a certain time period dictated by its timer suffer a corresponding latency before these requests are truly served. This approach explicitly characterizes the expected average latency of a caching system in the presence of delayed hits. This further enables us to derive a simple ranking function to quickly prioritize contents to minimizing latency. We implemented a LA-Cache prototype within Apache Traffic Server. The latency achieved by our implementations agrees closely with theoretical predictions of our model. Our experimental results using production traces show that LA-Cache reduces latencies by 5%-15% compared to state-of-the-art methods depending on backend RTTs. This work appeared in USENIX ATC 2022. Our code is available on GitHub: https://github.com/GYan58/la-cache\n- In summary, this project proposed new online cache dimensioning algorithms and our index-based cache dimensioning policy is of independent interests. The project also contributed to time-to-live (TTL) caching, which we leveraged to design latency-aware caching policy. Our algorithms are lightweight and prototyped with testbed implementation using production systems, hence, can be leveraged by CDN providers to improve system efficiency and quality of experience of customers. \n- The PI integrated related topics into undergraduate education. The PI initiated the DeepRacer Edge Cloud via capstone design project to bring machine learning into reality. Specifically, senior students leveraged cloud computing resources from the NSF CloudBank to demonstrate DeepRacers with RL. The PI also actively participated in STEM education, e.g., the STEM WEEK and summer camp at Binghamton University.\n \n \n\n\t\t\t\t\tLast Modified: 08/12/2023\n\n\t\t\t\t\tSubmitted by: Jian Li"
 }
}