{
 "awd_id": "2138773",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Preserve/Destroy Decisions for Simulation Data in Computational Physics and Beyond",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 85456.0,
 "awd_min_amd_letter_date": "2021-08-26",
 "awd_max_amd_letter_date": "2021-08-26",
 "awd_abstract_narration": "The scientific research community has been increasingly developing ways to share and re-use research data, thereby allowing more discoveries to be made from previous research investments. Much of the focus on sharing and reusability has been on experimental and observational data. This project addresses the equally vexing challenge of how to make best use and re-use of the massive data produced in computational simulations. Important research questions guiding this project include: the degree to which simulation results can be replicated; the advantages of storing the simulation data itself for others to reuse as compared to providing the computational software so that others can re-run the simulations; and understanding which software testing practices can facilitate the replication/reuse of simulation data and the simulation software that produces those data. The principal investigators will address these questions by performing extensive replication and software code testing on a set of computational physics simulation datasets and software code that they had gathered through a previous study. The project will produce publicly available, fully reproducible computational physics works as examples for publishing results in a way that the data and code are effectively reusable.\r\n\r\nThe principal investigators aim to improve understanding of, and increase, the reusability of the code and data associated with simulation-based research.  This project specifically aims to better inform data destroy/preservation decisions in the simulation context, toward improving the reusability and interoperability of simulation data and code. The project will also consider important questions such as how software engineering testing practices relate to computational physics practices, and how changes in computational environments affect code execution and the regeneration of simulation data. Ultimately, the results of this work are intended to guide the research community on how to best produce and disseminate research code. It is anticipated that the results for computational physics can be extended to develop general guidelines for simulation data and code sharing for other communities, the appropriate code testing to do so, and best practices for development of associated cyberinfrastructure and tools. \r\n\r\nThis project is supported by the National Science Foundation's Public Access Initiative which is managed by the NSF Office of Advanced Cyberinfrastructure on behalf of the Foundation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Victoria",
   "pi_last_name": "Stodden",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Victoria Stodden",
   "pi_email_addr": "stodden@usc.edu",
   "nsf_id": "000598131",
   "pi_start_date": "2021-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900890001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "CA37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "741400",
   "pgm_ele_name": "NSF Public Access Initiative"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 85456.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2661c798-7fff-f06c-9e5a-e125f5878d6a\">\n<p dir=\"ltr\"><span>There is an important tradeoff between the regeneration of simulation data from code and the long term archiving of the simulation data itself. Addressing the preserve/destroy tradeoff is important because simulation data can, in theory, be regenerated from the same code, potentially rendering storage of the simulation data unnecessary. We have explored this tradeoff by addressing code reliability over time to better understand how researchers should release their input datasets and code. Understanding this tradeoff allows the research community to improve reusability of simulation data and the associated code, and informs guidelines for code release. Throughout the course of this project, 10 major peer-reviewed journal articles were published, 6 research talks presented, 2 posters presented at conferences, and 6 software/reproduction packages published. We engaged 3 graduate students and provided research exposure and training for them.</span></p>\n<p dir=\"ltr\"><span>We advanced the intersection of two research areas: software testing and scientific research code development, in several ways. We proposed iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that software test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. This work was published in &ldquo;iFixFlakies: A Framework for Automatically Fixing Order-Dependent Flaky Tests&rdquo; by August Shi, Wing Lam, Reed Oei, Tao Xie, and Darko Marinov (ESEC/FSE 2019). We also carried out an empirical evaluation of 107 representative software tests in open source packages to understand why software tests may fail intermittently, and we found the novel result that in many cases the order of software tests is crucial to their consistent passing. We published these results in &ldquo;Understanding Reproducibility and Characteristics of Flaky Tests Through Test Reruns in Java Projects&rdquo; by W. Lam, S. Winter, A. Astorga, V. Stodden and D. Marinov in ISSRE 2020. We also distinguished the impact of specification vs implementation of software testing on whether the test passes or fails in an empirical effort that considered 200 open-source Java projects and detected 275 tests that unexpectedly fail due to wrong assumptions. A flaky test behavior that appears in scientific code would affect reproducibility of results by producing output that is different than expected. It also affects potential interoperability and executability of code in the future: when coders write tests that depend on specific implementations rather than a specification, the test may fail in the future as the software continues to develop or be deployed in different settings. We presented the tool DexFix to detect test failures due to wrong assumptions in the publication &ldquo;Domain-Specific Fixes for Flaky Tests with Wrong Assumptions on Underdetermined Specifications&rdquo; by P. Zhang, Y. Jiang, A. Wei, V. Stodden, D. Marinov and A. Shi at ICSE 2021.</span></p>\n<p dir=\"ltr\"><span>We developed and advanced the understanding of two novel approaches to code reliability and data and results sharing in the scientific research context: the well-defined notions of the &ldquo;Replication Package&rdquo; and the &ldquo;Scientific Testing.&rdquo; Our publications &ldquo;Three Empirical Principles for Computational Reproducibility and their Implementation: The Reproduction Package&rdquo;</span></p>\n<p dir=\"ltr\"><span>by M. S. Krafczyk, A. Shi, A. Bhaskar, D. Marinov, and V. Stodden in the Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences 2020, and &ldquo;Scientific Tests and Continuous Integration Strategies to Enhance Reproducibility in the Scientific Software Context&rdquo; by M. S. Krafczyk, A. Shi, A. Bhaskar, D. Marinov, and V. Stodden, in the Second International Workshop on Practical Reproducible Evaluation of Computer Systems (P-RECS'19) in June 2019 detail these conceptualizations. Based on these concepts, we then made several advances in the identification and characterization of guidance for research software and data publication that accompanies the publication of scientific findings. We implemented these guidances in the open source software tool ReproScreener that automatically checks the guidances in research projects.</span></p>\n<p dir=\"ltr\"><span>The project's aim is to better inform data destroy/preservation decisions in the simulation context, in particular with respect to ensuring reusability and interoperability of data and code. Through data replication and code testing, this project assesses characteristics of research code that improve its potential reusability over time and provides an open source software tool for the automatic detection of our proposed guidances Reproscreener (reproscreener.org). This work guides the research community regarding the production and dissemination of scientific research code.</span></p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/03/2023<br>\n\t\t\t\t\tModified by: Victoria&nbsp;Stodden</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThere is an important tradeoff between the regeneration of simulation data from code and the long term archiving of the simulation data itself. Addressing the preserve/destroy tradeoff is important because simulation data can, in theory, be regenerated from the same code, potentially rendering storage of the simulation data unnecessary. We have explored this tradeoff by addressing code reliability over time to better understand how researchers should release their input datasets and code. Understanding this tradeoff allows the research community to improve reusability of simulation data and the associated code, and informs guidelines for code release. Throughout the course of this project, 10 major peer-reviewed journal articles were published, 6 research talks presented, 2 posters presented at conferences, and 6 software/reproduction packages published. We engaged 3 graduate students and provided research exposure and training for them.\nWe advanced the intersection of two research areas: software testing and scientific research code development, in several ways. We proposed iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that software test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. This work was published in \"iFixFlakies: A Framework for Automatically Fixing Order-Dependent Flaky Tests\" by August Shi, Wing Lam, Reed Oei, Tao Xie, and Darko Marinov (ESEC/FSE 2019). We also carried out an empirical evaluation of 107 representative software tests in open source packages to understand why software tests may fail intermittently, and we found the novel result that in many cases the order of software tests is crucial to their consistent passing. We published these results in \"Understanding Reproducibility and Characteristics of Flaky Tests Through Test Reruns in Java Projects\" by W. Lam, S. Winter, A. Astorga, V. Stodden and D. Marinov in ISSRE 2020. We also distinguished the impact of specification vs implementation of software testing on whether the test passes or fails in an empirical effort that considered 200 open-source Java projects and detected 275 tests that unexpectedly fail due to wrong assumptions. A flaky test behavior that appears in scientific code would affect reproducibility of results by producing output that is different than expected. It also affects potential interoperability and executability of code in the future: when coders write tests that depend on specific implementations rather than a specification, the test may fail in the future as the software continues to develop or be deployed in different settings. We presented the tool DexFix to detect test failures due to wrong assumptions in the publication \"Domain-Specific Fixes for Flaky Tests with Wrong Assumptions on Underdetermined Specifications\" by P. Zhang, Y. Jiang, A. Wei, V. Stodden, D. Marinov and A. Shi at ICSE 2021.\nWe developed and advanced the understanding of two novel approaches to code reliability and data and results sharing in the scientific research context: the well-defined notions of the \"Replication Package\" and the \"Scientific Testing.\" Our publications \"Three Empirical Principles for Computational Reproducibility and their Implementation: The Reproduction Package\"\nby M. S. Krafczyk, A. Shi, A. Bhaskar, D. Marinov, and V. Stodden in the Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences 2020, and \"Scientific Tests and Continuous Integration Strategies to Enhance Reproducibility in the Scientific Software Context\" by M. S. Krafczyk, A. Shi, A. Bhaskar, D. Marinov, and V. Stodden, in the Second International Workshop on Practical Reproducible Evaluation of Computer Systems (P-RECS'19) in June 2019 detail these conceptualizations. Based on these concepts, we then made several advances in the identification and characterization of guidance for research software and data publication that accompanies the publication of scientific findings. We implemented these guidances in the open source software tool ReproScreener that automatically checks the guidances in research projects.\nThe project's aim is to better inform data destroy/preservation decisions in the simulation context, in particular with respect to ensuring reusability and interoperability of data and code. Through data replication and code testing, this project assesses characteristics of research code that improve its potential reusability over time and provides an open source software tool for the automatic detection of our proposed guidances Reproscreener (reproscreener.org). This work guides the research community regarding the production and dissemination of scientific research code.\n\n\n\t\t\t\t\tLast Modified: 08/03/2023\n\n\t\t\t\t\tSubmitted by: Victoria Stodden"
 }
}