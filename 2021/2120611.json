{
 "awd_id": "2120611",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SaTC: CORE: Small: Foundations for the Next Generation of Private Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 99995.0,
 "awd_amount": 115995.0,
 "awd_min_amd_letter_date": "2021-07-16",
 "awd_max_amd_letter_date": "2022-05-19",
 "awd_abstract_narration": "Recent advances in large-scale machine learning (ML) promise a range of benefits to society, but also introduce new risks.  One major risk is a loss of privacy for the individuals whose data powers the machine learning algorithms. There are now convincing demonstrations that algorithms for machine learning can reveal sensitive information about individuals in their training data by memorizing specific strings of sensitive text such as bank account numbers or through membership-inference attacks. In the recent years, a framework called differential privacy---a mathematically principled, quantitative notion of what it means for an algorithm to ensure privacy for the individuals who contribute training data---has led to significant progress towards privacy in machine learning. This progress offers a proof-of-concept that we can hope to enjoy some of the benefits of using machine learning on sensitive data, while measuring and limiting breaches of confidentiality.  This project will investigate and begin to make some of the fundamental advances that are necessary to make differentially private ML a viable technology.  The focus will be on laying the groundwork for differentially private ML for entire systems, rather than for standalone tasks, which have been the focus of prior work.  This project team comprising researchers with a broad range of expertise in ML, algorithms, systems, and cybersecurity, has planned a set of education tasks: public-facing set of course materials on differentially private machine learning and statistics and and an undergraduate-level textbook on differential privacy.\r\n\r\nThis project includes three technical thrusts that will lay the groundwork for future efforts to build private ML systems.  The first thrust will be to improve the foundational algorithms that enable differentially private ML on high-dimensional data.  The second thrust will be to build a bridge between algorithms for standalone ML tasks and algorithms for systems-level workloads of ML tasks, by developing differentially private algorithms for training many personalized models, which is a paradigmatic workload in ML.  The final thrust will consist of empirical work on auditing differentially private ML methods to understand how the real-world privacy costs compare to those predicted by the theory of differential privacy when these algorithms are used as part of realistic workloads, such as models that are continually updated with new data. This privacy auditing will also facilitate detecting unwanted memorization of training data in machine learning, and also provide more quantitative approaches to auditing differentially private algorithms based on membership-inference and data poisoning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhiwei Steven",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhiwei Steven Wu",
   "pi_email_addr": "zstevenwu@cmu.edu",
   "nsf_id": "000792262",
   "pi_start_date": "2021-07-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carneige Mellon University",
  "perf_str_addr": "5000 Forbes Avenue WQED Building",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 99995.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we have developed tools and algorithms that advanced the state of the art of privacy-preserving machine learning. In particular, our results have advanced the frontier for machine learning with formal differential privacy guarantees.&nbsp;</p>\n<p><br />The first set of outcomes in this project is a new set of algorithms for differentially private machine learning. The PI's research has conducted research on how to generate synthetic data that satisfies differential privacy guarantees while enabling downstream machine learning tasks. The project also provided new differentially private algorithms for multi-task learning and model personalization, which have been relatively underexplored in the differential privacy literature. The project has contributed to the line of work on differentially private reinforcement learning. Finally, we have also tackled fundamental statistical estimation under the stringent privacy criterion of local differential privacy, where the private data are not even collected by the data scientist.</p>\n<p><br />The second set of results in this project is new mechanisms for managing the privacy-accuracy trade-offs across multiple data analysis tasks. We provide a new set of privacy accounting mechanisms that enable data scientists to track the privacy loss while choosing the privacy risk parameters fully adaptively. We also developed new \"accuracy-first\" algorithm that aims to achieve a target accuracy level, while minimizing the resulting privacy loss from the computation.</p>\n<p><br />Finally, the project developed new algorithms for reconstruction attacks, which provide an effective way to audit systems that release statistical information.</p>\n<p><br />The project has also led to new educational material in a CMU course named \"Foundations of privacy.\"<br /><br /></p><br>\n<p>\n Last Modified: 12/21/2023<br>\nModified by: Zhiwei Steven&nbsp;Wu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we have developed tools and algorithms that advanced the state of the art of privacy-preserving machine learning. In particular, our results have advanced the frontier for machine learning with formal differential privacy guarantees.\n\n\n\nThe first set of outcomes in this project is a new set of algorithms for differentially private machine learning. The PI's research has conducted research on how to generate synthetic data that satisfies differential privacy guarantees while enabling downstream machine learning tasks. The project also provided new differentially private algorithms for multi-task learning and model personalization, which have been relatively underexplored in the differential privacy literature. The project has contributed to the line of work on differentially private reinforcement learning. Finally, we have also tackled fundamental statistical estimation under the stringent privacy criterion of local differential privacy, where the private data are not even collected by the data scientist.\n\n\n\nThe second set of results in this project is new mechanisms for managing the privacy-accuracy trade-offs across multiple data analysis tasks. We provide a new set of privacy accounting mechanisms that enable data scientists to track the privacy loss while choosing the privacy risk parameters fully adaptively. We also developed new \"accuracy-first\" algorithm that aims to achieve a target accuracy level, while minimizing the resulting privacy loss from the computation.\n\n\n\nFinally, the project developed new algorithms for reconstruction attacks, which provide an effective way to audit systems that release statistical information.\n\n\n\nThe project has also led to new educational material in a CMU course named \"Foundations of privacy.\"\n\n\t\t\t\t\tLast Modified: 12/21/2023\n\n\t\t\t\t\tSubmitted by: Zhiwei StevenWu\n"
 }
}