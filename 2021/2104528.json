{
 "awd_id": "2104528",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: AF: Optimization and sampling algorithms with provable generalization and runtime guarantees, with applications to deep learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2021-06-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 174187.0,
 "awd_amount": 174187.0,
 "awd_min_amd_letter_date": "2021-05-11",
 "awd_max_amd_letter_date": "2021-05-11",
 "awd_abstract_narration": "When training deep-learning and other machine-learning models, one would like to train in such a way that the generalization error of the trained model, that is, the error of the trained model when it is used to make predictions on a \u201ctest\u201d dataset which was not used to train the model, is as low as possible. Training algorithms with good generalization properties can lead to machine-learning models that are more robust to changes in the dataset, allow for robust predictions, and help mitigate algorithmic bias when the training dataset may not be fully representative of the diversity of the population dataset. Such algorithms can also lead to more stable training in settings such as distributed training and online learning. In practice, the choice of optimization algorithm that one uses to train the model can greatly affect both its training error and generalization error.  Unfortunately, there is a lack of optimization algorithms with provable guarantees on the generalization error.  This makes it difficult to design algorithms which provably achieve both a fast running time and low generalization error.  The aim of this project is to design novel algorithms for training deep-learning and other machine-learning models, and to prove guarantees on the running time, generalization error and related robustness properties of these algorithms.  To design and analyze such algorithms, this project brings together ideas from different areas of mathematics and computer science.\r\n\r\nThis project is designing novel optimization and Markov-chain sampling algorithms, for training deep-learning models as well as other machine-learning models.  It aims to prove guarantees on the generalization error and related robustness properties of these algorithms, and also to provide fast running-time guarantees.  Guaranteeing a low generalization error is especially challenging in deep learning, since the number of trainable parameters is oftentimes much larger than the size of the dataset, and the loss function used to train the model is nonconvex.  To prove stronger generalization and related robustness guarantees, the project team uses ideas from manifold learning and differential geometry to model the low-dimensional structure of datasets which arise in many machine learning applications. The project has three components.  One component is to design and analyze novel optimization algorithms for training deep learning models.  Another component is to design and analyze algorithms for multi-agent optimization problems, such as the min-max optimization problems which arise when training generative adversarial nets (GANs) as well as multi-agent optimization problems which arise when training meta-learning models.  Finally, in addition to optimization algorithms, it is also designing and analyzing Markov-chain sampling algorithms and related algorithms which are used to train Bayesian machine learning models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Oren",
   "pi_last_name": "Mangoubi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Oren Mangoubi",
   "pi_email_addr": "omangoubi@wpi.edu",
   "nsf_id": "000707503",
   "pi_start_date": "2021-05-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Worcester Polytechnic Institute",
  "inst_street_address": "100 INSTITUTE RD",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5088315000",
  "inst_zip_code": "016092280",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "WORCESTER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJNQME41NBU4"
 },
 "perf_inst": {
  "perf_inst_name": "Worcester Polytechnic Institute",
  "perf_str_addr": "100 Institute Rd.",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016092247",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 174187.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">The aim of this project was to design novel optimization and sampling algorithms for training deep-learning and other machine-learning models, and to prove guarantees on the running time, generalization error and related stability properties of these algorithms. To design and analyze such algorithms, this project brought together ideas from different areas of mathematics and computer science.</p>\n<p class=\"p1\">One outcome of this project was to develop stable algorithms for nonconvex min-max optimization, with provable convergence and polynomial-time runtime guarantees, and applications to training generative adversarial networks (GANs).&nbsp;&nbsp;Empirically, the proposed algorithms were found to converge on challenging nonconvex-nonconcave test functions and, when used to train GANs, to train stably on synthetic and real-world datasets and to avoid mode collapse.</p>\n<p class=\"p1\">Another outcome was to develop novel Markov chain-based algorithms for sampling from log-concave distributions, with convergence guarantees in the infinity-distance metric. &nbsp; The infinity-distance&nbsp;metric is a stronger metric than the total variation, Wasserstein, and KL divergence metrics which appear in the guarantees of many prior works on sampling, and sampling with guarantees in the infinity-distance metric allows for stronger privacy guarantees for differentially private sampling and optimization problems. The algorithms that were developed improve on the previous best runtimes for sampling from log-concave distributions with convergence guarantees in the infinity-distance metric.<span>&nbsp; </span>The developed algorithms also lead to better runtime bounds for problems in Bayesian inference and differentially private optimization.&nbsp;</p>\n<p class=\"p1\">This project also developed applications of deep learning to different areas of engineering. Specifically, the project developed applications of variational-inference Bayesian deep learning algorithms to meta-learning, with applications to urban traffic flow in a dynamic environment.<span>&nbsp; </span>Applications of deep learning algorithms to sub-surface imaging of soil moisture using ground-penetrating radar (GPR) were also developed.</p>\n<p class=\"p1\">Results from this project have led to multiple papers published in the proceedings of theoretical computer science and machine learning conferences, and to a masters thesis.<span>&nbsp; </span>The results of the project have also been disseminated through presentations at conferences and through invited talks at universities and in industry.<span>&nbsp; </span>Examples derived from this project were also included in a graduate course in deep learning and optimization developed by the PI.<span>&nbsp; </span>In addition, a hands-on exhibit and activities were designed and presented at a STEM festival for elementary and middle school students.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/03/2023<br>\n\t\t\t\t\tModified by: Oren&nbsp;Mangoubi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The aim of this project was to design novel optimization and sampling algorithms for training deep-learning and other machine-learning models, and to prove guarantees on the running time, generalization error and related stability properties of these algorithms. To design and analyze such algorithms, this project brought together ideas from different areas of mathematics and computer science.\nOne outcome of this project was to develop stable algorithms for nonconvex min-max optimization, with provable convergence and polynomial-time runtime guarantees, and applications to training generative adversarial networks (GANs).  Empirically, the proposed algorithms were found to converge on challenging nonconvex-nonconcave test functions and, when used to train GANs, to train stably on synthetic and real-world datasets and to avoid mode collapse.\nAnother outcome was to develop novel Markov chain-based algorithms for sampling from log-concave distributions, with convergence guarantees in the infinity-distance metric.   The infinity-distance metric is a stronger metric than the total variation, Wasserstein, and KL divergence metrics which appear in the guarantees of many prior works on sampling, and sampling with guarantees in the infinity-distance metric allows for stronger privacy guarantees for differentially private sampling and optimization problems. The algorithms that were developed improve on the previous best runtimes for sampling from log-concave distributions with convergence guarantees in the infinity-distance metric.  The developed algorithms also lead to better runtime bounds for problems in Bayesian inference and differentially private optimization. \nThis project also developed applications of deep learning to different areas of engineering. Specifically, the project developed applications of variational-inference Bayesian deep learning algorithms to meta-learning, with applications to urban traffic flow in a dynamic environment.  Applications of deep learning algorithms to sub-surface imaging of soil moisture using ground-penetrating radar (GPR) were also developed.\nResults from this project have led to multiple papers published in the proceedings of theoretical computer science and machine learning conferences, and to a masters thesis.  The results of the project have also been disseminated through presentations at conferences and through invited talks at universities and in industry.  Examples derived from this project were also included in a graduate course in deep learning and optimization developed by the PI.  In addition, a hands-on exhibit and activities were designed and presented at a STEM festival for elementary and middle school students.\n\n\t\t\t\t\tLast Modified: 10/03/2023\n\n\t\t\t\t\tSubmitted by: Oren Mangoubi"
 }
}