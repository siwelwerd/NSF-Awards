{
 "awd_id": "2101388",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Coding Techniques for Distributed Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2020-09-30",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 372267.0,
 "awd_amount": 388267.0,
 "awd_min_amd_letter_date": "2020-11-18",
 "awd_max_amd_letter_date": "2020-11-18",
 "awd_abstract_narration": "Modern machine learning models have achieved great success and have been widely deployed across many sectors. As the size of data used to train machine learning models keeps growing, it is now routine to use distributed computing infrastructures such as the cloud. This strategy allows the computation of training to be distributed among a large number of nodes hosted in the cloud, where each node processes a partition of the whole data set. However, the performance of nodes in the cloud is often unreliable, due to system failures, resource contention, load imbalance, etc., and that unreliability can significantly delay the training process. This project pursues a coding-based framework that not only tolerates the effects of faulty nodes, but also further enhances the performance of machine learning training by dynamically taking advantage of the resources available on all nodes, whether they are faulty or not. The outcomes of this project should lead to a significant performance boost for distributed training of machine learning models.\r\n\r\nTo enable the efficient use of distributed computing across unreliable infrastructure for training machine learning models from big data sets, the technical objectives of this project are divided into three levels. This project will first study coding theory for distributed matrix multiplication, a universal operation in various machine learning algorithms, and propose a coding framework with both fault tolerance and a significant performance boost. This framework will then be applied into parameter servers at the architecture level and deep neural networks at the model level, respectively. Combining these three parts, this work will lead to a practical coding framework that can efficiently scale out computation on heterogeneous unreliable nodes, where the coding schemes will be applied to distributed machine learning at different levels including fundamental arithmetic, architectures, and models.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Li",
   "pi_email_addr": "jun.li@qc.cuny.edu",
   "nsf_id": "000760317",
   "pi_start_date": "2020-11-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CUNY Queens College",
  "inst_street_address": "6530 KISSENA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "FLUSHING",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7189975400",
  "inst_zip_code": "113671575",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "NY06",
  "org_lgl_bus_name": "RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJABWGUJM228"
 },
 "perf_inst": {
  "perf_inst_name": "CUNY Queens College",
  "perf_str_addr": "65 30 Kissena Blvd",
  "perf_city_name": "Flushing",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "113671575",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NY06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 372267.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-24fdd9d4-7fff-3ceb-7b40-b883a461a411\">Modern machine learning models have achieved great successes and have been widely deployed across many sectors. As the size of data used to train machine learning models keeps growing, it is now routine to use distributed computing infrastructures such as the cloud. This strategy allows the computation of training to be distributed among a large number of nodes hosted in the cloud, where each node processes a partition of the whole data set. However, the performance of nodes in the cloud is often unreliable, due to system failures, resource contention, load imbalance, etc., and that unreliability can significantly delay the training process.&nbsp;</span></p>\n<p><span id=\"docs-internal-guid-24fdd9d4-7fff-3ceb-7b40-b883a461a411\">Intellectual Merits: This project proposes a series of novel coding techniques to tolerate the adversarial effects of unreliable nodes (i.e., stragglers). Moreover, our works achieve the following desirable features. </span></p>\n<p><span id=\"docs-internal-guid-24fdd9d4-7fff-3ceb-7b40-b883a461a411\">First, we</span>&nbsp;enhance the performance of the workload by dynamically taking advantage of resources on all nodes, no matter there are faulty nodes or not. Specifically, we have proposed a coding scheme that can exploit the partially finished results on stragglers for distributed matrix multiplication. We have also extended the result above where the coding scheme is constructed taking the execution order of tasks into account.</p>\n<p>Second, we achieve a flexible and dynamical tradeoff between multiple types of resources.&nbsp;<span id=\"docs-internal-guid-3965ffff-7fff-48cb-e840-a4f053504470\"><span>We have proposed local re-encoding for distributed matrix multiplication that can dynamically change its parameters without remote communication, and meanwhile achieve high numerical stability.</span></span></p>\n<p>Third, we design coding schmes working for a variety of workload ranging from matrix multiplication to training deep neural networks. Such workloads include matrix chain multiplication, batch matrix mulitplication, and asynchronous and synchronous gradient descent.</p>\n<p>Broader Impacts: First, through this project, we develops coding techniques starting from matrix multiplication and extending them to neural networks. Such techniques broads the applications of coded computing, and we release the code to the public. Second, we offer research opportunities to graduate and undergraduate students, many of whom have under-represented backgrounds. Thirds, the research results are also developed into teaching materials the PI taught at CUNY Queens College and Graduate Center.</p>\n<p><span><br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/27/2022<br>\n\t\t\t\t\tModified by: Jun&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern machine learning models have achieved great successes and have been widely deployed across many sectors. As the size of data used to train machine learning models keeps growing, it is now routine to use distributed computing infrastructures such as the cloud. This strategy allows the computation of training to be distributed among a large number of nodes hosted in the cloud, where each node processes a partition of the whole data set. However, the performance of nodes in the cloud is often unreliable, due to system failures, resource contention, load imbalance, etc., and that unreliability can significantly delay the training process. \n\nIntellectual Merits: This project proposes a series of novel coding techniques to tolerate the adversarial effects of unreliable nodes (i.e., stragglers). Moreover, our works achieve the following desirable features. \n\nFirst, we enhance the performance of the workload by dynamically taking advantage of resources on all nodes, no matter there are faulty nodes or not. Specifically, we have proposed a coding scheme that can exploit the partially finished results on stragglers for distributed matrix multiplication. We have also extended the result above where the coding scheme is constructed taking the execution order of tasks into account.\n\nSecond, we achieve a flexible and dynamical tradeoff between multiple types of resources. We have proposed local re-encoding for distributed matrix multiplication that can dynamically change its parameters without remote communication, and meanwhile achieve high numerical stability.\n\nThird, we design coding schmes working for a variety of workload ranging from matrix multiplication to training deep neural networks. Such workloads include matrix chain multiplication, batch matrix mulitplication, and asynchronous and synchronous gradient descent.\n\nBroader Impacts: First, through this project, we develops coding techniques starting from matrix multiplication and extending them to neural networks. Such techniques broads the applications of coded computing, and we release the code to the public. Second, we offer research opportunities to graduate and undergraduate students, many of whom have under-represented backgrounds. Thirds, the research results are also developed into teaching materials the PI taught at CUNY Queens College and Graduate Center.\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 12/27/2022\n\n\t\t\t\t\tSubmitted by: Jun Li"
 }
}