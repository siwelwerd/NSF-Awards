{
 "awd_id": "2141680",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Harnessing Accurate Bias in Large-Scale Language Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 278914.0,
 "awd_amount": 278914.0,
 "awd_min_amd_letter_date": "2021-08-19",
 "awd_max_amd_letter_date": "2021-08-19",
 "awd_abstract_narration": "Machine learning models reflect patterns in the data they are trained on, and can, unfortunately, exhibit negative social biases such as prejudice, sexism, or racism.  Most research seeks to mitigate this bias, but this work flips the paradigm and explores an alternative by asking: can the bias in machine learning models be harnessed for good? There is strong evidence that some language models exhibit a property called \"accurate bias\": the patterns captured by the models correlate strongly with human values, judgements, and opinions in ways that are accurately intertwined with time, geography, personal identity, and cultural milieu. In fact, the correlations are so strong and fine-grained that models exhibiting accurate bias can be studied as a surrogate for human subjects, implying researchers can derive actionable insight by experimenting on models in ways that are not possible with humans. By developing a robust methodology and best practices for extracting and analyzing the accurate bias in language models, it is possible to develop new tools for the social sciences, and could revolutionize any field that studies humans, such as psychology, cognitive science, or political science.\r\n\r\nTo accomplish these goals, this EArly Grant for Exploratory Research (EAGER) will systematically study language models to determine the possibilities and limitations of accurate bias. As an EAGER, these research activities will be highly exploratory, designed to amass preliminary results and develop technical proofs of concept to support future research. The work will blend methods from machine learning and social sciences to develop a preliminary theory of accurate bias, and a suite of accompanying methodological and technical best practices. By studying the feasibility of leveraging accurate bias in large-scale language models, this work could deliver fundamental insights into the values, opinions and thought processes of humans. This work could also deliver insights into how to improve language models, including improving their ability to reason symbolically, and a deeper understanding of the relationship between prompt engineering, data curation, fine-tuning, and the informativity of the final model.  Technical elements of our proposal, such as work on prompt engineering and controllable text generation, could have significant applicability outside the context of social science research, and stand on their own right as advances of interest to the machine learning community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Wingate",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Wingate",
   "pi_email_addr": "wingated@cs.byu.edu",
   "nsf_id": "000700514",
   "pi_start_date": "2021-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ethan",
   "pi_last_name": "Busby",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Ethan C Busby",
   "pi_email_addr": "ethan.busby@byu.edu",
   "nsf_id": "000722387",
   "pi_start_date": "2021-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nancy",
   "pi_last_name": "Fulda",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nancy Fulda",
   "pi_email_addr": "nfulda@cs.byu.edu",
   "nsf_id": "000820814",
   "pi_start_date": "2021-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Joshua",
   "pi_last_name": "Gubler",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Joshua R Gubler",
   "pi_email_addr": "jgub@byu.edu",
   "nsf_id": "000842906",
   "pi_start_date": "2021-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lisa",
   "pi_last_name": "Argyle",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Lisa P Argyle",
   "pi_email_addr": "lisa_argyle@byu.edu",
   "nsf_id": "000842908",
   "pi_start_date": "2021-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brigham Young University",
  "inst_street_address": "A-153 ASB",
  "inst_street_address_2": "",
  "inst_city_name": "PROVO",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8014223360",
  "inst_zip_code": "846021128",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "UT03",
  "org_lgl_bus_name": "BRIGHAM YOUNG UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JWSYC7RUMJD1"
 },
 "perf_inst": {
  "perf_inst_name": "Brigham Young University",
  "perf_str_addr": "A-285 ASB",
  "perf_city_name": "Provo",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "846021231",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "UT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 278914.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Language models such as ChatGPT are having significant impact throughout academia and industry. In this project, we are studying ways to leverage language models to improve society, by developing prosocial applications that allow us to strengthen democratic discourse and improve our ability to conduct ethical and accurate social science research.</p>\n<p>Highlights from the project include the following:</p>\n<p>* We have introduced the idea of \"algorithmic fidelity\". When a model has algorithmic fidelity, it faithfully reflects the attitudes, biases and judgements -- both good and bad -- of human subjects. As such, it can be used to enhance social research that relies on humans, and can potentially uncover patterns in human thought that are diffused across populations, cultures and ideologies.</p>\n<p>* We have shown that LLMs, such as GPT-3, have significant algorithmic fidelity in the area of political science, with an innovative \"silicon subjects\" methodology that constructs virtual human subjects with rich backstories; those simulated humans can then be compared to real humans.&nbsp; This idea has already spawned at least one startup dedicated to providing commercial services that provide simulated humans, and was an early paper in the now-burgeoning area of exploring the extent of algorithmic fidelity in LLMs.</p>\n<p>* We have used the concepts of algorithmic fidelity to help reduce diviseness in conversations and politically charged topics, such as illegal immigration and gun control.&nbsp; A major field study showed that using LLMs to rewrite conversational turns promotes feelings of being understood and improved conversation quality -- such dialogue is a necessary, although insufficient, condition for increasing mutual understanding, compromise, and coalition building.&nbsp; The fact that our interventions are implemented using LLMs suggests that they could potentially be deployed at scale, to temper conversations on social media.</p>\n<p>* We have also published several more technical papers that help solve important problems along the way, such as how to select good prompts for use with LLMs (by optimizing mutual information), and how to construct evaluation frameworks that elicit knowledge as cleanly as possible (by extensively evaluating multiple choice prompting).</p>\n<p>* We have multiple ongoing efforts to identify, isolate, and leverage algorithmic fidelity in other application areas, such as autism, racism, and political polarization.&nbsp; These efforts build on the idea of constructing silicon subjects, and simulates them as they recount virtual stories about their (virtual) lives; these stories are then mined for new social patterns and insights into actionable drivers that may help address pressing social problems.</p>\n<p>* We also have ongoing efforts to understand the extent to which LLMs are building models of humans, and the extent to which they are simply stitching together surface statistics of internet speech. Progress in this area is of fundamental interest to a wide variety of researchers, and involves careful and thoughtful construction of test cases designed to distinguish between observational statistics and causal / interventional models.</p>\n<p>This project has supported a wide variety of undergraduate, MS and PhD students.</p>\n<p>In terms of broader impacts, results from this project have been published in multiple top-tier venues including ACL, Political Analysis and PNAS; presented in about 20 conferences and invited talks; featured in multiple podcasts; translated into special \"Science for Kids\" articles; discussed extensively on Twitter; and even adopted by a major social media platform to reduce real-world toxicity.</p><br>\n<p>\n Last Modified: 07/15/2024<br>\nModified by: David&nbsp;Wingate</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nLanguage models such as ChatGPT are having significant impact throughout academia and industry. In this project, we are studying ways to leverage language models to improve society, by developing prosocial applications that allow us to strengthen democratic discourse and improve our ability to conduct ethical and accurate social science research.\n\n\nHighlights from the project include the following:\n\n\n* We have introduced the idea of \"algorithmic fidelity\". When a model has algorithmic fidelity, it faithfully reflects the attitudes, biases and judgements -- both good and bad -- of human subjects. As such, it can be used to enhance social research that relies on humans, and can potentially uncover patterns in human thought that are diffused across populations, cultures and ideologies.\n\n\n* We have shown that LLMs, such as GPT-3, have significant algorithmic fidelity in the area of political science, with an innovative \"silicon subjects\" methodology that constructs virtual human subjects with rich backstories; those simulated humans can then be compared to real humans. This idea has already spawned at least one startup dedicated to providing commercial services that provide simulated humans, and was an early paper in the now-burgeoning area of exploring the extent of algorithmic fidelity in LLMs.\n\n\n* We have used the concepts of algorithmic fidelity to help reduce diviseness in conversations and politically charged topics, such as illegal immigration and gun control. A major field study showed that using LLMs to rewrite conversational turns promotes feelings of being understood and improved conversation quality -- such dialogue is a necessary, although insufficient, condition for increasing mutual understanding, compromise, and coalition building. The fact that our interventions are implemented using LLMs suggests that they could potentially be deployed at scale, to temper conversations on social media.\n\n\n* We have also published several more technical papers that help solve important problems along the way, such as how to select good prompts for use with LLMs (by optimizing mutual information), and how to construct evaluation frameworks that elicit knowledge as cleanly as possible (by extensively evaluating multiple choice prompting).\n\n\n* We have multiple ongoing efforts to identify, isolate, and leverage algorithmic fidelity in other application areas, such as autism, racism, and political polarization. These efforts build on the idea of constructing silicon subjects, and simulates them as they recount virtual stories about their (virtual) lives; these stories are then mined for new social patterns and insights into actionable drivers that may help address pressing social problems.\n\n\n* We also have ongoing efforts to understand the extent to which LLMs are building models of humans, and the extent to which they are simply stitching together surface statistics of internet speech. Progress in this area is of fundamental interest to a wide variety of researchers, and involves careful and thoughtful construction of test cases designed to distinguish between observational statistics and causal / interventional models.\n\n\nThis project has supported a wide variety of undergraduate, MS and PhD students.\n\n\nIn terms of broader impacts, results from this project have been published in multiple top-tier venues including ACL, Political Analysis and PNAS; presented in about 20 conferences and invited talks; featured in multiple podcasts; translated into special \"Science for Kids\" articles; discussed extensively on Twitter; and even adopted by a major social media platform to reduce real-world toxicity.\t\t\t\t\tLast Modified: 07/15/2024\n\n\t\t\t\t\tSubmitted by: DavidWingate\n"
 }
}