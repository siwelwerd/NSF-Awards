{
 "awd_id": "2136915",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Biometric Authentication using Noncontact Cardiovascular Signals",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032920000",
 "po_email": "doliveir@nsf.gov",
 "po_sign_block_name": "Daniela Oliveira",
 "awd_eff_date": "2021-07-15",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 187723.0,
 "awd_amount": 187723.0,
 "awd_min_amd_letter_date": "2021-07-07",
 "awd_max_amd_letter_date": "2021-07-07",
 "awd_abstract_narration": "In our increasingly interconnected society, user authentication - procedures for verifying that individuals are who they claim to be - is important for improving the security of digital systems. Authentication protocols commonly involve combinations of something the user knows (such as a password) or has (such as a card or fob), and biometrics (such as the recognition of a fingerprint). This project develops novel biometric approaches based on the activity of the heart. Because shape and physiology of the cardiovascular system differ from person to person, it is possible to detect significant differences related to blood flow for different individuals. The project introduces methods for detecting information related to the cardiovascular system using a video camera, at the level of detail needed to perform user authentication. The project's broader significance is the potential use of these new techniques, as well as a dataset of measurements collected from several individuals for cybersecurity and health care research and applications.\r\n\r\nThe underlying phenomenon in this project is photoplethysmography, which is the use of illumination-based sensors to record local volumetric changes in peripheral blood circulation. The investigators use video cameras to obtain measurements of cardiovascular activity, with the motivation that noncontact measurements are more convenient and unobtrusive than using devices that touch the skin. An emphasis of the project is the use of machine-learning techniques to analyze signals from videos of the human face, to extract features that can distinguish individuals based solely on cardiovascular activity. The research includes characterization of camera parameters and illumination conditions. The primary goal is to demonstrate that the resulting system can be used for user authentication. Potential contributions of this project include improved security of digital systems, along with better understanding of the cardiovascular system.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amos",
   "pi_last_name": "Abbott",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Amos L Abbott",
   "pi_email_addr": "abbott@vt.edu",
   "nsf_id": "000348621",
   "pi_start_date": "2021-07-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Abhijit",
   "pi_last_name": "Sarkar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Abhijit Sarkar",
   "pi_email_addr": "asarkar1@vt.edu",
   "nsf_id": "000764089",
   "pi_start_date": "2021-07-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "1145 Perry Street",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 187723.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has led to the development of two new methods for automatically extracting information related to a person's cardiovascular system. Unlike common approaches that use sensors mounted on a fingertip or on other parts of the body, our new methods rely entirely on video cameras for data capture. The underlying principle is that the heart causes blood volume pulses (BVP) to travel throughout the body, and these pulses cause faint fluctuations in reflectance near the surface of the skin. These fluctuations can be captured by a camera even though they are not visible to the unaided eye.<br /><br />Figure 1 provides a high-level illustration of our approach. As a person faces a camera, visible regions of skin will exhibit small temporal changes in brightness within the video signal. The approach that we use to detect these changes is known as imaging photoplethysmography (iPPG), where the term photoplethysmography (PPG) refers to the use of light to detect volumetric changes in the body. This project has led to two new iPPG systems that can process a video and extract temporal waveforms that are significantly higher in accuracy than those produced by earlier systems. <br /><br />Our particular emphasis in this project has been to detect BVP signals with accuracy that is sufficiently high to support the task of biometric authentication. In contrast, most previous iPPG systems have employed a substantial amount of averaging, often with the goal of reporting heart rate that has been averaged over several seconds. Our work has focused on extracting individual pulses with relatively high fidelity, as shown in Figure 2. The figure shows an example PPG signal (shown in brown) that has been extracted by one of our systems, as compared with the ground-truth signal (shown in blue) that was obtained using a fingertip-mounted PPG sensor. Four blood volume pulses are shown, each comprising two peaks known as the systolic peak followed by the lower-amplitude diastolic peak. This level of signal quality is key to performing such tasks as authentication.<br /><br />Figure 3 shows more details about the system that produced this example PPG signal. This system first detects small regions of interest (ROI) within the face, and extracts temporal waveforms for each region for the separate color channels (red, green, and blue) from the camera. Central to this system is a deep neural-network model that we have trained to accept those waveforms as input and produce an estimated PPG signal as output. We developed postprocessing steps to deal with noise that is expected to be present in the output. These latter steps extract and align groups of individual pulses from the waveform. In the application that is illustrated in the figure, biometric authentication is performed by comparing the shape of a representative pulse with the BVP waveform shape that is expected for this person; a close match indicates verification of this person's identity.<br /><br />The primary outcomes of this project are as follows:<br /><br />- Our work is the first to demonstrate feasibility of performing biometric authentication using cardiovascular (PPG) signals that have been extracted from video of a person's face. This project demonstrated feasibility using two different system designs.<br /><br />- For the task of estimating heart rate from video of a person's face, both of our systems exhibited better performance than previous state-of-the-art systems.<br /><br />- Source code for both of our systems is now available to the research community.<br /><br />- The project supported our collection of a new dataset to facilitate more work in this field. This dataset consists of video recordings of 10 participants using one RGB camera and two NIR cameras, synchronized with data from a fingertip PPG sensor. This dataset will soon be made available to other researchers.<br /><br />Broader impacts of this work include the following:<br /><br />- The field of cybersecurity will benefit if this project's proof-of-concept work in biometric authentication is extended and combined with other authentication systems. A camera-based approach based on this work offers the potential for unintrusive, continuous authentication.<br /><br />- Health-care systems may also benefit from this work. For example, it is conceivable that the system developed in this project can be extended to monitor heart rate variability (HRV) and other vital signs. Camera-based approaches offer the potential for vital-sign monitoring that is more convenient and less intrusive than using existing approaches, such as specialized sensors and wearable devices.<br /><br />- This project has had educational impact in several respects. In particular, several graduate students contributed to this project directly, and they gained valuable lessons related to machine learning, signal processing, and biometrics. Aspects of the work will be incorporated into undergraduate courses. Also, some of the knowledge that has been gained from this project has been incorporated into tutorials being offered at professional conferences. <br /><br />- Finally, this project has supported technology transfer through open-source dissemination of our source code, and through the sharing of the new dataset that we collected.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/10/2023<br>\n\t\t\t\t\tModified by: Amos&nbsp;L&nbsp;Abbott</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688755219279_OverviewofiPPG--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688755219279_OverviewofiPPG--rgov-800width.jpg\" title=\"Figure 1. Illustration of imaging PPG.\"><img src=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688755219279_OverviewofiPPG--rgov-66x44.jpg\" alt=\"Figure 1. Illustration of imaging PPG.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1. High-level illustration of an imaging photoplethysmography (iPPG) system. When a person faces a video camera, portions of skin that are visible to the camera will exhibit small fluctuations in brightness because of blood volume pulses. An iPPG system attempts to extract this faint signal.</div>\n<div class=\"imageCredit\">Fulan Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Amos&nbsp;L&nbsp;Abbott</div>\n<div class=\"imageTitle\">Figure 1. Illustration of imaging PPG.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688743389291_Systemoverview--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688743389291_Systemoverview--rgov-800width.jpg\" title=\"Figure 3. System overview.\"><img src=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688743389291_Systemoverview--rgov-66x44.jpg\" alt=\"Figure 3. System overview.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3. One of our approaches. a) Video input. b) Regions are detected on the face. c) Intensity values are obtained as time series from input. d,e) A deep model estimates the PPG signal. f) Postprocessing removes outliers and extracts a group of normalized pulses. g) Authentication is performed.</div>\n<div class=\"imageCredit\">Fulan Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Amos&nbsp;L&nbsp;Abbott</div>\n<div class=\"imageTitle\">Figure 3. System overview.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688757436777_ExamplePPGsignal--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688757436777_ExamplePPGsignal--rgov-800width.jpg\" title=\"Figure 2. Example PPG signal.\"><img src=\"/por/images/Reports/POR/2023/2136915/2136915_10745576_1688757436777_ExamplePPGsignal--rgov-66x44.jpg\" alt=\"Figure 2. Example PPG signal.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2. Plot of an example PPG signal that was extracted by our system from video (shown in brown), as compared with Ground Truth from a fingertip PPG sensor (shown in blue). The figure shows a sequence of 4 blood volume pulses.</div>\n<div class=\"imageCredit\">Fulan Li</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Amos&nbsp;L&nbsp;Abbott</div>\n<div class=\"imageTitle\">Figure 2. Example PPG signal.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project has led to the development of two new methods for automatically extracting information related to a person's cardiovascular system. Unlike common approaches that use sensors mounted on a fingertip or on other parts of the body, our new methods rely entirely on video cameras for data capture. The underlying principle is that the heart causes blood volume pulses (BVP) to travel throughout the body, and these pulses cause faint fluctuations in reflectance near the surface of the skin. These fluctuations can be captured by a camera even though they are not visible to the unaided eye.\n\nFigure 1 provides a high-level illustration of our approach. As a person faces a camera, visible regions of skin will exhibit small temporal changes in brightness within the video signal. The approach that we use to detect these changes is known as imaging photoplethysmography (iPPG), where the term photoplethysmography (PPG) refers to the use of light to detect volumetric changes in the body. This project has led to two new iPPG systems that can process a video and extract temporal waveforms that are significantly higher in accuracy than those produced by earlier systems. \n\nOur particular emphasis in this project has been to detect BVP signals with accuracy that is sufficiently high to support the task of biometric authentication. In contrast, most previous iPPG systems have employed a substantial amount of averaging, often with the goal of reporting heart rate that has been averaged over several seconds. Our work has focused on extracting individual pulses with relatively high fidelity, as shown in Figure 2. The figure shows an example PPG signal (shown in brown) that has been extracted by one of our systems, as compared with the ground-truth signal (shown in blue) that was obtained using a fingertip-mounted PPG sensor. Four blood volume pulses are shown, each comprising two peaks known as the systolic peak followed by the lower-amplitude diastolic peak. This level of signal quality is key to performing such tasks as authentication.\n\nFigure 3 shows more details about the system that produced this example PPG signal. This system first detects small regions of interest (ROI) within the face, and extracts temporal waveforms for each region for the separate color channels (red, green, and blue) from the camera. Central to this system is a deep neural-network model that we have trained to accept those waveforms as input and produce an estimated PPG signal as output. We developed postprocessing steps to deal with noise that is expected to be present in the output. These latter steps extract and align groups of individual pulses from the waveform. In the application that is illustrated in the figure, biometric authentication is performed by comparing the shape of a representative pulse with the BVP waveform shape that is expected for this person; a close match indicates verification of this person's identity.\n\nThe primary outcomes of this project are as follows:\n\n- Our work is the first to demonstrate feasibility of performing biometric authentication using cardiovascular (PPG) signals that have been extracted from video of a person's face. This project demonstrated feasibility using two different system designs.\n\n- For the task of estimating heart rate from video of a person's face, both of our systems exhibited better performance than previous state-of-the-art systems.\n\n- Source code for both of our systems is now available to the research community.\n\n- The project supported our collection of a new dataset to facilitate more work in this field. This dataset consists of video recordings of 10 participants using one RGB camera and two NIR cameras, synchronized with data from a fingertip PPG sensor. This dataset will soon be made available to other researchers.\n\nBroader impacts of this work include the following:\n\n- The field of cybersecurity will benefit if this project's proof-of-concept work in biometric authentication is extended and combined with other authentication systems. A camera-based approach based on this work offers the potential for unintrusive, continuous authentication.\n\n- Health-care systems may also benefit from this work. For example, it is conceivable that the system developed in this project can be extended to monitor heart rate variability (HRV) and other vital signs. Camera-based approaches offer the potential for vital-sign monitoring that is more convenient and less intrusive than using existing approaches, such as specialized sensors and wearable devices.\n\n- This project has had educational impact in several respects. In particular, several graduate students contributed to this project directly, and they gained valuable lessons related to machine learning, signal processing, and biometrics. Aspects of the work will be incorporated into undergraduate courses. Also, some of the knowledge that has been gained from this project has been incorporated into tutorials being offered at professional conferences. \n\n- Finally, this project has supported technology transfer through open-source dissemination of our source code, and through the sharing of the new dataset that we collected.\n\n\t\t\t\t\tLast Modified: 07/10/2023\n\n\t\t\t\t\tSubmitted by: Amos L Abbott"
 }
}