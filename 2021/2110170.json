{
 "awd_id": "2110170",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "IIS:RI Theoretical Foundations of Reinforcement Learning: From Tabula Rasa to Function Approximation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2021-07-26",
 "awd_max_amd_letter_date": "2021-07-26",
 "awd_abstract_narration": "Reinforcement learning, a technique that trains intelligent agents to make decisions, has become the central algorithmic paradigm for various applications, such as robotics, healthcare, manufacturing production, game playing, and transportation. However, reinforcement learning is equally infamous for demanding significant amounts of data and computing resources. This project aims to contribute to the fundamental understanding of reinforcement learning to reveal its inherent difficulties and develop efficient algorithms with strong theoretical guarantees. The results of the project are readily applicable to solving practical resource-hungry problems. The success of this project also requires new algorithmic techniques and mathematical tools in a variety of disciplines. An education plan is integrated into this project; the investigator will develop new courses, mentor students, organize workshops, and deliver lessons to high school students through the University of Washington\u2019s Partner School program.\r\n\r\nThis project has two major components. The first thrust studies the most canonical setting, tabula rasa reinforcement learning. The investigator will identify fundamental limits and develop optimal algorithms for several problems of both theoretical and practical interests: worst-case complexity, adaptation to problem structure, and data collection for batch RL. The second thrust is motivated by the modern usage of RL, where function approximation is employed for generalization over a large state space. The investigator will systematically examine the necessary and sufficient conditions that permit efficient learning algorithms for three of the most popular function approximation schemes: value-based, policy-based, and model-based. For both thrusts, the investigator will utilize the inherent combinatorial structures of reinforcement learning to characterize its fundamental hardness and design efficient algorithms. In addition to theoretical developments, the project also aims to implement all algorithms developed as open-source software and evaluate them on benchmark simulation environments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Simon",
   "pi_last_name": "Du",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Simon Du",
   "pi_email_addr": "ssdu@cs.washington.edu",
   "nsf_id": "000841729",
   "pi_start_date": "2021-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to advance the fundamental understanding of reinforcement learning by addressing its inherent challenges and developing efficient algorithms with strong theoretical guarantees. The first thrust of the project focused on tackling the long-standing open problem of sample complexity in reinforcement learning within the tabular setting. We successfully developed new efficient algorithms that resolved this challenge and extended the results beyond the standard reward-maximization paradigm to encompass data collection and offline learning scenarios. Notably, in certain settings, we demonstrated that the sample complexity can be independent of the planning horizon, revealing that reinforcement learning may not be inherently more difficult than bandit problems under specific conditions.</p>\r\n<p>The second thrust of the project sought to bridge the gap between theoretical advancements and practical applications by systematically examining the necessary and sufficient conditions for efficient learning algorithms under three widely used function approximation frameworks: value-based, policy-based, and model-based. Through this effort, we developed novel, efficient algorithms with rigorous theoretical guarantees. To achieve these results, we introduced new algorithmic techniques and mathematical tools, drawing from diverse disciplines such as applied mathematics, information theory, optimization, statistics, and combinatorics.</p>\r\n<p>This project has led to publications in leading machine learning conferences, contributing to the broader dissemination of our findings. Additionally, it provided valuable training opportunities for four graduate students and one postdoctoral researcher in the PI&rsquo;s group, equipping them with cutting-edge knowledge and skills. Overall, the project has made significant strides in understanding and advancing reinforcement learning, with implications for both foundational research and real-world applications.</p><br>\n<p>\n Last Modified: 01/17/2025<br>\nModified by: Simon&nbsp;Du</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project aims to advance the fundamental understanding of reinforcement learning by addressing its inherent challenges and developing efficient algorithms with strong theoretical guarantees. The first thrust of the project focused on tackling the long-standing open problem of sample complexity in reinforcement learning within the tabular setting. We successfully developed new efficient algorithms that resolved this challenge and extended the results beyond the standard reward-maximization paradigm to encompass data collection and offline learning scenarios. Notably, in certain settings, we demonstrated that the sample complexity can be independent of the planning horizon, revealing that reinforcement learning may not be inherently more difficult than bandit problems under specific conditions.\r\n\n\nThe second thrust of the project sought to bridge the gap between theoretical advancements and practical applications by systematically examining the necessary and sufficient conditions for efficient learning algorithms under three widely used function approximation frameworks: value-based, policy-based, and model-based. Through this effort, we developed novel, efficient algorithms with rigorous theoretical guarantees. To achieve these results, we introduced new algorithmic techniques and mathematical tools, drawing from diverse disciplines such as applied mathematics, information theory, optimization, statistics, and combinatorics.\r\n\n\nThis project has led to publications in leading machine learning conferences, contributing to the broader dissemination of our findings. Additionally, it provided valuable training opportunities for four graduate students and one postdoctoral researcher in the PIs group, equipping them with cutting-edge knowledge and skills. Overall, the project has made significant strides in understanding and advancing reinforcement learning, with implications for both foundational research and real-world applications.\t\t\t\t\tLast Modified: 01/17/2025\n\n\t\t\t\t\tSubmitted by: SimonDu\n"
 }
}