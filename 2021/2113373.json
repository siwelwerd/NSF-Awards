{
 "awd_id": "2113373",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "A Transfer Learning Approach to Algorithmic Fairness",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yulia Gel",
 "awd_eff_date": "2021-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2021-08-02",
 "awd_max_amd_letter_date": "2021-08-02",
 "awd_abstract_narration": "In today's data-driven world, machine learning models are routinely used to make high-stakes decisions in criminal justice, education, lending, medicine, and many other areas. Although replacing humans with machine learning models appears to eliminate human biases in decision-making processes, they may perpetuate or even exacerbate biases in the training data. Such algorithmic biases are especially objectionable when they adversely affect underprivileged groups. In this project, we focus on detecting and mitigating algorithmic biases that are caused by sampling biases in the training data. The project also provides research training opportunities for graduate students. \r\n\r\nThere are three aims. First, the PIs identify gaps in the capabilities of existing algorithmic fairness practices for overcoming sampling biases in the training data. The PIs also study how current trends in the development of machine learning (ML) models (for example, data augmentation and overparameterization) can perpetuate and exacerbate algorithmic biases. Second, the PIs cast the fair machine learning problem as a transfer learning problem and leverage recent developments in transfer learning to detect and mitigate algorithmic biases caused by sampling bias. Third, the PIs consider how to collect training datasets that are more representative of the general population and beget ML models that are free from algorithmic biases. The ultimate goal is to broaden the appeal and adoption of algorithmic fairness practices among ML practitioners. The PIs plan to demonstrate that the transfer learning approach to algorithmic fairness avoids two barriers in the way of this ultimate goal: (i) it aligns the goal of algorithmic fairness with the goals of (possibly non-altruistic) ML practitioners by avoiding the apparent trade-off between accuracy and fairness, and (ii) it addresses the lack of consensus on the choice of algorithmic fairness practice in many ML tasks by providing an objective measure of the efficacy of such practices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuekai",
   "pi_last_name": "Sun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuekai Sun",
   "pi_email_addr": "yuekai@umich.edu",
   "nsf_id": "000758966",
   "pi_start_date": "2021-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Moulinath",
   "pi_last_name": "Banerjee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Moulinath Banerjee",
   "pi_email_addr": "moulib@umich.edu",
   "nsf_id": "000429273",
   "pi_start_date": "2021-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "Regents of the University of Michigan - Ann Arbor",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The standard approach to algorithmic fairness emphasizes the adherence to established fairness standards. For example, most previous work on algorithmic fairness develops mathematical definitions of fair decision-making and algorithms to enforce these definitions in an AI system. This&nbsp;<em>deontological</em>&nbsp;approach to algorithmic fairness (deonotology is an ethical theory that emphasizes duty, rules and adherence to principles) often overlooks the downstream&nbsp;<em>consequences</em>&nbsp;of adherence to fairness standards. It is also criticized for its inflexibility and propensity to become mired in moral uncertainty as a result of conflicting fairness standards in many applications. For example, the well-known \"impossibility of fairness\" shows that common mathematical definitions of fairness are fundamentally incompatible: it is (mathematically) impossible to satisfy one definition without violating other definitions.</p>\r\n<p>As an alternative, we advocate for a&nbsp;<em>consequentialist</em>&nbsp;approach to algorithmic fairness. Instead of adherence to established fairness standards, this approach views AI as components of sociotechnical systems and emphasizes alleviating injustices and improving social welfare. In applications where there are conflicting fairness standards, the consequentialist approach suggests that practitioners enforce the standard that maximizes social welfare. For example, in the debate on whether recidivism risk assessment algorithms should satisfy separation or sufficiency (two conflicting algorithmic fairness definitions), this approach suggests practitioners enforce the fairness definition that detains the fewest defendants while maintaining an allowable recidivism rate, thus maximizing social welfare. Compared to the deontological approach, the two main advantages of the consequentialist approach are:</p>\r\n<ol>\r\n<li>It enforces fairness standards&nbsp;<em>adaptively</em>&nbsp;by taking into account the downstream consequences of enforcing the standards (instead of rigidly adhering to a set of fairness standards);</li>\r\n<li>It provides practical guidance when there are conflicting fairness standards.</li>\r\n</ol>\r\n<p>This (more) consequentialist approach to algorithmic fairness offers a dynamic framework that prioritizes the impacts of fairness interventions. By focusing on outcomes, practitioners can navigate the complexities of algorithmic fairness&nbsp;with greater flexibility and precision, ultimately leading to more equitable and beneficial AI systems. This shift represents a critical evolution in the pursuit of algorithmic fairness, one that acknowledges the intricate interplay between fairness principles and their real-world consequences.</p>\r\n<p>In this project, we present two examples of the consequentialist approach to algorithmic fairness. First, we consider mitigating algorithmic biases caused by distribution shifts. In practice, algorithmic biases are often caused by sampling biases in the training data, which manifest themselves as distribution shifts between the training data and the population. Here, the consequentialist approach suggests that we keep in mind the social welfare of the (disadvantaged) users when mitigating algorithmic biases. We show that:</p>\r\n<ol>\r\n<li>Common algorithmic fairness interventions are not guaranteed to improve social welfare; in fact, they can reduce social welfare.</li>\r\n<li>It can be more beneficial to consider (algorithmic) bias mitigation as a distribution shift problem and to leverage methods for overcoming distribution shifts.</li>\r\n</ol>\r\n<p>An important benefit of the consequentialist approach to mitigating algorithmic biases caused by distribution shifts is that it can help (possibly non-altruistic) practitioners avoid the fairness-accuracy trade-off by aligning their incentives with the users' incentives. This alleviates one of the barriers in the way of broader adoption of algorithmic fairness practices.</p>\r\n<p>Second, we consider enforcing fairness when the users respond strategically to fairness interventions. Due to the strategic behavior of users, fairness interventions can have unintended consequences. For example, the Coate-Loury model from (labor) economics shows that affirmative action policies can harm disadvantaged users by confirming employers' prejudices against them. Here, the consequentialist approach suggests new theories and methods that leverage the users' strategic behavior to improve social welfare. For example, we prove that it is possible to avoid the impossibility of fairness with interventions that target substantive equality (substantive equality seeks to dismantle oppressive social hierarchies and structures): such interventions not only satisfy multiple (often incompatible) algorithmic fairness definitions, but also achieve substantive equality. This result provides a mathematical foundation for emerging theories of algorithmic justice and reparation.</p><br>\n<p>\n Last Modified: 01/10/2025<br>\nModified by: Yuekai&nbsp;Sun</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe standard approach to algorithmic fairness emphasizes the adherence to established fairness standards. For example, most previous work on algorithmic fairness develops mathematical definitions of fair decision-making and algorithms to enforce these definitions in an AI system. Thisdeontologicalapproach to algorithmic fairness (deonotology is an ethical theory that emphasizes duty, rules and adherence to principles) often overlooks the downstreamconsequencesof adherence to fairness standards. It is also criticized for its inflexibility and propensity to become mired in moral uncertainty as a result of conflicting fairness standards in many applications. For example, the well-known \"impossibility of fairness\" shows that common mathematical definitions of fairness are fundamentally incompatible: it is (mathematically) impossible to satisfy one definition without violating other definitions.\r\n\n\nAs an alternative, we advocate for aconsequentialistapproach to algorithmic fairness. Instead of adherence to established fairness standards, this approach views AI as components of sociotechnical systems and emphasizes alleviating injustices and improving social welfare. In applications where there are conflicting fairness standards, the consequentialist approach suggests that practitioners enforce the standard that maximizes social welfare. For example, in the debate on whether recidivism risk assessment algorithms should satisfy separation or sufficiency (two conflicting algorithmic fairness definitions), this approach suggests practitioners enforce the fairness definition that detains the fewest defendants while maintaining an allowable recidivism rate, thus maximizing social welfare. Compared to the deontological approach, the two main advantages of the consequentialist approach are:\r\n\r\nIt enforces fairness standardsadaptivelyby taking into account the downstream consequences of enforcing the standards (instead of rigidly adhering to a set of fairness standards);\r\nIt provides practical guidance when there are conflicting fairness standards.\r\n\r\n\n\nThis (more) consequentialist approach to algorithmic fairness offers a dynamic framework that prioritizes the impacts of fairness interventions. By focusing on outcomes, practitioners can navigate the complexities of algorithmic fairnesswith greater flexibility and precision, ultimately leading to more equitable and beneficial AI systems. This shift represents a critical evolution in the pursuit of algorithmic fairness, one that acknowledges the intricate interplay between fairness principles and their real-world consequences.\r\n\n\nIn this project, we present two examples of the consequentialist approach to algorithmic fairness. First, we consider mitigating algorithmic biases caused by distribution shifts. In practice, algorithmic biases are often caused by sampling biases in the training data, which manifest themselves as distribution shifts between the training data and the population. Here, the consequentialist approach suggests that we keep in mind the social welfare of the (disadvantaged) users when mitigating algorithmic biases. We show that:\r\n\r\nCommon algorithmic fairness interventions are not guaranteed to improve social welfare; in fact, they can reduce social welfare.\r\nIt can be more beneficial to consider (algorithmic) bias mitigation as a distribution shift problem and to leverage methods for overcoming distribution shifts.\r\n\r\n\n\nAn important benefit of the consequentialist approach to mitigating algorithmic biases caused by distribution shifts is that it can help (possibly non-altruistic) practitioners avoid the fairness-accuracy trade-off by aligning their incentives with the users' incentives. This alleviates one of the barriers in the way of broader adoption of algorithmic fairness practices.\r\n\n\nSecond, we consider enforcing fairness when the users respond strategically to fairness interventions. Due to the strategic behavior of users, fairness interventions can have unintended consequences. For example, the Coate-Loury model from (labor) economics shows that affirmative action policies can harm disadvantaged users by confirming employers' prejudices against them. Here, the consequentialist approach suggests new theories and methods that leverage the users' strategic behavior to improve social welfare. For example, we prove that it is possible to avoid the impossibility of fairness with interventions that target substantive equality (substantive equality seeks to dismantle oppressive social hierarchies and structures): such interventions not only satisfy multiple (often incompatible) algorithmic fairness definitions, but also achieve substantive equality. This result provides a mathematical foundation for emerging theories of algorithmic justice and reparation.\t\t\t\t\tLast Modified: 01/10/2025\n\n\t\t\t\t\tSubmitted by: YuekaiSun\n"
 }
}