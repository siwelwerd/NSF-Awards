{
 "awd_id": "2128145",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: New tools for studying structural and inductive bias in NLP models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2021-08-30",
 "awd_max_amd_letter_date": "2022-07-25",
 "awd_abstract_narration": "Modern natural language processing systems, based on neural networks trained using large amounts of text, are a key part of the infrastructure of the nation and the world. These systems  power practical tools like machine translation, web search, or automatic question answering, as well as research tools that help scientists and policy makers. These language processing models have made enormous progress in many ways, yet systems still fail unexpectedly, their successes cannot be explained, and their blind spots lead to biases. This project develops new tools for studying language models: why they work as well as they do, what their limitations are, and what distortions they introduce into language understanding, with the goal of improved systems and helping mitigate negative impacts on society.\r\n\r\nThis project develops and investigates four kinds of new analytic tools for studying the inductive biases of language models - the structural tendencies that determine what they can learn. The structural transfer-learning paradigm involves training language models on artificial languages that can be manipulated, to see which structural aspects improve performance on natural language. The challenge-task paradigm brings humans in the loop to develop new evaluations to study why and how language processing systems fail, such as on aspect of language that change over time. The new theoretical framework of sensitivity models the complexity of language processing tasks by measuring how responsive the classification is to minor changes in the input, demonstrating which tasks or examples are easy or hard. And new tools are introduced to measure how embeddings of words introduce structural distortions - exaggerations or understatements in word relationships - that can cause models to fail. Understanding the limitations of technology and what makes one system better or one task or dataset harder than another is a crucial step toward building better language processing systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Jurafsky",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel S Jurafsky",
   "pi_email_addr": "jurafsky@stanford.edu",
   "nsf_id": "000140878",
   "pi_start_date": "2021-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 117747.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 382253.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">This project studied large language models. These models are powerful and useful, so it is essential to understand why they work, where they fail, and the biases that are built into&nbsp;&nbsp;them, and&nbsp; to develop improvements and mitigations.&nbsp;&nbsp;We developed new analytic methods, and used them to conduct a series of experiments to uncover properties of language models.&nbsp;&nbsp;For example, we showed some aspects of human language that make&nbsp;&nbsp;language models learn better. One is when the text they are trained on has particular kinds of associations between words that are far apart in text, another is the use of a property of grammar called recursion, and a third has to do with whether the text has the right distribution of frequent and rare words. These facts help us understand language models, but they also have implications for cognitive science and linguistics and our understanding of human language and how people learn and represent it. We also discovered problems with language models. For example, we show that language models are overconfident, asserting answers strongly even when they are wrong. We find that multilingual language models turn out not to be equally multilingual: when they generate text, the grammar is over-influenced by English.&nbsp;&nbsp;And we find that in the health care domain, language models generate text or make decisions that are biased against some social groups.&nbsp;&nbsp;Finally, we developed new mechanisms to improve language models, such as better ways to generate text. The results of our project tell us more about how language models do what they do,&nbsp;provide hypotheses about how people themselves learn and represent language,&nbsp;uncover problems with them, and surface potential improvements.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/01/2024<br>\nModified by: Daniel&nbsp;S&nbsp;Jurafsky</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project studied large language models. These models are powerful and useful, so it is essential to understand why they work, where they fail, and the biases that are built intothem, and to develop improvements and mitigations.We developed new analytic methods, and used them to conduct a series of experiments to uncover properties of language models.For example, we showed some aspects of human language that makelanguage models learn better. One is when the text they are trained on has particular kinds of associations between words that are far apart in text, another is the use of a property of grammar called recursion, and a third has to do with whether the text has the right distribution of frequent and rare words. These facts help us understand language models, but they also have implications for cognitive science and linguistics and our understanding of human language and how people learn and represent it. We also discovered problems with language models. For example, we show that language models are overconfident, asserting answers strongly even when they are wrong. We find that multilingual language models turn out not to be equally multilingual: when they generate text, the grammar is over-influenced by English.And we find that in the health care domain, language models generate text or make decisions that are biased against some social groups.Finally, we developed new mechanisms to improve language models, such as better ways to generate text. The results of our project tell us more about how language models do what they do,provide hypotheses about how people themselves learn and represent language,uncover problems with them, and surface potential improvements.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/01/2024\n\n\t\t\t\t\tSubmitted by: DanielSJurafsky\n"
 }
}