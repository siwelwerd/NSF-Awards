{
 "awd_id": "2101052",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Decentralized Attribution and Secure Training of Generative Models",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925177",
 "po_email": "asquicci@nsf.gov",
 "po_sign_block_name": "Anna Squicciarini",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2021-08-06",
 "awd_max_amd_letter_date": "2021-08-06",
 "awd_abstract_narration": "Generative models describe real-world data distributions such as images, texts, and human motions, and are playing an essential role in a large and growing range of applications from photo editing to natural language processing to autonomous driving. There are two open challenges regarding the development and dissemination of generative models: (1) Adversarial applications of generative models have created concerning socio-technical disturbances (e.g., espionage operations and malicious impersonation); and (2) developing generative models using multiple proprietary datasets (which are needed to reduce data biases) raises privacy concerns about data leakage. Legislative efforts have recently been taken in the wake of these challenges, so far with limited consensus on the format of regulations and knowledge about their technological or social feasibility. To this end, this project will develop new mathematical theories and computational tools to assess the feasibility of two connected solutions to these challenges: Model attribution enforces the owners to be correctly identified based on their generated contents; secure training ensures zero data leakage during the collaborative training of attributable generative models. If successful, the outcomes of the project will provide technical guidance for future regulation design towards secure development and dissemination of generative models. Project results will be disseminated through a project website, open-source software, and public datasets. The impacts of the project will be broadened through educational activities, including new course modules on Artificial Intelligence (AI) security, undergraduate research projects, and outreach to the local community through lab tours, to prepare underrepresented groups with skills to mitigate risks from malicious impersonation and biased data/model representations targeting these groups.\r\n\r\nThis project will focus on synergistic research tasks towards decentralized model attribution and secure training of generative models. In the former, the research team will study the systematic design of a set of user-end generative models that can be certifiably attributed by a set of binary classifiers, which are stored in a decentralized manner to mitigate security risks. The technical feasibility of decentralized attribution will be measured by the tradeoffs between attributability, generation quality, and model capacity. In the latter, the research team will study secure multi-party training of generative models and the associated binary classifiers for attribution. Data privacy and training scalability will be balanced through the design of security-friendly model architectures and learning losses. New knowledge will be created that differentiates this project from the existing state-of-the-art literature in digital forensics and secure computation: (1) Sufficient conditions for decentralized attribution will be developed, which will reveal analytical connections between attributability, data geometry, model architecture, and generation quality. (2) The sufficient conditions will enable estimation of the capacity of attributable models for a given dataset and generation quality tolerance. (3) Feasibility of sublinear secure vector multiplication will be studied, which will fundamentally improve the scalability of secure collaborative training. (4) Privacy-friendly activation and loss functions will be designed for the training of user-end generative models and the classifiers for attribution.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yi",
   "pi_last_name": "Ren",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yi Ren",
   "pi_email_addr": "yiren@asu.edu",
   "nsf_id": "000688341",
   "pi_start_date": "2021-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yezhou",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yezhou Yang",
   "pi_email_addr": "yz.yang@asu.edu",
   "nsf_id": "000733585",
   "pi_start_date": "2021-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ni",
   "pi_last_name": "Trieu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ni Trieu",
   "pi_email_addr": "nitrieu@asu.edu",
   "nsf_id": "000826634",
   "pi_start_date": "2021-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "P.O. Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><h4>Introduction</h4>\r\n<p>This project aimed to address the challenges of securely training generative models and attributing generative models with robust watermarking techniques, advancing our understanding of privacy and attribution in AI systems. Supported by the National Science Foundation (NSF), the work has both intellectual merit and broader impacts, as detailed below.</p>\r\n<h4>Intellectual Merit</h4>\r\n<p>The intellectual merit of this project lies in its significant advancements in generative model training, watermarking techniques, and causal inference methodologies. Key outcomes include:</p>\r\n<ol>\r\n<li>\r\n<p><strong>Novel GenAI watermarking inspired by statistical phyiscs</strong>: We introduced robust methods for watermarking generative AI models using N-point correlation functions (NPCFs) and Quantization Index Modulation (QIM). These methods are highly resistant to geometric attacks and ensure minimal perceptual distortion in outputs.</p>\r\n</li>\r\n<li>\r\n<p><strong>New theories and methods for GenAI secure training</strong>: The project developed privacy-preserving GAN training protocols using multi-party computation (MPC), achieving a significant reduction in computational costs while maintaining data security. Additionally, a novel secure protocol for bivariate causal discovery, AITIA, was proposed, optimizing computational efficiency and accuracy.</p>\r\n</li>\r\n<li>\r\n<p><strong>Scholarly Contributions</strong>: The work resulted in multiple peer-reviewed publications, including presentations at prestigious conferences such as ICML, ICLR, CVPR, PETS, and CCS, contributing to the fields of secure AI, watermarking, and causal inference.</p>\r\n</li>\r\n</ol>\r\n<h4>Broader Impacts</h4>\r\n<p>Beyond academic contributions, the project achieved broader societal benefits by:</p>\r\n<ol>\r\n<li>\r\n<p><strong>Workforce Development</strong>: Supporting the training of four PhD students, equipping them with expertise in secure AI systems, cryptographic methods, and ethical considerations in AI.</p>\r\n</li>\r\n<li>\r\n<p><strong>Community Engagement</strong>: Collaborating with national supercomputing centers through the NAIRR program.</p>\r\n</li>\r\n<li>\r\n<p><strong>Public Dissemination</strong>: Communicating findings through public talks, accessible digital media, and open-source tools such as the AITIA protocol on GitHub.</p>\r\n</li>\r\n</ol>\r\n<h4>Summary of Outcomes</h4>\r\n<p>Throughout the life of the award, the project achieved significant milestones:</p>\r\n<ul>\r\n<li>\r\n<p>Conducted rigorous experiments on watermarking techniques, demonstrating significantly improved tradeoff among GenAI content attribution accuracy, generation quality, and key capacity under combined attacks.</p>\r\n</li>\r\n<li>\r\n<p>Developed privacy-preserving GAN training protocols, reducing training time by up to 16 times compared to full MPC implementations.</p>\r\n</li>\r\n<li>\r\n<p>Proposed AITIA, achieving a 3.6-340 times speedup in secure causal inference computations.</p>\r\n</li>\r\n<li>\r\n<p>Published 12 papers in top-tier conferences, advancing the state of the art in generative AI watermarking, secure training, and causal discovery.</p>\r\n</li>\r\n</ul>\r\n<p>In conclusion, this NSF-funded project has enriched scientific knowledge, trained future researchers, and contributed tools and insights for secure and ethical AI development. These outcomes demonstrate the vital role of fundamental research in addressing complex challenges and inspiring innovation for a better future.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/12/2025<br>\nModified by: Yi&nbsp;Ren</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "Introduction\r\n\n\nThis project aimed to address the challenges of securely training generative models and attributing generative models with robust watermarking techniques, advancing our understanding of privacy and attribution in AI systems. Supported by the National Science Foundation (NSF), the work has both intellectual merit and broader impacts, as detailed below.\r\nIntellectual Merit\r\n\n\nThe intellectual merit of this project lies in its significant advancements in generative model training, watermarking techniques, and causal inference methodologies. Key outcomes include:\r\n\r\n\r\n\n\nNovel GenAI watermarking inspired by statistical phyiscs: We introduced robust methods for watermarking generative AI models using N-point correlation functions (NPCFs) and Quantization Index Modulation (QIM). These methods are highly resistant to geometric attacks and ensure minimal perceptual distortion in outputs.\r\n\r\n\r\n\n\nNew theories and methods for GenAI secure training: The project developed privacy-preserving GAN training protocols using multi-party computation (MPC), achieving a significant reduction in computational costs while maintaining data security. Additionally, a novel secure protocol for bivariate causal discovery, AITIA, was proposed, optimizing computational efficiency and accuracy.\r\n\r\n\r\n\n\nScholarly Contributions: The work resulted in multiple peer-reviewed publications, including presentations at prestigious conferences such as ICML, ICLR, CVPR, PETS, and CCS, contributing to the fields of secure AI, watermarking, and causal inference.\r\n\r\n\r\nBroader Impacts\r\n\n\nBeyond academic contributions, the project achieved broader societal benefits by:\r\n\r\n\r\n\n\nWorkforce Development: Supporting the training of four PhD students, equipping them with expertise in secure AI systems, cryptographic methods, and ethical considerations in AI.\r\n\r\n\r\n\n\nCommunity Engagement: Collaborating with national supercomputing centers through the NAIRR program.\r\n\r\n\r\n\n\nPublic Dissemination: Communicating findings through public talks, accessible digital media, and open-source tools such as the AITIA protocol on GitHub.\r\n\r\n\r\nSummary of Outcomes\r\n\n\nThroughout the life of the award, the project achieved significant milestones:\r\n\r\n\r\n\n\nConducted rigorous experiments on watermarking techniques, demonstrating significantly improved tradeoff among GenAI content attribution accuracy, generation quality, and key capacity under combined attacks.\r\n\r\n\r\n\n\nDeveloped privacy-preserving GAN training protocols, reducing training time by up to 16 times compared to full MPC implementations.\r\n\r\n\r\n\n\nProposed AITIA, achieving a 3.6-340 times speedup in secure causal inference computations.\r\n\r\n\r\n\n\nPublished 12 papers in top-tier conferences, advancing the state of the art in generative AI watermarking, secure training, and causal discovery.\r\n\r\n\r\n\n\nIn conclusion, this NSF-funded project has enriched scientific knowledge, trained future researchers, and contributed tools and insights for secure and ethical AI development. These outcomes demonstrate the vital role of fundamental research in addressing complex challenges and inspiring innovation for a better future.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 01/12/2025\n\n\t\t\t\t\tSubmitted by: YiRen\n"
 }
}