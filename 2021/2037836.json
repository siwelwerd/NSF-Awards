{
 "awd_id": "2037836",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Drone Control in Turbulence via Reinforcement Learning",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2022-01-01",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 255882.0,
 "awd_amount": 255882.0,
 "awd_min_amd_letter_date": "2021-06-11",
 "awd_max_amd_letter_date": "2021-10-14",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project is to improve drone control. Current controllers are effective in specific environments but perform poorly in environments and flight conditions such as turbulence, thereby narrowing the scope in which drones can be used.  This project will advance a plug-and-play solution in which users can focus on higher-level tasks specific to their use case, like obstacle avoidance and route planning. This new controller has broad applications in both commercial and military settings: it enables stable flight across a wide array of environments, expands the flight envelope in turbulent conditions, and allows for longer missions due to increased control efficiency.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project addresses the problem of drone control in turbulence through the development of a reinforcement learning-based flight controller. The project will enlarge the design envelope for quadcopters as well as provide a system and environment for testing reinforcement learning algorithms that can be applied to other control problems. Contemporary systems rely on Proportional Integrative Derivative (PID) controllers as an essential part of stable flight. These PID controllers rely on holistically tuned, static functions to convert maneuvering commands into voltage changes across drone motors to meet the rotor\u2019s targeted rotation speed. In lieu of statically defined PID equations, this novel controller uses a reinforcement learning algorithm, which is a subset of machine learning where an agent is trained to select actions that maximize a reward across an environment. This technique has led to greater-than-human performance across a variety of different control and game theory tasks, but little is known about how these techniques fare when replacing PID control systems. The primary advantage the development of a reinforcement learning controller would have over simpler PID controllers is the ability for the user to view drone control at a higher level of abstraction, thus mitigating the need to focus on the minutiae of flight control for complex missions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Ben",
   "pi_last_name": "Hightower",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ben Hightower",
   "pi_email_addr": "benhightower@gmail.com",
   "nsf_id": "000815555",
   "pi_start_date": "2021-06-11",
   "pi_end_date": "2021-10-14"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anish",
   "pi_last_name": "Sambamurthy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anish Sambamurthy",
   "pi_email_addr": "anish.sambamurthy@gmail.com",
   "nsf_id": "000865042",
   "pi_start_date": "2021-10-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "AIFLY VENTURES",
  "inst_street_address": "19745 NORTHAMPTON DR",
  "inst_street_address_2": "",
  "inst_city_name": "SARATOGA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4088248539",
  "inst_zip_code": "950703333",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "AIFLY LABS INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "EE33TC6Z37P1"
 },
 "perf_inst": {
  "perf_inst_name": "AIFLY VENTURES",
  "perf_str_addr": "19745 NORTHAMPTON DR",
  "perf_city_name": "SARATOGA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950703333",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6856",
   "pgm_ref_txt": "ARTIFICIAL INTELL & COGNIT SCI"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 255882.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-8d9106c1-7fff-c874-f224-e9739bdfb768\"> </span></p>\n<p dir=\"ltr\"><span>Throughout the course of Phase I, our results show that reinforcement learning is a promising approach for improving drone control in turbulent conditions. The controller enabled drones to fly in wind conditions that were up to 2 times stronger than those handled by baseline controllers and had no convergence issues during training. This is a significant achievement as it demonstrates the ability of reinforcement learning to enhance the robustness and performance of drone control systems in challenging environments and can open up expanded opportunities for drone usage.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Transfer of the learned policies to physical drones enables the use of empirical data collection to further improve the simulation fidelity. The use of physical drones allow us to test the robustness of the learned policies and gather data that can be used to improve the simulation. This also allows expansion of the scope of flight conditions simulated and enables tests of robustness in a variety of real-world scenarios.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>These results demonstrate the potential for reinforcement learning to have a significant impact on the drone industry. The ability to fly in stronger winds and adapt to changing conditions can open up new opportunities for drone usage in various industries. The use of reinforcement learning can enhance the robustness and performance of drone control systems in challenging environments, and allows for the drone to continuously learn and adapt to changing conditions. Additionally, the next step of transferring the learned policies to physical drones and gathering data to improve the simulation will require further funding and resources. An enormous amount of data needs to be collected to train the models effectively, and collecting that data is costly, as it requires the use of physical drones and the necessary equipment to gather and process the data.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/14/2023<br>\n\t\t\t\t\tModified by: Anish&nbsp;Sambamurthy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThroughout the course of Phase I, our results show that reinforcement learning is a promising approach for improving drone control in turbulent conditions. The controller enabled drones to fly in wind conditions that were up to 2 times stronger than those handled by baseline controllers and had no convergence issues during training. This is a significant achievement as it demonstrates the ability of reinforcement learning to enhance the robustness and performance of drone control systems in challenging environments and can open up expanded opportunities for drone usage.\n\n \nTransfer of the learned policies to physical drones enables the use of empirical data collection to further improve the simulation fidelity. The use of physical drones allow us to test the robustness of the learned policies and gather data that can be used to improve the simulation. This also allows expansion of the scope of flight conditions simulated and enables tests of robustness in a variety of real-world scenarios.\n\n \nThese results demonstrate the potential for reinforcement learning to have a significant impact on the drone industry. The ability to fly in stronger winds and adapt to changing conditions can open up new opportunities for drone usage in various industries. The use of reinforcement learning can enhance the robustness and performance of drone control systems in challenging environments, and allows for the drone to continuously learn and adapt to changing conditions. Additionally, the next step of transferring the learned policies to physical drones and gathering data to improve the simulation will require further funding and resources. An enormous amount of data needs to be collected to train the models effectively, and collecting that data is costly, as it requires the use of physical drones and the necessary equipment to gather and process the data.\n\n\t\t\t\t\tLast Modified: 01/14/2023\n\n\t\t\t\t\tSubmitted by: Anish Sambamurthy"
 }
}