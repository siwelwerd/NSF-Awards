{
 "awd_id": "2125201",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: RI: Small: NL(V)P: Natural Language (Variety) Processing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 165949.0,
 "awd_amount": 165949.0,
 "awd_min_amd_letter_date": "2021-08-13",
 "awd_max_amd_letter_date": "2021-08-13",
 "awd_abstract_narration": "No language is a monolith. Languages vary richly across countries, regions, social classes, and other factors. Despite recent advances in natural language processing (NLP) technology for translating between languages, answering questions, or engaging in simple conversations, current approaches have largely focused only on \"standard\" varieties of languages. By ignoring other varieties, treating them essentially as statistical noise, current technologies neglect the millions of people who speak these varieties. \r\n\r\nThis project is creating ways to enable language technologies such as translation and question-answering systems, both to process and to generate fine-grained language varieties. The team will develop computational methods to automatically recognize features of different language varieties and then create approaches for integrating such linguistic information into the models powering language technologies. Additionally, the team will design methods to adapt models into varieties for which minimal training data may be available. The resulting suite of general methods will benefit diverse communities and less-privileged populations that speak underserved languages and varieties.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yulia",
   "pi_last_name": "Tsvetkov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yulia Tsvetkov",
   "pi_email_addr": "yuliats@cs.washington.edu",
   "nsf_id": "000728441",
   "pi_start_date": "2021-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 Brooklyn Ave NE",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 165949.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has made substantial progress on three overarching goals aimed at advancing the quality and fairness of language technologies by learning to identify, characterize, process, and generate language varieties and dialects.&nbsp;</p>\r\n<p>First, it introduced several resource and task benchmarks aiming to evaluate the state of the art in language technologies for dialects and underrepresented language varieties, as well as encourage broader NLP community to advance research on low-resource and dialectal NLP. These include&nbsp; benchmarks such as DIALECTBENCH, GlobalBench, BUFFET, FAVA, YORULECT, which together span hundreds of diverse dialects and multilingual tasks. Crucially, these new benchmarks have revealed significant performance and cost gaps between standard and non-standard varieties, highlighting the urgent need for more equitable natural language processing (NLP) approaches.</p>\r\n<p>Second, this project introduced multiple computational methods that effectively incorporate linguistic and social context into large multilingual neural networks, thereby improving their ability to handle diverse language varieties. For instance, a new zero-shot classification framework leverages descriptive label prompts to enable accurate classification of texts from a wide range of dialects and languages, bypassing the need for expensive annotations. A suite of interpretability techniques was developed to spotlight the specific lexical, syntactic, semantic, and stylistic cues that distinguish various dialects, thereby helping identify and reduce spurious correlations in model predictions. These advances have led to more robust cross-lingual and cross-dialect analysis, facilitating a deeper understanding of how dialectal features intersect with social and cultural factors.</p>\r\n<p>Finally, this project established adaptive language generation models that leverage language relationships to optimally serve users of less-common varieties. Key contributions include the development of diffusion-based language models, modular multi-LLM collaboration frameworks, and context-aware decoding methods that significantly increase faithfulness and adaptability in text generation.&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Intellectual Merit:</strong></p>\r\n<p>By systematically confronting the gaps in dialect detection, interpretability, and adaptive generation, the project has significantly advanced multilingual NLP research. It introduced novel models, benchmarks, and interpretability tools that lay the groundwork for more thorough understanding of complex language varieties, offering theoretical and methodological innovations. The project generated 43 publications at top NLP and machine learning conferences and journals, which have already been cited 1K+ times (as of December 2024). Paper awards and distinctions include a Best Social Impact Paper Award at ACL 2024, a Spotlight Paper at COLM 2024, an Oral Presentation (top 1%) at ICLR 2024, a Spotlight Paper at ICLR 2024, and a Best Paper Award at ACL 2023.</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Broader Impacts:</strong></p>\r\n<p><strong>&nbsp;</strong>The findings of this project hold particular promise for global communities that speak less-common dialects or low-resource languages, providing pathways for inclusive technology design in education, healthcare, and beyond. The project also contributed significantly to workforce development, supporting postdocs, graduate students, and undergraduates, many from traditionally underrepresented groups, and disseminating research findings through university courses, international conferences, and industry collaborations. By raising awareness of equity issues within both academic and corporate settings, this project is influencing language modeling design decisions to ensure that the benefits of language technologies extend to speakers of all varieties.</p><br>\n<p>\n Last Modified: 12/25/2024<br>\nModified by: Yulia&nbsp;Tsvetkov</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project has made substantial progress on three overarching goals aimed at advancing the quality and fairness of language technologies by learning to identify, characterize, process, and generate language varieties and dialects.\r\n\n\nFirst, it introduced several resource and task benchmarks aiming to evaluate the state of the art in language technologies for dialects and underrepresented language varieties, as well as encourage broader NLP community to advance research on low-resource and dialectal NLP. These include benchmarks such as DIALECTBENCH, GlobalBench, BUFFET, FAVA, YORULECT, which together span hundreds of diverse dialects and multilingual tasks. Crucially, these new benchmarks have revealed significant performance and cost gaps between standard and non-standard varieties, highlighting the urgent need for more equitable natural language processing (NLP) approaches.\r\n\n\nSecond, this project introduced multiple computational methods that effectively incorporate linguistic and social context into large multilingual neural networks, thereby improving their ability to handle diverse language varieties. For instance, a new zero-shot classification framework leverages descriptive label prompts to enable accurate classification of texts from a wide range of dialects and languages, bypassing the need for expensive annotations. A suite of interpretability techniques was developed to spotlight the specific lexical, syntactic, semantic, and stylistic cues that distinguish various dialects, thereby helping identify and reduce spurious correlations in model predictions. These advances have led to more robust cross-lingual and cross-dialect analysis, facilitating a deeper understanding of how dialectal features intersect with social and cultural factors.\r\n\n\nFinally, this project established adaptive language generation models that leverage language relationships to optimally serve users of less-common varieties. Key contributions include the development of diffusion-based language models, modular multi-LLM collaboration frameworks, and context-aware decoding methods that significantly increase faithfulness and adaptability in text generation.\r\n\n\n\r\n\n\nIntellectual Merit:\r\n\n\nBy systematically confronting the gaps in dialect detection, interpretability, and adaptive generation, the project has significantly advanced multilingual NLP research. It introduced novel models, benchmarks, and interpretability tools that lay the groundwork for more thorough understanding of complex language varieties, offering theoretical and methodological innovations. The project generated 43 publications at top NLP and machine learning conferences and journals, which have already been cited 1K+ times (as of December 2024). Paper awards and distinctions include a Best Social Impact Paper Award at ACL 2024, a Spotlight Paper at COLM 2024, an Oral Presentation (top 1%) at ICLR 2024, a Spotlight Paper at ICLR 2024, and a Best Paper Award at ACL 2023.\r\n\n\n\r\n\n\nBroader Impacts:\r\n\n\nThe findings of this project hold particular promise for global communities that speak less-common dialects or low-resource languages, providing pathways for inclusive technology design in education, healthcare, and beyond. The project also contributed significantly to workforce development, supporting postdocs, graduate students, and undergraduates, many from traditionally underrepresented groups, and disseminating research findings through university courses, international conferences, and industry collaborations. By raising awareness of equity issues within both academic and corporate settings, this project is influencing language modeling design decisions to ensure that the benefits of language technologies extend to speakers of all varieties.\t\t\t\t\tLast Modified: 12/25/2024\n\n\t\t\t\t\tSubmitted by: YuliaTsvetkov\n"
 }
}