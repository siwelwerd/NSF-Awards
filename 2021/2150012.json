{
 "awd_id": "2150012",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Weakly-Supervised Visual Scene Understanding: Combining Images and Videos, and Going Beyond Semantic Tags",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 500499.0,
 "awd_amount": 332659.0,
 "awd_min_amd_letter_date": "2021-09-23",
 "awd_max_amd_letter_date": "2022-05-24",
 "awd_abstract_narration": "The internet provides an endless supply of images and videos, replete with weakly-annotated meta-data such as text tags, GPS coordinates, timestamps, or social media sentiments. This huge resource of visual data provides an opportunity to create scalable and powerful recognition algorithms that do not depend on expensive human annotations. The research component of this project develops novel visual scene understanding algorithms that can effectively learn from such weakly-annotated visual data. The main novelty is to combine both images and videos together. The developed algorithms could have broad impact in numerous fields including AI, security, and agricultural sciences. In addition to scientific impact, the project performs complementary educational and outreach activities. Specifically, it provides mentorship to high school, undergraduate, and graduate students, teaches new undergraduate and graduate computer vision courses that have been lacking at UC Davis, and organizes an international workshop on weakly-supervised visual scene understanding.\r\n\r\nThis project develops novel algorithms to advance weakly-supervised visual scene understanding in two complementary ways: (1) learning jointly with both images and videos to take advantage of their complementarity, and (2) learning from weak supervisory signals that go beyond standard semantic tags such as timestamps, captions, and relative comparisons. Specifically, it investigates novel approaches to advance tasks like fully-automatic video object segmentation, weakly-supervised object detection, unsupervised learning of object categories, and mining of localized patterns in the image/video data that are correlated with the weak supervisory signal. Throughout, the project explores ways to understand and mitigate noise in the weak labels and to overcome the domain differences between images and videos.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yong Jae",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yong Jae Lee",
   "pi_email_addr": "yongjaelee@cs.wisc.edu",
   "nsf_id": "000678292",
   "pi_start_date": "2021-09-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 North Park Street Suite 640",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 15563.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 111193.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 167524.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 38379.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to develop&nbsp;novel weakly-supervised computer vision algorithms. In particular, it investigated two main thrusts: (1) Enhancing the input data by learning jointly from weakly-labeled images and videos, to combine complementary advantages of both domains: diverse and high-quality information from images, and motion and temporal information from videos; and (2) Enhancing the weak supervisory signal by going beyond semantic tags (i.e., object/scene/action/attribute labels) to learn from weak annotations such as captions, timestamps, GPS coordinates, and relative comparisons.</p>\n<p>In terms of intellectual merit, there were broadly three key areas of technical contributions. The first is the development of novel weakly-supervised algorithms for visual understanding. The second is the development of novel algorithms for controllable image generation. The third is the development of novel multimodal vision-language assistants. The work produced 38 peer reviewed papers in top-tier computer vision and machine learning conferences, and new publicly available codebases for the algorithms which are linked from https://pages.cs.wisc.edu/~yongjaelee/. The research results were also regularly presented by the PI at international meetings and university seminars.</p>\n<p>In terms of broader impact, the main project outcomes were graduate student mentorship and training, outreach activities to promote wider participation of underrepresented students in CS and STEM education, and broad scientific impact of the algorithms. In particular, the project helped train MS and PhD students in conducting and presenting research in the topics of this project. Several MS and PhD students completed their degrees and accepted new PhD, postdoc, and research industry positions. The project's outreach component contributed to efforts that widen underrepresented student participation in STEM.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/15/2024<br>\nModified by: Yong Jae&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project was to developnovel weakly-supervised computer vision algorithms. In particular, it investigated two main thrusts: (1) Enhancing the input data by learning jointly from weakly-labeled images and videos, to combine complementary advantages of both domains: diverse and high-quality information from images, and motion and temporal information from videos; and (2) Enhancing the weak supervisory signal by going beyond semantic tags (i.e., object/scene/action/attribute labels) to learn from weak annotations such as captions, timestamps, GPS coordinates, and relative comparisons.\n\n\nIn terms of intellectual merit, there were broadly three key areas of technical contributions. The first is the development of novel weakly-supervised algorithms for visual understanding. The second is the development of novel algorithms for controllable image generation. The third is the development of novel multimodal vision-language assistants. The work produced 38 peer reviewed papers in top-tier computer vision and machine learning conferences, and new publicly available codebases for the algorithms which are linked from https://pages.cs.wisc.edu/~yongjaelee/. The research results were also regularly presented by the PI at international meetings and university seminars.\n\n\nIn terms of broader impact, the main project outcomes were graduate student mentorship and training, outreach activities to promote wider participation of underrepresented students in CS and STEM education, and broad scientific impact of the algorithms. In particular, the project helped train MS and PhD students in conducting and presenting research in the topics of this project. Several MS and PhD students completed their degrees and accepted new PhD, postdoc, and research industry positions. The project's outreach component contributed to efforts that widen underrepresented student participation in STEM.\n\n\n\t\t\t\t\tLast Modified: 07/15/2024\n\n\t\t\t\t\tSubmitted by: Yong JaeLee\n"
 }
}