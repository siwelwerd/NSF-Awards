{
 "awd_id": "2044822",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Supporting Interaction with Dynamic Virtual Reality Experiences for People with Visual Impairments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-08-15",
 "awd_exp_date": "2022-05-31",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 6360.0,
 "awd_min_amd_letter_date": "2021-03-12",
 "awd_max_amd_letter_date": "2022-06-28",
 "awd_abstract_narration": "Virtual reality (VR) is a computer-generated simulation that inserts a person into a 3D environment. VR has applications in education, training, rehabilitation, tourism, and other domains. However, because many VR experiences rely on vision, making them inaccessible to people with visual impairments, research is needed in how to make these experiences more widely available. One key barrier to accessibility is that people using VR frequently interact with objects in the simulation that move. This project will develop methods for incorporating sound, vibration, and other cues to help people with visual impairments locate and interact with virtual objects more effectively. Based on this work, the project team will create guidelines and tools software developers can use to create more accessible VR experiences, allowing people with visual impairments to reap their educational, immersive, and social benefits. The team will work closely with people with visual impairments as both participants and collaborators, including providing opportunities for youth with visual impairments to participate in computing through an annual computing camp.\r\n\r\nThe goal of this project is to help people with visual impairments understand and act on virtual reality environments that have moving targets. To meet these goals, the team will focus on three key components of VR interaction: 1) helping people accurately select nearby targets on a vertical plane, 2) accurately depicting the time-to-contact of approaching targets, and 3) using positional metaphors to indicate the location of multiple virtual targets simultaneously. Addressing each objective requires innovation in how to provide information to users with visual impairments, how to process input from users with visual impairments, and how to model a user's intent. The team will develop the virtual reality experiences using user-centered design approaches and conduct both lab and longitudinal evaluations of the solutions to each challenge. The project's education plan includes teaching computing concepts to youth with visual impairments, disseminating the research to less technology-centric fields, and integrating accessibility topics into computing coursework. This research will address unsolved problems in making virtual reality accessible to people with visual impairments, set guidelines for accessible virtual reality experiences based on empirical evidence, and develop an open-source toolkit.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Rector",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle K Rector",
   "pi_email_addr": "kyle-rector@uiowa.edu",
   "nsf_id": "000754390",
   "pi_start_date": "2021-03-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Iowa",
  "inst_street_address": "105 JESSUP HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IOWA CITY",
  "inst_state_code": "IA",
  "inst_state_name": "Iowa",
  "inst_phone_num": "3193352123",
  "inst_zip_code": "522421316",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IA01",
  "org_lgl_bus_name": "THE UNIVERSITY OF IOWA",
  "org_prnt_uei_num": "",
  "org_uei_num": "Z1H9VJS8NG16"
 },
 "perf_inst": {
  "perf_inst_name": "University of Iowa",
  "perf_str_addr": "2 GILMORE HALL",
  "perf_city_name": "IOWA CITY",
  "perf_st_code": "IA",
  "perf_st_name": "Iowa",
  "perf_zip_code": "522421320",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 6359.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aims to make dynamic virtual reality experiences more accessible to people who are blind through a novel technological design. <span>Specifically, the focus was on dynamic targets. The first project involved presenting targets at different elevations.</span></p>\n<p>To explore finding targets at different elevations, an accessible whac-a-mole game was developed that uses audio cues for gameplay and to provide scaffolds. A depth camera was used to track the player. The player holds onto vibrating game controllers and wears over-ear headphones.</p>\n<p>A 5x5 vertical grid that is positioned in front of the player, centered on their chest. Body measurements were used from the depth camera to size the grid such that a player could reach all 25 targets. A random mole pops out, signified by a spatialized piano note, and the player has two seconds to hit it. The player uses their hearing to determine the position of the mole and their hand to &ldquo;whack&rdquo; the mole. For left-to-right, the piano notes span from the left ear to the right ear. For elevation, it plays piano notes that increase in pitch as the elevation increases. The player can move and rotate their head to better locate the sound.</p>\n<p>For the scaffolds, there are declarative hints and imperative hints. These cues are played after the piano note, but before a mole sound. For the declarative hints, the software states the elevation of the mole with respect to the person&rsquo;s body (i.e., head, neck, chest, stomach, hips). This elevation is spatialized in 5 locations from left to right: 1) left ear, 2) louder in left ear than right ear, 3) both ears equally, 4) louder in right ear than left, and 5) right ear.</p>\n<p>For the imperative hints, the software chooses which hand is closer to the target (i.e., right for right moles, left for left moles, and a random choice between both hands for center moles). It then states the hand and how far it should move up or down (e.g., &ldquo;right up 2,&rdquo; &ldquo;left down 1,&rdquo; where the units are in feet). The command is spatialized from left to right like the declarative hints.</p>\n<p>The game advanced in difficulty by removing scaffolds and speeding up the moles.</p>\n<p>The intellectual merit of this research is that it could impact research in accessible virtual reality experiences and accessible gaming experiences for people with visual impairments.&nbsp;<span>This research has the potential to help people develop virtual reality experiences, to teach body movements for gameplay, real-world exercise, and sports.</span></p>\n<p dir=\"ltr\"><span>The broader impact of the research is that it could provide more accessible virtual reality experiences for people with visual impairments. Female researchers were involved in this work.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/23/2022<br>\n\t\t\t\t\tModified by: Kyle&nbsp;K&nbsp;Rector</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aims to make dynamic virtual reality experiences more accessible to people who are blind through a novel technological design. Specifically, the focus was on dynamic targets. The first project involved presenting targets at different elevations.\n\nTo explore finding targets at different elevations, an accessible whac-a-mole game was developed that uses audio cues for gameplay and to provide scaffolds. A depth camera was used to track the player. The player holds onto vibrating game controllers and wears over-ear headphones.\n\nA 5x5 vertical grid that is positioned in front of the player, centered on their chest. Body measurements were used from the depth camera to size the grid such that a player could reach all 25 targets. A random mole pops out, signified by a spatialized piano note, and the player has two seconds to hit it. The player uses their hearing to determine the position of the mole and their hand to \"whack\" the mole. For left-to-right, the piano notes span from the left ear to the right ear. For elevation, it plays piano notes that increase in pitch as the elevation increases. The player can move and rotate their head to better locate the sound.\n\nFor the scaffolds, there are declarative hints and imperative hints. These cues are played after the piano note, but before a mole sound. For the declarative hints, the software states the elevation of the mole with respect to the person\u2019s body (i.e., head, neck, chest, stomach, hips). This elevation is spatialized in 5 locations from left to right: 1) left ear, 2) louder in left ear than right ear, 3) both ears equally, 4) louder in right ear than left, and 5) right ear.\n\nFor the imperative hints, the software chooses which hand is closer to the target (i.e., right for right moles, left for left moles, and a random choice between both hands for center moles). It then states the hand and how far it should move up or down (e.g., \"right up 2,\" \"left down 1,\" where the units are in feet). The command is spatialized from left to right like the declarative hints.\n\nThe game advanced in difficulty by removing scaffolds and speeding up the moles.\n\nThe intellectual merit of this research is that it could impact research in accessible virtual reality experiences and accessible gaming experiences for people with visual impairments. This research has the potential to help people develop virtual reality experiences, to teach body movements for gameplay, real-world exercise, and sports.\nThe broader impact of the research is that it could provide more accessible virtual reality experiences for people with visual impairments. Female researchers were involved in this work.\n\n\t\t\t\t\tLast Modified: 09/23/2022\n\n\t\t\t\t\tSubmitted by: Kyle K Rector"
 }
}