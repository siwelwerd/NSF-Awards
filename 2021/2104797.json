{
 "awd_id": "2104797",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: Graph Neural Networks: A Feature and Structure Learning Approach",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2021-05-06",
 "awd_max_amd_letter_date": "2021-05-06",
 "awd_abstract_narration": "Many data in the real world can be naturally represented by graphs, such as social networks, citation networks, chemical compounds, and biological networks. The graph structure is an effective way to express relationships among a collection of items. Using graphs to represent these data, we can obtain an advanced understanding of the complex data structures embedded in the raw data. Graph mining using machine learning methods yields many exciting discoveries in various fields. Existing graph deep learning methods, such as graph convolutional networks (GNN), conduct local information aggregation with local structural operations. They usually stack multiple graph convolutional layers to enable a larger receptive field, which is straightforward but can result in several issues, including over-fitting and over-smoothing. These issues are critical to graph neural networks but are not well investigated. The objective of this project is to develop advanced graph learning methods without going deep and complex. This project also facilitates integrating graph learning algorithms into existing curricula of Machine Learning courses at both undergraduate and graduate levels.\r\n\r\n\r\nSpecifically, this project focuses on how to enlarge receptive fields without going deep effectively. This project develops a set of advanced convolution, pooling, and un-pooling operations on graphs. The advanced graph convolution layer utilizes teleport functions to select highly relevant nodes at the global scope. In this layer, a teleport function computes relevance between the center node and other nodes beyond the local neighborhood. The nodes with particular relevance are teleported for the center node, enabling the center node to gather information from a broader neighborhood. The new graph pooling layer uses an attention operation to produce a better-connected coarsened graph with more graph topology information. In particular, it leverages an attention operator to generate ranking scores for node selection, capturing the graph connectivity information. The proposed graph un-pooling layer utilizes an attention operator to initialize restored nodes. Altogether, this project provides a systematic GNN study on graph feature and structure learning and is expected to advance GNNs development significantly.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hongyang",
   "pi_last_name": "Gao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hongyang Gao",
   "pi_email_addr": "hygao@iastate.edu",
   "nsf_id": "000829405",
   "pi_start_date": "2021-05-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Iowa State University",
  "inst_street_address": "1350 BEARDSHEAR HALL",
  "inst_street_address_2": "515 MORRILL ROAD",
  "inst_city_name": "AMES",
  "inst_state_code": "IA",
  "inst_state_name": "Iowa",
  "inst_phone_num": "5152945225",
  "inst_zip_code": "500112103",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IA04",
  "org_lgl_bus_name": "IOWA STATE UNIVERSITY OF SCIENCE AND TECHNOLOGY",
  "org_prnt_uei_num": "DQDBM7FGJPC5",
  "org_uei_num": "DQDBM7FGJPC5"
 },
 "perf_inst": {
  "perf_inst_name": "Iowa State University",
  "perf_str_addr": "1138 Pearson",
  "perf_city_name": "Ames",
  "perf_st_code": "IA",
  "perf_st_name": "Iowa",
  "perf_zip_code": "500112103",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project advanced the field of graph learning by developing novel techniques and theoretical insights, which enhanced the capabilities of graph neural networks (GNNs) without relying on extensive stacking of convolutional layers. One key outcome was the creation of a motif-based graph learning approach, effectively connecting multiple datasets via shared graph motifs. This innovation has shown potential in applications such as drug discovery, where analyzing complex molecular structures is essential.</p>\n<p><br />We also developed new theoretical understandings of deep learning models, particularly in relation to their convergence and optimization under low-data circumstances. This includes insights from our theoretical work, such as <em>A Global Convergence Theory for Deep ReLU Implicit Networks via Over-Parameterization</em> and <em>Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models</em>, which contribute to a deeper understanding of the behavior of deep learning models and their ability to generalize. These advancements offer novel perspectives on optimizing neural networks, especially in few-shot learning scenarios, and highlight the critical role of over-parameterization and model width in improving performance in low-data settings.</p>\n<p><br />For low-data setting, we introduced <em>Meta-AdaM</em>, a meta-learned adaptive optimizer with momentum designed for few-shot learning. Unlike existing methods that focus on model initialization or learning rates, Meta-AdaM incorporates weight-update history and momentum to improve convergence in low-data scenarios. The double look-ahead mechanism enables rapid convergence, similar to many-shot settings. Experiments on benchmark datasets demonstrate its effectiveness in few-shot tasks.</p>\n<p>Educationally, project findings were integrated into the advanced deep learning curriculum (COMS 673), offering students hands-on experience with graph learning techniques. The project supported the training of graduate students, including one who went on to a faculty position, highlighting its role in fostering the next generation of researchers. Overall, this project has made lasting contributions to graph learning research, interdisciplinary applications, and advanced educational experiences.</p><br>\n<p>\n Last Modified: 11/11/2024<br>\nModified by: Hongyang&nbsp;Gao</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project advanced the field of graph learning by developing novel techniques and theoretical insights, which enhanced the capabilities of graph neural networks (GNNs) without relying on extensive stacking of convolutional layers. One key outcome was the creation of a motif-based graph learning approach, effectively connecting multiple datasets via shared graph motifs. This innovation has shown potential in applications such as drug discovery, where analyzing complex molecular structures is essential.\n\n\n\nWe also developed new theoretical understandings of deep learning models, particularly in relation to their convergence and optimization under low-data circumstances. This includes insights from our theoretical work, such as A Global Convergence Theory for Deep ReLU Implicit Networks via Over-Parameterization and Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models, which contribute to a deeper understanding of the behavior of deep learning models and their ability to generalize. These advancements offer novel perspectives on optimizing neural networks, especially in few-shot learning scenarios, and highlight the critical role of over-parameterization and model width in improving performance in low-data settings.\n\n\n\nFor low-data setting, we introduced Meta-AdaM, a meta-learned adaptive optimizer with momentum designed for few-shot learning. Unlike existing methods that focus on model initialization or learning rates, Meta-AdaM incorporates weight-update history and momentum to improve convergence in low-data scenarios. The double look-ahead mechanism enables rapid convergence, similar to many-shot settings. Experiments on benchmark datasets demonstrate its effectiveness in few-shot tasks.\n\n\nEducationally, project findings were integrated into the advanced deep learning curriculum (COMS 673), offering students hands-on experience with graph learning techniques. The project supported the training of graduate students, including one who went on to a faculty position, highlighting its role in fostering the next generation of researchers. Overall, this project has made lasting contributions to graph learning research, interdisciplinary applications, and advanced educational experiences.\t\t\t\t\tLast Modified: 11/11/2024\n\n\t\t\t\t\tSubmitted by: HongyangGao\n"
 }
}