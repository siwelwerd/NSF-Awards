{
 "awd_id": "2053423",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Fusing Massive Disparate Data and Fast Surrogate Models for Probabilistic Quantification of Uncertain Hazards",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 140000.0,
 "awd_amount": 140000.0,
 "awd_min_amd_letter_date": "2021-03-11",
 "awd_max_amd_letter_date": "2023-05-10",
 "awd_abstract_narration": "Mitigating the impact of natural hazards, such as volcanic eruptions, earthquakes, or infectious diseases, rests on our ability to accurately quantify hazard risks in advance of their occurrence. This project will tackle this challenge and develop a new computationally feasible framework to integrate disparate field observations and computer simulations.  The new framework will deliver substantial upgrades in computational efficiency for natural hazard quantification. One testbed will be the 2018 eruption of the Kilauea Volcano in Hawaii, which injured 23 people and destroyed more than 700 dwellings. For this event, extensive field observations from disparate sources, such as radar satellites, global navigation satellite system receivers, borehole tiltmeters, and seismometers, as well as large-scale computer simulations, will be used to analyze methods for volcanic hazard quantification. The methods developed in the project will be implemented in open-source software available to a wide community of scientists and engineers. The project is complemented by training for both graduate and undergraduate students.\r\n                                                                            \r\nThe first major roadblock for precisely quantifying uncertain natural hazards is the computational scalability of computer simulations, as they often require the numerical solution of partial differential equations on massive spatio-temporal domains with multi-dimensional input. This challenge will be overcome by developing Gaussian process (GP) emulators as a computationally feasible surrogate model to approximate outcomes of computer experiments. This approach is appealing because it not only includes parallel predictions with linear computational order with respect to the number of coordinates, but it also leverages the correlation between coordinates to enable fast predictive sampling. The second computational challenge is in fusing disparate data from multiple sources to calibrate physical models. The project will address this challenge by quantifying uncertainty in data processing and estimating the discrepancy between the physical model and reality to allow for data integration. While this project focuses on applications in natural hazard quantification, the new GP emulator, computational tools for model calibration, and data integration methods will more generally extend the applicability of data science and machine learning algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mengyang",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mengyang Gu",
   "pi_email_addr": "mengyang@pstat.ucsb.edu",
   "nsf_id": "000744583",
   "pi_start_date": "2021-03-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "3227 Cheadle Hall",
  "perf_city_name": "Santa Barbara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931062050",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 47846.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 49281.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 42873.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has two main aims to advance statistical estimation and uncertainty quantification. The first aim is to develop accurate surrogate models or emulators for approximating expensive computer simulations with massive data, widely used for reproducing complex systems in scientific discovery processes. Second, as modern measurements can have different types or sources of data, the second aim is to integrate disparate data for data inversion and model calibration. For both aims, increasing the computational scalability of the algorithm without sacrificing accuracy in prediction and uncertainty quantification is a focal point. These two aims are motivated by various real-world applications in physics, materials science, and epidemiology.</p>\n<p>&nbsp;</p>\n<p>This project generated three main findings toward the first aim. First, estimated uncertainty from probabilistic models can be used to determine whether additional samples are required in specific input regions to control the predictive error in predictions. The adaptive design criterion developed in this project can not only reduce the number of samples to be collected, but also ensure the predictive error is reliably controlled (see the publication with doi:10.1063/5.0121805). Both are particularly useful for predicting nonlinear maps with a large input space. Second, input augmentation and decomposition can enhance the predictive accuracy or reduce the computational cost of surrogate models. For instance, using a derived input based on the free input variables enhances the predictive accuracy of the surrogate model, which is used to emulate elastic continuum mechanical models of ground deformation due to pressure changes of magma reservoirs (doi:10.1029/2024JH000161). Third, large matrix inversion and multiplication represent a key computation bottleneck in statistics and domain science. This project integrates fast and exact algorithms, such as the Kalman filter, fast Fourier transform, and general Schur algorithm in statistical models and tools. These algorithms motivate the development of new fast algorithms, such as fast matrix multiplication algorithms with applications in estimating particle interaction kernel functions (doi:10.51387/22-NEJSDS13).</p>\n<p>&nbsp;</p>\n<p>This project led to two major outcomes for the second aim. First, this project develops new approaches for integrating disparate data from multiple sources for model calibration (doi:10.1080/00401706.2023.2182365), and these methods are implemented in an R package for data inversion and model calibration (doi: 10.32614/RJ-2023-085). This package integrates both scalar and vector surrogate models, such as the parallel partial Gaussian process surrogate, and enables the users to emulate expensive computer models with massive outcomes and calibrate their models by field data in one line of code. Second, this project develops new approaches for constructing probabilistic models and fast algorithms to automate data inversion. For instance, differential dynamic microscopy (DDM) is a popular physical approach that uses the Fourier basis method to extract information from optical microscopy videos. However, a Fourier basis range must be chosen in a case-by-case manner. This project develops uncertainty quantification methods (doi:10.1103/PhysRevE.104.034610) and more efficient estimators that naturally weigh the information on&nbsp; Fourier basis functions to improve DDM. Furthermore, the general Schur algorithm is used for accelerating the computation without approximation.</p>\n<p>&nbsp;</p>\n<p>This project led to the development of several new packages, such as DDM-UQ and SKFCPD, available on GitHub and CRAN, respectively, and substantial improvement of the RobustGaSP package available in CRAN (for R version) and GitHub (for MATLAB and Python versions), and RobustCalibration available in CRAN. These new tools of computational model emulation, calibration, and change detection have broad impacts in a wide range of areas, including geophysics, soft condensed matter physics, epidemiology, materials sciences, and power system management. Furthermore, the project supports the fundamental development of fast and exact algorithms for matrix inversion and multiplication, and the preliminary results may help overcome the computational bottleneck of surrogate models without sacrificing accuracy.</p><br>\n<p>\n Last Modified: 08/26/2024<br>\nModified by: Mengyang&nbsp;Gu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project has two main aims to advance statistical estimation and uncertainty quantification. The first aim is to develop accurate surrogate models or emulators for approximating expensive computer simulations with massive data, widely used for reproducing complex systems in scientific discovery processes. Second, as modern measurements can have different types or sources of data, the second aim is to integrate disparate data for data inversion and model calibration. For both aims, increasing the computational scalability of the algorithm without sacrificing accuracy in prediction and uncertainty quantification is a focal point. These two aims are motivated by various real-world applications in physics, materials science, and epidemiology.\n\n\n\n\n\nThis project generated three main findings toward the first aim. First, estimated uncertainty from probabilistic models can be used to determine whether additional samples are required in specific input regions to control the predictive error in predictions. The adaptive design criterion developed in this project can not only reduce the number of samples to be collected, but also ensure the predictive error is reliably controlled (see the publication with doi:10.1063/5.0121805). Both are particularly useful for predicting nonlinear maps with a large input space. Second, input augmentation and decomposition can enhance the predictive accuracy or reduce the computational cost of surrogate models. For instance, using a derived input based on the free input variables enhances the predictive accuracy of the surrogate model, which is used to emulate elastic continuum mechanical models of ground deformation due to pressure changes of magma reservoirs (doi:10.1029/2024JH000161). Third, large matrix inversion and multiplication represent a key computation bottleneck in statistics and domain science. This project integrates fast and exact algorithms, such as the Kalman filter, fast Fourier transform, and general Schur algorithm in statistical models and tools. These algorithms motivate the development of new fast algorithms, such as fast matrix multiplication algorithms with applications in estimating particle interaction kernel functions (doi:10.51387/22-NEJSDS13).\n\n\n\n\n\nThis project led to two major outcomes for the second aim. First, this project develops new approaches for integrating disparate data from multiple sources for model calibration (doi:10.1080/00401706.2023.2182365), and these methods are implemented in an R package for data inversion and model calibration (doi: 10.32614/RJ-2023-085). This package integrates both scalar and vector surrogate models, such as the parallel partial Gaussian process surrogate, and enables the users to emulate expensive computer models with massive outcomes and calibrate their models by field data in one line of code. Second, this project develops new approaches for constructing probabilistic models and fast algorithms to automate data inversion. For instance, differential dynamic microscopy (DDM) is a popular physical approach that uses the Fourier basis method to extract information from optical microscopy videos. However, a Fourier basis range must be chosen in a case-by-case manner. This project develops uncertainty quantification methods (doi:10.1103/PhysRevE.104.034610) and more efficient estimators that naturally weigh the information on Fourier basis functions to improve DDM. Furthermore, the general Schur algorithm is used for accelerating the computation without approximation.\n\n\n\n\n\nThis project led to the development of several new packages, such as DDM-UQ and SKFCPD, available on GitHub and CRAN, respectively, and substantial improvement of the RobustGaSP package available in CRAN (for R version) and GitHub (for MATLAB and Python versions), and RobustCalibration available in CRAN. These new tools of computational model emulation, calibration, and change detection have broad impacts in a wide range of areas, including geophysics, soft condensed matter physics, epidemiology, materials sciences, and power system management. Furthermore, the project supports the fundamental development of fast and exact algorithms for matrix inversion and multiplication, and the preliminary results may help overcome the computational bottleneck of surrogate models without sacrificing accuracy.\t\t\t\t\tLast Modified: 08/26/2024\n\n\t\t\t\t\tSubmitted by: MengyangGu\n"
 }
}