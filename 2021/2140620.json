{
 "awd_id": "2140620",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Augmented 360 Video for Situation Awareness in Firefighting",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2021-07-28",
 "awd_max_amd_letter_date": "2021-07-28",
 "awd_abstract_narration": "Fire incidents have caused substantial injuries, illness, and death on both firefighters and civilians. Poor communications between commanders in the control center and firefighters working at emergency sites are frequently cited as the determining factor in fatality and loss reports of firefighter operations. Traditionally, remote commanders visualize emergency sites and lead the operation using videos captured by helmet cameras of firefighters. Unfortunately, these video systems suffer from a fundamental problem, i.e., commanders are only able to see a single view of the emergency site at a time. This limitation restricts the situation awareness of commanders and leads to productivity and safety issues such as miscommunication of locations and failure to identify dangerous events. This project combines 360-degree videos and augmented reality to enable remote commanders to achieve 360-degree situation awareness of the entire emergency site in all viewing directions and enhance firefighting productivity and safety. The 360-degree video viewing benefits various emergency response communities in planning and training. Other research outcomes, including open datasets and software, are widely disseminated through publications, presentations, and websites to contribute to the computer science and engineering communities.  Educational activities including undergraduate research as well as fire safety training for K-12 students are also planned to enhance the impacts of this project. \r\n \r\nThis project will design and develop augmented 360 video technology to enable remote commanders to switch viewports within a 360-degree scene and visualize machine-detected events of interest. It will investigate an augmented 360 video viewing system to enable panoramic situation awareness for incident command in firefighter operations through these steps: (1) to address commanders\u2019 needs and requirements for situation awareness in firefighter videos, interviews will be performed to identify and categorize important objects and events in firefighter response and their technology preferences; (2) to fill in the knowledge gap of how to enable automatic machine detection of important events in firefighter videos, a view of interest detection model will be designed detects target objects and events; and (3) to provide panoramic situation awareness to commanders, an augmented 360 video viewing system for remote incident command will be developed and evaluated through a field study.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhisheng",
   "pi_last_name": "Yan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhisheng Yan",
   "pi_email_addr": "zyan4@gmu.edu",
   "nsf_id": "000755484",
   "pi_start_date": "2021-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Mason University",
  "inst_street_address": "4400 UNIVERSITY DR",
  "inst_street_address_2": "",
  "inst_city_name": "FAIRFAX",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7039932295",
  "inst_zip_code": "220304422",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "VA11",
  "org_lgl_bus_name": "GEORGE MASON UNIVERSITY",
  "org_prnt_uei_num": "H4NRWLFCDF43",
  "org_uei_num": "EADLFP7Z72E5"
 },
 "perf_inst": {
  "perf_inst_name": "George Mason University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "220304422",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "VA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9594bd1b-7fff-209f-1db0-c25aabd3a7ce\"> </span></p>\r\n<p><span id=\"docs-internal-guid-185969f3-7fff-acbc-ef61-0dd2504e1227\"> </span></p>\r\n<p dir=\"ltr\"><span>The lack of situation awareness for remote commanders has caused firefighting command issues such as miscommunication of locations and failure to identify dangerous events. Live 2D video-based command systems have been introduced to improve traditional voice-based systems by allowing remote commanders to visualize emergency scenes and lead the operation. However, commanders are only able to see a single view of the scene at a time. In this project, we propose augmented 360 videos that enable remote commanders to interact with the entire panoramic scene and visualize objects of interest outside of their current view. The key challenge is how to detect important objects on 360 videos. Traditional video analytics models designed for 2D everyday videos do not work well for 360 firefighting videos. New research is needed to find unique solutions.</span></p>\r\n<p dir=\"ltr\"><span>In order to understand the needs of incident commanders in visualizing objects of interest in firefighting, we first conducted an interview with commanders from the Illinois Fire Service Institute (IFSI) and summarized the objects, actions, and events of interest as well as their priorities. After two site visits to IFSI, we created the first 360 video dataset for firefighting operations by utilizing our customized annotation tool for 360 video object detection which can address the unique distortion of objects in 360 videos.&nbsp;</span></p>\r\n<p>To identify the gap between existing 2D object detectors and our 360 video object detector, we conducted a study to investigate the model accuracy and computation complexity of existing object detectors on 360 firefighting videos. We then designed a low-latency high-accuracy object detection approach for 360 firefighter response videos, achieving 4x speedup, 25% memory usage reduction, and slightly better accuracy, compared to state-of-the-art approaches.</p>\r\n<p dir=\"ltr\"><span>We worked with researchers and firefighters from IFSI to evaluate the proposed augmented 360 videos using the Viewing and Query Service (VQS) tool we built. The domain experts were generally satisfied with the interface, and the object and action detection results of VQS and would like to use the technologies in their firefighter training sessions for debriefing purposes. We further extended the VQS tool into a low-latency 360 video analytics framework for future services and applications. In addition to the VQS tool, we have created a network wifi heatmap for IFSI to provide information to the IFSI leadership what kind of outdoor bandwidth they can expect when considering streaming of 360 videos for real-time viewing of training exercises.&nbsp;</span></p>\r\n<p><span>Regarding the broader impacts, this project generated a unique 360 firefighting video dataset and the VQS tool that has been provided to stakeholders in UIUC, GMU, and IFSI. The annotation tool for object detection on 360 videos has been made available to the public at GitHub. Other research algorithms and results of this project were disseminated at reputed conferences and journals as well as invited talks, conference presentations, and an interdisciplinary retreat held at IFSI in 2023. These resources could provide an opportunity for firefighting communities to improving their everyday routines. All publicly available project outcomes including published papers and GitHub research repositories are summarized on the project website at </span><a href=\"https://mason.gmu.edu/~zyan4/projects/eager2021\"><span>https://mason.gmu.edu/~zyan4/projects/eager2021</span></a><span>. Furthermore, outcomes resulting from this research were incorporated into graduate courses, such as Immersive Media Analytics and Computing at GMU and Advanced Topics of IoT at UIUC, to enrich the student learning experience through real-world examples.</span></p><br>\n<p>\n Last Modified: 01/07/2025<br>\nModified by: Zhisheng&nbsp;Yan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\n \r\n\n\nThe lack of situation awareness for remote commanders has caused firefighting command issues such as miscommunication of locations and failure to identify dangerous events. Live 2D video-based command systems have been introduced to improve traditional voice-based systems by allowing remote commanders to visualize emergency scenes and lead the operation. However, commanders are only able to see a single view of the scene at a time. In this project, we propose augmented 360 videos that enable remote commanders to interact with the entire panoramic scene and visualize objects of interest outside of their current view. The key challenge is how to detect important objects on 360 videos. Traditional video analytics models designed for 2D everyday videos do not work well for 360 firefighting videos. New research is needed to find unique solutions.\r\n\n\nIn order to understand the needs of incident commanders in visualizing objects of interest in firefighting, we first conducted an interview with commanders from the Illinois Fire Service Institute (IFSI) and summarized the objects, actions, and events of interest as well as their priorities. After two site visits to IFSI, we created the first 360 video dataset for firefighting operations by utilizing our customized annotation tool for 360 video object detection which can address the unique distortion of objects in 360 videos.\r\n\n\nTo identify the gap between existing 2D object detectors and our 360 video object detector, we conducted a study to investigate the model accuracy and computation complexity of existing object detectors on 360 firefighting videos. We then designed a low-latency high-accuracy object detection approach for 360 firefighter response videos, achieving 4x speedup, 25% memory usage reduction, and slightly better accuracy, compared to state-of-the-art approaches.\r\n\n\nWe worked with researchers and firefighters from IFSI to evaluate the proposed augmented 360 videos using the Viewing and Query Service (VQS) tool we built. The domain experts were generally satisfied with the interface, and the object and action detection results of VQS and would like to use the technologies in their firefighter training sessions for debriefing purposes. We further extended the VQS tool into a low-latency 360 video analytics framework for future services and applications. In addition to the VQS tool, we have created a network wifi heatmap for IFSI to provide information to the IFSI leadership what kind of outdoor bandwidth they can expect when considering streaming of 360 videos for real-time viewing of training exercises.\r\n\n\nRegarding the broader impacts, this project generated a unique 360 firefighting video dataset and the VQS tool that has been provided to stakeholders in UIUC, GMU, and IFSI. The annotation tool for object detection on 360 videos has been made available to the public at GitHub. Other research algorithms and results of this project were disseminated at reputed conferences and journals as well as invited talks, conference presentations, and an interdisciplinary retreat held at IFSI in 2023. These resources could provide an opportunity for firefighting communities to improving their everyday routines. All publicly available project outcomes including published papers and GitHub research repositories are summarized on the project website at https://mason.gmu.edu/~zyan4/projects/eager2021. Furthermore, outcomes resulting from this research were incorporated into graduate courses, such as Immersive Media Analytics and Computing at GMU and Advanced Topics of IoT at UIUC, to enrich the student learning experience through real-world examples.\t\t\t\t\tLast Modified: 01/07/2025\n\n\t\t\t\t\tSubmitted by: ZhishengYan\n"
 }
}