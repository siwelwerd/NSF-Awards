{
 "awd_id": "2130250",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Creating an Unsupervised Interpretable Representation of the World Through Concept Disentanglement",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2021-11-01",
 "awd_exp_date": "2024-10-31",
 "tot_intn_awd_amt": 169345.0,
 "awd_amount": 169345.0,
 "awd_min_amd_letter_date": "2021-07-07",
 "awd_max_amd_letter_date": "2021-07-07",
 "awd_abstract_narration": "Humans are able to break down a large entity into smaller and simpler concepts, just from having seen many objects and their relationships. Reproducing this type of behavior in a machine learning model has several benefits. In particular, it could lead to computational ways of representing the world that are interpretable yet powerful. These new representations could be used within machine learning algorithms, allowing the algorithms to be more robust and more likely to generalize when the underlying situations change. For instance, if an algorithm has found a collection of parts that an object is typically comprised of, then it can use those parts to identify this type of object even when it is in an unusual setting, or when the object itself is unusual. This new way of representing the world will allow more robust and generalizable machine learning models. This will be particularly helpful for difficult challenges in computer vision, including problems related to vision systems in automated vehicles, analysis of medical time-series, and materials science problems related to the understanding of material properties and discovery of new materials.\r\n\r\nSpecifically, the main goal of this project is learning with interpretable learned concepts using a disentangled neural network. The approach breaks the problem down into three steps that each could be manageable, and each step can be checked and improved independently of the other steps. The steps are to decompose each observation into local parts, identify possible concepts by looking at common relationships between the local parts, and align the proposed concepts, based on their semantic meaning, within a disentangled neural network. The discovered concepts will be interpretable and can be used as features for many downstream tasks. The disentangled neural networks built from these concepts could potentially generalize more easily to new situations than other approaches.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cynthia",
   "pi_last_name": "Rudin",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Cynthia D Rudin",
   "pi_email_addr": "cynthia@cs.duke.edu",
   "nsf_id": "000031776",
   "pi_start_date": "2021-07-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St, Suite 710",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 169345.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-f301471d-7fff-db8c-a297-ba3cb88da11c\"> </span></p>\r\n<p dir=\"ltr\"><span>This project contributed substantially to foundational areas in interpretable machine learning, including algorithms for finding extremely sparse machine learning (ML) models, focusing on algorithms for producing Rashomon sets (sets of near-optimal models), as well as interpretable deep learning models. The project also includes theory and applications of interpretable machine learning.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>Three important forms of extremely sparse models are sparse decision trees, sparse generalized additive models, and risk scores. Decision trees are simple if-then rule models. Generalized additive models and risk scores are used commonly in criminal justice and healthcare, where &ldquo;points&rdquo; are given for each of a set of factors and the total score determines the estimated risk.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>These types of ML models are extremely practical once created, but creating them is challenging, even with fast algorithms. This is due to the &ldquo;interaction bottleneck&rdquo; that arises because humans must interact with machine learning algorithms, and this process can be tedious and time-consuming. In the standard paradigm of creating ML models, one would use a standard ML algorithm, which produces a model, have the human evaluate the model, identify a flaw, and add constraints to the algorithm to force it to generate a different model, iterating this process until the model is acceptable. This standard process is often extremely inefficient, often taking weeks or months per iteration.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>This project provides an alternative to the standard ML paradigm. Algorithms in the new paradigm provide Rashomon sets, which are large sets of good (i.e., near-optimal) models. These Rashomon sets are accessed by human-computer interfaces, which allow humans to explore the Rashomon set directly and choose an acceptable model. The new paradigm requires much more computation up front, since a large set of good models (not just one) needs to be found, stored, and visualized; however, the benefits of the new paradigm are tremendous in resolving the interaction bottleneck. New algorithms in this paradigm were introduced that produce Rashomon sets for sparse decision trees, sparse generalized additive models, and risk scores.&nbsp;&nbsp;&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>Another area of research studied in this project is the question of why interpretable models are as accurate as black box models for datasets with uncertain outcomes (noisy input-output relationships). Results from this project establish a &ldquo;path&rdquo; between increased noise and the existence of simpler-yet-accurate models.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>This project also contributed work on interpretable neural networks. The interpretable neural networks take two forms: prototype models that use case-based reasoning, and a model that considers symmetry. </span></p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/03/2025<br>\nModified by: Cynthia&nbsp;D&nbsp;Rudin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThis project contributed substantially to foundational areas in interpretable machine learning, including algorithms for finding extremely sparse machine learning (ML) models, focusing on algorithms for producing Rashomon sets (sets of near-optimal models), as well as interpretable deep learning models. The project also includes theory and applications of interpretable machine learning.\r\n\n\nThree important forms of extremely sparse models are sparse decision trees, sparse generalized additive models, and risk scores. Decision trees are simple if-then rule models. Generalized additive models and risk scores are used commonly in criminal justice and healthcare, where points are given for each of a set of factors and the total score determines the estimated risk.\r\n\n\nThese types of ML models are extremely practical once created, but creating them is challenging, even with fast algorithms. This is due to the interaction bottleneck that arises because humans must interact with machine learning algorithms, and this process can be tedious and time-consuming. In the standard paradigm of creating ML models, one would use a standard ML algorithm, which produces a model, have the human evaluate the model, identify a flaw, and add constraints to the algorithm to force it to generate a different model, iterating this process until the model is acceptable. This standard process is often extremely inefficient, often taking weeks or months per iteration.\r\n\n\nThis project provides an alternative to the standard ML paradigm. Algorithms in the new paradigm provide Rashomon sets, which are large sets of good (i.e., near-optimal) models. These Rashomon sets are accessed by human-computer interfaces, which allow humans to explore the Rashomon set directly and choose an acceptable model. The new paradigm requires much more computation up front, since a large set of good models (not just one) needs to be found, stored, and visualized; however, the benefits of the new paradigm are tremendous in resolving the interaction bottleneck. New algorithms in this paradigm were introduced that produce Rashomon sets for sparse decision trees, sparse generalized additive models, and risk scores.\r\n\n\nAnother area of research studied in this project is the question of why interpretable models are as accurate as black box models for datasets with uncertain outcomes (noisy input-output relationships). Results from this project establish a path between increased noise and the existence of simpler-yet-accurate models.\r\n\n\nThis project also contributed work on interpretable neural networks. The interpretable neural networks take two forms: prototype models that use case-based reasoning, and a model that considers symmetry. \r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 02/03/2025\n\n\t\t\t\t\tSubmitted by: CynthiaDRudin\n"
 }
}