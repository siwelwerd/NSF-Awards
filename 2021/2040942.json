{
 "awd_id": "2040942",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Organizing Crowd Audits to Detect Bias in Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2021-02-01",
 "awd_exp_date": "2025-01-31",
 "tot_intn_awd_amt": 625000.0,
 "awd_amount": 681000.0,
 "awd_min_amd_letter_date": "2021-01-25",
 "awd_max_amd_letter_date": "2024-04-23",
 "awd_abstract_narration": "Machine learning development teams often struggle to detect and mitigate harmful stereotypes due to their own blind spots, particularly when ML systems are deployed globally. These kinds of representation harms cannot be easily quantified using today\u2019s automated techniques or fairness metrics, and require knowledge of specific social, cultural, and historical contexts. The researchers team will develop a crowd audit service that harnesses the power of volunteers and crowd workers to identify specific cases of bias and unfairness in machine learning systems, generalize those to systematic failures, and synthesize and prioritize these findings in a form that is readily actionable by development teams. Success in the research team\u2019s work will lead to new ways to identify bias and unfairness in machine learning systems, thus improving trust and reliability in these systems. The research team\u2019s work will be shared through a public web site that will make it easy for journalists, policy makers, researchers, and the public at large to engage in understanding algorithmic bias as well as participating in finding unfair behaviors in machine learning systems. \r\n\r\nThis project will explore three major research questions. The first is investigating new techniques for recruiting and incentivizing participation from a diverse crowd. The second is developing new and effective forms of guidance for crowd workers for finding instances and generalizing instances of bias. The third is designing new ways of synthesizing findings from the crowd so that development teams can understand and productively act on. The outputs of this research will include developing a taxonomy of harms; designing and evaluating new kinds of tools to help the crowd tag, discuss, and generalize representation harms; synthesizing new design practices in algorithmic socio-technical platforms in which these platforms can provide users with the opportunity to identify and report observed unfair system behaviors via the platform itself; and gathering new data sets consisting of unfair ML system behaviors identified by the crowd. These datasets will support future research into the design of crowd auditing systems, the nature of representation harms in ML systems, and for future ML teams working on similar kinds of systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Hong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jason Hong",
   "pi_email_addr": "jasonh@cs.cmu.edu",
   "nsf_id": "000255506",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nihar",
   "pi_last_name": "Shah",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nihar Shah",
   "pi_email_addr": "nihars@andrew.cmu.edu",
   "nsf_id": "000753892",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Adam",
   "pi_last_name": "Perer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adam Perer",
   "pi_email_addr": "adamperer@cmu.edu",
   "nsf_id": "000793964",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "Holstein",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth J Holstein",
   "pi_email_addr": "kjholste@cs.cmu.edu",
   "nsf_id": "000818904",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Motahhare",
   "pi_last_name": "Eslami",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Motahhare Eslami",
   "pi_email_addr": "meslami@andrew.cmu.edu",
   "nsf_id": "000832693",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon University",
  "perf_str_addr": "5000 Forbes Ave WQED Building",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  },
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0757",
   "pgm_ref_txt": "COOP PLAN OPs & SERVICES"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "121Z",
   "pgm_ref_txt": "FAIROS RCN"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 641000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-56925479-7fff-fd8e-a8f5-a5fc199c85e5\"> </span></p>\r\n<p dir=\"ltr\"><span>AI systems are being used in education, healthcare, finance, art, social programs, and more. However, a great deal of past work has found that AI systems can inadvertently exhibit algorithmic bias against people based on demographics like race, gender, and age. This bias can result in negative stereotypes, unfair allocation of resources, and poor outcomes due to skewed decisions.</span></p>\r\n<p dir=\"ltr\"><span>Interestingly, people are already organically coming together to audit AI systems for fairness and bias. The primary goals of this project were to (a) better understand how everyday people audit these kinds of systems today, and (b) use this understanding to develop new tools to help people evaluate and audit AI systems. As part of this second thrust, we built WeAudit.org to teach the general public about algorithmic bias and offer a suite of tools to facilitate collective crowd audits, with a focus on text-to-image Generative AI.</span></p>\r\n<p dir=\"ltr\"><span>We conducted numerous interviews, diary studies, and workshops, analyzed thousands of tweets of existing organic audits of systems, and ran controlled experiments to better understand how people naturally think of bias in AI systems and how they work together to find and report on problems. This work resulted in a theoretical model of how audits start and progress, as well as a better understanding of strengths and weaknesses of crowd audits. For example, we found that participants from marginalized gender or sexual orientation groups were more likely to rate images that were biased against their groups as more severely harmful, but belonging to a marginalized race did not have a similar pattern.</span></p>\r\n<p dir=\"ltr\"><span>We also interviewed many commercial AI developers to understand how they audit their systems and see what concerns they have about crowd audits. One example concern is that preliminary public crowd audits of a system may make their company look bad even if the system has not yet been formally deployed.</span></p>\r\n<p dir=\"ltr\"><span>Lastly, we studied how frontline workers use existing algorithms for prioritizing housing for homeless individuals. We used comic book storyboarding for soliciting feedback from frontline workers and from homeless individuals about who should be prioritized for housing. This work helped influence some states in investigating their use of algorithms for human services.</span></p>\r\n<p dir=\"ltr\"><span>To help developers evaluate AI systems, we built and evaluated Deblinder, a tool that lets an analyst gather failure reports for AI systems from the crowd (similar to bug reports but for AI systems), cluster similar failure reports together, and hypothesize about more general failures. We also developed Zeno, which embodies a philosophy of behavior-driven AI development, helping developers evaluate their AI models on specific behaviors (subsets of cases) rather than just a single overall metric. For example, for a computer vision AI model, instead of a single accuracy metric, one could use Zeno to test how well the model works for people with dark skin, light skin, glasses, long hair, and more. Zeno is open source and deployed at</span><a href=\"https://zenoml.com/\"><span> </span></a><a href=\"https://zenoml.com\"><span>https://zenoml.com</span></a><span>, and has been used by over 500 people auditing over 15,000 models in machine learning courses and in industry.</span></p>\r\n<p dir=\"ltr\"><span>We developed WeAudit to facilitate crowd audits for text-to-image Generative AI (</span><a href=\"http://weaudit.org/\"><span>http://weaudit.org</span></a><span>). WeAudit offers several tools for auditing. TAIGA (</span><a href=\"https://taiga.weaudit.org/\"><span>https://taiga.weaudit.org</span></a><span>) generates a set of images using the same prompt and lets you compare generated images against a different prompt. For example, a user can compare \"kindergarten teacher with students\" (which often generates many images of young white women teachers) and \"professors with students\" (which often generates many images of older white men). Users can discuss comparisons using our discussion boards. Another tool is Ouroboros, which uses AI to help audit AI. A user can generate a dozen images from the same prompt and then use computer vision to show users the quantitative distributions of age, gender, and skin color. Lastly, MIRAGE lets people compare what images different AI models generate based on the exact same prompt (</span><a href=\"https://mirage.weaudit.org/\"><span>https://mirage.weaudit.org</span></a><span>).</span></p>\r\n<p><span>Results of this research have been used in several courses at Carnegie Mellon University. Over 200 students have learned about AI bias and participated in AI auditing using WeAudit in courses that the PIs have taught. This research has helped support 4 PhD students</span><span> </span><span>, 6 REU students, and a large number of master&rsquo;s and undergrad students from computer science, psychology, business, design, and more. We have been working with researchers at University of Notre Dame at their Technology Ethics Lab (joint with IBM) on WeAudit, to develop more features and do testing with students. We are continuing this work on crowd auditing with Seoul National University (SNU) as part of a new joint Human-Centered AI Research Center.</span></p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/14/2025<br>\nModified by: Jason&nbsp;Hong</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963246958_comic_storyboarding--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963246958_comic_storyboarding--rgov-800width.png\" title=\"Comicboard storyboarding for early-stage feedback on AI Systems\"><img src=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963246958_comic_storyboarding--rgov-66x44.png\" alt=\"Comicboard storyboarding for early-stage feedback on AI Systems\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We developed a set of comicboards based on eight major stages of an AI system\ufffds design, as shown in the top row. We then used these comicboards as scaffolding to elicit participant\ufffds feedback around each stage. This specific comicboard was created for the model definition stage, based on detailed te</div>\n<div class=\"imageCredit\">TS Kuo, H Shen, J Geum, N Jones, JI Hong, H Zhu, K Holstein, in \ufffdUnderstanding Frontline Workers\ufffd and Unhoused Individuals\ufffd Perspectives on AI Used in Homeless Services\ufffd (CHI 2023)</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Hong\n<div class=\"imageTitle\">Comicboard storyboarding for early-stage feedback on AI Systems</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963068895_weaudit_workflow--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963068895_weaudit_workflow--rgov-800width.jpg\" title=\"WeAudit AI Auditing Workflow\"><img src=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963068895_weaudit_workflow--rgov-66x44.jpg\" alt=\"WeAudit AI Auditing Workflow\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">WeAudit supports a workflow to help people investigate and deliberate on potential biases in AI systems. WeAudit has several features to help with these two activities, including pairwise comparisons of images created from generative AI (a), a history sidebar to see one\ufffds past work (b), worked examp</div>\n<div class=\"imageCredit\">Wesley Hanwen Deng, Motahhare Eslami, Ken Holstein, Jason Hong, in \"WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI\" (CSCW 2025, forthcoming)</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Hong\n<div class=\"imageTitle\">WeAudit AI Auditing Workflow</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963174922_zeno--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963174922_zeno--rgov-800width.png\" title=\"Zeno user interface for evaluating behaviors of AI models\"><img src=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963174922_zeno--rgov-66x44.png\" alt=\"Zeno user interface for evaluating behaviors of AI models\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Zeno (see https://zenoml.com) is a system to help developers evaluate behaviors of their AI models over time. From interviews with many AI developers, we found that in addition to evaluating overall metrics (such as accuracy) over time, developers also evaluated their models on specific behaviors (o</div>\n<div class=\"imageCredit\">\ufffdA Cabrera, E Fu, D Bertucci, K Holstein, A Talwalkar, JI Hong, A Perer, in \ufffdZeno: An interactive framework for behavioral evaluation of machine learning\ufffd (CHI 2023)</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Hong\n<div class=\"imageTitle\">Zeno user interface for evaluating behaviors of AI models</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741962495219_weaudit_interface1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741962495219_weaudit_interface1--rgov-800width.png\" title=\"Screenshot of TAIGA, Tool for Auditing Images Generated from AI\"><img src=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741962495219_weaudit_interface1--rgov-66x44.png\" alt=\"Screenshot of TAIGA, Tool for Auditing Images Generated from AI\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">TAIGA (https://taiga.weaudit.org) is a tool that makes it easy to see multiple images generated from a prompt, and compare against images generated by other prompts. This approach makes it easier to compare and contrast prompts, do counterfactuals, and find potential kinds of bias in generative AI.</div>\n<div class=\"imageCredit\">Wesley Hanwen Deng, Motahhare Eslami, Ken Holstein, Jason Hong, in \"WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI\" (CSCW 2025, forthcoming)</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Hong\n<div class=\"imageTitle\">Screenshot of TAIGA, Tool for Auditing Images Generated from AI</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963015617_weaudit_interface2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963015617_weaudit_interface2--rgov-800width.jpg\" title=\"WeAudit AI Auditing Interfaces\"><img src=\"/por/images/Reports/POR/2025/2040942/2040942_10714031_1741963015617_weaudit_interface2--rgov-66x44.jpg\" alt=\"WeAudit AI Auditing Interfaces\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image shows multiple parts of WeAudit (https://weaudit.org). Users can do side-by-side comparisons with TAIGA (a) and see a history of their past prompts (b). To help support newcomers, users can also see worked examples (c) as well as hints about what other people audited (d). Users can also c</div>\n<div class=\"imageCredit\">Wesley Hanwen Deng, Motahhare Eslami, Ken Holstein, Jason Hong, in \"WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI\" (CSCW 2025, forthcoming)</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Hong\n<div class=\"imageTitle\">WeAudit AI Auditing Interfaces</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nAI systems are being used in education, healthcare, finance, art, social programs, and more. However, a great deal of past work has found that AI systems can inadvertently exhibit algorithmic bias against people based on demographics like race, gender, and age. This bias can result in negative stereotypes, unfair allocation of resources, and poor outcomes due to skewed decisions.\r\n\n\nInterestingly, people are already organically coming together to audit AI systems for fairness and bias. The primary goals of this project were to (a) better understand how everyday people audit these kinds of systems today, and (b) use this understanding to develop new tools to help people evaluate and audit AI systems. As part of this second thrust, we built WeAudit.org to teach the general public about algorithmic bias and offer a suite of tools to facilitate collective crowd audits, with a focus on text-to-image Generative AI.\r\n\n\nWe conducted numerous interviews, diary studies, and workshops, analyzed thousands of tweets of existing organic audits of systems, and ran controlled experiments to better understand how people naturally think of bias in AI systems and how they work together to find and report on problems. This work resulted in a theoretical model of how audits start and progress, as well as a better understanding of strengths and weaknesses of crowd audits. For example, we found that participants from marginalized gender or sexual orientation groups were more likely to rate images that were biased against their groups as more severely harmful, but belonging to a marginalized race did not have a similar pattern.\r\n\n\nWe also interviewed many commercial AI developers to understand how they audit their systems and see what concerns they have about crowd audits. One example concern is that preliminary public crowd audits of a system may make their company look bad even if the system has not yet been formally deployed.\r\n\n\nLastly, we studied how frontline workers use existing algorithms for prioritizing housing for homeless individuals. We used comic book storyboarding for soliciting feedback from frontline workers and from homeless individuals about who should be prioritized for housing. This work helped influence some states in investigating their use of algorithms for human services.\r\n\n\nTo help developers evaluate AI systems, we built and evaluated Deblinder, a tool that lets an analyst gather failure reports for AI systems from the crowd (similar to bug reports but for AI systems), cluster similar failure reports together, and hypothesize about more general failures. We also developed Zeno, which embodies a philosophy of behavior-driven AI development, helping developers evaluate their AI models on specific behaviors (subsets of cases) rather than just a single overall metric. For example, for a computer vision AI model, instead of a single accuracy metric, one could use Zeno to test how well the model works for people with dark skin, light skin, glasses, long hair, and more. Zeno is open source and deployed at https://zenoml.com, and has been used by over 500 people auditing over 15,000 models in machine learning courses and in industry.\r\n\n\nWe developed WeAudit to facilitate crowd audits for text-to-image Generative AI (http://weaudit.org). WeAudit offers several tools for auditing. TAIGA (https://taiga.weaudit.org) generates a set of images using the same prompt and lets you compare generated images against a different prompt. For example, a user can compare \"kindergarten teacher with students\" (which often generates many images of young white women teachers) and \"professors with students\" (which often generates many images of older white men). Users can discuss comparisons using our discussion boards. Another tool is Ouroboros, which uses AI to help audit AI. A user can generate a dozen images from the same prompt and then use computer vision to show users the quantitative distributions of age, gender, and skin color. Lastly, MIRAGE lets people compare what images different AI models generate based on the exact same prompt (https://mirage.weaudit.org).\r\n\n\nResults of this research have been used in several courses at Carnegie Mellon University. Over 200 students have learned about AI bias and participated in AI auditing using WeAudit in courses that the PIs have taught. This research has helped support 4 PhD students , 6 REU students, and a large number of masters and undergrad students from computer science, psychology, business, design, and more. We have been working with researchers at University of Notre Dame at their Technology Ethics Lab (joint with IBM) on WeAudit, to develop more features and do testing with students. We are continuing this work on crowd auditing with Seoul National University (SNU) as part of a new joint Human-Centered AI Research Center.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 03/14/2025\n\n\t\t\t\t\tSubmitted by: JasonHong\n"
 }
}