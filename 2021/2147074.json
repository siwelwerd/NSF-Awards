{
 "awd_id": "2147074",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Emotion-Aware Internet-of-Things Based on Analysis of Speech and Physiological Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2021-08-15",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 497763.0,
 "awd_amount": 278398.0,
 "awd_min_amd_letter_date": "2021-09-23",
 "awd_max_amd_letter_date": "2021-09-23",
 "awd_abstract_narration": "The Internet of Things describes a network of devices capable of connecting hundreds of billions of devices, and then sensing and communicating information required for a wide range of uses such as healthcare, vehicular systems, and industrial environments. As one of the most natural ways of communication, speech will increasingly be used as the primary form of interaction between humans and Internet of Things devices. In recent years, research has shown that there are clear links between the emotional and mental state of an individual and certain patterns in the individual's speech. If these patterns are detected in a timely fashion, it is possible to build emotion-aware Internet of Things solutions. This could be used to adapt a system to better meet the needs of the user, to prevent human error, to detect and prevent potentially malicious user activities, and to initiate medical interventions. Therefore, the overarching goal of this project is to advance speech-based emotion analysis to enable the design of such emotion-aware Internet of Things solutions. The project will also enrich the team's ongoing outreach and educational goals, including mentorship of minority and high-school students, revision of existing and development of new courses aligned with the research challenges in the project, and tight integration of research activities and undergraduate education.\r\n\r\nThe technical challenges in the project are organized into three main thrusts. First, the project will develop and evaluate multi-modal emotion detection systems, where speech analysis is coupled with other physiological metrics such as heart rate, galvanic skin response, or skin temperature, to more accurately determine an individual's emotional state. Second, the work will apply the concept of topic modeling to perform context-aware analysis of speech data, which will also assist in differentiating short-term emotions (i.e., the current mood of an individual) from long-term emotions (e.g., depression). Topic modeling is an increasingly popular technique to learn, recognize, and extract the topics of spoken commands or conversations, providing additional context information for more accurate emotion analysis. The primary outcomes of the first two thrusts will be new insights into the design and development of emotion-aware systems. However, to achieve this goal, a comprehensive database containing speech and physiological data (annotated with the emotional states of the users) will be required, and therefore, the third thrust of the project will build such a database. When completed, this database will contain speech samples and other data from over 500 individuals and the database will be made available to the general scientific community to advance research beyond the team's institution.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christian",
   "pi_last_name": "Poellabauer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christian Poellabauer",
   "pi_email_addr": "cpoellab@cs.fiu.edu",
   "nsf_id": "000128116",
   "pi_start_date": "2021-09-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Florida International University",
  "inst_street_address": "11200 SW 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "MIAMI",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3053482494",
  "inst_zip_code": "331992516",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "FL26",
  "org_lgl_bus_name": "FLORIDA INTERNATIONAL UNIVERSITY",
  "org_prnt_uei_num": "Q3KCVK5S9CP1",
  "org_uei_num": "Q3KCVK5S9CP1"
 },
 "perf_inst": {
  "perf_inst_name": "Florida International University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "331990001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "FL26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 246398.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>It is expected that over the next few years, billions of electronic devices will be connected to the Internet-of-Things (IoT), forming a vast infrastructure of smart systems providing thousands of different applications and services. We will interact with these devices every day, e.g., to monitor and control our health, home, means of transportation, exercise regimen, social interactions, and almost every other aspect of our daily activities. Many of these applications and services will be tightly integrated with everyday products such as household items, entertainment systems, vehicles, homes, security systems, etc. As one of the most natural ways of communication, speech has recently found growing interest as the primary mode of interaction between humans and IoT devices. The topic of speech processing itself has been studied since the 1960s and is very well researched. However, recently, a rapidly expanding body of literature points towards clear links between subtle changes in speech and various mental conditions as well as neurological diseases and injuries. Psychological conditions that express themselves via speech changes include mood disorders, depression, stress (including post-traumatic stress disorder), and anxiety, while neurological problems encompass the many neurodegenerative conditions (such as dementia, Parkinson's, ALS, and Alzheimer's), neurodevelopmental conditions (such as autism spectrum disorder), traumatic brain injuries, but also impaired cognitive abilities that may be due to intoxication or lack of sleep. This opens the door to many new opportunities, e.g., IoT systems based on speech could detect emotional distress in the speaker to prevent human error or malicious activities. A speech based IoT system could be part of a telehealth system that autonomously and transparently works to diagnose a health issue or monitor disease progression or effectiveness of treatment. Emotion-awareness of IoT could also be used to adapt a system&rsquo;s behavior, functionality, and services based on the user&rsquo;s current psychological state, level of anxiety, or frustration to maximize the effectiveness of the human-machine interactions. For example, many software systems, including entertainment, gaming, and learning systems, could monitor the level of user frustration to adapt the virtual experience to maximize satisfaction or to find the optimal balance of excitement and challenge without stress for learning experiences (known as the Yerkes-Dodson Law of Arousal).</p>\n<p>However, a fundamental obstacle to reaching such goals is the lack of data, specifically data that includes not just speech, but other measures and data points that can be used to establish ground truth information, provide context, or make speech-based health analysis more accurate and reliable. Toward this end, the project collected data from 150 adults, each participating in the data collection for about four weeks, during which they answered a spoken daily smartphone mental health assessment using the smartphone&rsquo;s microphone. In addition, they also wore a Fitbit Charge 5 day and night, which was used to collect electrocardiograms (ECG) once per day, activity information (e.g., calories burnt, metabolic rate, step counts, intensity levels, etc.), heart rate data, and sleep data. The project has also resulted in the development of a data collection framework that makes it possible to deploy algorithms on smartphones and Fitbit trackers remotely, dynamically reconfigure the sensing parameters, provide individualized information regarding compliance (e.g., when devices are not being worn sufficiently), pre-process the incoming data (e.g., to remove noise in the data), and analyze the data. Analysis of the data focused initially on the sleep information, which showed very strong correlation with the mental health information provided in the smartphone health assessments, e.g., negative emotional states strongly impact the quality of sleep such as reduced phases of rapid eye movements (REM), increased restlessness, and increased heart rate. Strong negative emotions are also shown to affect several acoustic features found in the voice recordings, such as several of the Mel-frequency cepstral coefficients extracted from the voice data. Overall, the study has resulted in a data set containing data from 95 male and 55 female participants ages 19-51, almost 50,000 hours of recorded daytime activity, almost 3,000 sleep episodes (nights), about 1,500 survey submissions, and, most importantly, about 24 hours of speech recordings in total. After careful cleaning and processing, a subset of the anonymous data set will be released to other researchers in the second half of 2024. To date, the project has produced 13 peer-reviewed publications, with further papers in development. Several graduate and undergraduate students received training during the data collection and analysis phases of the project.</p><br>\n<p>\n Last Modified: 11/10/2023<br>\nModified by: Christian&nbsp;Poellabauer</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIt is expected that over the next few years, billions of electronic devices will be connected to the Internet-of-Things (IoT), forming a vast infrastructure of smart systems providing thousands of different applications and services. We will interact with these devices every day, e.g., to monitor and control our health, home, means of transportation, exercise regimen, social interactions, and almost every other aspect of our daily activities. Many of these applications and services will be tightly integrated with everyday products such as household items, entertainment systems, vehicles, homes, security systems, etc. As one of the most natural ways of communication, speech has recently found growing interest as the primary mode of interaction between humans and IoT devices. The topic of speech processing itself has been studied since the 1960s and is very well researched. However, recently, a rapidly expanding body of literature points towards clear links between subtle changes in speech and various mental conditions as well as neurological diseases and injuries. Psychological conditions that express themselves via speech changes include mood disorders, depression, stress (including post-traumatic stress disorder), and anxiety, while neurological problems encompass the many neurodegenerative conditions (such as dementia, Parkinson's, ALS, and Alzheimer's), neurodevelopmental conditions (such as autism spectrum disorder), traumatic brain injuries, but also impaired cognitive abilities that may be due to intoxication or lack of sleep. This opens the door to many new opportunities, e.g., IoT systems based on speech could detect emotional distress in the speaker to prevent human error or malicious activities. A speech based IoT system could be part of a telehealth system that autonomously and transparently works to diagnose a health issue or monitor disease progression or effectiveness of treatment. Emotion-awareness of IoT could also be used to adapt a systems behavior, functionality, and services based on the users current psychological state, level of anxiety, or frustration to maximize the effectiveness of the human-machine interactions. For example, many software systems, including entertainment, gaming, and learning systems, could monitor the level of user frustration to adapt the virtual experience to maximize satisfaction or to find the optimal balance of excitement and challenge without stress for learning experiences (known as the Yerkes-Dodson Law of Arousal).\n\n\nHowever, a fundamental obstacle to reaching such goals is the lack of data, specifically data that includes not just speech, but other measures and data points that can be used to establish ground truth information, provide context, or make speech-based health analysis more accurate and reliable. Toward this end, the project collected data from 150 adults, each participating in the data collection for about four weeks, during which they answered a spoken daily smartphone mental health assessment using the smartphones microphone. In addition, they also wore a Fitbit Charge 5 day and night, which was used to collect electrocardiograms (ECG) once per day, activity information (e.g., calories burnt, metabolic rate, step counts, intensity levels, etc.), heart rate data, and sleep data. The project has also resulted in the development of a data collection framework that makes it possible to deploy algorithms on smartphones and Fitbit trackers remotely, dynamically reconfigure the sensing parameters, provide individualized information regarding compliance (e.g., when devices are not being worn sufficiently), pre-process the incoming data (e.g., to remove noise in the data), and analyze the data. Analysis of the data focused initially on the sleep information, which showed very strong correlation with the mental health information provided in the smartphone health assessments, e.g., negative emotional states strongly impact the quality of sleep such as reduced phases of rapid eye movements (REM), increased restlessness, and increased heart rate. Strong negative emotions are also shown to affect several acoustic features found in the voice recordings, such as several of the Mel-frequency cepstral coefficients extracted from the voice data. Overall, the study has resulted in a data set containing data from 95 male and 55 female participants ages 19-51, almost 50,000 hours of recorded daytime activity, almost 3,000 sleep episodes (nights), about 1,500 survey submissions, and, most importantly, about 24 hours of speech recordings in total. After careful cleaning and processing, a subset of the anonymous data set will be released to other researchers in the second half of 2024. To date, the project has produced 13 peer-reviewed publications, with further papers in development. Several graduate and undergraduate students received training during the data collection and analysis phases of the project.\t\t\t\t\tLast Modified: 11/10/2023\n\n\t\t\t\t\tSubmitted by: ChristianPoellabauer\n"
 }
}