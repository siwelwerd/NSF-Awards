{
 "awd_id": "2128685",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Workshop on Devices-to-Systems for In-Memory Computing, being held Virtual at the University of Cincinnati, Cincinnati, Ohio, May 11-12, 2021.",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Usha Varshney",
 "awd_eff_date": "2021-04-15",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 25236.0,
 "awd_amount": 25236.0,
 "awd_min_amd_letter_date": "2021-04-15",
 "awd_max_amd_letter_date": "2021-05-21",
 "awd_abstract_narration": "In-memory computing architectures are envisioned to be enablers for implementing data-driven algorithms using Machine Learning and Artificial Intelligence for applications, such as environment monitoring, healthcare, internet of things, mobile devices, communications, drones, and robots having tremendous societal impacts. Currently, a major question lies in understanding relative trade-offs between various available devices, identifying the most promising semiconductor device options, and research directions for developing novel devices for optimum functionality in-memory computing. This NSF workshop is being organized to explore novel devices and processes, enabling circuits and systems, integration and manufacturing, and semiconductor workforce development to establish US long-term leadership. The significance of this workshop includes identifying future nanoscale devices that will offer high-performance, low-cost, engineered and scalable functionality for performing in-memory computing operations at ultra-low power. The workshop will feature a dedicated session and panel discussions on education and workforce development that will include identification of challenges in semiconductor workforce development and future needs, interdisciplinary curriculum development in Electrical Engineering and Computer Science, efficient use of opensource platforms for in-memory devices and supporting technologies, equipment needs, and training of future scientists in these areas. The findings from this workshop will be summarized in a report and posted on workshop website for dissemination to the public at large. Additionally, the findings will be published as a peer-reviewed paper in an open access journal for broader dissemination to the scientific community. \r\n\r\nThe technical significance of this workshop is to provide a platform for discussions on identifying the next generation of nanoscale devices to enable in-memory computing. The overall objective is to explore and identify the scientific issues and technological challenges associated with the underpinnings of in-memory computing. The workshop will invite leading researchers to discuss challenges associated with the currently available devices such as Static Random Access Memory (SRAM) devices, Dynamic Random Access Memory (DRAM) devices, Flash Memory devices, Magnetic Random Access Memory (MRAM) devices, Spin Torque Transfer Random Access Memory (STTRAM) devices, Resistive Random Access Memory (RRAM) devices, Phase Change Memory (PCM) devices, Ferroelectrics-based devices, and other memristive devices to discuss most potential candidates for in-memory computing. Thereafter, the workshop will host panel discussions to identify promising device technologies and other aspects of in-memory computing such as compatibility with Complementary Metal Oxide Semiconductor (CMOS) front end of line (FEOL), back end of line (BEOL), circuits and systems integration, and challenges with semiconductor manufacturing and packaging. The speakers and panelists will be selected from academia, semiconductor industry, federal research laboratories, and small and large business companies. The workshop will be organized on virtual platform and will consist of four technical sessions, one session on education and workforce development, and an open discussion on various other aspects such as role of open access platforms for semiconductor devices and manufacturing, and technology translation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rashmi",
   "pi_last_name": "Jha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rashmi Jha",
   "pi_email_addr": "jhari@ucmail.uc.edu",
   "nsf_id": "000505218",
   "pi_start_date": "2021-04-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hai",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hai Li",
   "pi_email_addr": "hai.li@duke.edu",
   "nsf_id": "000538107",
   "pi_start_date": "2021-04-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dmitri",
   "pi_last_name": "Strukov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dmitri Strukov",
   "pi_email_addr": "strukov@ece.ucsb.edu",
   "nsf_id": "000539747",
   "pi_start_date": "2021-04-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Cincinnati Main Campus",
  "inst_street_address": "2600 CLIFTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CINCINNATI",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "5135564358",
  "inst_zip_code": "452202872",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OH01",
  "org_lgl_bus_name": "CINCINNATI UNIV OF",
  "org_prnt_uei_num": "DZ4YCZ3QSPR5",
  "org_uei_num": "DZ4YCZ3QSPR5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Cincinnati Main Campus",
  "perf_str_addr": "",
  "perf_city_name": "Cininnati",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "452210222",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OH01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "151700",
   "pgm_ele_name": "EPMD-ElectrnPhoton&MagnDevices"
  },
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "103E",
   "pgm_ref_txt": "Energy efficient electronics"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 25236.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The data-driven models for computing have become increasingly important with the advent of Big Data with applications areas ranging from internet of things (IoTs), natural language processing, bio-informatics, climate and natural disaster predictions, communications, and cyber-security. Machine learning and Artificial Intelligence are prominent data-driven approaches for computing that are catching much attention. On one hand, Graphics Processing Units (GPUs, based on advanced CMOS devices with High Bandwidth Memories) are becoming increasingly powerful processors to meet the computing needs of these applications, yet, on the other hand, the fundamental limitations of memory-wall bottleneck still persists crippling the execution of these algorithms and stopping them from reaching their true potentials. As a result of these bottlenecks, the training time and carbon footprint of running these algorithms on GPUs are still much higher than ideally desired. Additionally, edge-computing demands of integrating neuromorphic processors in edge devices (such as sensors) with low-latency and ultra-low power consumptions are not yet met by existing techniques.&nbsp;&nbsp; Because of these limitations, in-memory computing (IMC) approaches have become critical to advancing this area. This workshop focused on identifying the scientific issues and technological challenges associated with the underpinnings of in-memory computing including novel devices and processes, enabling circuits and systems, integration and manufacturing, and semiconductor workforce development in the USA.<strong><em>&nbsp;</em></strong></p>\n<p>The workshop was organized as 5 sessions for a total of 2 days. The session-1 was on CMOS Front-End-Of-Line (FEOL) type devices for in-memory Computing, session-2 was on CMOS Back-End-Of-Line (BEOL) type devices for in-memory Computing, session-3 was Semiconductor Education and Workforce development in the US, session-4 was on Circuit and Systems Aspects of In-Memory Computing, and finally, session-5 was on Semiconductor manufacturing, Packaging, and Integration. The subject matter experts in respective areas were invited as speakers and panelists. The workshop was attended by more than 80 attendees representing academia, government labs, and semiconductor industry in the US.</p>\n<p>&nbsp;For conventional memory technologies (such as SRAMs, DRAMs, Flash), major recommendations included addressing the impact of Process Voltage Temperature (PVT) variabilities and other non-idealities of the devices and understanding its impact on inferencing accuracy, search for completely novel in-memory computing applications that capitalizes on variabilities and can be resilient like brain, increasing the density of emerging devices such as magnetic RAMs, and availability of software tools for in-memory computing, and opportunities for tape-out.</p>\n<p>&nbsp;For emerging memories, major challenges were identified as scaling the array sizes due to the lack of access devices. For RRAM devices, lack of unified device model that accurately encompasses non-idealities such as drifts, variability, reliability under various operating conditions and choice of materials systems was seen as a challenge. On the other hand, these devices also offer some unique features such as short-term states (dynamic memristor), and variability. Though variability is traditionally considered bad for traditional techniques of computing, ML algorithms benefit from certain amount of variability. Scalability of architectures with these devices and more complex datasets need to be explored.&nbsp;</p>\n<p>&nbsp;From session on workforce development, it was concluded that leadership in semiconductor research, design, and manufacturing requires access to the best and brightest scientists and engineers from around the world. In the global race for talent, the U.S. educational system is lagging behind to produce a sufficient number of workers and students with the necessary STEM expertise.&nbsp; With US to keep the dominant market share and maintain the innovation edge, it is imperative to invest in education of highly skilled workforce. The mission is now to dramatically expand the pipeline of talented workers ready to fill the significant workforce deficits reported by companies worldwide. &nbsp;The panel presented the academic and industry perspectives on how to develop and enhance the educational programs in the U.S.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/02/2023<br>\n\t\t\t\t\tModified by: Rashmi&nbsp;Jha</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe data-driven models for computing have become increasingly important with the advent of Big Data with applications areas ranging from internet of things (IoTs), natural language processing, bio-informatics, climate and natural disaster predictions, communications, and cyber-security. Machine learning and Artificial Intelligence are prominent data-driven approaches for computing that are catching much attention. On one hand, Graphics Processing Units (GPUs, based on advanced CMOS devices with High Bandwidth Memories) are becoming increasingly powerful processors to meet the computing needs of these applications, yet, on the other hand, the fundamental limitations of memory-wall bottleneck still persists crippling the execution of these algorithms and stopping them from reaching their true potentials. As a result of these bottlenecks, the training time and carbon footprint of running these algorithms on GPUs are still much higher than ideally desired. Additionally, edge-computing demands of integrating neuromorphic processors in edge devices (such as sensors) with low-latency and ultra-low power consumptions are not yet met by existing techniques.   Because of these limitations, in-memory computing (IMC) approaches have become critical to advancing this area. This workshop focused on identifying the scientific issues and technological challenges associated with the underpinnings of in-memory computing including novel devices and processes, enabling circuits and systems, integration and manufacturing, and semiconductor workforce development in the USA. \n\nThe workshop was organized as 5 sessions for a total of 2 days. The session-1 was on CMOS Front-End-Of-Line (FEOL) type devices for in-memory Computing, session-2 was on CMOS Back-End-Of-Line (BEOL) type devices for in-memory Computing, session-3 was Semiconductor Education and Workforce development in the US, session-4 was on Circuit and Systems Aspects of In-Memory Computing, and finally, session-5 was on Semiconductor manufacturing, Packaging, and Integration. The subject matter experts in respective areas were invited as speakers and panelists. The workshop was attended by more than 80 attendees representing academia, government labs, and semiconductor industry in the US.\n\n For conventional memory technologies (such as SRAMs, DRAMs, Flash), major recommendations included addressing the impact of Process Voltage Temperature (PVT) variabilities and other non-idealities of the devices and understanding its impact on inferencing accuracy, search for completely novel in-memory computing applications that capitalizes on variabilities and can be resilient like brain, increasing the density of emerging devices such as magnetic RAMs, and availability of software tools for in-memory computing, and opportunities for tape-out.\n\n For emerging memories, major challenges were identified as scaling the array sizes due to the lack of access devices. For RRAM devices, lack of unified device model that accurately encompasses non-idealities such as drifts, variability, reliability under various operating conditions and choice of materials systems was seen as a challenge. On the other hand, these devices also offer some unique features such as short-term states (dynamic memristor), and variability. Though variability is traditionally considered bad for traditional techniques of computing, ML algorithms benefit from certain amount of variability. Scalability of architectures with these devices and more complex datasets need to be explored. \n\n From session on workforce development, it was concluded that leadership in semiconductor research, design, and manufacturing requires access to the best and brightest scientists and engineers from around the world. In the global race for talent, the U.S. educational system is lagging behind to produce a sufficient number of workers and students with the necessary STEM expertise.  With US to keep the dominant market share and maintain the innovation edge, it is imperative to invest in education of highly skilled workforce. The mission is now to dramatically expand the pipeline of talented workers ready to fill the significant workforce deficits reported by companies worldwide.  The panel presented the academic and industry perspectives on how to develop and enhance the educational programs in the U.S. \n\n \n\n\t\t\t\t\tLast Modified: 02/02/2023\n\n\t\t\t\t\tSubmitted by: Rashmi Jha"
 }
}