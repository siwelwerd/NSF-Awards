{
 "awd_id": "2101107",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: RUI: Generating Haptics in Telerobotics through Perception Complementarities during Physical Distancing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2021-06-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 174386.0,
 "awd_amount": 174386.0,
 "awd_min_amd_letter_date": "2021-05-13",
 "awd_max_amd_letter_date": "2021-05-13",
 "awd_abstract_narration": "Haptic feedback, the sense of touch and awareness of movement, is a fundamental sensory pathway for human everyday life and plays a particularly essential role in object manipulation. In a physically distanced or miniaturized world, telepresence using robot proxies can assist humans with remote tasks, yet an ongoing issue is the limited haptic capabilities due to the lack of high-fidelity, low-cost haptic interfaces or sensors. As a plethora of information exists in the digital age, mostly through images or video streams, the ability to interpret the sense of touch from vision enables new opportunities. This project explores software solutions that can estimate contact force/torque from visual data and train robots to replicate force sensitive soft body manipulation tasks. This software agent will be trained to transmit real-time force/torque estimation without the need for haptic sensors. Promising application domains include providing surgeons with vision-based haptic feedback during robot-assisted minimally invasive surgery, short term telepresence demand for emergencies or disaster response, and teleoperating a companion robot to perform a haptically enabled virtual hug or a remote handshake. The goal is to create a novel software solution that helps humans stay connected, complete remote tasks through intelligent touch estimation, and lower the barrier to entry for haptic teleoperation by reducing accessibility and hardware requirements. \r\n\r\nThis project leverages preliminary endeavors in vision-based force estimation in robot assisted minimally invasive surgery (RMIS). Meanwhile, the technology will evaluate the accuracy of the artificial force, analyze added benefits or limitations of the artificial haptic information, explore the sim-to-real transfer learning capabilities of the proposed framework through Variational Autoencoder-Generative Adversarial Networks (VAE-GANs) and prioritize cross-robot support by ensuring software compatibility with Robot Operating System (ROS), Collaborative Robotics Toolkit (CRTK) and Asynchronous Multi-Body Framework (AMBF) for dynamic simulation and visualization. The wide spectrum of applications ensure significant potential of this forward looking research to impact telerobotics in soft object manipulation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yun-Hsuan",
   "pi_last_name": "Su",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yun-Hsuan Su",
   "pi_email_addr": "msu@mtholyoke.edu",
   "nsf_id": "000830392",
   "pi_start_date": "2021-05-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Mount Holyoke College",
  "inst_street_address": "50 COLLEGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "SOUTH HADLEY",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135382000",
  "inst_zip_code": "010751423",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MA01",
  "org_lgl_bus_name": "THE TRUSTEES OF MOUNT HOLYOKE COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "XTLND4KQ2QA6"
 },
 "perf_inst": {
  "perf_inst_name": "Mount Holyoke College",
  "perf_str_addr": "50 College Street",
  "perf_city_name": "South Hadley",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010756456",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MA01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9229",
   "pgm_ref_txt": "RES IN UNDERGRAD INST-RESEARCH"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 174386.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Project Overview</strong></p>\n<p>The primary objective of this research was to develop an innovative framework for generating force feedback in teleoperated robotic systems, particularly when traditional haptic hardware is unavailable. This project aimed to support multimodal feedback experiences for human operators during robot manipulation tasks by utilizing indirect force estimation, thereby improving how robots perceive and interact with soft, deformable objects in various task environments.&nbsp;Our research can be divided into three main thrusts:</p>\n<ol>\n<li><strong>Full      Haptic Interfaces</strong>: This      involved developing data-driven approaches to estimate robot-object      interaction dynamics and ensuring the accuracy of multimodal sensory      information by identifying inconsistencies among recorded haptic,      kinematic, and visual data. This enhances the reliability and security of      the teleoperation framework while also recovering lost information in the      presence of noise or disruptions.<br /><br /></li>\n<li><strong>Vision-Inferred      Haptic Feedback</strong>: We investigated how robots      could perform effective haptic-sensitive manipulation of soft objects      without relying on physical haptic sensors, utilizing vision-based force      estimation methods instead. This effort involved extensive simulation      training and the development of strategies to apply this knowledge to      real-world scenarios.<br /><br /></li>\n<li><strong>Haptic      Command Without Local Devices</strong>:      This focused on capturing human motion through visual input and      translating it into robotic movement, enabling robots to replicate the      force and torque exerted by a human operator.<br /><br /></li>\n</ol>\n<p><strong>Key Accomplishments</strong></p>\n<p>Throughout the project period project, significant progress was made:</p>\n<ol>\n<li><strong>Multimodal      Consistency and Perception Complementarities</strong>: We developed a deep learning framework to predict multimodal      data inconsistencies in robot manipulation tasks, improving the robot's      ability to make informed decisions based on available sensory feedback,      cross-modal information inference, noise detection, and distortion      reconstruction. Through this process, we discovered that the quality of      cross-modal prediction heavily depends on how well the available sensory      feedback (in our case, vision) is optimized for haptic relevance.      Specifically, we identified the potential to create a dynamic camera pose      adjustment algorithm that optimizes the likelihood of accurately      predicting interaction forces.</li>\n<li><strong>Digital      Twin Technology</strong>: Our work resulted in the      development of a digital twin of the Raven-II surgical robot, enabling      precise interactions with soft objects in a simulated environment. We      achieved outstanding kinematic synchronization with an accuracy of less      than 2 mm. This technology is essential for safely training and testing      surgical robots. Given that the AMBF is a physics-enabled simulation      platform, future efforts will focus on dynamic synchronization, real-time      tissue contact detection, coordinated camera movements, and autonomous tissue      property identification.</li>\n<li><strong>Improved      Vision-Based Force Estimation Building Blocks</strong>: We made modularized improvments in the vision-based force estimation pipeline in robot-assisted      minimally invasive surgery involves.<br /><br />(a) Image Segmentation: locating the surgical tool-tissue contact,<br />(b) Dynamic Scene Reconstruction: estimate local tissue deformation due to contact,<br />(c) Force Estimation: given prior knowledge of tissue properties, predict the contact force resulting in such level of deformation.<br /><br />For (a), we enhanced the accuracy of surgical tool segmentation in endoscopic images by employing morphological data preprocessing, tailored iterative training methods, which led to better segmentation outcomes. For (b), we implemented a occupancy network-based deep learning model capable of reconstructing complex surgical environments from a single camera perspective, providing reconstructions that can aid surgeons in minimally invasive procedures. Next steps include training the model in high-fidelity simulated datasets with timestamped 3D reconstruction ground truth.<br /><strong><br /></strong></li>\n<li><strong>Frequency      of Vision-Based Force Estimation</strong>:      Our research demonstrated the capability of predicting force at a high      frequency (up to 500 Hz) with minimal error. This advancement is crucial      for enhancing real-time telerobot performances in dynamic and intricate      tasks.&nbsp;<br /><br /></li>\n</ol>\n<p><strong>Broader Impacts</strong></p>\n<p>The outcomes of this project extend beyond the laboratory. By improving robotic haptic feedback systems, we enhance the potential for robots to assist in surgical environments, providing surgeons with better tools for minimally invasive procedures, ultimately leading to improved patient outcomes. The methodologies developed also <strong>contribute to the fields of robotics and artificial intelligence, </strong>paving the way for future innovations.</p>\n<p>Furthermore, we have produced comprehensive timestamped<strong> multimodal robot manipulation datasets</strong> that contain visual, kinematics, and ground truth haptic information, made available to the research community, promoting collaboration and knowledge sharing in robotic manipulation research. All source code and algorithms developed in this project is open source on GitHub. Finally, the <strong>educational experiences provided to numerous undergraduate students</strong> involved in this project fostered the next generation of scientists and engineers, preparing them to contribute to advancements in technology. Several of which presented at IEEE international conference venues, and have decided to continue graduate studies due to the transformative experiences.</p>\n<p>&nbsp;</p>\n<p><strong>Conclusion</strong></p>\n<p>In summary, this project has made significant strides in advancing robotic manipulation through innovative approaches to inferred haptic feedback, benefiting both the academic community and practical applications in medical technology. We identified the critical role of actively seeking available sensory feedback in ways that optimize the cross-modal inference power. We look forward to continuing this research and exploring new frontiers in human-robot interaction in deformable and dynamic environments.</p><br>\n<p>\n Last Modified: 10/29/2024<br>\nModified by: Yun-Hsuan&nbsp;Su</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197529275__41A6105_min--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197529275__41A6105_min--rgov-800width.jpg\" title=\"Dataset collection on abdominal phantom\"><img src=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197529275__41A6105_min--rgov-66x44.jpg\" alt=\"Dataset collection on abdominal phantom\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Melody's undergraduate students collecting multimodal datasets using a handheld surgical instrument on an abdominal phantom.</div>\n<div class=\"imageCredit\">Mount Holyoke College</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Yun-Hsuan&nbsp;Su\n<div class=\"imageTitle\">Dataset collection on abdominal phantom</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730198845584_IMG_20220525_175858--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730198845584_IMG_20220525_175858--rgov-800width.jpg\" title=\"Poster booth discussion at ICRA 2022\"><img src=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730198845584_IMG_20220525_175858--rgov-66x44.jpg\" alt=\"Poster booth discussion at ICRA 2022\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Melody and her research student, Heidi, discussing their work with interested researchers in the field after paper presentation at ICRA 2022.</div>\n<div class=\"imageCredit\">Kevin Huang</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Yun-Hsuan&nbsp;Su\n<div class=\"imageTitle\">Poster booth discussion at ICRA 2022</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730199392615_PXL_20240603_173138435--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730199392615_PXL_20240603_173138435--rgov-800width.jpg\" title=\"ISMR 2024 workshop presentation.\"><img src=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730199392615_PXL_20240603_173138435--rgov-66x44.jpg\" alt=\"ISMR 2024 workshop presentation.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Melody as a ISMR 2024 workshop speaker at The 3rd Holistic Forum of Medical Robotic Junior Professors: From Rehabilitation to Surgical Robots.</div>\n<div class=\"imageCredit\">Andy Lewis</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Yun-Hsuan&nbsp;Su\n<div class=\"imageTitle\">ISMR 2024 workshop presentation.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197758610__41A6088_min--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197758610__41A6088_min--rgov-800width.jpg\" title=\"Fine tuning Raven-II surgical robot parameters.\"><img src=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197758610__41A6088_min--rgov-66x44.jpg\" alt=\"Fine tuning Raven-II surgical robot parameters.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An IMeRo lab student researcher fine-tuning Raven-II surgical robot parameters to minimize trajectory discrepancies with its digital twin.</div>\n<div class=\"imageCredit\">Mount Holyoke College</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Yun-Hsuan&nbsp;Su\n<div class=\"imageTitle\">Fine tuning Raven-II surgical robot parameters.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197173896__41A6099_min--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197173896__41A6099_min--rgov-800width.jpg\" title=\"Experiments on the Raven-II surgical robot.\"><img src=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197173896__41A6099_min--rgov-66x44.jpg\" alt=\"Experiments on the Raven-II surgical robot.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Melody's undergraduate students conducting Raven-II surgical robot experiments in the IMeRo research lab at Mount Holyoke College.</div>\n<div class=\"imageCredit\">Mount Holyoke College</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Yun-Hsuan&nbsp;Su\n<div class=\"imageTitle\">Experiments on the Raven-II surgical robot.</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197367532__41A6124_min--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197367532__41A6124_min--rgov-800width.jpg\" title=\"Students testing the AMBF simulated Raven-II.\"><img src=\"/por/images/Reports/POR/2024/2101107/2101107_10733641_1730197367532__41A6124_min--rgov-66x44.jpg\" alt=\"Students testing the AMBF simulated Raven-II.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Melody's undergraduate students testing out the AMBF simulated Raven-II digital twin in the IMeRo research lab at Mount Holyoke College.</div>\n<div class=\"imageCredit\">Mount Holyoke College</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Yun-Hsuan&nbsp;Su\n<div class=\"imageTitle\">Students testing the AMBF simulated Raven-II.</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nProject Overview\n\n\nThe primary objective of this research was to develop an innovative framework for generating force feedback in teleoperated robotic systems, particularly when traditional haptic hardware is unavailable. This project aimed to support multimodal feedback experiences for human operators during robot manipulation tasks by utilizing indirect force estimation, thereby improving how robots perceive and interact with soft, deformable objects in various task environments.Our research can be divided into three main thrusts:\n\nFull      Haptic Interfaces: This      involved developing data-driven approaches to estimate robot-object      interaction dynamics and ensuring the accuracy of multimodal sensory      information by identifying inconsistencies among recorded haptic,      kinematic, and visual data. This enhances the reliability and security of      the teleoperation framework while also recovering lost information in the      presence of noise or disruptions.\n\n\nVision-Inferred      Haptic Feedback: We investigated how robots      could perform effective haptic-sensitive manipulation of soft objects      without relying on physical haptic sensors, utilizing vision-based force      estimation methods instead. This effort involved extensive simulation      training and the development of strategies to apply this knowledge to      real-world scenarios.\n\n\nHaptic      Command Without Local Devices:      This focused on capturing human motion through visual input and      translating it into robotic movement, enabling robots to replicate the      force and torque exerted by a human operator.\n\n\n\n\n\nKey Accomplishments\n\n\nThroughout the project period project, significant progress was made:\n\nMultimodal      Consistency and Perception Complementarities: We developed a deep learning framework to predict multimodal      data inconsistencies in robot manipulation tasks, improving the robot's      ability to make informed decisions based on available sensory feedback,      cross-modal information inference, noise detection, and distortion      reconstruction. Through this process, we discovered that the quality of      cross-modal prediction heavily depends on how well the available sensory      feedback (in our case, vision) is optimized for haptic relevance.      Specifically, we identified the potential to create a dynamic camera pose      adjustment algorithm that optimizes the likelihood of accurately      predicting interaction forces.\nDigital      Twin Technology: Our work resulted in the      development of a digital twin of the Raven-II surgical robot, enabling      precise interactions with soft objects in a simulated environment. We      achieved outstanding kinematic synchronization with an accuracy of less      than 2 mm. This technology is essential for safely training and testing      surgical robots. Given that the AMBF is a physics-enabled simulation      platform, future efforts will focus on dynamic synchronization, real-time      tissue contact detection, coordinated camera movements, and autonomous tissue      property identification.\nImproved      Vision-Based Force Estimation Building Blocks: We made modularized improvments in the vision-based force estimation pipeline in robot-assisted      minimally invasive surgery involves.\n\n(a) Image Segmentation: locating the surgical tool-tissue contact,\n(b) Dynamic Scene Reconstruction: estimate local tissue deformation due to contact,\n(c) Force Estimation: given prior knowledge of tissue properties, predict the contact force resulting in such level of deformation.\n\nFor (a), we enhanced the accuracy of surgical tool segmentation in endoscopic images by employing morphological data preprocessing, tailored iterative training methods, which led to better segmentation outcomes. For (b), we implemented a occupancy network-based deep learning model capable of reconstructing complex surgical environments from a single camera perspective, providing reconstructions that can aid surgeons in minimally invasive procedures. Next steps include training the model in high-fidelity simulated datasets with timestamped 3D reconstruction ground truth.\n\n\nFrequency      of Vision-Based Force Estimation:      Our research demonstrated the capability of predicting force at a high      frequency (up to 500 Hz) with minimal error. This advancement is crucial      for enhancing real-time telerobot performances in dynamic and intricate      tasks.\n\n\n\n\n\nBroader Impacts\n\n\nThe outcomes of this project extend beyond the laboratory. By improving robotic haptic feedback systems, we enhance the potential for robots to assist in surgical environments, providing surgeons with better tools for minimally invasive procedures, ultimately leading to improved patient outcomes. The methodologies developed also contribute to the fields of robotics and artificial intelligence, paving the way for future innovations.\n\n\nFurthermore, we have produced comprehensive timestamped multimodal robot manipulation datasets that contain visual, kinematics, and ground truth haptic information, made available to the research community, promoting collaboration and knowledge sharing in robotic manipulation research. All source code and algorithms developed in this project is open source on GitHub. Finally, the educational experiences provided to numerous undergraduate students involved in this project fostered the next generation of scientists and engineers, preparing them to contribute to advancements in technology. Several of which presented at IEEE international conference venues, and have decided to continue graduate studies due to the transformative experiences.\n\n\n\n\n\nConclusion\n\n\nIn summary, this project has made significant strides in advancing robotic manipulation through innovative approaches to inferred haptic feedback, benefiting both the academic community and practical applications in medical technology. We identified the critical role of actively seeking available sensory feedback in ways that optimize the cross-modal inference power. We look forward to continuing this research and exploring new frontiers in human-robot interaction in deformable and dynamic environments.\t\t\t\t\tLast Modified: 10/29/2024\n\n\t\t\t\t\tSubmitted by: Yun-HsuanSu\n"
 }
}