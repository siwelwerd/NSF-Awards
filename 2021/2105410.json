{
 "awd_id": "2105410",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "The Temporal Organization of Prosody in Multimodal Speech and Co-speech Gestures",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": "7032927376",
 "po_email": "jwmirand@nsf.gov",
 "po_sign_block_name": "Josie Welkom Miranda",
 "awd_eff_date": "2022-01-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 138000.0,
 "awd_amount": 138000.0,
 "awd_min_amd_letter_date": "2021-08-14",
 "awd_max_amd_letter_date": "2021-08-14",
 "awd_abstract_narration": "This award was provided as part of NSF\u2019s Social, Behavioral and Economic Sciences Postdoctoral Research Fellowships (SPRF) program. The goal of the SPRF program is to prepare promising, early career doctoral-level scientists for scientific careers in academia, industry or private sector, and government. SPRF awards involve two years of training under the sponsorship of established scientists and encourage Postdoctoral Fellows to perform independent research. NSF seeks to promote the participation of scientists from all segments of the scientific community, including those from underrepresented groups, in its research programs and activities; the postdoctoral period is considered to be an important level of professional development in attaining this goal. Each Postdoctoral Fellow must address important scientific questions that advance their respective disciplinary fields. Under the sponsorship of Dr. Jelena Krivokapi\u0107 at the University of Michigan, this postdoctoral fellowship award supports an early career scientist examining the nature of coordination between multimodal speech events. In everyday communication, speech and gestures that accompany speech, referred to as co-speech gestures, interact in a principled way. While much of interdisciplinary research has examined the multifaceted nature of verbal and non-verbal behaviors in human communication, our understanding of the multimodal prosodic phenomena remains limited. The goal of the proposed research is to illuminate how language users integrate prosody into different modalities while producing words and phrases. Prosodic structure groups words into larger phrases and expresses prominence (highlighting important or new information). Specifically, this project examines how prosodic information is embodied in co-speech gestures occurring at phrasal edges and under prominence, what coordination patterns of speech and co-speech gestures emerge from the prosodic structure that is specific to a language, and if and how interacting language users exhibit prosodic accommodation in co-speech gestures. By using cutting-edge instruments and analysis tools, multimodal data (pitch, movement of the vocal tract, head, eyebrow, hand) are assessed via concurrent recordings of audio, video, and kinematic information from individual speakers of typologically different languages (English and Korean) participating in various speech tasks including solo and interactive speech activities.\r\n\r\nThe proposed research examines how the speech signal and the accompanying bodily movements produced by a language user are mediated by prosodic structure of the language. This project utilizes cutting-edge techniques to collect the necessary multimodal data and characterizes the temporal relation between co-speech beat gestures, pitch gestures, and consonant and vowel gestures as well as detailing their kinematic properties at phrasal boundaries and under prominence. Further, a series of multimodal experiments is designed for two prosodically different languages\u2014a language with no lexical stress and no pitch accent and a language with stress and marking prominence with pitch accents to reveal the cross-linguistic and/or language-specific aspects of speech and co-speech gesture coordination. This research also conducts a real-time accommodation study designed for pairs of speakers participating in communicative speech tasks, with the broad goal of gaining information about the emergence of prosodic accommodation across modalities. To date, the proposed work is the first to investigate accommodation processes in both speech and co-speech gestures. This work can advance our understanding of the precise nature of the speech and co-speech gesture coordination and of how different modalities are coordinated to embody information grouping and highlighting at the phrasal level, broadening the empirical and theoretical base for models of articulatory organization and linguistic accounts of prosody. The results serve to support the development of theoretical models of prosodic structure of typologically different languages that take into account these different modalities of communication. The proposed work produces a substantial database of the movement signals of multimodal articulators with synchronous audio and video recordings that will be freely distributed to the research and education community. Broadly, this research contributes to knowledge and perspectives for future exploration of multimodality in communication in sign language, education, and engineering and clinical applications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yoonjeong",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yoonjeong Lee",
   "pi_email_addr": "",
   "nsf_id": "000840760",
   "pi_start_date": "2021-08-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jelena",
   "pi_last_name": "Krivokapic",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jelena Krivokapic",
   "pi_email_addr": "",
   "nsf_id": "000674368",
   "pi_start_date": "2021-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Lee, Yoonjeong",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Los Angeles",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "",
  "inst_zip_code": "90095",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan",
  "perf_str_addr": null,
  "perf_city_name": "",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091220",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "040Y00",
   "pgm_ele_name": "(SPRF-FR) SBE Postdoctoral Res"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "7137",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 138000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In everyday communication, speech and gestures that accompany speech, referred to as 'co-speech gestures,' interact in a systematic manner. The primary objective of the proposed research is to investigate how language users seamlessly incorporate linguistic prosody, particularly the organization of linguistic grouping and highlighting, specific to the language being spoken across various modalities and communication contexts. Throughout the duration of the funding period, our work has focused on examining how speech and co-speech gestures align temporally in two linguistically distinct languages, Korean and English through a set of multimodal experiments. Additionally, we have conducted investigations into voice quality across a diverse range of individuals from various communication contexts and cultural and linguistic backgrounds, including English, Korean, Thai, White Hmong, and Gujarati, in order to deepen our comprehension of these complex communication processes.</p>\n<p>Throughout the grant period, we conducted a series of experiments utilizing electromagnetic articulography (EMA) paired with camcorders to capture multimodal physical signals from speakers of American English and Seoul Korean. These experiments aimed to investigate how prosodic information manifests in co-speech gestures occurring at phrasal edges and under prominence, as well as to explore the temporal coordination patterns of speech and co-speech gestures specific to each language.&nbsp;</p>\n<p>The outcomes of our research award have been multifaceted and impactful, spanning various aspects of language communication, with a particular focus on the role of linguistic structure in gesture coordination. Our findings provided insights into the interplay between vocal tract actions and co-speech gestures, particularly in relation to the specific linguistic structure exhibited by each language. Notably, our results demonstrate synchronous coordination between co-speech manual beat gestures and both phrase-edge segment and tone gestures in Korean, suggesting that the multimodal expression of phrasal prominence is driven by linguistic structure. We conclude that prosodic structure is expressed through co-speech gestures, indicating linguistically driven coordination rather than biomechanical coupling alone.</p>\n<p>Addressing the challenges of data processing and analysis, we developed a novel analysis tool to facilitate the automatic labeling of complex time-series data, which has broad applications in future research on language multimodality.</p>\n<p>Moreover, our investigation into voice quality among individuals with diverse linguistic backgrounds has yielded valuable insights into communication dynamics. Our findings, drawn from a range of communication contexts and languages such as English, Korean, Thai, White Hmong, and Gujarati, shed light on the critical factors shaping voice variability. We deduce that variations in voice characteristics are influenced by factors such as phonation biology, language-specific phonological patterns, and individual idiosyncrasies.</p>\n<p>Overall, our research has significantly contributed to advancing our understanding of language communication and gesture coordination, with implications for diverse fields such as linguistics, cognitive science, and communication disorders. Through our dissemination efforts, we aim to share our findings with broader communities of interest and continue pushing the boundaries of knowledge in this important area of research. Thus, our work lays the groundwork for future exploration of multimodality in communication across different languages and modalities.</p><br>\n<p>\n Last Modified: 04/30/2024<br>\nModified by: Yoonjeong&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn everyday communication, speech and gestures that accompany speech, referred to as 'co-speech gestures,' interact in a systematic manner. The primary objective of the proposed research is to investigate how language users seamlessly incorporate linguistic prosody, particularly the organization of linguistic grouping and highlighting, specific to the language being spoken across various modalities and communication contexts. Throughout the duration of the funding period, our work has focused on examining how speech and co-speech gestures align temporally in two linguistically distinct languages, Korean and English through a set of multimodal experiments. Additionally, we have conducted investigations into voice quality across a diverse range of individuals from various communication contexts and cultural and linguistic backgrounds, including English, Korean, Thai, White Hmong, and Gujarati, in order to deepen our comprehension of these complex communication processes.\n\n\nThroughout the grant period, we conducted a series of experiments utilizing electromagnetic articulography (EMA) paired with camcorders to capture multimodal physical signals from speakers of American English and Seoul Korean. These experiments aimed to investigate how prosodic information manifests in co-speech gestures occurring at phrasal edges and under prominence, as well as to explore the temporal coordination patterns of speech and co-speech gestures specific to each language.\n\n\nThe outcomes of our research award have been multifaceted and impactful, spanning various aspects of language communication, with a particular focus on the role of linguistic structure in gesture coordination. Our findings provided insights into the interplay between vocal tract actions and co-speech gestures, particularly in relation to the specific linguistic structure exhibited by each language. Notably, our results demonstrate synchronous coordination between co-speech manual beat gestures and both phrase-edge segment and tone gestures in Korean, suggesting that the multimodal expression of phrasal prominence is driven by linguistic structure. We conclude that prosodic structure is expressed through co-speech gestures, indicating linguistically driven coordination rather than biomechanical coupling alone.\n\n\nAddressing the challenges of data processing and analysis, we developed a novel analysis tool to facilitate the automatic labeling of complex time-series data, which has broad applications in future research on language multimodality.\n\n\nMoreover, our investigation into voice quality among individuals with diverse linguistic backgrounds has yielded valuable insights into communication dynamics. Our findings, drawn from a range of communication contexts and languages such as English, Korean, Thai, White Hmong, and Gujarati, shed light on the critical factors shaping voice variability. We deduce that variations in voice characteristics are influenced by factors such as phonation biology, language-specific phonological patterns, and individual idiosyncrasies.\n\n\nOverall, our research has significantly contributed to advancing our understanding of language communication and gesture coordination, with implications for diverse fields such as linguistics, cognitive science, and communication disorders. Through our dissemination efforts, we aim to share our findings with broader communities of interest and continue pushing the boundaries of knowledge in this important area of research. Thus, our work lays the groundwork for future exploration of multimodality in communication across different languages and modalities.\t\t\t\t\tLast Modified: 04/30/2024\n\n\t\t\t\t\tSubmitted by: YoonjeongLee\n"
 }
}