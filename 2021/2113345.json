{
 "awd_id": "2113345",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Collaborative: Using Machine Learning to Build More Resilient and Transparent Computer Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-01-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 333320.0,
 "awd_amount": 244069.0,
 "awd_min_amd_letter_date": "2021-02-25",
 "awd_max_amd_letter_date": "2021-02-25",
 "awd_abstract_narration": "Machine learning algorithms are increasingly part of everyday life: they help power the ads that we see while browsing the web, self-driving aids in modern cars, and even weather prediction and critical infrastructure. We rely on these algorithms in part because they perform better than alternatives and they can be easy to customize to new applications. Many machine learning algorithms also have a big weakness: it is difficult to understand how and why they compute the answers they provide. This opaqueness means that the answers we get from a machine learning algorithm could be subtly biased or even completely wrong, and yet we might not realize it. This project's goal is to make machine learning algorithms easier to understand, as well as to leverage some of the techniques used by attackers to trick machine learning algorithms into making mistakes to build computer systems that are more resistant to attack. In addition to making fundamental contributions to how machine learning algorithms are designed and used, the project includes outreach efforts that will entice students to gain hands-on experience with machine learning tools.\r\n\r\nThis project focuses on deep neural networks (DNNs).  A groundswell of research within the past five years has demonstrated the propensity of these models to being evaded by inputs created to fool them -- so called \"adversarial examples.\" These types of attacks leverage DNNs' opacity: while DNNs can perform remarkably well on some classification tasks, they often defy simple explanations of how they do so, and indeed can leverage features for doing so that humans might find surprising. This project leverages DNNs and the attacks against them to gain insights into how to build more resilient computer systems. Specifically, the project will use DNNs to model adversaries trying to attack computer systems and then \"attack\" these DNNs to learn how to improve these systems' resilience to attack. This modeling will be done using Generative Adversarial Nets (GANs), in which \"generator\" and \"discriminator\" models compete. Central to this vision are the abilities to evade DNNs under constraints and to extract explanations from them about how they perform classification. Consequently, this project will make fundamental advances both in developing better methods to deceive DNNs and in improving this important machine-learning tool.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Reiter",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Reiter",
   "pi_email_addr": "michael.reiter@duke.edu",
   "nsf_id": "000161868",
   "pi_start_date": "2021-02-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 244069.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-b53f3b5f-7fff-d5aa-73f3-4fb88b06fa2a\"> </span></p>\n<p dir=\"ltr\"><span>The goal of this project has been to evaluate the suitability of, and to improve the use of, ML algorithms for building more resilient computer systems and computer-security defenses.&nbsp; A central aspect of this research has been on understanding the robustness of an ML classifier to attempts to mislead it by providing test instances, or \"adversarial examples\", that have been carefully perturbed to induce misclassifications (so-called \"evasion\" attacks).&nbsp; When the ML classifier represents a defender, robustness of the classifier means that it is harder for an adversary to fool.&nbsp; When the ML classifier represents an adversary, successful adversarial examples against the model represent system behaviors that deceive the adversary.&nbsp; This project made a number of important contributions to the understanding of such evasion attacks:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>The project developed a general technique for producing adversarial examples, while satisfying other constraints or objectives for those inputs. Unlike previous approaches to evasion attacks, this technique can capture both constraints that can be modeled mathematically (e.g., using an objective function) as well as constraints that are hard to model mathematically, but for which it is possible to verify that they have been met.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>The project developed more effective algorithms for generating evasive inputs for an ML classifier.&nbsp; In particular, one algorithm developed in the project was shown to be more successful in generating evasive inputs than state-of-the-art attacks while consuming less time to do so. Another algorithm developed in the project leveraged a generalized notion of evasion attack (also developed within the project) to more effectively generate evasive inputs relevant for various real-world scenarios.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>The project thoroughly explored the application of these principles to an important problem in computer system defense, namely malware detection.&nbsp; The project developed novel evasive attacks on malware detectors that classify raw executables, showing that existing approaches to malware classification were vulnerable to evasion.&nbsp; The project additionally demonstrated the utility of adversarial training, i.e., including malware instances generated to be evasive into the training of the model, to make the models more robust to evasion.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>The project demonstrated the utility of ML models to both attacking and defending a well-known tool for detecting credential database breaches.&nbsp; This method of detection inserts decoy passwords (\"honeywords\") into a credential database that, if ever submitted in login attempts, alert the site to its breach, since no users should know those honeywords.&nbsp; ML plays a role in such designs both (i) for generating honeywords for each account so as to be deceptive to the attacker who breaches the database, and (ii) for identifying a user's chosen password from among the passwords associated with his/her account in a stolen database.</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>This project served as the locus of training for several Ph.D. students and additionally supported projects involving undergraduate and M.S. students.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/16/2023<br>\nModified by: Michael&nbsp;Reiter</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThe goal of this project has been to evaluate the suitability of, and to improve the use of, ML algorithms for building more resilient computer systems and computer-security defenses. A central aspect of this research has been on understanding the robustness of an ML classifier to attempts to mislead it by providing test instances, or \"adversarial examples\", that have been carefully perturbed to induce misclassifications (so-called \"evasion\" attacks). When the ML classifier represents a defender, robustness of the classifier means that it is harder for an adversary to fool. When the ML classifier represents an adversary, successful adversarial examples against the model represent system behaviors that deceive the adversary. This project made a number of important contributions to the understanding of such evasion attacks:\n\n\n\n\nThe project developed a general technique for producing adversarial examples, while satisfying other constraints or objectives for those inputs. Unlike previous approaches to evasion attacks, this technique can capture both constraints that can be modeled mathematically (e.g., using an objective function) as well as constraints that are hard to model mathematically, but for which it is possible to verify that they have been met.\n\n\n\n\nThe project developed more effective algorithms for generating evasive inputs for an ML classifier. In particular, one algorithm developed in the project was shown to be more successful in generating evasive inputs than state-of-the-art attacks while consuming less time to do so. Another algorithm developed in the project leveraged a generalized notion of evasion attack (also developed within the project) to more effectively generate evasive inputs relevant for various real-world scenarios.\n\n\n\n\nThe project thoroughly explored the application of these principles to an important problem in computer system defense, namely malware detection. The project developed novel evasive attacks on malware detectors that classify raw executables, showing that existing approaches to malware classification were vulnerable to evasion. The project additionally demonstrated the utility of adversarial training, i.e., including malware instances generated to be evasive into the training of the model, to make the models more robust to evasion.\n\n\n\n\nThe project demonstrated the utility of ML models to both attacking and defending a well-known tool for detecting credential database breaches. This method of detection inserts decoy passwords (\"honeywords\") into a credential database that, if ever submitted in login attempts, alert the site to its breach, since no users should know those honeywords. ML plays a role in such designs both (i) for generating honeywords for each account so as to be deceptive to the attacker who breaches the database, and (ii) for identifying a user's chosen password from among the passwords associated with his/her account in a stolen database.\n\n\n\n\nThis project served as the locus of training for several Ph.D. students and additionally supported projects involving undergraduate and M.S. students.\n\n\n\n\n\t\t\t\t\tLast Modified: 12/16/2023\n\n\t\t\t\t\tSubmitted by: MichaelReiter\n"
 }
}