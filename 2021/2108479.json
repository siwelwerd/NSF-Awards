{
 "awd_id": "2108479",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Fast, Low-Memory Embeddings for Tensor Data with Applications",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922948",
 "po_email": "slevine@nsf.gov",
 "po_sign_block_name": "Stacey Levine",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 156426.0,
 "awd_amount": 156426.0,
 "awd_min_amd_letter_date": "2021-05-17",
 "awd_max_amd_letter_date": "2023-06-07",
 "awd_abstract_narration": "Many data processing tasks, such as image, video, and music compression and classification, involve finding compact representations of data files. Compactly representing such files is generally a good idea for many practical reasons. Compressed music and image files (MP3, JPG, etc.) are much faster and cheaper to communicate and store than the originals. In classification applications, a small number of informative file features are often selected from larger categories of data in order to help boost accuracy and efficiency (this is akin to only focusing on the voice in a song when the aim is to identify the singer). In more extreme situations, data signals may be so large or change so rapidly that they cannot be stored or analyzed at all without first being quickly compressed. Many interesting problems of this type exist in research areas related to algorithms for internet data analysis aimed at, for example, quickly detecting particular types of large-scale cyber-attacks. As part of this project, the investigators will develop and implement new faster compression and data analysis techniques for complex data, which can then be used to facilitate faster data processing in a myriad of large-scale data processing applications. The project will also have educational benefits aimed at increasing the representation of students from under-represented and under-served groups in STEM research fields. This will be accomplished by the investigators hosting and mentoring research projects for undergraduate students from diverse backgrounds who will apply the compression and data analysis techniques developed as part of this research to specific application data, for example, to analyze and better understand Lyme disease data.\r\n\r\nThis research includes a rich new class of practical Johnson-Lindenstrauss (JL) maps for vector data that cannot only be applied to vectors faster than Fast Fourier Transform time serially but are also trivially parallelizable. The embeddings will be randomized, and their analysis will be supported by the development of novel concentration inequalities based on generic chaining and supremum of chaos approaches for structured tensor data embeddings. These techniques will then allow, for example, the construction of new fast and memory efficient embeddings with the Tensor Restricted Isometry Property of value in the analysis of large tensor data. In addition, the research will develop new nonlinear bi-Lipchitz extensions of linear modewise JL-maps for tensor data capable of preserving distances between all low rank tensors in a given database and all other lower rank tensors, even outside of the database. These new nonlinear embeddings techniques will allow improved theoretical guarantees for space-constrained learning and classification with polynomial kernels. Finally, these embeddings will also be applied to address data-intensive problems in quantum many-body theory and nuclear physics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Deanna",
   "pi_last_name": "Needell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Deanna Needell",
   "pi_email_addr": "deanna@math.ucla.edu",
   "nsf_id": "000600078",
   "pi_start_date": "2021-05-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Elizaveta",
   "pi_last_name": "Rebrova",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elizaveta Rebrova",
   "pi_email_addr": "elre@princeton.edu",
   "nsf_id": "000840866",
   "pi_start_date": "2021-05-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "520 Portola Plaza MS 7620C",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 32997.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 56074.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 67355.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As the amount of data we collect and use in various applications grows in leaps and bounds, we need better ways to handle and analyze large-scale data. In many cases, this data comes in different forms and with different attributes or dimensions, making tensors -- the multi-dimensional generalization of matrices (arrays) -- a natural way to represent it. Tensors are widely used in fields like medicine, communications, and data science.<br />While matrices are well-studied and understood mathematically, the techniques for working with tensors are more complex and challenging, especially when it comes to acquiring, compressing, storing, reconstructing, and analyzing them. However, many types of tensor data have underlying patterns and structures that allow for more efficient computation, which is becoming increasingly important as the size of data grows.</p>\n<p>Our work focuses on creating methods to compress and reconstruct tensor data efficiently while maintaining the key information in the data. Indeed, the field of compressed sensing focuses on acquiring and compressing high dimensional data that has low dimensional structure in such a way that the data never needs to be acquired in its entirety at all (which is prohibitive in many large-scale settings). Unfortunately, the theory this field provides for tensor data requires one to \"unfold\" the tensor (imagine concatenating row after row of a table and making it into one long string) before compression. Then, the compression operator is applied to that&nbsp;unfolding, rather than the tensor itself. In addition to being unnatural and ignoring the&nbsp;intrinsic&nbsp;structure of the tensor, it means that the operator itself needs to be massive -- its dimension needing to be the number of entries in the entire tensor! This is analogous to using a&nbsp;vacuum&nbsp;storage bag to compress packed clothes that then require you to bring a large-sized vacuum (much larger than the clothes themselves!) with you in order to decompress them. Clearly there must be a better way!</p>\n<p>For this reason, our work has focused on the construction of compression techniques that do not require tensor&nbsp;unfoldings. We provide the methods for constructing the compression operators, methods for reconstruction (decompression), and theoretical guarantees for accuracy that match those methods that require&nbsp;unfoldings. To the best of our knowledge, this work is the first to provide such guarantees while also being computationally feasible and not requiring&nbsp;unfoldings, but being applied \"modewise\" to the data instead. To continue the analogy of the vacuum storage bags, we now only need to bring a very small vacuum in order to decompress your packed clothes! We have extended this work to other mathematical problems like the least-squares problems and linear&nbsp;feasbilitiy&nbsp;problems, which have many applications in data science.</p>\n<p>This grant also partially funded summer undergraduate research programs in which the students developed techniques related to this grant and applied them to medical and criminal justice datasets coming from our community partners. It also funded a PhD student who worked on these methods and their theoretical guarantees. All work was collaborative with the co-PIs&nbsp;at Michigan State (and now Princeton where one co-PI currently is), and publications and software have been made publicly available.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/09/2024<br>\nModified by: Deanna&nbsp;Needell</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2108479/2108479_10733864_1725920631842_tensor_image--rgov-214x142.PNG\" original=\"/por/images/Reports/POR/2024/2108479/2108479_10733864_1725920631842_tensor_image--rgov-800width.PNG\" title=\"Example of a (3-mode) tensor\"><img src=\"/por/images/Reports/POR/2024/2108479/2108479_10733864_1725920631842_tensor_image--rgov-66x44.PNG\" alt=\"Example of a (3-mode) tensor\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visualization of a tensor with three modes (corresponding to documents, words, time) that might arise from e.g. text data.</div>\n<div class=\"imageCredit\">Lara Kassab, UCLA</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Deanna&nbsp;Needell\n<div class=\"imageTitle\">Example of a (3-mode) tensor</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs the amount of data we collect and use in various applications grows in leaps and bounds, we need better ways to handle and analyze large-scale data. In many cases, this data comes in different forms and with different attributes or dimensions, making tensors -- the multi-dimensional generalization of matrices (arrays) -- a natural way to represent it. Tensors are widely used in fields like medicine, communications, and data science.\nWhile matrices are well-studied and understood mathematically, the techniques for working with tensors are more complex and challenging, especially when it comes to acquiring, compressing, storing, reconstructing, and analyzing them. However, many types of tensor data have underlying patterns and structures that allow for more efficient computation, which is becoming increasingly important as the size of data grows.\n\n\nOur work focuses on creating methods to compress and reconstruct tensor data efficiently while maintaining the key information in the data. Indeed, the field of compressed sensing focuses on acquiring and compressing high dimensional data that has low dimensional structure in such a way that the data never needs to be acquired in its entirety at all (which is prohibitive in many large-scale settings). Unfortunately, the theory this field provides for tensor data requires one to \"unfold\" the tensor (imagine concatenating row after row of a table and making it into one long string) before compression. Then, the compression operator is applied to thatunfolding, rather than the tensor itself. In addition to being unnatural and ignoring theintrinsicstructure of the tensor, it means that the operator itself needs to be massive -- its dimension needing to be the number of entries in the entire tensor! This is analogous to using avacuumstorage bag to compress packed clothes that then require you to bring a large-sized vacuum (much larger than the clothes themselves!) with you in order to decompress them. Clearly there must be a better way!\n\n\nFor this reason, our work has focused on the construction of compression techniques that do not require tensorunfoldings. We provide the methods for constructing the compression operators, methods for reconstruction (decompression), and theoretical guarantees for accuracy that match those methods that requireunfoldings. To the best of our knowledge, this work is the first to provide such guarantees while also being computationally feasible and not requiringunfoldings, but being applied \"modewise\" to the data instead. To continue the analogy of the vacuum storage bags, we now only need to bring a very small vacuum in order to decompress your packed clothes! We have extended this work to other mathematical problems like the least-squares problems and linearfeasbilitiyproblems, which have many applications in data science.\n\n\nThis grant also partially funded summer undergraduate research programs in which the students developed techniques related to this grant and applied them to medical and criminal justice datasets coming from our community partners. It also funded a PhD student who worked on these methods and their theoretical guarantees. All work was collaborative with the co-PIsat Michigan State (and now Princeton where one co-PI currently is), and publications and software have been made publicly available.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 09/09/2024\n\n\t\t\t\t\tSubmitted by: DeannaNeedell\n"
 }
}