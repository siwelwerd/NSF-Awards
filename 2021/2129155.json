{
 "awd_id": "2129155",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: HCC: Small: Leveraging a Wrapped Haptic Display to Communicate Robot Learning and Accelerate Human Teaching",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2025-09-30",
 "tot_intn_awd_amt": 249489.0,
 "awd_amount": 249489.0,
 "awd_min_amd_letter_date": "2021-09-02",
 "awd_max_amd_letter_date": "2021-09-02",
 "awd_abstract_narration": "Humans excel at teaching physical activities and tasks through demonstration and physical correction (think of a coach guiding an athlete through a desired motion), and human learners often use verbal and nonverbal signals to communicate their understanding or confusion. Similar approaches to teaching have been used between humans and robots, allowing humans to naturally demonstrate tasks, like cooking or furniture assembly, and correct errors in the motions of the robots. While robots have made great advances in understanding the demonstrations and corrections from human teachers, they have lacked an effective way to communicate what they do and do not understand, so human teachers may not know when or if a robot is ready to carry out a task by itself. This project will address this communication gap by developing new ways for robot arms to communicate to human teachers as they learn.  The investigators will attach haptic skin displays (arrays of controllable bubbles) to a robotic arm and create touch sensations to communicate the robot\u2019s understanding or confusion to the human teacher.  Better and more understandable communication from robots as they learn will allow users to train or retrain robots more efficiently, and to better know when they are ready to deploy, without needing specialized knowledge about how robots function. These features will make robot arms more attractive tools for small and mid-sized manufacturers, allowing them to flexibly automate some manufacturing tasks. The lessons learned about communicating the learning state of robotic arms can also be applied to other computer and robotic systems, making the opaque process of robot and computer learning more comprehensible and giving the opportunity to catch and correct errors.\r\n\r\nThe goal of this project is to characterize how humans perceive haptic skin displays wrapped around robot arms, and to formalize how robots capture and communicate feedback through these haptic arrays. Prior work enables robots to learn from physical demonstrations; however, it is equally important to make this learning transparent to the human teacher. This project will advance transparent and interpretable robot learning from an algorithmic and haptic perspective. The team of investigators will i) characterize the types and patterns of exploratory haptic feedback the human perceives, ii) embed the robot's complex and high-dimensional reward learning into low-dimensional haptic feedback, and iii) model how humans interpret the robot's feedback.  These steps will ultimately provide human teachers with an awareness of the robot\u2019s understanding and thereby improve their demonstrations. Each contribution will be evaluated in human subject studies with a commercial robot arm. This project has the potential to advance robotics in small and mid-sized manufacturing by making the process of teaching robot arms intuitive, transparent, and user-friendly.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Laura",
   "pi_last_name": "Blumenschein",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Laura H Blumenschein",
   "pi_email_addr": "lhblumen@purdue.edu",
   "nsf_id": "000852297",
   "pi_start_date": "2021-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072114",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 249489.0
  }
 ],
 "por": null
}