{
 "awd_id": "2040926",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Quantifying and Mitigating Disparities in Language Technologies",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 375000.0,
 "awd_amount": 383000.0,
 "awd_min_amd_letter_date": "2021-01-25",
 "awd_max_amd_letter_date": "2023-12-04",
 "awd_abstract_narration": "Advances in natural language processing (NLP) technology now make it possible to perform many tasks through natural language or over natural language data -- automatic systems can answer questions, perform web search, or command our computers to perform specific tasks. However, ``language'' is not monolithic; people vary in the language they speak, the dialect they use, the relative ease with which they produce language, or the words they choose with which to express themselves. In benchmarking of NLP systems however, this linguistic variety is generally unattested. Most commonly tasks are formulated using canonical American English, designed with little regard for whether systems will work on language of any other variety. In this work we ask a simple question: can we measure the extent to which the diversity of language that we use affects the quality of results that we can expect from language technology systems? This will allow for the development and deployment of fair accuracy measures for a variety of tasks regarding language technology, encouraging advances in the state of the art in these technologies to focus on all, not just a select few.\r\n\r\nSpecifically, this work focuses on four aspects of this overall research question. First, we will develop a general-purpose methodology for quantifying how well particular language technologies work across many varieties of language. Measures over multiple speakers or demographics are combined to benchmarks that can drive progress in development of fair metrics for language systems, tailored to the specific needs of design teams. Second, we will move beyond simple accuracy measures, and directly quantify the effect that the accuracy of systems has on users in terms of relative utility derived from using the system. These measures of utility will be incorporated in our metrics for system success. Third, we focus on the language produced by people from varying demographic groups, predicting system accuracies from demographics. Finally, we will examine novel methods for robust learning of NLP systems across language or dialectal boundaries, and examine the effect that these methods have on increasing accuracy for all users.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Graham",
   "pi_last_name": "Neubig",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Graham Neubig",
   "pi_email_addr": "gneubig@andrew.cmu.edu",
   "nsf_id": "000732016",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Bigham",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey Bigham",
   "pi_email_addr": "jbigham@cmu.edu",
   "nsf_id": "000541549",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Geoff",
   "pi_last_name": "Kaufman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Geoff Kaufman",
   "pi_email_addr": "gfk@andrew.cmu.edu",
   "nsf_id": "000701788",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yulia",
   "pi_last_name": "Tsvetkov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yulia Tsvetkov",
   "pi_email_addr": "yuliats@cs.washington.edu",
   "nsf_id": "000728441",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Antonios",
   "pi_last_name": "Anastasopoulos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Antonios Anastasopoulos",
   "pi_email_addr": "antonis@gmu.edu",
   "nsf_id": "000832909",
   "pi_start_date": "2021-01-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 375000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Advances in natural language processing (NLP) technology now make it possible to perform many tasks through natural language or over natural language data -- automatic systems can answer questions, perform web search, or command our computers to perform specific tasks. However, ``language'' is not monolithic; people vary in the language they speak, the dialect they use, the relative ease with which they produce language, or the words they choose with which to express themselves. In benchmarking of NLP systems however, this linguistic variety is generally unattested. Most commonly tasks are formulated using canonical American English, designed with little regard for whether systems will work on language of any other variety. In this work we asked a simple question: can we measure the extent to which the diversity of language that we use affects the quality of results that we can expect from language technology systems? This will allow for the development and deployment of fair accuracy measures for a variety of tasks regarding language technology, encouraging advances in the state of the art in these technologies to focus on all, not just a select few.</p>\n<p><br />Specifically, this work focused on several aspects of this overall research question:</p>\n<ul>\n<li>First, the project developed a general-purpose methodology for quantifying how well particular language technologies work across many varieties of language. It built benchmarks that considered the overall speaking population for various language varieties and built a benchmark GlobalBench.</li>\n<li>Second, it moved beyond simple accuracy measures, and directly quantified the effect that the accuracy of systems has on users in terms of relative utility derived from using the system. For instance, it examined the disparate effect of mistakes in voice recognition on people of various demographics.</li>\n<li>Third, it examined novel methods for robust learning of NLP systems across language or dialectal boundaries, and examine the effect that these methods have on increasing accuracy for all users. For instance, it created techniques that can generate text in many different language varieties.</li>\n</ul><br>\n<p>\n Last Modified: 02/02/2024<br>\nModified by: Graham&nbsp;Neubig</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAdvances in natural language processing (NLP) technology now make it possible to perform many tasks through natural language or over natural language data -- automatic systems can answer questions, perform web search, or command our computers to perform specific tasks. However, ``language'' is not monolithic; people vary in the language they speak, the dialect they use, the relative ease with which they produce language, or the words they choose with which to express themselves. In benchmarking of NLP systems however, this linguistic variety is generally unattested. Most commonly tasks are formulated using canonical American English, designed with little regard for whether systems will work on language of any other variety. In this work we asked a simple question: can we measure the extent to which the diversity of language that we use affects the quality of results that we can expect from language technology systems? This will allow for the development and deployment of fair accuracy measures for a variety of tasks regarding language technology, encouraging advances in the state of the art in these technologies to focus on all, not just a select few.\n\n\n\nSpecifically, this work focused on several aspects of this overall research question:\n\nFirst, the project developed a general-purpose methodology for quantifying how well particular language technologies work across many varieties of language. It built benchmarks that considered the overall speaking population for various language varieties and built a benchmark GlobalBench.\nSecond, it moved beyond simple accuracy measures, and directly quantified the effect that the accuracy of systems has on users in terms of relative utility derived from using the system. For instance, it examined the disparate effect of mistakes in voice recognition on people of various demographics.\nThird, it examined novel methods for robust learning of NLP systems across language or dialectal boundaries, and examine the effect that these methods have on increasing accuracy for all users. For instance, it created techniques that can generate text in many different language varieties.\n\t\t\t\t\tLast Modified: 02/02/2024\n\n\t\t\t\t\tSubmitted by: GrahamNeubig\n"
 }
}