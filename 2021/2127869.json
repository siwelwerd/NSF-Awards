{
 "awd_id": "2127869",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: New Directions in Probabilistic Deep Learning: Exponential Families, Bayesian Nonparametrics and Empirical Bayes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 499756.0,
 "awd_amount": 499756.0,
 "awd_min_amd_letter_date": "2021-07-28",
 "awd_max_amd_letter_date": "2021-07-28",
 "awd_abstract_narration": "Deep learning (DL) provides a powerful paradigm for modern machine learning (ML), with applications in a range of areas, such as natural language processing, computer vision and robotics. DL has proven powerful because it can capture complex relationships between input and output, and it enjoys efficient algorithms for analyzing massive datasets. But DL also has limitations. Currently, DL often provides \u2018black box\u2019 predictions. Black box indicates that the results do not involve clearly articulated assumptions or the degree of certainty in the results. To use ML in important applications, it is crucial to know from which assumptions the methods are based. Second, basic DL methods provide point predictions, but do not provide uncertainty about them. For ML to be safely deployed in critical decision-making systems, ML methods must provide calibrated measurement about the reliability of its predictions. Finally, all of these issues mean that DL does not provide easily interpretable predictions. Interpretability is important for understanding how ML makes mistakes, for deploying ML in high-stakes settings that require accountability, and when using ML predictions in the service of scientific understanding. This interdisciplinary project addresses these issues by using the rigorous methodology of probabilistic ML and applied Bayesian statistics to form interpretable DL models. Models that will be based on clearly stated assumptions and provide calibrated uncertainty about their predictions. This research aims to solve open problems in DL, provide mathematical clarity to some of its empirically proven ideas, and expand its reach to probabilistic modeling for broad applications in astronomy, language modeling for the computational social sciences, and electronic healthcare records.\r\n\r\nThe project will adapt modern ideas in DL for modern probabilistic models of complex datasets through research in two topics. The first topic develops the foundations of probabilistic deep learning, clarifying how deep neural network models draw from classical ideas like exponential families and generalized linear models, and expanding DL to Bayesian nonparametric models of infinite depth. The second topic develops empirical Bayes representation learning. Representation learning, a cornerstone of DL, is about finding low-dimensional descriptions of high-dimensional data. But from a statistical perspective, the problem is that many representations can accurately capture the distribution of the data. This project will explore how the powerful concept of empirical Bayes, a classical statistical idea that blends frequentist and Bayesian thinking, provides a natural framework for defining good representations. Through new theory, algorithms, and software, this project will significantly expand the capabilities of DL.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Blei",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Blei",
   "pi_email_addr": "david.blei@columbia.edu",
   "nsf_id": "000114130",
   "pi_start_date": "2021-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "500 West 120th Street, Mudd 430",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 499756.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"flex max-w-full flex-col flex-grow\">\n<div class=\"min-h-8 text-message flex w-full flex-col items-end gap-2 whitespace-normal break-words [.text-message+&amp;]:mt-5\">\n<div class=\"flex w-full flex-col gap-1 empty:hidden first:pt-[3px]\">\n<div class=\"markdown prose w-full break-words dark:prose-invert light\">\n<p>Our research explores advanced methods in statistical modeling, machine learning, and data analysis to address challenges in diverse fields, including healthcare, economics, and artificial intelligence. Key themes focus on making predictions, understanding causality, and evaluating model performance.</p>\n<p>In public health, one model assesses advances techniques for analyzing individual health records by improving text-based medical predictions. In economics, a model called CAREER uses large-scale resume data to predict job changes and income trends, helping labor economists understand career patterns more accurately.</p>\n<p>The studies also tackle challenges in causal inference&mdash;understanding \"what if\" scenarios in fields like medicine and social sciences. New algorithms help uncover complex causal relationships, allowing scientists to more confidently study factors that influence outcomes, even with limited or complex data.</p>\n<p>Further contributions enhance machine learning's reliability. For example, new \"density-aware\" layers in neural networks help gauge the reliability of predictions by identifying unusual data, which is critical in safety-focused applications like autonomous driving. Another model improves how we test artificial intelligence systems for bias by examining how large language models handle moral questions.</p>\n<p>Lastly, there are innovations in model evaluation, with new tools to ensure that statistical models accurately represent the data they&rsquo;re meant to predict. From refining Bayesian inference&mdash;a framework for updating predictions with new information&mdash;to designing more precise ways of sampling in complex simulations, these studies advance both the theory and practice of machine learning and statistical inference, pushing towards more robust and trustworthy applications across science and society.</p>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n Last Modified: 11/10/2024<br>\nModified by: David&nbsp;Blei</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\n\nOur research explores advanced methods in statistical modeling, machine learning, and data analysis to address challenges in diverse fields, including healthcare, economics, and artificial intelligence. Key themes focus on making predictions, understanding causality, and evaluating model performance.\n\n\nIn public health, one model assesses advances techniques for analyzing individual health records by improving text-based medical predictions. In economics, a model called CAREER uses large-scale resume data to predict job changes and income trends, helping labor economists understand career patterns more accurately.\n\n\nThe studies also tackle challenges in causal inferenceunderstanding \"what if\" scenarios in fields like medicine and social sciences. New algorithms help uncover complex causal relationships, allowing scientists to more confidently study factors that influence outcomes, even with limited or complex data.\n\n\nFurther contributions enhance machine learning's reliability. For example, new \"density-aware\" layers in neural networks help gauge the reliability of predictions by identifying unusual data, which is critical in safety-focused applications like autonomous driving. Another model improves how we test artificial intelligence systems for bias by examining how large language models handle moral questions.\n\n\nLastly, there are innovations in model evaluation, with new tools to ensure that statistical models accurately represent the data theyre meant to predict. From refining Bayesian inferencea framework for updating predictions with new informationto designing more precise ways of sampling in complex simulations, these studies advance both the theory and practice of machine learning and statistical inference, pushing towards more robust and trustworthy applications across science and society.\n\n\n\n\t\t\t\t\tLast Modified: 11/10/2024\n\n\t\t\t\t\tSubmitted by: DavidBlei\n"
 }
}