{
 "awd_id": "2104404",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:Small:Collaborative Research: Understanding Human-Object Interactions from First-person and Third-person Videos",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2022-07-31",
 "tot_intn_awd_amt": 228357.0,
 "awd_amount": 228357.0,
 "awd_min_amd_letter_date": "2020-11-13",
 "awd_max_amd_letter_date": "2020-11-13",
 "awd_abstract_narration": "Ubiquitous cameras, together with ever increasing computing resources, are dramatically changing the nature of visual data and their analysis. Cities are adopting networked camera systems for policing and intelligent resource allocation, and individuals are recording their lives using wearable devices. For these camera systems to become truly smart and useful for people, it is crucial that they understand interesting objects in the scene and detect ongoing activities/events, while jointly considering continuous 24/7 videos from multiple sources. Such object-level and activity-level awareness in hospitals, elderly homes, and public places would provide assistive and quality-of-life technology for disabled and elderly people, provide intelligent surveillance systems to prevent crimes, and allow smart usage of environmental resources.  This project will investigate novel computer vision algorithms that combine 1st-person videos (from wearable cameras) and 3rd-person videos (from static environmental cameras) for joint recognition of humans, objects, and their interactions. The key idea is to combine the two views' complementary and unique advantages for joint visual scene understanding. To this end, it will create a new dataset, and develop new algorithms that learn to recognize objects jointly across the views, learn human-object and human-human relationships through the two views, and anonymize the videos to preserve users' privacies. The project will provide new algorithms that have the potential to benefit applications in smart environments, security, and quality-of-life assistive technologies. The project will also perform complementary educational and outreach activities that engage students in research and STEM.\r\n\r\nThis project will develop novel algorithms that learn from joint 1st-person videos (from wearable cameras) and 3rd-person videos (from static environmental cameras) for joint recognition of humans, objects, and their interactions. The 1st-person view is ideal for object recognition, while the 3rd-person view is ideal for human activity recognition. Thus, this project will investigate unique solutions to challenging problems that would otherwise be difficult to overcome when analyzing each viewpoint in isolation. The main research directions will be: (1) creating a benchmark 1st-person and 3rd-person video dataset to investigate this new problem; and developing algorithms that (2) learn to establish object and human correspondences between the two views; (3) learn object-action relationships across the views; and (4) anonymize the visual data for privacy-preserving visual recognition.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Ryoo",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Michael S Ryoo",
   "pi_email_addr": "mryoo@cs.stonybrook.edu",
   "nsf_id": "000711682",
   "pi_start_date": "2020-11-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117944001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 228357.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to develop novel computer vision algorithms that can learn from joint 1st-person videos captured from wearable cameras and 3rd-person videos captured from static environmental cameras for recognition of humans, objects, and their interactions. The 1st-person view is ideal for object recognition, while the 3rd-person view is more beneficial for analysis of humans. Thus, this project investigated unique solutions to challenging problems that would otherwise be difficult to overcome when analyzing each viewpoint in isolation.&nbsp;</p>\n<p><br />In terms of intellectual merit, there were broadly three key areas of technical contributions. The first is the development of novel representation learning algorithms for 1st person and 3rd person video e.g., to model human activities. The second is the development of novel algorithms for learning disentangled and robust representations. The third is the development of algorithms that can learn from minimal human supervision. The work produced 23 peer reviewed papers in top-tier computer vision, machine learning, and robotics conferences, and new publicly available codebases for the algorithms which are linked from https://pages.cs.wisc.edu/~yongjaelee/ and http://michaelryoo.com/. The research results were also regularly presented by the PIs at international meetings and university seminars.</p>\n<p><br />In terms of broader impact, the main project outcomes were graduate student mentorship and training, outreach activities to promote wider participation of underrepresented students in CS and STEM education, and broad scientific impact of the algorithms. In particular, the project helped train MS and PhD students in conducting and presenting research in the topics of this project. Several MS and PhD students completed their degrees and accepted new PhD and research industry positions. The project's outreach component contributed to efforts that widen underrepresented student participation in STEM.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/20/2023<br>\n\t\t\t\t\tModified by: Michael&nbsp;S&nbsp;Ryoo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to develop novel computer vision algorithms that can learn from joint 1st-person videos captured from wearable cameras and 3rd-person videos captured from static environmental cameras for recognition of humans, objects, and their interactions. The 1st-person view is ideal for object recognition, while the 3rd-person view is more beneficial for analysis of humans. Thus, this project investigated unique solutions to challenging problems that would otherwise be difficult to overcome when analyzing each viewpoint in isolation. \n\n\nIn terms of intellectual merit, there were broadly three key areas of technical contributions. The first is the development of novel representation learning algorithms for 1st person and 3rd person video e.g., to model human activities. The second is the development of novel algorithms for learning disentangled and robust representations. The third is the development of algorithms that can learn from minimal human supervision. The work produced 23 peer reviewed papers in top-tier computer vision, machine learning, and robotics conferences, and new publicly available codebases for the algorithms which are linked from https://pages.cs.wisc.edu/~yongjaelee/ and http://michaelryoo.com/. The research results were also regularly presented by the PIs at international meetings and university seminars.\n\n\nIn terms of broader impact, the main project outcomes were graduate student mentorship and training, outreach activities to promote wider participation of underrepresented students in CS and STEM education, and broad scientific impact of the algorithms. In particular, the project helped train MS and PhD students in conducting and presenting research in the topics of this project. Several MS and PhD students completed their degrees and accepted new PhD and research industry positions. The project's outreach component contributed to efforts that widen underrepresented student participation in STEM.\n\n\t\t\t\t\tLast Modified: 02/20/2023\n\n\t\t\t\t\tSubmitted by: Michael S Ryoo"
 }
}