{
 "awd_id": "2120087",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Computational Methods for Speech Analysis",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927269",
 "po_email": "ceavey@nsf.gov",
 "po_sign_block_name": "Cheryl Eavey",
 "awd_eff_date": "2021-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 249318.0,
 "awd_amount": 249318.0,
 "awd_min_amd_letter_date": "2021-07-23",
 "awd_max_amd_letter_date": "2021-07-23",
 "awd_abstract_narration": "This research project will develop tools for testing hypotheses about human communication. Researchers generally study human communication from textual transcripts which omit vocal tone. The project will directly address the disconnect between the data-generating process - in which speakers and listeners use the auditory channel to convey both textual and non-textual signals - and the widespread practice of discarding speech audio. The investigators will extend their prior speech model, The Model of Audio and Speech Structure, to address some limitations of the model. In particular, the statistical extensions will accommodate multiple speakers and allow for the joint modeling of text and tone. To demonstrate the value of the statistical extensions, the model will be applied to two original video corpora - police body-worn camera footage and campaign speeches for federal office. New software will be developed that makes it easy for researchers to quickly annotate a large amount of speech audio. The browser-based tools will enable automatic and manual segmentation, along with labeling. Multiple graduate students will gain experience in computationally intensive research and software development. The tools to be developed will be incorporated into ongoing public-private collaborations to improve oversight of police officers in the field.\r\n\r\nThis research project will extend the Model of Audio and Speech Structure (MASS), which analyzes conversation as a nested stochastic process in which (i) the flow of conversation unfolds as a sequence of utterances transitioning between speakers and their vocal tones, based on contextual covariates; and (ii) the auditory signal within each utterance unfolds as a hidden Markov model that transitions between phonemes which generate sound. The model enables social scientists to test hypotheses about how conversations are structured by fixed covariates (e.g., speaker gender, conversation role) and time-varying covariates (e.g., exogenous external stimuli, endogenous conversation trajectory such as the previous speaker's tone). In its current implementation, however, MASS has two key limitations: First, it uses resource-intensive human annotations of tone for each speaker, which limits application to contexts with many unique speakers, such as police body-worn camera footage. This project will develop extensions allowing the model to borrow strength by partial pooling across speakers with similar speech profiles. Second, MASS incorporates text as externally given metadata. The project will develop a new approach for joint modeling of text and audio which will incorporate a dynamic topic model into the flow-of-conversation layer of MASS. The investigators will conduct two applications to demonstrate the value of the multi-speaker and joint text-audio modeling extensions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Lucas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Lucas",
   "pi_email_addr": "christopher.lucas@wustl.edu",
   "nsf_id": "000842543",
   "pi_start_date": "2021-07-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dean",
   "pi_last_name": "Knox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dean Knox",
   "pi_email_addr": "dcknox@upenn.edu",
   "nsf_id": "000843444",
   "pi_start_date": "2021-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "133300",
   "pgm_ele_name": "Methodology, Measuremt & Stats"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 249318.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div id=\"WACViewPanel_ClipboardElement\" class=\"WACEditing EditMode EditingSurfaceBody FireFox WACViewPanel_DisableLegacyKeyCodeAndCharCode usehover\" style=\"overflow: hidden; visibility: visible; direction: ltr;\">\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 0px 0px 16px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: normal; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">This project aimed to advance the scientific study of multimodal communication, particularly in political communication. While much research examines political communication through audio and video, it often focuses strictly on the text of communication, ignoring the audio-visual channel. We develop</span><span class=\"NormalTextRun SCXW94088597 BCX2\">ed</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> causal frameworks that formalize quantities of interest in multimodal settings, complemented with substantive studies exploring when and where non-textual modes of communication are most impactful, as well as contexts in which their influence may be overstated.</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 11pt; line-height: 20.925px; font-family: Arial,;\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 16px 0px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: normal; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">The most significant achievement of this project is the development of a formal framework for analyzing causal effects of multimodal communication, with an application to political speech during U.S. presidential campaigns. This framework integrates text, audio, and visual elements to quantify their distinct and combined effects on voter perception. To assess the importance of these non-textual channels, we </span><span class=\"NormalTextRun SCXW94088597 BCX2\">conducted</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> a naturalistic experiment using campaign \"</span><span class=\"FindHit SCXW94088597 BCX2\">catch</span><span class=\"NormalTextRun SCXW94088597 BCX2\">phrases\" and a controlled audio conjoint experiment built in collaboration with voice actors. The findings underscore that vocal delivery impacts evaluations of political candidates. This framework </span><span class=\"NormalTextRun SCXW94088597 BCX2\">establishes</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> a methodological foundation for future research on multimodal communication and is conditionally accepted at the </span></span><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: italic; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">Journal of the Royal Statistical Society: Series A.</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 11pt; line-height: 20.925px; font-family: Arial,;\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 16px 0px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: normal; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">In a second study, we assessed the credibility and impact of political deepfakes, experimentally comparing them to other forms of misinformation. Results showed that deepfakes are not uniquely credible compared to text or audio misinformation but can erode trust in media, especially that which runs counter to pre-existing beliefs. Specifically, we found that people often misidentify real videos as deepfakes, highlighting the broader challenge of </span><span class=\"NormalTextRun SCXW94088597 BCX2\">maintaining</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> media credibility in the presence of AI-generated content. This research directly informs policy debates on digital misinformation and is forthcoming at the </span></span><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: italic; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">Journal of Politics.</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 11pt; line-height: 20.925px; font-family: Arial,;\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 16px 0px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: normal; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">Another output of the project is the CANDOR corpus, a vast multimodal dataset of over </span><span class=\"NormalTextRun SCXW94088597 BCX2\">850 hours</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> of conversational video, audio, and transcripts. This dataset </span><span class=\"NormalTextRun SCXW94088597 BCX2\">provides</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> detailed insights into conversational dynamics, including turn-taking, emotional expression, and post-conversation reflections. It provides public access to the largest corpus of its type to date, and it has been used to support interdisciplinary research on a range of topics </span><span class=\"NormalTextRun SCXW94088597 BCX2\">such as </span><span class=\"NormalTextRun SCXW94088597 BCX2\">conversation</span>'<span class=\"NormalTextRun SCXW94088597 BCX2\">s effects on psychological well-being, the formation of perceptions and meta-perceptions through social interaction, and computational modeling of conversational behavior through large-language-model dialogue agents. This study is published in </span></span><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: italic; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">Science Advances.</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 11pt; line-height: 20.925px; font-family: Arial,;\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 16px 0px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: normal; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">We also extended and released open-source tools for video annotation. One is built in </span><span class=\"NormalTextRun SCXW94088597 BCX2\">Shiny and</span><span class=\"NormalTextRun SCXW94088597 BCX2\"> can be used to label both local and remote data. A second is built in Python for the annotation of local data.</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 11pt; line-height: 20.925px; font-family: Arial,;\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 16px 0px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"color: #000000; font-size: 11pt; font-style: normal; text-decoration: none; line-height: 20.925px; font-family: Arial,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">This project&rsquo;s outcomes have advanced our understanding of political communication by providing new causal frameworks, experimental insights, and practical tools. Finally, the award also developed the careers of multiple graduate students, some of whom are </span><span class=\"NormalTextRun SCXW94088597 BCX2\">already </span><span class=\"NormalTextRun SCXW94088597 BCX2\">successfully transitioning into faculty positions at leading research universities.</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 11pt; line-height: 20.925px; font-family: Arial,;\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW94088597 BCX2\" style=\"direction: ltr;\">\r\n<p class=\"Paragraph SCXW94088597 BCX2\" style=\"font-weight: normal; font-style: normal; vertical-align: baseline; background-color: transparent; color: windowtext; text-align: left; margin: 0px 0px 10.6667px; padding-left: 0px; padding-right: 0px; text-indent: 0px;\" lang=\"EN-US\"><span class=\"TextRun SCXW94088597 BCX2\" style=\"font-size: 12pt; line-height: 26.7375px; font-family: Aptos,;\" lang=\"EN-US\"><span class=\"NormalTextRun SCXW94088597 BCX2\">&nbsp;</span></span><span class=\"EOP SCXW94088597 BCX2\" style=\"font-size: 12pt; line-height: 26.7375px; font-family: Aptos,;\"> <br /></span></p>\r\n</div>\r\n</div><br>\n<p>\n Last Modified: 11/29/2024<br>\nModified by: Christopher&nbsp;Lucas</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\r\n\r\n\n\nThis project aimed to advance the scientific study of multimodal communication, particularly in political communication. While much research examines political communication through audio and video, it often focuses strictly on the text of communication, ignoring the audio-visual channel. We developed causal frameworks that formalize quantities of interest in multimodal settings, complemented with substantive studies exploring when and where non-textual modes of communication are most impactful, as well as contexts in which their influence may be overstated.\r\n\r\n\r\n\n\nThe most significant achievement of this project is the development of a formal framework for analyzing causal effects of multimodal communication, with an application to political speech during U.S. presidential campaigns. This framework integrates text, audio, and visual elements to quantify their distinct and combined effects on voter perception. To assess the importance of these non-textual channels, we conducted a naturalistic experiment using campaign \"catchphrases\" and a controlled audio conjoint experiment built in collaboration with voice actors. The findings underscore that vocal delivery impacts evaluations of political candidates. This framework establishes a methodological foundation for future research on multimodal communication and is conditionally accepted at the Journal of the Royal Statistical Society: Series A.\r\n\r\n\r\n\n\nIn a second study, we assessed the credibility and impact of political deepfakes, experimentally comparing them to other forms of misinformation. Results showed that deepfakes are not uniquely credible compared to text or audio misinformation but can erode trust in media, especially that which runs counter to pre-existing beliefs. Specifically, we found that people often misidentify real videos as deepfakes, highlighting the broader challenge of maintaining media credibility in the presence of AI-generated content. This research directly informs policy debates on digital misinformation and is forthcoming at the Journal of Politics.\r\n\r\n\r\n\n\nAnother output of the project is the CANDOR corpus, a vast multimodal dataset of over 850 hours of conversational video, audio, and transcripts. This dataset provides detailed insights into conversational dynamics, including turn-taking, emotional expression, and post-conversation reflections. It provides public access to the largest corpus of its type to date, and it has been used to support interdisciplinary research on a range of topics such as conversation's effects on psychological well-being, the formation of perceptions and meta-perceptions through social interaction, and computational modeling of conversational behavior through large-language-model dialogue agents. This study is published in Science Advances.\r\n\r\n\r\n\n\nWe also extended and released open-source tools for video annotation. One is built in Shiny and can be used to label both local and remote data. A second is built in Python for the annotation of local data.\r\n\r\n\r\n\n\nThis projects outcomes have advanced our understanding of political communication by providing new causal frameworks, experimental insights, and practical tools. Finally, the award also developed the careers of multiple graduate students, some of whom are already successfully transitioning into faculty positions at leading research universities.\r\n\r\n\r\n\n\n \n\r\n\r\n\t\t\t\t\tLast Modified: 11/29/2024\n\n\t\t\t\t\tSubmitted by: ChristopherLucas\n"
 }
}