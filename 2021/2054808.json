{
 "awd_id": "2054808",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Deep Learning and Random Forests for High-Dimensional Regression",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 157960.0,
 "awd_amount": 157960.0,
 "awd_min_amd_letter_date": "2020-10-19",
 "awd_max_amd_letter_date": "2021-08-03",
 "awd_abstract_narration": "This project aims to investigate two of the most widely used and state-of-the-art methods for high-dimensional regression: deep neural networks and random forests. Despite their widespread implementation, pinning down their theoretical properties has eluded researchers until recently. The proposed research aims to add to the growing body of literature on their analysis, by both developing tools of theoretical value and providing guarantees and guidance for practitioners and applied scientists who use these popular methods frequently in their work.\r\n\r\nThe success of multi-layer networks has largely been buoyed by their ability to generalize well despite being able to fit most datasets, given enough parameters. This phenomenon is particularly striking when the input dimension is far greater than the available sample size, as is the case with many modern applications in molecular biology, medical imaging, and astrophysics, to name a few. A major component of the proposed work will be to obtain complexity bounds for classes of deep neural networks with controls on the size of their weights, which can then be used to bound generalization error and statistical risk. These complexity bounds reveal the role of complexity penalization, which is based on certain norms of the weights of the network. Motivated by these observations, another stream of the proposed research seeks to provide statistical guarantees of certain complexity penalized estimators and their adaptive properties. Current theoretical results for random forests are either for stylized versions of those that are used in practice or are asymptotic in nature and it is therefore difficult to determine the quality of convergence as a function of the parameters of the random forest. Furthermore, the setting for the analysis of more practical implementations of random forests is limited to structured, fixed-dimensional regression function classes. Given these restrictions, the first component of the proposal aims to investigate how random forests behave in the high-dimensional regime when the number of predictors grows with the sample size. Another research objective is to isolate and study families of flexible high-dimensional regression functions for which finite sample convergence rates can be established. The final endeavor of this project is to connect popular measures of variable importance  to the bias of random forests. Since variable importance measures are used for assessing the role each predictor variable plays in influencing the output, this connection will partially explain why random forests are adaptive to sparsity. The relationship will also help to theoretically motivate variable importance measures as useful tools for model interpretability.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Klusowski",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jason M Klusowski",
   "pi_email_addr": "jason.klusowski@princeton.edu",
   "nsf_id": "000791723",
   "pi_start_date": "2020-10-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "Off. of Research & Proj. Admin.",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 10132.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 73213.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 74615.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">In the era of complex decision-making, the need for simple, robust, and efficient tools to extract actionable insights from vast amounts of data is more pressing than ever. The PI's project aimed to address this challenge by advancing the theory and application of decision trees and random forests, two powerful methods in statistics and machine learning. Over the course of the grant, the PI made significant strides in understanding the theoretical foundations of these methods and improving their performance in practical settings.</p>\n<p class=\"p1\">One of the key outcomes of the project was the development of a theoretical framework for studying the performance of decision trees, random forests, and derived measures of variables importance. These new analytical tools rely on connections between decision trees and sequential greedy approximation algorithms for convex optimization problems.&nbsp;</p>\n<p class=\"p1\">Using this framework, the PI demonstrated that popular implementations of decision trees and random forests are accurate for both regression and classification tasks, even when a very large number of input variables, relevant or not, are observed and included in the model. The work also revealed how various tuning parameters affect performance, leading to practical insights about they should be chosen. Finally, the PI showed how simple measures of variable importance derived from decision tree methodology can be used to identity relevant input variables in a nonlinear predictive model, with finite-sample guarantees.</p>\n<p class=\"p1\">These findings have important implications for various fields which rely on tree-based procedures, including bioinformatics, finance, and healthcare. For instance, in bioinformatics, the PI's work has implications for tree-based methods in analyzing genomic data and identify patterns in gene expression, leading to advancements in personalized medicine and drug discovery. In finance, the PI's improved understanding of decision trees and random forests can enhance financial modeling and risk management practices, ultimately contributing to a more stable and efficient financial system. In healthcare, the techniques developed by the PI can be used to improve disease diagnosis and treatment planning, leading to better patient outcomes and more efficient healthcare delivery.</p>\n<p class=\"p2\">In conclusion, the PI's project has produced significant outcomes that advance our understanding of decision trees and random forests and their applications. By improving the reliability, interpretability, and performance of these methods, results of this project will empower decision-makers with powerful tools to navigate the complexities of the digital age and drive innovation and progress.</p><br>\n<p>\n Last Modified: 02/28/2024<br>\nModified by: Jason&nbsp;M&nbsp;Klusowski</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn the era of complex decision-making, the need for simple, robust, and efficient tools to extract actionable insights from vast amounts of data is more pressing than ever. The PI's project aimed to address this challenge by advancing the theory and application of decision trees and random forests, two powerful methods in statistics and machine learning. Over the course of the grant, the PI made significant strides in understanding the theoretical foundations of these methods and improving their performance in practical settings.\n\n\nOne of the key outcomes of the project was the development of a theoretical framework for studying the performance of decision trees, random forests, and derived measures of variables importance. These new analytical tools rely on connections between decision trees and sequential greedy approximation algorithms for convex optimization problems.\n\n\nUsing this framework, the PI demonstrated that popular implementations of decision trees and random forests are accurate for both regression and classification tasks, even when a very large number of input variables, relevant or not, are observed and included in the model. The work also revealed how various tuning parameters affect performance, leading to practical insights about they should be chosen. Finally, the PI showed how simple measures of variable importance derived from decision tree methodology can be used to identity relevant input variables in a nonlinear predictive model, with finite-sample guarantees.\n\n\nThese findings have important implications for various fields which rely on tree-based procedures, including bioinformatics, finance, and healthcare. For instance, in bioinformatics, the PI's work has implications for tree-based methods in analyzing genomic data and identify patterns in gene expression, leading to advancements in personalized medicine and drug discovery. In finance, the PI's improved understanding of decision trees and random forests can enhance financial modeling and risk management practices, ultimately contributing to a more stable and efficient financial system. In healthcare, the techniques developed by the PI can be used to improve disease diagnosis and treatment planning, leading to better patient outcomes and more efficient healthcare delivery.\n\n\nIn conclusion, the PI's project has produced significant outcomes that advance our understanding of decision trees and random forests and their applications. By improving the reliability, interpretability, and performance of these methods, results of this project will empower decision-makers with powerful tools to navigate the complexities of the digital age and drive innovation and progress.\t\t\t\t\tLast Modified: 02/28/2024\n\n\t\t\t\t\tSubmitted by: JasonMKlusowski\n"
 }
}