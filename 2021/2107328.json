{
 "awd_id": "2107328",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Collaborative Research: Situated Visual Information Spaces",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922706",
 "po_email": "ccaragea@nsf.gov",
 "po_sign_block_name": "Cornelia Caragea",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 403456.0,
 "awd_amount": 403456.0,
 "awd_min_amd_letter_date": "2021-07-07",
 "awd_max_amd_letter_date": "2023-07-31",
 "awd_abstract_narration": "The aim of this project is to enable people to effectively visualize information about the world in augmented reality. Augmented reality is potentially the next big social benefit from computer technologies, because it allows visual information to be embedded - or \u2018situated\u2019 - into the real world. This allows people using smartphones and smartglasses to see data around them in the correct real-world context. However, unlike when visualizing data on a regular computer or smartphone display, where a designer has complete control over how the application looks and feels, augmented reality visualizations are inherently overlaid on the real world. As such, visualizations must be capable of reacting to different real-world environments including dynamic scenes, and for there to be design recommendations that say how visualizations should react to different environments. This project will scientifically investigate visualization for augmented reality, study the efficacy of different approaches, create design recommendations, and then build a software system that can apply these recommendations to help design and run effective visualization applications. The proposed approach will be experimentally validated in the sports and healthcare domains.\r\n\r\nSituated visual information spaces fuse the digital information world with the physical world of objects, people, locations, and environments using augmented reality. To realize this, three scientific and design challenges will be tackled: (1) Situated visualization, interaction, and collaboration, which requires intuitive in-situ data visualizations, physical and digital interfaces for natural user interactions, and schemes for collaboration in augmented reality. Novel situated visual embedding methods will be studied for spatial and non-spatial data in dynamic environmental and situational contexts. These visualizations will automatically adapt to the physical environment, digital entities, users, and tasks while using perceptually and cognitively effective methods that do not overwhelm the user. (2) Design via constraints, where software reduces the increased complexity of creating visualizations that adapt to real-world environments. This software is aimed at visualization designers and evaluates guidelines as constraints, then balances these to provide recommendations for appropriate data and designs for the current environment. (3) Situated applications, where two wellness applications in healthcare and sports will be developed and evaluated in partnership with respective domain experts. Within them, these domains cover a spectrum of different techniques, tasks, and users. These applications will help to define an achievable research scope, drive it with motivated stakeholders, and present best-practices via use cases.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hanspeter",
   "pi_last_name": "Pfister",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hanspeter Pfister",
   "pi_email_addr": "pfister@seas.harvard.edu",
   "nsf_id": "000185558",
   "pi_start_date": "2021-07-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "President and Fellows of Harvard College",
  "perf_str_addr": "33 Oxford Street",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021385369",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 141587.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 133834.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 128035.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-159037ee-7fff-590e-64ab-d9209c40567f\">\r\n<p dir=\"ltr\"><span>Situated visual information spaces (sVIS) employ augmented reality (AR) and mixed reality (MR) technologies to visualize and analyze information about the physical world that was previously hidden, inaccessible, or difficult to integrate using existing computational interfaces. The techniques developed toward this goal address essential aspects of visualization design, user interaction, and collaboration, enabling the intelligent integration of digital data into physical environments. To demonstrate sVIS, the investigators have developed new tools for high-profile sports such as basketball, and applications in healthcare, such as biomedical visualization and AR-assisted surgery. These tools illustrate how digital information can seamlessly enhance real-world experiences, providing users with intuitive and contextually relevant insights that augment human information-processing capability.</span></p>\r\n<p dir=\"ltr\"><span>At a foundational level, this project investigated how virtual labels should relate to their physical referents in AR.&nbsp; The project designed methods that adapt labels to environmental conditions such as lighting, clutter, and color similarity, maintaining visibility without obstructing critical real-world details. For dynamic scenarios, the investigators used deep reinforcement learning capabilities in their work RL- LABEL to optimize label placement by predicting object and user movement, subsequently minimizing occlusions for improved label perception. These innovations advance the usefulness of AR interfaces in real-world, dynamic conditions.</span></p>\r\n<p dir=\"ltr\"><span>Basic techniques have been advanced through systems like Data PerformAR, which allows control of visualization elements using staging and gestures or even embeds data visualizations directly onto presenters&rsquo; bodies. This is used to create immersive and engaging presentations that make data more intuitive and comprehensible for viewers. The investigators also explored using large immersive display systems and AR auxiliary displays to support big data visual analytics in collaborative settings. This allows users to seamlessly integrate personal AR devices with large-scale visualizations, enhancing focus and context in tasks requiring detailed exploration of high-dimensional data.</span></p>\r\n<p dir=\"ltr\"><span>Applications in sports, healthcare, and education have highlighted the practical value of this work. Namely, Sportify, an AR-based tool, combines narratives and visualizations to explain basketball tactics, deepening fans' understanding and engagement. The ARrow tool provides real-time feedback on rowing techniques, while the VIRD tool supports immersive analysis of badminton matches through 3D reconstructions and spatial visualizations. These applications demonstrate how situated visualizations can improve training outcomes and spectator experiences. Healthcare efforts addressed challenges in visualizing complex scientific data through optical see-through headsets, enhancing clarity and precision under varying lighting and background conditions. The VoxAR technique extended this work by adapting volume-rendered medical data for optical see-through displays, improving doctors&rsquo; and medical domain experts&rsquo; perception and interpretation of intricate information, rendered situated in their clinical surroundings.</span></p>\r\n<p dir=\"ltr\"><span>A significant aim of sVIS has been to create the foundations for a generation of &lsquo;AR natives&rsquo; who will design and use visual information spaces that are complex, multi-view, and adaptive, and to enhance human capacity to process information throughout our everyday work and play experiences. As a result, throughout the project, the investigators developed open-source tools such as iBall and VIRD, and scientific publications that provide</span><span> </span><span>data-driven recommendations for designing adaptive visualizations. The investigators have moreover developed a framework for understanding user behavior in extended reality (XR) environments. This framework, known as explainable XR (EXR), standardizes the recording and analysis of multimodal user interactions, enabling insights into how users interact with AR and XR systems. The framework integrates session recording, web-based visual analytics, and advanced models for analyzing user behavior, contributing to the development of more effective AR systems.</span><span> </span><span>To understand visual attention in AR and XR environments, the investigators have conducted experiments using eye-tracking data and have contributed to the identification of key spatial factors&mdash;such as eccentricity and visual angle size&mdash;that influence user attention. This research informed the creation of predictive models for visual attention, which can guide the design of AR systems that align with users&rsquo; perceptual and cognitive processes. These insights are critical for optimizing user experience and ensuring that visualizations are both intuitive and effective. The project has laid critical groundwork for future AR systems that enhance human understanding and interaction with digital information by addressing foundational challenges in adaptive visualization and user interaction. The results contribute to the growing body of knowledge in visualization science, providing a pathway for continued innovation and the development of immersive technologies that improve daily life and professional practice.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>Further, the project has demonstrated sVIS technologies for industrial and educational contexts as well. For instance, the project investigated the impact of XR visual guidance on user performance in virtual assembly tasks, showing a significant reduction in task completion times and errors when guided by AR-based instructions. This finding highlights the potential of AR to improve productivity and accuracy in complex, real-world tasks. Additionally, educational integration of the project&rsquo;s findings fostered a new generation of designers and developers equipped to create adaptive AR systems. Course materials, research opportunities, and outreach efforts targeted underrepresented groups, promoting diversity and inclusion in AR development.</span></p>\r\n</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/16/2025<br>\nModified by: Hanspeter&nbsp;Pfister</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nSituated visual information spaces (sVIS) employ augmented reality (AR) and mixed reality (MR) technologies to visualize and analyze information about the physical world that was previously hidden, inaccessible, or difficult to integrate using existing computational interfaces. The techniques developed toward this goal address essential aspects of visualization design, user interaction, and collaboration, enabling the intelligent integration of digital data into physical environments. To demonstrate sVIS, the investigators have developed new tools for high-profile sports such as basketball, and applications in healthcare, such as biomedical visualization and AR-assisted surgery. These tools illustrate how digital information can seamlessly enhance real-world experiences, providing users with intuitive and contextually relevant insights that augment human information-processing capability.\r\n\n\nAt a foundational level, this project investigated how virtual labels should relate to their physical referents in AR. The project designed methods that adapt labels to environmental conditions such as lighting, clutter, and color similarity, maintaining visibility without obstructing critical real-world details. For dynamic scenarios, the investigators used deep reinforcement learning capabilities in their work RL- LABEL to optimize label placement by predicting object and user movement, subsequently minimizing occlusions for improved label perception. These innovations advance the usefulness of AR interfaces in real-world, dynamic conditions.\r\n\n\nBasic techniques have been advanced through systems like Data PerformAR, which allows control of visualization elements using staging and gestures or even embeds data visualizations directly onto presenters bodies. This is used to create immersive and engaging presentations that make data more intuitive and comprehensible for viewers. The investigators also explored using large immersive display systems and AR auxiliary displays to support big data visual analytics in collaborative settings. This allows users to seamlessly integrate personal AR devices with large-scale visualizations, enhancing focus and context in tasks requiring detailed exploration of high-dimensional data.\r\n\n\nApplications in sports, healthcare, and education have highlighted the practical value of this work. Namely, Sportify, an AR-based tool, combines narratives and visualizations to explain basketball tactics, deepening fans' understanding and engagement. The ARrow tool provides real-time feedback on rowing techniques, while the VIRD tool supports immersive analysis of badminton matches through 3D reconstructions and spatial visualizations. These applications demonstrate how situated visualizations can improve training outcomes and spectator experiences. Healthcare efforts addressed challenges in visualizing complex scientific data through optical see-through headsets, enhancing clarity and precision under varying lighting and background conditions. The VoxAR technique extended this work by adapting volume-rendered medical data for optical see-through displays, improving doctors and medical domain experts perception and interpretation of intricate information, rendered situated in their clinical surroundings.\r\n\n\nA significant aim of sVIS has been to create the foundations for a generation of AR natives who will design and use visual information spaces that are complex, multi-view, and adaptive, and to enhance human capacity to process information throughout our everyday work and play experiences. As a result, throughout the project, the investigators developed open-source tools such as iBall and VIRD, and scientific publications that provide data-driven recommendations for designing adaptive visualizations. The investigators have moreover developed a framework for understanding user behavior in extended reality (XR) environments. This framework, known as explainable XR (EXR), standardizes the recording and analysis of multimodal user interactions, enabling insights into how users interact with AR and XR systems. The framework integrates session recording, web-based visual analytics, and advanced models for analyzing user behavior, contributing to the development of more effective AR systems. To understand visual attention in AR and XR environments, the investigators have conducted experiments using eye-tracking data and have contributed to the identification of key spatial factorssuch as eccentricity and visual angle sizethat influence user attention. This research informed the creation of predictive models for visual attention, which can guide the design of AR systems that align with users perceptual and cognitive processes. These insights are critical for optimizing user experience and ensuring that visualizations are both intuitive and effective. The project has laid critical groundwork for future AR systems that enhance human understanding and interaction with digital information by addressing foundational challenges in adaptive visualization and user interaction. The results contribute to the growing body of knowledge in visualization science, providing a pathway for continued innovation and the development of immersive technologies that improve daily life and professional practice.\r\n\n\nFurther, the project has demonstrated sVIS technologies for industrial and educational contexts as well. For instance, the project investigated the impact of XR visual guidance on user performance in virtual assembly tasks, showing a significant reduction in task completion times and errors when guided by AR-based instructions. This finding highlights the potential of AR to improve productivity and accuracy in complex, real-world tasks. Additionally, educational integration of the projects findings fostered a new generation of designers and developers equipped to create adaptive AR systems. Course materials, research opportunities, and outreach efforts targeted underrepresented groups, promoting diversity and inclusion in AR development.\r\n\r\n\n\n\t\t\t\t\tLast Modified: 01/16/2025\n\n\t\t\t\t\tSubmitted by: HanspeterPfister\n"
 }
}