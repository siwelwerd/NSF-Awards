{
 "awd_id": "2104758",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII : HCC : Smart Acoustic Surfaces as Multimodal Interfaces",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-06-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 174824.0,
 "awd_amount": 174824.0,
 "awd_min_amd_letter_date": "2021-03-22",
 "awd_max_amd_letter_date": "2021-05-21",
 "awd_abstract_narration": "Over the course of the last decade, smart devices have become essential components of daily life. However, apart from conversational assistants, most smart devices and environments rely primarily on presenting information visually, a poor fit for many situations where users' visual attention is on something besides the device. One way to support these situations is to let people use sound or touch for interactions. However, adding separate microphones, loudspeakers, and vibration mechanisms using conventional means can impose extra costs and reduce the device\u2019s durability and aesthetics. This project seeks to develop alternative technologies for recording and reproducing spatial audio through bending vibrations on flat surfaces ranging from smartphones to video walls. These vibrations can also allow the smart acoustic surface to serve as a touch interface. The advantage of this approach lies in the fact that the surface is already part of the device, allowing the device to maintain its durability and aesthetics while incorporating these new features. Adding spatial audio and haptic feedback to OLED displays, smartphones, and video walls will provide fundamental technology to improve people's ability to navigate complex data sets such as menus and maps, and enable a greater sense of immersion for remote applications such as video conference calls. \r\n\r\nThe project develops a framework for designing surfaces such as the display screen of a device to serve as acoustic and vibrotactile interfaces. The research objectives will address the following three challenges by employing the vibro-acoustics of extended surfaces: (1) spatial audio reproduction, (2) spatial audio capture, and (3) haptic feedback and touch sensing. The vibrations of the extended surface induced by acoustic and touch interactions with the structure can be detected using arrays of sensors distributed on the surface. Similarly, arrays of force actuators distributed on the surface will induce bending vibrations that cause the structure to radiate sound. Loudspeaker and microphone array processing methods will be adapted for the vibration actuators and sensors with applications including noise reduction, sound-field control, and beamforming. Machine learning techniques will be used to identify source-location features stemming from vibration patterns induced by acoustic and touch interactions with the structure. Acoustic recordings and scanning laser vibrometer measurements will be used to analyze the effectiveness of the framework in real-world settings.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Heilemann",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Michael C Heilemann",
   "pi_email_addr": "mheilema@ur.rochester.edu",
   "nsf_id": "000787346",
   "pi_start_date": "2021-03-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "Computer Studies Building, Room 611",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146270140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 174824.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As computing technology has continued to advance, the limitation on the performance of smartphones, tablets, and video screens is being set by a user&rsquo;s ability to navigate the large amount of visual information presented on the display of the device rather than the available computing power. There is strong evidence to suggest that devices that combine additional modalities such as spatial sound and haptic feedback can improve user&rsquo;s ability to perform search tasks, and give a greater sense of immersion. However, conventional means of incorporating these additional modalities, especially spatial sound, requires physical installations of loudspeaker and/or microphone arrays that are not compatible with the compact designs of modern displays, and can affect the aesthetics of the space in which they are deployed. This project laid the groundwork for new systems to capture and render spatial audio, and touch inputs using the vibrations of flat surfaces, which can allow these modes of interaction to be deployed without disturbing the built environment.</p>\n<p>Surfaces such as display panels can be turned into loudspeakers by affixing force transducers on the back to drive sound-producing vibrations. As part of this project, we demonstrated that an array of force transducers may be used to localize these vibrations to particular regions of the surface as primary audio sources. This gives the potential for a number of intriguing multimedia applications such as display systems that align audio sources with their corresponding visual images.</p>\n<p>We also demonstrated that the human voice can be captured by recording the vibrations induced by speech signals in various structures within a space. An extremely low-cost version of this vibration microphone was demonstrated using an inexpensive piezoelectric disk bonded to a panel. The resonant modes of the surface inevitably degrade the audio quality by introducing reverberation into the recorded signal. However, a widely-employed commercial speech-to-text service was able to transcribe a series of recorded speech signals with a word-error-rate (WER) only 6% higher than the same speech signals captured by a studio microphone. The WER can be reduced by recording the vibrations of surfaces with greater internal damping. This suggests that structural microphones can be an effective means of communicating with smart devices without affecting the durability or aesthetics of the device.&nbsp;</p>\n<p>One benefit that arises from recording audio signals with a structural vibration sensor is that the resonant modes of the structure provide spatial information about the source such as its position relative to the surface. Typical methods of inferring the direction of an audio source require multi-microphone arrays. We applied machine learning methods to correctly infer the direction of speech signals to a tolerance of 5 degrees with above 90% accuracy for a panel using just one structural sensor. This can greatly reduce the cost of systems for capturing spatial audio, and has implications for other audio applications such as direction-based signal enhancement, event detection, and source localization/tracking. It is interesting to note that since the resonances of the structure provide the spatial information, a tradeoff exists between the quality of the recorded audio and the microphone&rsquo;s ability to accurately infer the direction of the source.&nbsp;</p>\n<p>In a similar way, the resonant modes of the structure also provide spatial information about how and where a surface is touched. Our work in this area was limited to a basic proof-of-concept during this project, but we demonstrated that the touch location of a stylus on a panel could be accurately classified to 1 cm boxes with 100% accuracy using a single vibration sensor. Vibration-based touch systems can allow for large displays such as televisions or information kiosks to be equipped with touch sensing capabilities without incurring the large costs associated with scaling conventional touch-sensing technologies. Future work in this area will include developing a system to recognize gestures such as swipes, pinches, and multi-touch inputs. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/08/2023<br>\nModified by: Michael&nbsp;C&nbsp;Heilemann</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs computing technology has continued to advance, the limitation on the performance of smartphones, tablets, and video screens is being set by a users ability to navigate the large amount of visual information presented on the display of the device rather than the available computing power. There is strong evidence to suggest that devices that combine additional modalities such as spatial sound and haptic feedback can improve users ability to perform search tasks, and give a greater sense of immersion. However, conventional means of incorporating these additional modalities, especially spatial sound, requires physical installations of loudspeaker and/or microphone arrays that are not compatible with the compact designs of modern displays, and can affect the aesthetics of the space in which they are deployed. This project laid the groundwork for new systems to capture and render spatial audio, and touch inputs using the vibrations of flat surfaces, which can allow these modes of interaction to be deployed without disturbing the built environment.\n\n\nSurfaces such as display panels can be turned into loudspeakers by affixing force transducers on the back to drive sound-producing vibrations. As part of this project, we demonstrated that an array of force transducers may be used to localize these vibrations to particular regions of the surface as primary audio sources. This gives the potential for a number of intriguing multimedia applications such as display systems that align audio sources with their corresponding visual images.\n\n\nWe also demonstrated that the human voice can be captured by recording the vibrations induced by speech signals in various structures within a space. An extremely low-cost version of this vibration microphone was demonstrated using an inexpensive piezoelectric disk bonded to a panel. The resonant modes of the surface inevitably degrade the audio quality by introducing reverberation into the recorded signal. However, a widely-employed commercial speech-to-text service was able to transcribe a series of recorded speech signals with a word-error-rate (WER) only 6% higher than the same speech signals captured by a studio microphone. The WER can be reduced by recording the vibrations of surfaces with greater internal damping. This suggests that structural microphones can be an effective means of communicating with smart devices without affecting the durability or aesthetics of the device.\n\n\nOne benefit that arises from recording audio signals with a structural vibration sensor is that the resonant modes of the structure provide spatial information about the source such as its position relative to the surface. Typical methods of inferring the direction of an audio source require multi-microphone arrays. We applied machine learning methods to correctly infer the direction of speech signals to a tolerance of 5 degrees with above 90% accuracy for a panel using just one structural sensor. This can greatly reduce the cost of systems for capturing spatial audio, and has implications for other audio applications such as direction-based signal enhancement, event detection, and source localization/tracking. It is interesting to note that since the resonances of the structure provide the spatial information, a tradeoff exists between the quality of the recorded audio and the microphones ability to accurately infer the direction of the source.\n\n\nIn a similar way, the resonant modes of the structure also provide spatial information about how and where a surface is touched. Our work in this area was limited to a basic proof-of-concept during this project, but we demonstrated that the touch location of a stylus on a panel could be accurately classified to 1 cm boxes with 100% accuracy using a single vibration sensor. Vibration-based touch systems can allow for large displays such as televisions or information kiosks to be equipped with touch sensing capabilities without incurring the large costs associated with scaling conventional touch-sensing technologies. Future work in this area will include developing a system to recognize gestures such as swipes, pinches, and multi-touch inputs. \n\n\n\t\t\t\t\tLast Modified: 12/08/2023\n\n\t\t\t\t\tSubmitted by: MichaelCHeilemann\n"
 }
}