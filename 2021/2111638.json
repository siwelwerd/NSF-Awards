{
 "awd_id": "2111638",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Agent-Based Identification of Constitutive Relationships from Large Manufacturing Datasets",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922936",
 "po_email": "emirowsk@nsf.gov",
 "po_sign_block_name": "Ela Mirowski",
 "awd_eff_date": "2021-08-01",
 "awd_exp_date": "2023-01-31",
 "tot_intn_awd_amt": 255964.0,
 "awd_amount": 255964.0,
 "awd_min_amd_letter_date": "2021-07-29",
 "awd_max_amd_letter_date": "2021-07-29",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project is to improve utilization of data resources. While data is an integral part of contemporary business\u2014used to inform strategic, technical, and financial decisions\u2014data collection remains federated in many fields, including manufacturing, because logistical, practical, and strategic hurdles prevent centralization. Consequently, these data resources quickly become isolated. No longer FAIR (Findable, Accessible, Interoperable, Reusable), the value of data so expensive to collect is lost. The proposed technology addresses two major concerns facing effective utilization of federated data. First, it develops a unified interface to analyze and explore federated data, without sacrificing control over data access. Second, it integrates machine learning with an understanding of the physical system.  \r\n\r\nThe proposed technology is a mathematically rigorous translation between neural networks and the constitutive relationships describing the underlying physics. The two approaches will leverage measurements of a process environment, including time, temperature, and pressure, as well as mechanical strength or chemical reactivity. Neural networks, which are general and easy to train, estimate system behavior through statistical correlations, which is ideal for repetitive, complex systems, such as manufacturing processes; but they require increasingly large and diverse datasets to expand the conditions under which they are reliable. In contrast, constitutive relationships, which often take years to develop, can be used to predict how a system will behave under new conditions.  This system will integrate both approaches.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Branden",
   "pi_last_name": "Kappes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Branden Kappes",
   "pi_email_addr": "branden.kappes@contextualize.us.com",
   "nsf_id": "000767517",
   "pi_start_date": "2021-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CONTEXTUALIZE, LLC",
  "inst_street_address": "2679 W MAIN ST",
  "inst_street_address_2": "STE 300-758",
  "inst_city_name": "LITTLETON",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3039082653",
  "inst_zip_code": "801201950",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "CO06",
  "org_lgl_bus_name": "CONTEXTUALIZE, LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "P1MCBT1JQL15"
 },
 "perf_inst": {
  "perf_inst_name": "CONTEXTUALIZE LLC",
  "perf_str_addr": "483 E. 2nd Ave.",
  "perf_city_name": "Castle Rock",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "801089212",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CO04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8037",
   "pgm_ref_txt": "Advanced Manufacturing"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 255964.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;Data impacts every sector of society. It is collected to document what has happened and interpreted to understand why. Data comes not just from databases, data lakes, and shared folders. There are myriad sources for this data: a person or several people, a software program, a piece of equipment, or a database. Each source has peculiar characteristics that require special attention. There is no escaping that these peculiarities must be considered during data collection, processing, and interpretation and handling those peculiarities requires special knowledge; knowledge that may come from formal training or may simply be a unique familiarity with a process or procedure. Capturing domain expertise ensures that handling those peculiarities happens only once so that the data and its analysis may be made easily accessible and easily usable by every member of an organization, team, group, collaborator, or other stakeholder.</p>\n<p>Understanding data can take many forms, from an informal impression to expert analysis to formal mathematical and statistical modeling using tools like machine learning and artificial intelligence. But by whatever mechanism that data is understood, that understanding is predicated on having a complete and thorough view of the data. The goal of SBIR 2111678 was first to enable understanding through the consolidation and integration of data from many, disparate sources and, second, to bridge the divide between expert analysis and statistical modeling.</p>\n<p>Both the origin and destination of data impact how that data will be used. Sophisticated users?that is, users with a solid foundational in a particular field?can work with less processed data. Those with expertise outside the field may require expert guidance to develop actionable insights from a specialized data resource. Carta was developed to bridge this gap and bring dynamic and up-to-date insights to every type of data consumer.</p>\n<p>The result of this project was the development of Carta, a platform that seamlessly integrates data with domain expertise. And while closing the loop on expert analysis and statistical modeling remains a grand challenge, Carta provides the foundational infrastructure to capture and convey the expertise needed to transform that data into information and, from information, understanding. Carta has also generated specific IP around the handling and integration of disparate data sources, identifying statistically sound correlations between temporal and spatial data through novel and exclusive data fusion.</p>\n<p>However, not every aspect of SBIR 2111678 was an unqualified success. Paraphrasing Nelson Mandela, I never lose. I either win or I learn. The most ambitious goal of this project was to attempt to develop first a mathematical and then a numerical approach to map descriptive statistical machine learning algorithms with explanatory analytical formulae. A rigorous solution proved to be mathematically impossible within the confines of modern mathematical theories, including number theory, functional analysis, and group theory. Having recognized this as a possibility even at the proposal stage, preliminary work included a numerical approximation that was applied to known functions, performed well, and appeared promising. However, this early success met with the hard reality that even small deviations from a mathematical ideal result in prohibitive and insurmountable numerical instabilities. Once a more nearly closed-form solution was deemed impossible, novel approximations quickly returned to known methods that leverage the very same trained machine learning models and analytical formulations that accomplish the same task at a much lower computational burden.</p>\n<p>Despite its early-stage development, Contextualize has already started to apply the infrastructure developed under this National Science Foundation SBIR to data collected by a variety of organization and data modalities. It has been used by several universities, national laboratories, and commercial enterprise to examine not only their own data, but to incorporate data from their collaborators and partners.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/18/2023<br>\n\t\t\t\t\tModified by: Branden&nbsp;Kappes</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n Data impacts every sector of society. It is collected to document what has happened and interpreted to understand why. Data comes not just from databases, data lakes, and shared folders. There are myriad sources for this data: a person or several people, a software program, a piece of equipment, or a database. Each source has peculiar characteristics that require special attention. There is no escaping that these peculiarities must be considered during data collection, processing, and interpretation and handling those peculiarities requires special knowledge; knowledge that may come from formal training or may simply be a unique familiarity with a process or procedure. Capturing domain expertise ensures that handling those peculiarities happens only once so that the data and its analysis may be made easily accessible and easily usable by every member of an organization, team, group, collaborator, or other stakeholder.\n\nUnderstanding data can take many forms, from an informal impression to expert analysis to formal mathematical and statistical modeling using tools like machine learning and artificial intelligence. But by whatever mechanism that data is understood, that understanding is predicated on having a complete and thorough view of the data. The goal of SBIR 2111678 was first to enable understanding through the consolidation and integration of data from many, disparate sources and, second, to bridge the divide between expert analysis and statistical modeling.\n\nBoth the origin and destination of data impact how that data will be used. Sophisticated users?that is, users with a solid foundational in a particular field?can work with less processed data. Those with expertise outside the field may require expert guidance to develop actionable insights from a specialized data resource. Carta was developed to bridge this gap and bring dynamic and up-to-date insights to every type of data consumer.\n\nThe result of this project was the development of Carta, a platform that seamlessly integrates data with domain expertise. And while closing the loop on expert analysis and statistical modeling remains a grand challenge, Carta provides the foundational infrastructure to capture and convey the expertise needed to transform that data into information and, from information, understanding. Carta has also generated specific IP around the handling and integration of disparate data sources, identifying statistically sound correlations between temporal and spatial data through novel and exclusive data fusion.\n\nHowever, not every aspect of SBIR 2111678 was an unqualified success. Paraphrasing Nelson Mandela, I never lose. I either win or I learn. The most ambitious goal of this project was to attempt to develop first a mathematical and then a numerical approach to map descriptive statistical machine learning algorithms with explanatory analytical formulae. A rigorous solution proved to be mathematically impossible within the confines of modern mathematical theories, including number theory, functional analysis, and group theory. Having recognized this as a possibility even at the proposal stage, preliminary work included a numerical approximation that was applied to known functions, performed well, and appeared promising. However, this early success met with the hard reality that even small deviations from a mathematical ideal result in prohibitive and insurmountable numerical instabilities. Once a more nearly closed-form solution was deemed impossible, novel approximations quickly returned to known methods that leverage the very same trained machine learning models and analytical formulations that accomplish the same task at a much lower computational burden.\n\nDespite its early-stage development, Contextualize has already started to apply the infrastructure developed under this National Science Foundation SBIR to data collected by a variety of organization and data modalities. It has been used by several universities, national laboratories, and commercial enterprise to examine not only their own data, but to incorporate data from their collaborators and partners.\n\n\t\t\t\t\tLast Modified: 03/18/2023\n\n\t\t\t\t\tSubmitted by: Branden Kappes"
 }
}