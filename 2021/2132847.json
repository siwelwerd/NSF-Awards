{
 "awd_id": "2132847",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI/Collaborative Research: Robot-Assisted Feeding: Towards Efficient, Safe, and Personalized Caregiving Robots",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2022-01-01",
 "awd_exp_date": "2025-12-31",
 "tot_intn_awd_amt": 508543.0,
 "awd_amount": 508543.0,
 "awd_min_amd_letter_date": "2021-09-08",
 "awd_max_amd_letter_date": "2021-09-08",
 "awd_abstract_narration": "The goal of this project is to develop caregiving robots that can provide long-term assistance with activities of daily living (ADLs) to people with mobility limitations. Despite great strides taken towards sustainable solutions in controlled environments, robots are far from ready for adoption in real home environments as long-term caregiving solutions. Key factors could be the over-reliance on full autonomy in tasks that require dynamic physical and social interactions in unstructured environments as well as the lack of personalized assistance. Based on the central tenet that robots need to optimize both physical and social interactions to provide efficient, safe, and personalized assistance for ADLs, this work will focus on developing robot-assisted feeding as a long-term caregiving solution for a person with upper-extremity disability in an unstructured, real-home environment. Successful feeding consists of bite acquisition (i.e., picking up a food item) and bite transfer (i.e., moving it into the mouth). This project develops methods to integrate these activities towards the development of an intelligent and personalized robot-assisted feeding system. The models leverage multimodal feedback to develop human-in-the-loop control policies that adapt to a range of human and environmental factors. Realizing that full autonomy can be challenging in unstructured and dynamic environments, the methods will leverage expert human feedback while minimizing the cognitive load and interweave them intelligently with autonomy to arrive at a long-term caregiving solution. This work will have a direct impact on the lives, health, and comfort of millions of people in the world who live with motor impairments. Developing policies that consider the human in the loop at every step and learn from their feedback through multiple modalities will have an impact on many other human-robot interaction domains including but not limited to assistive teleoperation.\r\n\r\nThis project will advance the state of the art of robotics from both a technical and algorithmic perspective. First, novel bite acquisition algorithms will be developed that are capable of picking up deformable objects in unstructured settings. Second, bite-transfer algorithms will be developed that learn from physical feedback provided on the robot or on the utensil when transferring food inside of a person's mouth. Finally, the research team will develop active and adaptive algorithms that tap into other sources of data, such as comparisons or language instructions, to intelligently improve the acquisition and transfer algorithms and personalize the feeding experience. This work leverages the idea of learning from multimodal human feedback---specifically by embracing physical interactions rather than trying to avoid them---to better manipulate and transfer food. The assistive acquisition and transfer algorithms will be extensively evaluated through human subject studies. The algorithms will be implemented on multiple high-degree-of-freedom robotic platforms across labs. Planned user studies and low-level implementations will advance the state of robotics outside of assistive feeding, particularly towards other ADLs or Instrumental ADLs (IADLs) in home settings, such as meal-preparation, cooking, and housework.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dorsa",
   "pi_last_name": "Sadigh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dorsa Sadigh",
   "pi_email_addr": "dorsa@cs.stanford.edu",
   "nsf_id": "000769760",
   "pi_start_date": "2021-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 Jane Stanford Way",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 508543.0
  }
 ],
 "por": null
}