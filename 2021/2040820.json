{
 "awd_id": "2040820",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Computational Modeling of the Internal Structure of Events",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924770",
 "po_email": "rtheodor@nsf.gov",
 "po_sign_block_name": "Rachel M. Theodore",
 "awd_eff_date": "2021-09-15",
 "awd_exp_date": "2025-02-28",
 "tot_intn_awd_amt": 103495.0,
 "awd_amount": 103495.0,
 "awd_min_amd_letter_date": "2021-08-12",
 "awd_max_amd_letter_date": "2021-08-12",
 "awd_abstract_narration": "Human language is a powerful tool for conveying information about complex, multi-faceted events at different levels of specificity: in the space of a breath, we can move from talking about a complex event as a whole to a targeted discussion of its many parts and their inter-relationships. Understanding how we convey such complex information using language is critical to improving not only our scientific understanding of human linguistic capacities, but also the ability of artificial intelligence systems to extract knowledge about the world from the massive bodies of text humans generate every day, and ultimately to improve their ability to serve humanity's needs. With the goal of advancing both aims, this project develops foundational resources and cutting-edge deep learning-based artificial intelligence systems for extracting knowledge from those resources.\r\n\r\nTo achieve that goal, the project develops a broad-coverage, automatic method for mapping a description of an event to a rich representation of the relationships among that event's parts: its event structure. It has two main components: (i) it collects behavioral data and text corpus annotations for key aspects of the event structure of verbal, adjectival, and nominal predicates in English; and (ii) it develops and implements a general deep learning-based computational model of event structure, trained using those data. The lexicon and corpus produced under this proposal will be annotated for properties of events that are central in current linguistic theories of tense, grammatical aspect, and lexical aspect: (i) does the event have a natural endpoint (running a race) or not (simply running around)?; (ii) does the event happen at an instant (hitting a ball) or over time (building a house); (iii) what are the event's preconditions and results?; (iv) are those results permanent (killing a mosquito) or transient (opening a door, which can be closed again)?; (v) do they come about gradually (cleaning a table) or abruptly (scuffing a table)?; (vi) does the event consist of indivisible parts (individual claps in applause) or not (being red)?; (vii) are those parts similar (tapping on glass) or dissimilar (shopping for clothes); and (viii) do event parts correspond to participant parts (writing a book) or not (combining ingredients)? On the basis of these annotations, a computational model will be developed and implemented that jointly induces (a) distinct senses of a predicate (running a race v. running a company); (b) the event structure class(es) associated with those senses; (c) the event structure properties associated with those classes; and (d) a mapping from the event's parts to its participants and temporal/causal structure. This model will integrate Bayesian hierarchical models with recent advances in deep learning and will enable explicit quantitative comparison of alternative theoretical assumptions, such as the number of event structure classes and properties that must be posited to best explain the data.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lelia",
   "pi_last_name": "Glass",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lelia Glass",
   "pi_email_addr": "lelia.glass@modlangs.gatech.edu",
   "nsf_id": "000830777",
   "pi_start_date": "2021-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue, NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 103495.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2f8fc519-7fff-3a00-933a-ab0e1a3ca0ee\"> </span></p>\r\n<p dir=\"ltr\"><span>This grant represents a collaboration between the University of Rochester and Georgia Institute of Technology. The Georgia Tech portion is sunsetting while the Rochester team is requesting a no-cost extension. This report describes an interim conclusion from the Georgia Tech side.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>This project aims to gather annotations of text describing events for inferences that a human draws from it. For example, given the event description &ldquo;The package was delivered,&rdquo; we annotate how long the event lasted, what changed as a result, and what steps it involved. Using distinctions highlighted by the philosopher Zeno Vendler, we annotate event descriptions as dynamic or static, durative or instantaneous, and boundless or bounded &ndash; distinctions widely believed to shape the inferences that we draw about how events unfold over time and what sorts of steps or changes they entail. We use these annotations to train models that generalize to new data. Such annotations advance computational language understanding as well as theories about verb meaning.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>We began by gathering such annotations on corpus data from news and blogs from the English Web Treebank&nbsp; (Gantt, Glass, and White 2022, *Transactions of the Association for Computational Linguistics*, 2022).&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>The next phase of our project focused on the meanings of verbs, the words most strongly associated with events. The meaning of a verb lies in its contribution to a sentence, and multiple meanings of the same verb can only be distinguished in context (&ldquo;run a race&rdquo; versus &ldquo;run a company&rdquo;), so we have to place verbs into sentences to annotate them . But we also want to hold constant all of the other information in a sentence (tense marking, relative clauses, and so on), which is difficult in uncontrolled corpus data.&nbsp;&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Our strategy (described in a submission to *Glossa Psycholinguistics*) is to use large language models to write simple, controlled sentences such as &ldquo;The athlete ran the race&rdquo; or &ldquo;The manager ran the company.&rdquo;&nbsp; Then we aim to gather annotations of such sentences to create a dictionary of all English verbs and the Vendler-style inferences drawn from them in controlled contexts, allowing us to test theoretical claims about the behavior of vast classes of verbs.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>In the first phase of this undertaking, we developed a protocol for prompting Meta&rsquo;s Llama-2 model to write these controlled sentences. To address multiple meanings of the same verb (&ldquo;run a race&rdquo; versus &ldquo;run a company&rdquo;), we tried two strategies: we used the expert-written PropBank sense lexicon, and we asked Llama-2 to enumerate the senses of each verb. We used sense definitions from these two sources as input to a prompt asking the model to provide example sentences of the form &ldquo;The NOUN VERBed the NOUN.&rdquo; We gathered annotations of these sentences to ensure that they are natural (following the structural rules of English) and typical (describing expected situations), and that pairs of sentences meant to reflect distinct senses of a verb are judged as more different than pairs meant to reflect the same sense, to show that our sentence-generating tool works as desired. In addition to providing controlled data for our planned annotation of the whole English verb lexicon, our tool may </span><span>be useful for lexicographers, psychologists, or instructional designers who wish to automatically generate example sentences for desired verbs.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>This phase of our project took longer than expected because we changed course to adapt to new generative language models. Now, the Rochester team is moving on to use these automatically-written sentences to gather annotations of all the inferences that we are targeting. &nbsp; These data will be integrated into Aaron White&rsquo;s Universal Decompositional Semantics initiative, which publicizes data of interest to lexical semanticists and those interested in automatically extracting information from text.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Along the way, reflecting evolving research interests, co-PI Lelia Glass published several spinoff projects exploring social and subjective dimensions of event descriptions in text &ndash; aiming to explain why subjective adjectives such as &ldquo;cute&rdquo; are more likely than objective ones (&ldquo;red&rdquo;) to appear as the predicate of a sentence (&ldquo;the red dress is cute&rdquo;); why the seemingly neutral verb &ldquo;cause&rdquo; tends to appear with emotionally negative complements (&ldquo;cause cancer, cause problems&rdquo;); and why people who agree versus disagree with contentious political slogans (&ldquo;Free Palestine&rdquo;) tend to interpret those slogans differently. These projects use data from corpora and web surveys to explore how people formulate and understand sentences describing events, shaped by their subjective attitudes towards those events.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>This project employed several student researchers: three undergraduates on the Georgia Tech side, and two Ph.D. students at Rochester. These students developed hands-on experience in data science, project management, statistical modeling, computer programming, human subjects research, the prompting of language models, and academic writing. Three of these students are women, and two received undergraduate degrees in the humanities, so this project helped to welcome researchers from diverse backgrounds to learn in-demand skills. </span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/21/2025<br>\nModified by: Lelia&nbsp;Glass</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThis grant represents a collaboration between the University of Rochester and Georgia Institute of Technology. The Georgia Tech portion is sunsetting while the Rochester team is requesting a no-cost extension. This report describes an interim conclusion from the Georgia Tech side.\r\n\n\n\r\n\n\nThis project aims to gather annotations of text describing events for inferences that a human draws from it. For example, given the event description The package was delivered, we annotate how long the event lasted, what changed as a result, and what steps it involved. Using distinctions highlighted by the philosopher Zeno Vendler, we annotate event descriptions as dynamic or static, durative or instantaneous, and boundless or bounded  distinctions widely believed to shape the inferences that we draw about how events unfold over time and what sorts of steps or changes they entail. We use these annotations to train models that generalize to new data. Such annotations advance computational language understanding as well as theories about verb meaning.\r\n\n\nWe began by gathering such annotations on corpus data from news and blogs from the English Web Treebank (Gantt, Glass, and White 2022, *Transactions of the Association for Computational Linguistics*, 2022).\r\n\n\n\r\n\n\nThe next phase of our project focused on the meanings of verbs, the words most strongly associated with events. The meaning of a verb lies in its contribution to a sentence, and multiple meanings of the same verb can only be distinguished in context (run a race versus run a company), so we have to place verbs into sentences to annotate them . But we also want to hold constant all of the other information in a sentence (tense marking, relative clauses, and so on), which is difficult in uncontrolled corpus data.\r\n\n\n\r\n\n\nOur strategy (described in a submission to *Glossa Psycholinguistics*) is to use large language models to write simple, controlled sentences such as The athlete ran the race or The manager ran the company. Then we aim to gather annotations of such sentences to create a dictionary of all English verbs and the Vendler-style inferences drawn from them in controlled contexts, allowing us to test theoretical claims about the behavior of vast classes of verbs.\r\n\n\n\r\n\n\nIn the first phase of this undertaking, we developed a protocol for prompting Metas Llama-2 model to write these controlled sentences. To address multiple meanings of the same verb (run a race versus run a company), we tried two strategies: we used the expert-written PropBank sense lexicon, and we asked Llama-2 to enumerate the senses of each verb. We used sense definitions from these two sources as input to a prompt asking the model to provide example sentences of the form The NOUN VERBed the NOUN. We gathered annotations of these sentences to ensure that they are natural (following the structural rules of English) and typical (describing expected situations), and that pairs of sentences meant to reflect distinct senses of a verb are judged as more different than pairs meant to reflect the same sense, to show that our sentence-generating tool works as desired. In addition to providing controlled data for our planned annotation of the whole English verb lexicon, our tool may be useful for lexicographers, psychologists, or instructional designers who wish to automatically generate example sentences for desired verbs.\r\n\n\n\r\n\n\nThis phase of our project took longer than expected because we changed course to adapt to new generative language models. Now, the Rochester team is moving on to use these automatically-written sentences to gather annotations of all the inferences that we are targeting.  These data will be integrated into Aaron Whites Universal Decompositional Semantics initiative, which publicizes data of interest to lexical semanticists and those interested in automatically extracting information from text.\r\n\n\n\r\n\n\nAlong the way, reflecting evolving research interests, co-PI Lelia Glass published several spinoff projects exploring social and subjective dimensions of event descriptions in text  aiming to explain why subjective adjectives such as cute are more likely than objective ones (red) to appear as the predicate of a sentence (the red dress is cute); why the seemingly neutral verb cause tends to appear with emotionally negative complements (cause cancer, cause problems); and why people who agree versus disagree with contentious political slogans (Free Palestine) tend to interpret those slogans differently. These projects use data from corpora and web surveys to explore how people formulate and understand sentences describing events, shaped by their subjective attitudes towards those events.\r\n\n\n\r\n\n\nThis project employed several student researchers: three undergraduates on the Georgia Tech side, and two Ph.D. students at Rochester. These students developed hands-on experience in data science, project management, statistical modeling, computer programming, human subjects research, the prompting of language models, and academic writing. Three of these students are women, and two received undergraduate degrees in the humanities, so this project helped to welcome researchers from diverse backgrounds to learn in-demand skills. \r\n\n\n\t\t\t\t\tLast Modified: 04/21/2025\n\n\t\t\t\t\tSubmitted by: LeliaGlass\n"
 }
}