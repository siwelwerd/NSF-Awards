{
 "awd_id": "2151463",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: OAC: Data Collection Infrastructure for Panoramic Video Monitoring in Wildlife Science",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922625",
 "po_email": "jjli@nsf.gov",
 "po_sign_block_name": "Juan Li",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 171838.0,
 "awd_min_amd_letter_date": "2021-09-16",
 "awd_max_amd_letter_date": "2021-09-16",
 "awd_abstract_narration": "Wildlife monitoring has significant scientific and societal impacts. By utilizing remote cameras, biologists and ecologists can monitor and manage wildlife in order to prevent the transmission of zoonotic disease from animals and the invasion of wildlife on crops and livestock. However, current cyberinfrastructure (CI) in wildlife monitoring is limited to normal angle videos with a limited field of view and has caused missing the recording of important events that occurred outside of the direction being filmed. Moreover, existing remote cameras only allow the recording of short videos for a few minutes and thus cannot document many hours of wildlife activity in the monitoring zone. This project proposes methods for panoramic video monitoring that capture 360 degree uninterrupted videos to document complete wildlife activities. The project will allow wildlife scientists to access high fidelity monitoring data in both the spatial and temporal domains. Panoramic videos will not only capture comprehensive details on and near the monitoring site, but also depict the monitoring context of the data collection. The abundant research data and metadata embedded in panoramic videos will enhance the productivity of biologists and ecologists. If successful, the proposed wildlife monitoring CI will accelerate the adoption of panoramic data collection in other field research such as agriculture and archeology. The research outcomes, including the datasets generated and the software developed, will provide an interdisciplinary opportunity for undergraduate research, course curriculum development, and high school outreach activities, especially for underrepresented groups.\r\n\r\nThis project investigates a video collection cyberinfrastructure to enable panoramic wildlife monitoring. The design objective is to archive days to weeks of high resolution video data for long lived monitoring under the limited storage and energy constraints of remote cameras. To this end, this project proposes a framework for collaborative local and networked storage. First, we propose camera computing strategies to understand the scientific value of monitoring content and maximally compress the video with negligible overhead. This would mitigate the overall need for storage. Second, we propose a networked storage scheme to address the intermittent nature of the network in the wild, where only partial video is transported while the remaining video is generated in the receiver. We then schedule compressed video tiles for local storage or networked storage by orchestrating the storage, network and battery resources. Finally, we will develop and deploy the panoramic video monitoring in real wildlife research. We will validate the CI on the Savannah River site and assist wildlife scientists to study the impacts of animal interaction on disease transmission.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhisheng",
   "pi_last_name": "Yan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhisheng Yan",
   "pi_email_addr": "zyan4@gmu.edu",
   "nsf_id": "000755484",
   "pi_start_date": "2021-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Mason University",
  "inst_street_address": "4400 UNIVERSITY DR",
  "inst_street_address_2": "",
  "inst_city_name": "FAIRFAX",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7039932295",
  "inst_zip_code": "220304422",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "VA11",
  "org_lgl_bus_name": "GEORGE MASON UNIVERSITY",
  "org_prnt_uei_num": "H4NRWLFCDF43",
  "org_uei_num": "EADLFP7Z72E5"
 },
 "perf_inst": {
  "perf_inst_name": "George Mason University",
  "perf_str_addr": "4400 UNIVERSITY DR",
  "perf_city_name": "Fairfax",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "220304422",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "VA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "026Y00",
   "pgm_ele_name": "CRII CISE Research Initiation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 171838.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Wildlife is important for the environment, but its decline has led to more diseases and damage. Remote cameras are essential for wildlife monitoring, but existing 2D cameras have limited field of view. We propose panoramic video monitoring to capture complete wildlife activities. However, managing the large amount of 360 video data is a challenge. Traditional compression and transport methods do not work well for 360 video collection. New research is needed to find solutions for these unique challenges.</p>\n<p>In order to minimize the storage requirements for wildlife video data, we first designed a set of lightweight deep-learning-based bandwidth-efficient compression frameworks for transporting camera-sensed videos. Our spatial-adaptive compression algorithm reduced the transmitted size of JPEG images by a factor of 9. Our low-level-features-based compression algorithm reduced up to 40% bandwidth and achieved 2x speedup compared to exiting compression methods.</p>\n<p>To understand the end-to-end 360 video system for system optimization, we built a live 360 video streaming prototype. We identified a set of pitfalls in the evaluation methods used in the 360 video research community, such as arbitrary performance metrics, test videos, bandwidth environment. To address this, we recommended guidelines for evaluating 360 video streaming based on quality of experience metrics and common datasets. Additionally, we conducted a measurement study to identify latency issues and bottleneck in a live 360 video streaming system.</p>\n<p>Building on our findings, we designed content-driven and inpainting-based video transport systems for networked video storage. These transport systems utilized artificial intelligence to reduce the amount of compressed data that must be transmitted, without compromising the quality of delivered videos. Experimental results from outdoor cellular networks simulating wildlife monitoring scenarios showed that our content-driven design reduced video stall duration by 65% and stall count by 46%, while saving 31% more bandwidth than existing systems. Similarly, our inpainting-based transport system achieved 50% improvement in video viewing experience under the same bandwidth and hardware resources.</p>\n<p>Regarding the broader impacts, this project generated a 360 wildlife video dataset that is made publicly available. Our live 360 video streaming prototype and the panoramic video saliency detection model were also released to the public. Other research algorithms and results of this project were disseminated at reputed conferences and journals as well as invited talks and conference presentations. These resources could provide an opportunity for wildlife and computing communities to improving their everyday routines. All project outcomes including published papers and GitHub research repositories are summarized on the project website at&nbsp;<a href=\"https://mason.gmu.edu/~zyan4/projects/crii.html\">https://mason.gmu.edu/~zyan4/projects/crii.html</a>. Furthermore, this project involved undergraduate students and minority students in the research activities to broadening the participation in computing. Moreover, outcomes resulting from this research were incorporated into the PI?s course, Immersive Media Analytics and Computing, to enrich the student learning experience through real-world examples.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/14/2023<br>\n\t\t\t\t\tModified by: Zhisheng&nbsp;Yan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWildlife is important for the environment, but its decline has led to more diseases and damage. Remote cameras are essential for wildlife monitoring, but existing 2D cameras have limited field of view. We propose panoramic video monitoring to capture complete wildlife activities. However, managing the large amount of 360 video data is a challenge. Traditional compression and transport methods do not work well for 360 video collection. New research is needed to find solutions for these unique challenges.\n\nIn order to minimize the storage requirements for wildlife video data, we first designed a set of lightweight deep-learning-based bandwidth-efficient compression frameworks for transporting camera-sensed videos. Our spatial-adaptive compression algorithm reduced the transmitted size of JPEG images by a factor of 9. Our low-level-features-based compression algorithm reduced up to 40% bandwidth and achieved 2x speedup compared to exiting compression methods.\n\nTo understand the end-to-end 360 video system for system optimization, we built a live 360 video streaming prototype. We identified a set of pitfalls in the evaluation methods used in the 360 video research community, such as arbitrary performance metrics, test videos, bandwidth environment. To address this, we recommended guidelines for evaluating 360 video streaming based on quality of experience metrics and common datasets. Additionally, we conducted a measurement study to identify latency issues and bottleneck in a live 360 video streaming system.\n\nBuilding on our findings, we designed content-driven and inpainting-based video transport systems for networked video storage. These transport systems utilized artificial intelligence to reduce the amount of compressed data that must be transmitted, without compromising the quality of delivered videos. Experimental results from outdoor cellular networks simulating wildlife monitoring scenarios showed that our content-driven design reduced video stall duration by 65% and stall count by 46%, while saving 31% more bandwidth than existing systems. Similarly, our inpainting-based transport system achieved 50% improvement in video viewing experience under the same bandwidth and hardware resources.\n\nRegarding the broader impacts, this project generated a 360 wildlife video dataset that is made publicly available. Our live 360 video streaming prototype and the panoramic video saliency detection model were also released to the public. Other research algorithms and results of this project were disseminated at reputed conferences and journals as well as invited talks and conference presentations. These resources could provide an opportunity for wildlife and computing communities to improving their everyday routines. All project outcomes including published papers and GitHub research repositories are summarized on the project website at https://mason.gmu.edu/~zyan4/projects/crii.html. Furthermore, this project involved undergraduate students and minority students in the research activities to broadening the participation in computing. Moreover, outcomes resulting from this research were incorporated into the PI?s course, Immersive Media Analytics and Computing, to enrich the student learning experience through real-world examples.\n\n \n\n\t\t\t\t\tLast Modified: 07/14/2023\n\n\t\t\t\t\tSubmitted by: Zhisheng Yan"
 }
}