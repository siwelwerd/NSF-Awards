{
 "awd_id": "2106216",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Self-Adaptive Optimization Algorithms with Fast Convergence via  Geometry-Adapted Hyper-Parameter Scheduling",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2021-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 411248.0,
 "awd_amount": 411248.0,
 "awd_min_amd_letter_date": "2021-05-28",
 "awd_max_amd_letter_date": "2021-05-28",
 "awd_abstract_narration": "Machine-learning and artificial-intelligence techniques have been widely applied in modern society to enhance quality of lifr. In these applications, machine-learning models such as neural networks are trained on a large dataset using various optimization algorithms, which iteratively adjust the model parameters and converge to a good model. In particular, the convergence of these optimization algorithms often relies on choosing a good set of hyper-parameters. For example, one important algorithm hyper-parameter is the step size, which controls the scale of the update applied to the model parameters in every iteration, and it must be carefully chosen to avoid slow convergence and possible divergence.  In practice, these algorithm hyper-parameters either are guided by optimization theory or are set through manual fine-tuning. While theory-guided algorithm hyper-parameters often rely on certain unknown geometrical information of the model and are often too conservative, resulting in result in slow convergence, manually fine-tuned algorithm hyper-parameters critically depend on the specific application and algorithm, and often introduce much computation overhead. This project aims to address these issues by developing a principled, computation-light and effective hyper-parameter scheduling scheme for different types of optimization algorithms to achieve fast and stable convergence. The developed adapted hyper-parameter scheduling scheme is intended to facilitate machine-learning practitioners tuning the algorithm hyper-parameters and dynamically adapt them to the ongoing optimization process. This has further positive impact on implementation of large-scale machine learning applications such as autonomous driving, training adversary-robust models, robust decision making in finance and control, etc. \r\n\r\nIn this project, the researchers are developing a principled and efficient algorithm hyper-parameter scheduling framework that jointly adapts different algorithm hyper-parameters to the local geometry of the nonconvex objective function for a variety of popular optimization algorithms, and corroborate them with strong theoretical convergence guarantees in nonconvex machine learning. Specifically, the researchers are developing such geometry-adapted hyper-parameter scheduling scheme for deterministic optimization algorithms, including first-order gradient-based algorithms, accelerated gradient algorithms and second-order Newton-type algorithms. The researchers are developing new analysis tools that advance the understanding of the relation between hyper-parameters and the dynamic optimization process. Iteration and computation complexities of these algorithms is being established in nonconvex optimization. Based on this development, the researchers are extending the adapted hyper-parameter scheduling scheme to stochastic optimization algorithms, which use mini-batch random sampling and therefore necessitate a joint scheduling of step-size and batch size. Analysis of sample complexity and high probability convergence guarantee is being established for these algorithms. Furthermore, these developments are guiding the design of adapted hyper-parameter scheduling scheme for gradient-based minimax optimization algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yi",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yi Zhou",
   "pi_email_addr": "yi.zhou@tamu.edu",
   "nsf_id": "000805869",
   "pi_start_date": "2021-05-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Utah",
  "inst_street_address": "201 PRESIDENTS CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SALT LAKE CITY",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8015816903",
  "inst_zip_code": "841129049",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "UT01",
  "org_lgl_bus_name": "UNIVERSITY OF UTAH",
  "org_prnt_uei_num": "",
  "org_uei_num": "LL8GLEVH6MG3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Utah",
  "perf_str_addr": "75 S 2000 E",
  "perf_city_name": "Salt lake city",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "841128930",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "UT01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 411248.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Project scope: Artificial intelligence (AI) techniques have been widely applied in modern society to enhance life quality. In these applications, AI models such as neural networks are trained on a large dataset using optimization algorithms, whose convergence often relies on fine-tuned hyper-parameters that are chosen either by theory or through manual fine-tuning. While theory-guided hyper-parameters are often too conservative that result in slow convergence, manually fine-tuned hyper-parameters, on the other hand, are often time-consuming. This project aims to develop a principled, computation-light and effective hyper-parameter scheduling scheme for different types of optimization algorithms to achieve a fast and stable convergence with theoretical guarantee.</p>\n<p>&nbsp;</p>\n<p>Research outcome: This project developed a principled and efficient algorithm hyper-parameter scheduling framework that jointly adapts different algorithm hyper-parameters to the local geometry of the objective function for a variety of popular optimization algorithms, and corroborate them with strong theoretical convergence guarantees. Specifically, we developed geometry-adapted hyper-parameter scheduling schemes for deterministic optimization algorithms, including first-order gradient-based algorithms, accelerated gradient algorithms and second-order Newton-type algorithms. In particular, we have developed new analysis tools that advance the understanding of the relation between hyper-parameters and optimization process, and we studied the iteration and computation complexities of these algorithms. Based on this development, we further extended the adapted hyper-parameter scheduling schemes to stochastic optimization algorithms, which use mini-batch random sampling and therefore a joint scheduling of step-size and batch-size is needed. Analysis of sample complexity and high probability convergence guarantee has been established for these algorithms. Furthermore, these developments have inspired the design of adapted hyper-parameter scheduling scheme for gradient-based minimax optimization algorithms. The developed adapted hyper-parameter scheduling schemes have assisted AI practitioners tune algorithm hyper-parameters in an easy way.</p>\n<p>&nbsp;</p>\n<p>Publication outcome: This project generated 11 conference and journal papers in top-tier AI venues, and 2 Ph.D. thesis under the supervision of the PI. The project also supported the PI and the students travel to international conferences and disseminate the research outcome.</p>\n<p>&nbsp;</p>\n<p>Student supervision: Two Ph.D. students were supported by this project. Both students had successfully defended their thesis in 2023 and 2024, respectively. They are now postdoc research fellow at University of Maryland.</p>\n<p>&nbsp;</p>\n<p>Outreach: During the project period, the PI has interacted with K-12 students through the Engineering Day event hold by the College of Engineering. The PI presented research outcomes to the students via intuitive animations to attract their interest. In the summer of 2021 and 2022, the PI participated in the department high school summer research program and supervised three female students from multiple local high schools. The PI developed a two-month education plan that teaches the students basic machine learning skills and guides them developing programs for accomplishing various machine learning tasks.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/01/2024<br>\nModified by: Yi&nbsp;Zhou</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nProject scope: Artificial intelligence (AI) techniques have been widely applied in modern society to enhance life quality. In these applications, AI models such as neural networks are trained on a large dataset using optimization algorithms, whose convergence often relies on fine-tuned hyper-parameters that are chosen either by theory or through manual fine-tuning. While theory-guided hyper-parameters are often too conservative that result in slow convergence, manually fine-tuned hyper-parameters, on the other hand, are often time-consuming. This project aims to develop a principled, computation-light and effective hyper-parameter scheduling scheme for different types of optimization algorithms to achieve a fast and stable convergence with theoretical guarantee.\n\n\n\n\n\nResearch outcome: This project developed a principled and efficient algorithm hyper-parameter scheduling framework that jointly adapts different algorithm hyper-parameters to the local geometry of the objective function for a variety of popular optimization algorithms, and corroborate them with strong theoretical convergence guarantees. Specifically, we developed geometry-adapted hyper-parameter scheduling schemes for deterministic optimization algorithms, including first-order gradient-based algorithms, accelerated gradient algorithms and second-order Newton-type algorithms. In particular, we have developed new analysis tools that advance the understanding of the relation between hyper-parameters and optimization process, and we studied the iteration and computation complexities of these algorithms. Based on this development, we further extended the adapted hyper-parameter scheduling schemes to stochastic optimization algorithms, which use mini-batch random sampling and therefore a joint scheduling of step-size and batch-size is needed. Analysis of sample complexity and high probability convergence guarantee has been established for these algorithms. Furthermore, these developments have inspired the design of adapted hyper-parameter scheduling scheme for gradient-based minimax optimization algorithms. The developed adapted hyper-parameter scheduling schemes have assisted AI practitioners tune algorithm hyper-parameters in an easy way.\n\n\n\n\n\nPublication outcome: This project generated 11 conference and journal papers in top-tier AI venues, and 2 Ph.D. thesis under the supervision of the PI. The project also supported the PI and the students travel to international conferences and disseminate the research outcome.\n\n\n\n\n\nStudent supervision: Two Ph.D. students were supported by this project. Both students had successfully defended their thesis in 2023 and 2024, respectively. They are now postdoc research fellow at University of Maryland.\n\n\n\n\n\nOutreach: During the project period, the PI has interacted with K-12 students through the Engineering Day event hold by the College of Engineering. The PI presented research outcomes to the students via intuitive animations to attract their interest. In the summer of 2021 and 2022, the PI participated in the department high school summer research program and supervised three female students from multiple local high schools. The PI developed a two-month education plan that teaches the students basic machine learning skills and guides them developing programs for accomplishing various machine learning tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 07/01/2024\n\n\t\t\t\t\tSubmitted by: YiZhou\n"
 }
}