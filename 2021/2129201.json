{
 "awd_id": "2129201",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: HCC: Small: Leveraging a Wrapped Haptic Display to Communicate Robot Learning and Accelerate Human Teaching",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 249688.0,
 "awd_amount": 249688.0,
 "awd_min_amd_letter_date": "2021-09-02",
 "awd_max_amd_letter_date": "2021-09-02",
 "awd_abstract_narration": "Humans excel at teaching physical activities and tasks through demonstration and physical correction (think of a coach guiding an athlete through a desired motion), and human learners often use verbal and nonverbal signals to communicate their understanding or confusion. Similar approaches to teaching have been used between humans and robots, allowing humans to naturally demonstrate tasks, like cooking or furniture assembly, and correct errors in the motions of the robots. While robots have made great advances in understanding the demonstrations and corrections from human teachers, they have lacked an effective way to communicate what they do and do not understand, so human teachers may not know when or if a robot is ready to carry out a task by itself. This project will address this communication gap by developing new ways for robot arms to communicate to human teachers as they learn.  The investigators will attach haptic skin displays (arrays of controllable bubbles) to a robotic arm and create touch sensations to communicate the robot\u2019s understanding or confusion to the human teacher.  Better and more understandable communication from robots as they learn will allow users to train or retrain robots more efficiently, and to better know when they are ready to deploy, without needing specialized knowledge about how robots function. These features will make robot arms more attractive tools for small and mid-sized manufacturers, allowing them to flexibly automate some manufacturing tasks. The lessons learned about communicating the learning state of robotic arms can also be applied to other computer and robotic systems, making the opaque process of robot and computer learning more comprehensible and giving the opportunity to catch and correct errors.\r\n\r\nThe goal of this project is to characterize how humans perceive haptic skin displays wrapped around robot arms, and to formalize how robots capture and communicate feedback through these haptic arrays. Prior work enables robots to learn from physical demonstrations; however, it is equally important to make this learning transparent to the human teacher. This project will advance transparent and interpretable robot learning from an algorithmic and haptic perspective. The team of investigators will i) characterize the types and patterns of exploratory haptic feedback the human perceives, ii) embed the robot's complex and high-dimensional reward learning into low-dimensional haptic feedback, and iii) model how humans interpret the robot's feedback.  These steps will ultimately provide human teachers with an awareness of the robot\u2019s understanding and thereby improve their demonstrations. Each contribution will be evaluated in human subject studies with a commercial robot arm. This project has the potential to advance robotics in small and mid-sized manufacturing by making the process of teaching robot arms intuitive, transparent, and user-friendly.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dylan",
   "pi_last_name": "Losey",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Dylan P Losey",
   "pi_email_addr": "losey@vt.edu",
   "nsf_id": "000837733",
   "pi_start_date": "2021-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "635 Price's Fork Road",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240610001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 249688.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-0cc1e6f9-7fff-b93f-e856-60bbf55c938b\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In order for humans to seamlessly interact with robots and other intelligent systems, we need to make sure that humans and robots understand one another. Recent research has developed a variety of algorithms that robots can use to learn from humans. Under these algorithms the human demonstrates how they want the robot to behave --- e.g., the human physically shows the robot how to complete a task --- and the robot learns from these human demonstrations. Overall, robot learning enables information to transfer from humans to robots. But what about the opposite direction? In this project we developed a new channel that robots can leverage to immediately communicate what they are learning back to human teachers. Specifically, we created a haptic display that naturally wraps around robot arms. When humans physically teach robots new tasks, they feel this haptic display under their hands throughout the teaching process: the robot can inflate or deflate the haptic display on command without interfering with the human's physical demonstration. In practice, our haptic display behaves like a notification system --- the robot can inflate the display (applying pressure to the human's hands) to notify the human when it is confused about what it should be learning, and deflate the display when it is confident. An image of this haptic array wrapped around a robot arm is shown below:</span></p>\r\n<p><br /><br /></p>\r\n<p style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"border: none; display: inline-block; overflow: hidden; width: 433px; height: 253px;\"><img style=\"margin-left: 0px; margin-top: 0px;\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXftItdcZGAC6tv4VvRylr-Aoi5QBowjB0z6VjfTksU4t0-bCsbFIu2gUu85S2pWz3FkxPf6FOUK9puRN_rIqEkc2J4v5SHN3rjQWKVp5pbCklSHq6Ehmn-4kT-T0WaWjutG8P8tww?key=VtV8sdoiZGf9JXiA-3jMFZlD\" alt=\"\" width=\"433\" height=\"253\" /></span></span></p>\r\n<p>&nbsp;</p>\r\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project resulted in three fundamental advances in knowledge. 1) First, we developed the haptic displays, and conducted user studies to characterize how easily humans can detect feedback rendered by these displays. We used the results from these experiments to refine our haptic arrays and reach a system that humans can naturally and intuitively perceive when interacting with robot arms without interrupting their demonstrations. 2) Second, we developed algorithms that convert what the robot is learning into signals that can be rendered by our haptic display. Robot learning is often complex and nonlinear --- imagine matrices of thousands of numbers --- and so conveying every aspect of what the robot is learning quickly becomes infeasible. Instead, our approach breaks down the robot's learning into a few intuitive signals that humans can easily grasp: e.g., the robot's overall confidence. Across our experiments, robots were able to render simplified learning signals in real-time on the haptic array, and users correctly interpreted these signals to figure out what the robot had and had not learned. 3) Third, we measured how our approach for communicating robot learning impacted human-robot interaction. We found that communicating what the robot is learning has benefits for both the human teacher and the robot learner. From the human's perspective, when users know what the robot has learned they trust the robot more, and are able to accurately determine when the robot is ready to be deployed. From the robot's perspective, users that are aware of the robot's confusion are able to target their demonstrations on parts of the task the robot does not know. This targeting teaching enables the robot to learn the task more quickly with less human oversight. An outline of our three main research contributions is shown below:</span></p>\r\n<p>&nbsp;</p>\r\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"border: none; display: inline-block; overflow: hidden; width: 624px; height: 155px;\"><img style=\"margin-left: 0px; margin-top: 0px;\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXculBbdIGsViiL9_qSxRh_JxSc7-m9yCKVSPRmdU52hzB3rJ_gl6MHZTXPVQ0XkBG6XKCGi6JIklvpMOXg4LDY4Gh-PAnlOaw8cXPvvUDgjCjYBATfD2qyUp-bN6jn3-Njx6NtYow?key=VtV8sdoiZGf9JXiA-3jMFZlD\" alt=\"\" width=\"624\" height=\"155\" /></span></span></p>\r\n<p>&nbsp;</p>\r\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our project had broader impacts on education, robotics, and human well-being. Better and more understandable communication enables users to train robots more efficiently without needing specialized knowledge about how robots function. These features make robot arms more attractive tools for small and mid-sized manufacturers, allowing them to flexibly automate manufacturing tasks. This project partially supported three graduate students and two undergraduate students at Virginia Tech. Overall, the project resulted in one Ph.D. degree and two M.S. degrees. In addition, the project team organized and hosted a workshop at the 2023 IEEE International Conference on Robotics and Automation (ICRA): at this workshop over 100 professionals convened to discuss recent advances towards robots that communicate their learning. Our research products include four peer-reviewed journal articles and four peer-reviewed conference publications. Each of these papers are freely available to the public through the PIs websites and online repositories. In addition, the project team produced YouTube videos to document our progress and capture how robots can use haptic arrays to communicate with human teachers.</span></p>\r\n<p>&nbsp;</p>\r\n<p style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"border: none; display: inline-block; overflow: hidden; width: 425px; height: 318px;\"><img style=\"margin-left: 0px; margin-top: 0px;\" src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeU6QPvHbRMHkDaoEWMgXFTw4r-YwLmFjYCZ9oAcX_psUgY2zoc0tggnnT0g_dsmYxTg0g1W4VOMURTYMEVzCe_bsWKD-6I6vS6dFbfVHGLw251KHxqdT8cZa8YQLsdECkzrew06g?key=VtV8sdoiZGf9JXiA-3jMFZlD\" alt=\"\" width=\"425\" height=\"318\" /></span></span></p>\r\n<p>&nbsp;</p>\r\n<p style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #333333; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">To broaden participation in STEM, the PIs hosted groups of K-12 students during each year of the project. During these visits local elementary, middle, and high school students interacted with robot arms. We designed games where the visiting students taught the robot new tasks while receiving feedback through the haptic arrays: we asked the students if they could predict whether the robot had or had not learned their task. </span><span style=\"font-size: 11pt; font-family: 'Palatino Linotype',serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">In addition, PI Losey presented this research at outreach events on Virginia Tech campus, including Slush Rush (an event for first-year undergraduates learning about engineering) and Women's Preview Weekend (a weekend event for incoming female undergraduates in engineering).</span></p><br>\n<p>\n Last Modified: 01/24/2025<br>\nModified by: Dylan&nbsp;P&nbsp;Losey</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2129201/2129201_10770400_1737740004657_figure1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2129201/2129201_10770400_1737740004657_figure1--rgov-800width.png\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2025/2129201/2129201_10770400_1737740004657_figure1--rgov-66x44.png\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Wrapping haptic displays around a robot arm. When the human physically teaches the robot new tasks, they can feel what the robot has learned based on how the haptic bags inflate and deflate.</div>\n<div class=\"imageCredit\">Dylan Losey</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Dylan&nbsp;P&nbsp;Losey\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2129201/2129201_10770400_1737740102296_figure2_1_--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2129201/2129201_10770400_1737740102296_figure2_1_--rgov-800width.png\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2025/2129201/2129201_10770400_1737740102296_figure2_1_--rgov-66x44.png\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Outline of our key contributions to knowledge. We consider settings where humans are teaching robots new tasks. We develop haptic displays to communicate what the robot is learning, and explore how this feedback affects the way the human teaches the robot.</div>\n<div class=\"imageCredit\">Dylan Losey</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Dylan&nbsp;P&nbsp;Losey\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn order for humans to seamlessly interact with robots and other intelligent systems, we need to make sure that humans and robots understand one another. Recent research has developed a variety of algorithms that robots can use to learn from humans. Under these algorithms the human demonstrates how they want the robot to behave --- e.g., the human physically shows the robot how to complete a task --- and the robot learns from these human demonstrations. Overall, robot learning enables information to transfer from humans to robots. But what about the opposite direction? In this project we developed a new channel that robots can leverage to immediately communicate what they are learning back to human teachers. Specifically, we created a haptic display that naturally wraps around robot arms. When humans physically teach robots new tasks, they feel this haptic display under their hands throughout the teaching process: the robot can inflate or deflate the haptic display on command without interfering with the human's physical demonstration. In practice, our haptic display behaves like a notification system --- the robot can inflate the display (applying pressure to the human's hands) to notify the human when it is confused about what it should be learning, and deflate the display when it is confident. An image of this haptic array wrapped around a robot arm is shown below:\r\n\n\n\n\n\r\n\n\n\r\n\n\n\r\n\n\nThis project resulted in three fundamental advances in knowledge. 1) First, we developed the haptic displays, and conducted user studies to characterize how easily humans can detect feedback rendered by these displays. We used the results from these experiments to refine our haptic arrays and reach a system that humans can naturally and intuitively perceive when interacting with robot arms without interrupting their demonstrations. 2) Second, we developed algorithms that convert what the robot is learning into signals that can be rendered by our haptic display. Robot learning is often complex and nonlinear --- imagine matrices of thousands of numbers --- and so conveying every aspect of what the robot is learning quickly becomes infeasible. Instead, our approach breaks down the robot's learning into a few intuitive signals that humans can easily grasp: e.g., the robot's overall confidence. Across our experiments, robots were able to render simplified learning signals in real-time on the haptic array, and users correctly interpreted these signals to figure out what the robot had and had not learned. 3) Third, we measured how our approach for communicating robot learning impacted human-robot interaction. We found that communicating what the robot is learning has benefits for both the human teacher and the robot learner. From the human's perspective, when users know what the robot has learned they trust the robot more, and are able to accurately determine when the robot is ready to be deployed. From the robot's perspective, users that are aware of the robot's confusion are able to target their demonstrations on parts of the task the robot does not know. This targeting teaching enables the robot to learn the task more quickly with less human oversight. An outline of our three main research contributions is shown below:\r\n\n\n\r\n\n\n\r\n\n\n\r\n\n\nOur project had broader impacts on education, robotics, and human well-being. Better and more understandable communication enables users to train robots more efficiently without needing specialized knowledge about how robots function. These features make robot arms more attractive tools for small and mid-sized manufacturers, allowing them to flexibly automate manufacturing tasks. This project partially supported three graduate students and two undergraduate students at Virginia Tech. Overall, the project resulted in one Ph.D. degree and two M.S. degrees. In addition, the project team organized and hosted a workshop at the 2023 IEEE International Conference on Robotics and Automation (ICRA): at this workshop over 100 professionals convened to discuss recent advances towards robots that communicate their learning. Our research products include four peer-reviewed journal articles and four peer-reviewed conference publications. Each of these papers are freely available to the public through the PIs websites and online repositories. In addition, the project team produced YouTube videos to document our progress and capture how robots can use haptic arrays to communicate with human teachers.\r\n\n\n\r\n\n\n\r\n\n\n\r\n\n\nTo broaden participation in STEM, the PIs hosted groups of K-12 students during each year of the project. During these visits local elementary, middle, and high school students interacted with robot arms. We designed games where the visiting students taught the robot new tasks while receiving feedback through the haptic arrays: we asked the students if they could predict whether the robot had or had not learned their task. In addition, PI Losey presented this research at outreach events on Virginia Tech campus, including Slush Rush (an event for first-year undergraduates learning about engineering) and Women's Preview Weekend (a weekend event for incoming female undergraduates in engineering).\t\t\t\t\tLast Modified: 01/24/2025\n\n\t\t\t\t\tSubmitted by: DylanPLosey\n"
 }
}