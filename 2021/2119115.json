{
 "awd_id": "2119115",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CCRI:NEW: Research Infrastructure for Real-TIme Computer Vision and Decision Making via Mobile Robots",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 301586.0,
 "awd_amount": 301586.0,
 "awd_min_amd_letter_date": "2021-08-10",
 "awd_max_amd_letter_date": "2021-08-10",
 "awd_abstract_narration": "This project will create a research infrastructure for computer vision and real-time control of autonomous mobile robots (both aerial and ground). The infrastructure includes four integrated components: (1) A Purdue laboratory decorated as miniature cities. (2) Simulators that reflect the physical laboratory. (3) Programmable aerial robots with the same interface as the simulators. (4) Sample solutions for research on artificial intelligence, computer vision, and robot control for evaluation and comparison. This infrastructure will be available to the research community in multiple ways: (1) Users can evaluate their solutions with the simulators in a safe virtual environment. (2) Users can upload their control programs and this team will launch the robots inside Purdue's laboratory. Users can observe the robots remotely using the high-speed cameras already deployed in the laboratory. (3) Users can bring their own robots to the laboratory and conduct experiments. (4) This project will create competitions for researchers to demonstrate their solutions using autonomous mobile robots in simulated emergency and rescue scenarios. The competitions will use miniature buildings and people for the robots to recognize and count objects (such as number of people, vehicles, and houses), assess situations (such as the number of collapsed bridges), while avoiding obstacles.\r\n\r\nThis infrastructure will be available for investigating a wide range of research topics, including (1) real-time computer vision and control. The decorated laboratory will allow researchers to evaluate their solutions for real-time vision and control methods using active computer vision, navigation, and semantic segmentation in a three-dimensional environment. (2) simulation of robot fleets. Users can evaluate and improve their methods in a safe virtual environment before deployment. (3) This infrastructure will integrate virtual and physical environments so that solutions running in the simulators can be ported directly to the physical robots for experiments. (4) collision avoidance, multi-robot coordination, emergency response, computer security, and efficient machine learning on embedded systems. (5) agriculture, city planning, emergency response, and inspection of civil structures. This project will build STEM talents because autonomous robots and visual data are naturally appealing to the general public. With the simulators, students at all levels can participate without the cost of purchasing physical robots. This research infrastructure will reduce the barriers to innovations. This infrastructure will also encourage innovations in machine learning that are efficient in energy and can be ported to resource constrained embedded systems such as aerial robots. The project will engage a broader audience including K-12 students as well because of the many applications described above.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kristen",
   "pi_last_name": "Grauman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Kristen L Grauman",
   "pi_email_addr": "grauman@cs.utexas.edu",
   "nsf_id": "000282504",
   "pi_start_date": "2021-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Austin",
  "perf_str_addr": "2317 Speedway, D9500",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121810",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 301586.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-a8031d78-7fff-6898-e4c8-216dc7df6ce8\">\r\n<p dir=\"ltr\"><span>In this multi-institution project, the team from the University of Texas at Austin pursued advances in active vision, focusing on two main areas: audio-visual navigation and low-shot transfer learning for active perception policies. The work aims to enhance intelligent agent behavior and environment representation learning.</span></p>\r\n<p dir=\"ltr\"><span>In the realm of audio-visual navigation, the team developed methods to improve intelligent agent behavior for separating sound sources in an environment. This involves using audio-visual cues to navigate and identify different sound sources effectively. We also created a method for constructing an acoustic model of an unmapped indoor scene using a mobile agent equipped with visual and acoustic sensors. This model allows jointly constructing the scene's acoustic model and occupancy map on-the-fly. Additionally, the team developed a reinforcement learning (RL) policy that leverages audio-visual sensor streams to guide agent navigation and determine optimal positions for sampling acoustic data. This approach outperforms traditional navigation agents and state-of-the-art methods in diverse indoor environments. To improve the performance of audio-visual navigation policies trained in simulation when transferred to a physical robot, the team introduced an audio-visual navigation policy that disentangles navigation into acoustic field prediction and waypoint navigation. We also developed a frequency-adaptive strategy to select the best frequency band for prediction, improving performance on real data and validating results on a physical robot platform.</span></p>\r\n<p dir=\"ltr\"><span>In the area of low-shot transfer learning for active perception policies, the team presented a unified approach to visual navigation using a novel modular transfer learning model. This model can leverage experience from one source task and apply it to multiple target tasks with various goal modalities, enabling zero-shot experience learning.&nbsp; Experiments show that this approach learns faster, generalizes better, and outperforms state-of-the-art models. This work led to next steps in learning general spatial representations from audio-visual correspondences in egocentric videos. Using a masked auto-encoder framework, the model can synthesize masked multi-channel audio from unmasked audio and egocentric video, learning useful spatial features such as sound source location and scene layout/geometry.</span></p>\r\n<p dir=\"ltr\"><span>The broader impact and intellectual merit of this research are as follows. The team's work on audio-visual navigation and low-shot transfer learning enhances the capabilities of intelligent agents in navigating and perceiving their environments. This has broad applications in household robotics, autonomous vehicles, and other fields requiring advanced navigation and perception systems. By developing models that can perform zero-shot transfer and learn from minimal interactions, the work addresses the high cost and inefficiency associated with traditional reinforcement learning methods, contributing to more efficient and scalable solutions for various tasks.</span></p>\r\n<p dir=\"ltr\"><span>The practical applicability demonstrated through the sim2real strategies developed in this project ensure that the developed models and policies are not only theoretically sound but also practically viable. The introduction of self-supervised methods for learning spatial audio-visual correspondences in egocentric videos represents a significant advancement in the field, improving performance on multiple downstream tasks such as active speaker detection and spatial audio denoising and showcasing the potential for broader applications in audio-visual processing.</span></p>\r\n<p dir=\"ltr\"><span>Overall, the team's research at UT Austin contributes to the advancement of active vision and intelligent agent behavior, with significant implications for both theoretical research and practical applications. This project&rsquo;s developments in audio-visual navigation and low-shot transfer learning help pave the way for more efficient, adaptable, and capable intelligent systems.</span></p>\r\n</span></p><br>\n<p>\n Last Modified: 02/21/2025<br>\nModified by: Kristen&nbsp;L&nbsp;Grauman</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nIn this multi-institution project, the team from the University of Texas at Austin pursued advances in active vision, focusing on two main areas: audio-visual navigation and low-shot transfer learning for active perception policies. The work aims to enhance intelligent agent behavior and environment representation learning.\r\n\n\nIn the realm of audio-visual navigation, the team developed methods to improve intelligent agent behavior for separating sound sources in an environment. This involves using audio-visual cues to navigate and identify different sound sources effectively. We also created a method for constructing an acoustic model of an unmapped indoor scene using a mobile agent equipped with visual and acoustic sensors. This model allows jointly constructing the scene's acoustic model and occupancy map on-the-fly. Additionally, the team developed a reinforcement learning (RL) policy that leverages audio-visual sensor streams to guide agent navigation and determine optimal positions for sampling acoustic data. This approach outperforms traditional navigation agents and state-of-the-art methods in diverse indoor environments. To improve the performance of audio-visual navigation policies trained in simulation when transferred to a physical robot, the team introduced an audio-visual navigation policy that disentangles navigation into acoustic field prediction and waypoint navigation. We also developed a frequency-adaptive strategy to select the best frequency band for prediction, improving performance on real data and validating results on a physical robot platform.\r\n\n\nIn the area of low-shot transfer learning for active perception policies, the team presented a unified approach to visual navigation using a novel modular transfer learning model. This model can leverage experience from one source task and apply it to multiple target tasks with various goal modalities, enabling zero-shot experience learning. Experiments show that this approach learns faster, generalizes better, and outperforms state-of-the-art models. This work led to next steps in learning general spatial representations from audio-visual correspondences in egocentric videos. Using a masked auto-encoder framework, the model can synthesize masked multi-channel audio from unmasked audio and egocentric video, learning useful spatial features such as sound source location and scene layout/geometry.\r\n\n\nThe broader impact and intellectual merit of this research are as follows. The team's work on audio-visual navigation and low-shot transfer learning enhances the capabilities of intelligent agents in navigating and perceiving their environments. This has broad applications in household robotics, autonomous vehicles, and other fields requiring advanced navigation and perception systems. By developing models that can perform zero-shot transfer and learn from minimal interactions, the work addresses the high cost and inefficiency associated with traditional reinforcement learning methods, contributing to more efficient and scalable solutions for various tasks.\r\n\n\nThe practical applicability demonstrated through the sim2real strategies developed in this project ensure that the developed models and policies are not only theoretically sound but also practically viable. The introduction of self-supervised methods for learning spatial audio-visual correspondences in egocentric videos represents a significant advancement in the field, improving performance on multiple downstream tasks such as active speaker detection and spatial audio denoising and showcasing the potential for broader applications in audio-visual processing.\r\n\n\nOverall, the team's research at UT Austin contributes to the advancement of active vision and intelligent agent behavior, with significant implications for both theoretical research and practical applications. This projects developments in audio-visual navigation and low-shot transfer learning help pave the way for more efficient, adaptable, and capable intelligent systems.\r\n\t\t\t\t\tLast Modified: 02/21/2025\n\n\t\t\t\t\tSubmitted by: KristenLGrauman\n"
 }
}