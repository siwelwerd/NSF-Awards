{
 "awd_id": "2124685",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research: Evaluating the Promise and Pitfalls of Benchmarking in Machine Learning Research",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Mary Feeney",
 "awd_eff_date": "2021-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 20003.0,
 "awd_amount": 20003.0,
 "awd_min_amd_letter_date": "2021-07-27",
 "awd_max_amd_letter_date": "2021-07-27",
 "awd_abstract_narration": "This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2).\r\n\r\nThe scientific and commercial success of machine learning (ML) has spurred government and corporate sponsors to invest billions of dollars in machine learning research. Despite this massive investment, there is limited quantitative research on how the ML field measures progress: a process called \u201cbenchmarking.\u201d Benchmarking is the act of comparing algorithms on a quantitative metric after training them on the same benchmark dataset. Benchmarks organize ML researchers around common tasks. Achieving \u201cstate of the art\u201d performance on an important benchmark can spark new research trajectories and advance careers: consider the 2012 success of \u201cAlexNet\u201d in a prominent computer vision task, which helped to launch current interest in deep learning. However, the practice of benchmarking has already engendered criticism that this near-ubiquitous research culture does not push the field towards socially beneficial outcomes, and leads to overinvestment in methods that maximize performance on academic datasets but are environmentally unsustainable or harm the public when used in the real world. This dissertation research will provide a comprehensive analysis of the strengths and weaknesses of benchmarking practices with respect to several public aims: accelerating innovation in science, increasing equity within the field, and promoting ethical research (i.e., an orientation toward research that benefits society and avoids harms). By blending sociological analysis, computational methods for extracting and analyzing benchmarking data from thousands of papers, and in-depth qualitative interviews, this research will produce an understanding of benchmarking culture in ML research that combines breadth and quantitative rigor with depth and interpretive nuance. This project has significant implications for government and corporate funders, researchers, and society more broadly. \r\n\r\nThe dissertation consists of three subprojects. The first subproject explores evidence that benchmarking culture has stymied innovation by favoring utilization of the same datasets across multiple tasks and by incentivizing researchers to underinvest on nascent benchmarks and overinvest on mature ones. The second subproject explores how patterns in the adoption of benchmarks and rewards for state-of-the-art performance interact with status and resources to create inequities in the field. It tests the hypothesis that high-status researchers and institutions have disproportionate power to set the field\u2019s research agenda by introducing benchmarks, while garnering disproportionate citations for state-of-the-art achievements. Both of these phenomena have the potential to create a \u201cMatthew Effect\u201d that disadvantages under-represented and under-resourced researchers/institutions. These subprojects use network science, natural language processing, and manual coding to create a large dataset of benchmarks and progress on those benchmarks across multiple ML task communities. The third subproject consists of qualitative interviews with ML researchers across career stages and expertise to gain first-hand perspectives on benchmarking culture and assess reforms to improve research ethics and societal outcomes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jacob",
   "pi_last_name": "Foster",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Jacob G Foster",
   "pi_email_addr": "foster@soc.ucla.edu",
   "nsf_id": "000699374",
   "pi_start_date": "2021-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bernard",
   "pi_last_name": "Koch",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Bernard J Koch",
   "pi_email_addr": "bernardkoch@ucla.edu",
   "nsf_id": "000849967",
   "pi_start_date": "2021-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "264 Haines Hall",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125Y00",
   "pgm_ele_name": "Science of Science"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "1V21",
   "app_name": "R&RA ARP Act DEFC V",
   "app_symb_id": "040100",
   "fund_code": "010V2122DB",
   "fund_name": "R&RA ARP Act DEFC V",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 20003.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-79f03a5c-7fff-a4df-4e5d-fb0ddfb4601a\"> </span></p>\n<p dir=\"ltr\"><strong>Introduction:</strong><span> AI has emerged as one of the most influential technologies of the 21st century. Understanding (and optimizing) how AI researchers develop these technologies is thus crucial to the deployment of efficient, ethical, and equitable AI algorithms in the real world. The purpose of this grant was to better understand the social, epistemic, and ethical processes through which the field of AI research evaluates and measures scientific progress. In particular, the project focused on the origins and implications of an evaluation approach called \"benchmarking.\"</span></p>\n<p dir=\"ltr\"><span>Benchmarking is a scientific evaluation practice in which researchers compete to build algorithms to solve particular tasks (e.g., machine translation, facial recognition, biomedical question answering). Success is measured through the accuracy each algorithm achieves on a pre-chosen testing dataset (e.g., the MCAT). In AI research, benchmarking has largely replaced peer review as the primary measure of progress. This is because the advancement of state-of-the-art benchmarking scores over time provides a simple, easy-to-understand measure of where the field stands on a given task.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><strong>Results:</strong><span> The grant yielded two papers. The first paper explores the dynamics of benchmark dataset usage across 43,140 AI research papers published between 2015 and 2020. We find that AI research communities are increasingly concentrated on fewer and fewer datasets and that these datasets have been introduced by researchers situated within a small number of elite institutions. Concentration on datasets and institutions has implications for the trajectory of the field and the safe deployment of AI algorithms (see Broader Impacts).</span></p>\n<p>Drawing on 45 interviews with AI researchers, government policy makers, and tech executives, the second paper explains the origins of benchmarking and the implications of its replacement of peer review. Benchmarking was introduced by DARPA/NIST in the late 1980s to increase accountability by scientists after the collapse of \"symbolic\" AI in the 1980s. This pivot was important because benchmarking ceded scientists&rsquo; autonomy to external actors (government, and later, major IT firms) to decide the course of scientific research. Moreover, emphasizing accuracy over other scientific values (e.g., parsimony, novelty)&nbsp; incentivized the development of certain types of algorithms over others. The paper subsequently explains how these changes shaped the takeover of the field by deep learning algorithms in the 2010s, which dominate AI today.</p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><strong>Broader Impacts:</strong><span> In the first project, understanding the dynamics of benchmark dataset usage has both epistemic and ethical implications. From an epistemic perspective, the overuse of a few benchmark datasets risks invalidating benchmarking as a legitimate measure of progress. The overuse of benchmarking datasets means that scientists could be making more/less progress than they think on tasks because their datasets do not represent real-world diversity.&nbsp;</span></p>\n<p dir=\"ltr\">Poorly measured algorithms can also cause social harms when they behave unreliably upon deployment. For example, the repeated arrests of black men who were misidentified by facial recognition algorithms is partially due to benchmarks datasets that lacked enough people of color. Finally, this project produced the first paper to show quantitatively the breadth of influence large tech companies have on the direction of the field. This has implications both for scientific efficiency, but also for the inclusion of voices beyond industry research in deciding which technologies should be valued and prioritized.&nbsp;</p>\n<p>The second project is important because it provides insight into why AI research has become singularly dominated by deep learning. Deep learning has made incredible accomplishments through gains in accuracy, but it also has many disadvantages compared to other machine learning approaches that were being explored a decade ago: it is&nbsp; hard to interpret/explain, data-hungry, and comparatively unsustainable. Our study shows how the exclusive valorization of accuracy over other epistemic values incentivized focus on deep learning over other approaches. Moreover, the fact that benchmarking ceded autonomy to external actors was instrumental to the rise of large IT firms in AI/ML research; they alone possess the data to build large deep learning models and have become the dominant driver of research within the field. These findings suggest ways federal funders might diversify AI research beyond large-scale deep learning that might have different strengths and weaknesses.</p>\n<p>This DDRIG grant was instrumental in helping Bernard Koch complete his dissertation, disseminate his research, and advance his career. The first paper in this project was the first social science paper ever to win a \"Best Award\" at NeurIPS, one of the top three publication venues in AI research (awarded to 14 papers out 9,122 submissions). The visibility of this publication led to invitations to speak not only at universities, but at Apple, Microsoft Research Asia, and Spotify Research. The second paper is being prepared for submission, and Bernard hopes to write a condensed version of it for a major media venue. Lastly, these papers were instrumental in him finding a tenure-track faculty position at the University of Chicago, where he will continue researching organizational and policy issues in AI.</p><br>\n<p>\n Last Modified: 12/28/2023<br>\nModified by: Bernard&nbsp;Koch</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nIntroduction: AI has emerged as one of the most influential technologies of the 21st century. Understanding (and optimizing) how AI researchers develop these technologies is thus crucial to the deployment of efficient, ethical, and equitable AI algorithms in the real world. The purpose of this grant was to better understand the social, epistemic, and ethical processes through which the field of AI research evaluates and measures scientific progress. In particular, the project focused on the origins and implications of an evaluation approach called \"benchmarking.\"\n\n\nBenchmarking is a scientific evaluation practice in which researchers compete to build algorithms to solve particular tasks (e.g., machine translation, facial recognition, biomedical question answering). Success is measured through the accuracy each algorithm achieves on a pre-chosen testing dataset (e.g., the MCAT). In AI research, benchmarking has largely replaced peer review as the primary measure of progress. This is because the advancement of state-of-the-art benchmarking scores over time provides a simple, easy-to-understand measure of where the field stands on a given task.\n\n\n\n\n\nResults: The grant yielded two papers. The first paper explores the dynamics of benchmark dataset usage across 43,140 AI research papers published between 2015 and 2020. We find that AI research communities are increasingly concentrated on fewer and fewer datasets and that these datasets have been introduced by researchers situated within a small number of elite institutions. Concentration on datasets and institutions has implications for the trajectory of the field and the safe deployment of AI algorithms (see Broader Impacts).\n\n\nDrawing on 45 interviews with AI researchers, government policy makers, and tech executives, the second paper explains the origins of benchmarking and the implications of its replacement of peer review. Benchmarking was introduced by DARPA/NIST in the late 1980s to increase accountability by scientists after the collapse of \"symbolic\" AI in the 1980s. This pivot was important because benchmarking ceded scientists autonomy to external actors (government, and later, major IT firms) to decide the course of scientific research. Moreover, emphasizing accuracy over other scientific values (e.g., parsimony, novelty) incentivized the development of certain types of algorithms over others. The paper subsequently explains how these changes shaped the takeover of the field by deep learning algorithms in the 2010s, which dominate AI today.\n\n\n\n\n\nBroader Impacts: In the first project, understanding the dynamics of benchmark dataset usage has both epistemic and ethical implications. From an epistemic perspective, the overuse of a few benchmark datasets risks invalidating benchmarking as a legitimate measure of progress. The overuse of benchmarking datasets means that scientists could be making more/less progress than they think on tasks because their datasets do not represent real-world diversity.\n\n\nPoorly measured algorithms can also cause social harms when they behave unreliably upon deployment. For example, the repeated arrests of black men who were misidentified by facial recognition algorithms is partially due to benchmarks datasets that lacked enough people of color. Finally, this project produced the first paper to show quantitatively the breadth of influence large tech companies have on the direction of the field. This has implications both for scientific efficiency, but also for the inclusion of voices beyond industry research in deciding which technologies should be valued and prioritized.\n\n\nThe second project is important because it provides insight into why AI research has become singularly dominated by deep learning. Deep learning has made incredible accomplishments through gains in accuracy, but it also has many disadvantages compared to other machine learning approaches that were being explored a decade ago: it is hard to interpret/explain, data-hungry, and comparatively unsustainable. Our study shows how the exclusive valorization of accuracy over other epistemic values incentivized focus on deep learning over other approaches. Moreover, the fact that benchmarking ceded autonomy to external actors was instrumental to the rise of large IT firms in AI/ML research; they alone possess the data to build large deep learning models and have become the dominant driver of research within the field. These findings suggest ways federal funders might diversify AI research beyond large-scale deep learning that might have different strengths and weaknesses.\n\n\nThis DDRIG grant was instrumental in helping Bernard Koch complete his dissertation, disseminate his research, and advance his career. The first paper in this project was the first social science paper ever to win a \"Best Award\" at NeurIPS, one of the top three publication venues in AI research (awarded to 14 papers out 9,122 submissions). The visibility of this publication led to invitations to speak not only at universities, but at Apple, Microsoft Research Asia, and Spotify Research. The second paper is being prepared for submission, and Bernard hopes to write a condensed version of it for a major media venue. Lastly, these papers were instrumental in him finding a tenure-track faculty position at the University of Chicago, where he will continue researching organizational and policy issues in AI.\t\t\t\t\tLast Modified: 12/28/2023\n\n\t\t\t\t\tSubmitted by: BernardKoch\n"
 }
}