{
 "awd_id": "2136396",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps:  Video Based Interface for the Live Labelling of Emotional Responses",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922160",
 "po_email": "rshuman@nsf.gov",
 "po_sign_block_name": "Ruth Shuman",
 "awd_eff_date": "2021-07-15",
 "awd_exp_date": "2022-12-31",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2021-07-16",
 "awd_max_amd_letter_date": "2021-07-16",
 "awd_abstract_narration": "The broader impact/commercial potential of this I-Corps project is to provide an affordable, wearable device capable of detecting emotional states in real time.  Currently, there are few methods to objectively evaluate emotional states in real time and in real-life settings. The proposed software and hardware platform can potentially be used by entertainment venues to enhance the customer experience to increase sales, improve venue management and safety, and support decision making in scheduling.  The system can also be used to provide real time measurements of audience emotion and assess content preferences. \r\n\r\nThis I-Corps project develops a software and hardware platform centered on a wearable device for the unconstrained, contactless and live detection of physiological correlates of peak emotion. The technology uses individualized signals along with video-sourced data to provide real-time measures of consumers' emotional state. The project develops an approach combining innovative data collection and analysis strategies with cutting-edge software and affordable, scalable hardware for contactless, image-based detection of emotional states in real-time. The technology provides a wireless, easy-to-deploy, and wearable device capable of intelligent processing with limited power, well-suited for large scale data acquisition in real-world environments. Finally, this project also innovates in data science by combining real-time physiological signals from individual consumers with aggregate video-recordings in order to model group engagement and predict behavior.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Pablo",
   "pi_last_name": "Ripolles",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pablo Ripolles",
   "pi_email_addr": "pr82@nyu.edu",
   "nsf_id": "000749382",
   "pi_start_date": "2021-07-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "6 Washington Place",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100036603",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1654",
   "pgm_ref_txt": "HUMAN COMPUTER INTERFACE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Emotions are central to human life, influencing our behavior from the day we are born. They help us to act and avoid danger, and they are crucially involved in our decision-making. Importantly, emotion and engagement are well-known predictors of consumer preferences and purchasing decisions. This is especially true for commercial enterprises in which human emotions play a crucial role, such as in the music industry (e.g., emotions are related to ticket sales and preferred venues, among others). Despite the importance of emotion and engagement in sales in the music business, there are currently few means to objectively evaluate these measures<em> in real time and in real-life settings</em> (e.g., a live music event) in an accurate, affordable, and computationally inexpensive manner. Emotions not only affect human behavior, but also induce biological and physiological changes that can be <em>objectively </em>measured. In this vein, research in the music domain has focused on the quantification of a particular index of <em>strong emotional </em>responses: <em>chills</em>, also called <em><span><span>frissons</span></span></em> or <em>shivers down the spine</em>. Chills are usually quantified using techniques that are extremely susceptible to movement, electronic noise, and other environmental factors. Thus, it has traditionally been difficult to measure chills objectively and reliably <em>in real-word testbeds</em>. This I-Corps project assessed the market opportunity of a <em>wearable device </em>for the <em>unconstrained</em>, <em><span><span>contactless</span>,</span></em> and <em>real-time</em><span> detection of one of the core physiological correlates of chills during live music events: the <span>piloerection</span> of the skin (e.g., </span><em>goosebumps</em><span>). Our technology uses individualized signals capturing <span>piloerection?that</span> is, peak moments of <span>emotion?from</span> a live audience to provide real-time measures of crowd (i.e., consumer) engagement. During this project we completed more than 400 customer discovery interviews (including venue managers, audiences, and other industry stakeholders) across multiple cities well-known for their live music scene (New York, Philadelphia, and Nashville). This allowed our team to understand the ecosystem in which our product can be successfully marketed and led to the creation of a start-up, <span>ViBILLER</span>, <span>LLC</span>, registered in New York state. <span>ViBILLER</span> has an approved intellectual property license from New York University that allows the company to use the technology developed and has been successful at raising non-dilutive funding during the period of this award. &nbsp;Regarding broader impacts, this project has contributed to the advancement of several disciplines, including computer science, music technology, and human behavior and cognition. The project has also contributed to STEM workforce development through the mentoring and training of a postdoctoral associate and a research scientist in cross-disciplinary research and through outreach to high-schoolers in the New York city area. The team has also received extensive training in entrepreneurship both during the I-Corps period and after, as <span>ViBILLER</span> was accepted in several programs during the period of the award including the Max <span>Stenbeck</span> Venture Equity Program and New York University Tech Venture Accelerator.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/07/2023<br>\n\t\t\t\t\tModified by: Pablo&nbsp;Ripolles</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2136396/2136396_10749502_1694129804905_CHILLERLiveEvent_1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2136396/2136396_10749502_1694129804905_CHILLERLiveEvent_1--rgov-800width.jpg\" title=\"CHILLER at a live event 1\"><img src=\"/por/images/Reports/POR/2023/2136396/2136396_10749502_1694129804905_CHILLERLiveEvent_1--rgov-66x44.jpg\" alt=\"CHILLER at a live event 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Wearable device detecting the appearance of goosebumps in real time. The green LEDs light up with the intensity of the piloerection of the skin. This picture was taken at a live music event during customer discovery in Brooklyn (NYC)</div>\n<div class=\"imageCredit\">ViBILLER</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Pablo&nbsp;Ripolles</div>\n<div class=\"imageTitle\">CHILLER at a live event 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2136396/2136396_10749502_1694129859449_CHILLERLiveEvent_2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2136396/2136396_10749502_1694129859449_CHILLERLiveEvent_2--rgov-800width.jpg\" title=\"CHILLER at a live event 2\"><img src=\"/por/images/Reports/POR/2023/2136396/2136396_10749502_1694129859449_CHILLERLiveEvent_2--rgov-66x44.jpg\" alt=\"CHILLER at a live event 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Wearable device detecting the appearance of goosebumps in real time. The green LEDs light up with the intensity of the piloerection of the skin. This picture was taken at a live music event during customer discovery in Brooklyn (NYC)</div>\n<div class=\"imageCredit\">ViBILLER</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Pablo&nbsp;Ripolles</div>\n<div class=\"imageTitle\">CHILLER at a live event 2</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nEmotions are central to human life, influencing our behavior from the day we are born. They help us to act and avoid danger, and they are crucially involved in our decision-making. Importantly, emotion and engagement are well-known predictors of consumer preferences and purchasing decisions. This is especially true for commercial enterprises in which human emotions play a crucial role, such as in the music industry (e.g., emotions are related to ticket sales and preferred venues, among others). Despite the importance of emotion and engagement in sales in the music business, there are currently few means to objectively evaluate these measures in real time and in real-life settings (e.g., a live music event) in an accurate, affordable, and computationally inexpensive manner. Emotions not only affect human behavior, but also induce biological and physiological changes that can be objectively measured. In this vein, research in the music domain has focused on the quantification of a particular index of strong emotional responses: chills, also called frissons or shivers down the spine. Chills are usually quantified using techniques that are extremely susceptible to movement, electronic noise, and other environmental factors. Thus, it has traditionally been difficult to measure chills objectively and reliably in real-word testbeds. This I-Corps project assessed the market opportunity of a wearable device for the unconstrained, contactless, and real-time detection of one of the core physiological correlates of chills during live music events: the piloerection of the skin (e.g., goosebumps). Our technology uses individualized signals capturing piloerection?that is, peak moments of emotion?from a live audience to provide real-time measures of crowd (i.e., consumer) engagement. During this project we completed more than 400 customer discovery interviews (including venue managers, audiences, and other industry stakeholders) across multiple cities well-known for their live music scene (New York, Philadelphia, and Nashville). This allowed our team to understand the ecosystem in which our product can be successfully marketed and led to the creation of a start-up, ViBILLER, LLC, registered in New York state. ViBILLER has an approved intellectual property license from New York University that allows the company to use the technology developed and has been successful at raising non-dilutive funding during the period of this award.  Regarding broader impacts, this project has contributed to the advancement of several disciplines, including computer science, music technology, and human behavior and cognition. The project has also contributed to STEM workforce development through the mentoring and training of a postdoctoral associate and a research scientist in cross-disciplinary research and through outreach to high-schoolers in the New York city area. The team has also received extensive training in entrepreneurship both during the I-Corps period and after, as ViBILLER was accepted in several programs during the period of the award including the Max Stenbeck Venture Equity Program and New York University Tech Venture Accelerator.\n\n \n\n\t\t\t\t\tLast Modified: 09/07/2023\n\n\t\t\t\t\tSubmitted by: Pablo Ripolles"
 }
}