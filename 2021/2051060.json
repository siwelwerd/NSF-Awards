{
 "awd_id": "2051060",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "REU Site: Collaborative: Making Augmented and Virtual Reality Accessible",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2021-05-15",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 241557.0,
 "awd_amount": 241557.0,
 "awd_min_amd_letter_date": "2021-05-12",
 "awd_max_amd_letter_date": "2021-05-12",
 "awd_abstract_narration": "Augmented and virtual reality (XR) technologies are becoming popular consumer products. However, at present XR devices, development tools, and content pose major accessibility challenges for people with disabilities. How can you describe a 3D virtual environment to a person with a visual impairment? How should you present captions in 3D to a person who is hard-of-hearing? How can a person with a motor control disability navigate a 3D world?   \t\r\n\r\nThis REU Site brings together Profs. Shiri Azenkot and Brian Smith, experts in accessibility, along with Prof. Steven Feiner, an augmented reality and 3D interaction pioneer. The site will train students to be future leaders in XR accessibility. Students will receive training in accessibility and XR, engage in the research process from ideation to presentations, and join a community of scholars and practitioners who will provide career advice and mentorship. In addition, the projects undertaken by REU students will form the basis for new collaborations among the investigators and other collaborators at Columbia and Cornell. The site will recruit students from underrepresented backgrounds, especially students with disabilities. The target is to have a mixed-ability cohort where half the students have a disability and half do not. Students will participate in a range of formal and informal learning, including an XR development \"bootcamp,\" conversations with XR experts across academia and industry, and presentations on their work progress. Students will work on projects at the intersection of accessibility and XR. Examples of possible projects include designing descriptions that provide overviews of virtual environments for people who are blind and designing effective navigation techniques in a virtual environment, also for people who are blind. \r\n\r\nThe broader impacts of this proposal are two-fold. First, it supports the NSF\u2019s mission of broadening participation by focusing recruitment efforts on students from underrepresented groups, especially students with disabilities. Second, the focus of the research will benefit people with disabilities around the world. As XR becomes an increasingly popular consumer platform, the REU research will help ensure that it provides an equal user experience to people regardless of their disability status.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shiri",
   "pi_last_name": "Azenkot",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shiri Azenkot",
   "pi_email_addr": "shiri.azenkot@cornell.edu",
   "nsf_id": "000690878",
   "pi_start_date": "2021-05-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "2 West Loop Rd",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100441501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "113900",
   "pgm_ele_name": "RSCH EXPER FOR UNDERGRAD SITES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9250",
   "pgm_ref_txt": "REU SITE-Res Exp for Ugrd Site"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 241557.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-16d50293-7fff-5951-b659-cb5ad7b9d82e\"> </span></p>\r\n<p dir=\"ltr\"><span>Augmented and virtual reality (XR) technologies have become popular consumer products. However, at present, these technologies&mdash;devices, development and authoring tools, and content&mdash;pose barriers for people with disabilities. How can you describe a 3D virtual environment to a person with a visual impairment? How should you present captions in 3D to a person who is hard-of-hearing? How can a person with a motor control disability navigate a 3D world?</span></p>\r\n<p dir=\"ltr\"><span>Over the course of this Research Experience for Undergraduates program, Cornell Tech and Columbia University partnered to train over 20 aspiring undergraduate students to be future leaders in XR accessibility. They received training in both emerging extended reality technologies as well as core accessibility principles, and were mentored by scholars and practitioners with experience researching and implementing new accessibility features. These students made meaningful contributions to projects exploring these scientific frontiers, exploring topics like gesture-based feedback for virtual environments, accessible sports broadcasting, neurodiverse-friendly social VR, and AI guides for supporting blind people in virtual environments.</span></p>\r\n<p dir=\"ltr\"><span>This research experience supported NSF&rsquo;s mission of broadening participation by supporting students from under-represented groups, especially students with disabilities, in their continuing research endeavors. In addition, the resulting research conducted by students will benefit people with disabilities around the world. As XR becomes an increasingly popular consumer platform, the research these students have conducted will help ensure that it provides an equal user experience to people regardless of their disability status.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/13/2025<br>\nModified by: Shiri&nbsp;Azenkot</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501360416_Accessible_Sports_Broadcast--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501360416_Accessible_Sports_Broadcast--rgov-800width.png\" title=\"Towards Accessible Sports Broadcasts for Blind and Low-Vision Viewers\"><img src=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501360416_Accessible_Sports_Broadcast--rgov-66x44.png\" alt=\"Towards Accessible Sports Broadcasts for Blind and Low-Vision Viewers\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Immersive A/V conveys players\ufffd positions and actions as detected by computer vision-based video analysis, allowing BLV viewers to visualize the action.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Towards Accessible Sports Broadcasts for Blind and Low-Vision Viewers</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501206215_Gaming_Spatial_Awareness_Tools--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501206215_Gaming_Spatial_Awareness_Tools--rgov-800width.png\" title=\"Uncovering Visually Impaired Gamers\ufffd Preferences for Spatial Awareness Tools Within Video Games\"><img src=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501206215_Gaming_Spatial_Awareness_Tools--rgov-66x44.png\" alt=\"Uncovering Visually Impaired Gamers\ufffd Preferences for Spatial Awareness Tools Within Video Games\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustrations of the four spatial awareness tools we implemented within Dungeon Escape. These approaches \ufffd the smartphone map, the whole-room shockwave, the directional scanner, and the simple audio menu \ufffd represent a broad range of designs for facilitating spatial awareness for VIPs.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Uncovering Visually Impaired Gamers\ufffd Preferences for Spatial Awareness Tools Within Video Games</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501151817_Tanisha_Shende--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501151817_Tanisha_Shende--rgov-800width.jpg\" title=\"Tanisha Shende, REU Student\"><img src=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501151817_Tanisha_Shende--rgov-66x44.jpg\" alt=\"Tanisha Shende, REU Student\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Tanisha Shende worked on Enhancing Social VR Accessibility for Users With ADHD and Autism as part of her REU experience, exploring how to make social VR more friendly for neurodiverse users.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Tanisha Shende, REU Student</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501573019_Feiner1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501573019_Feiner1--rgov-800width.png\" title=\"Touchscreen Application for Use By Disabled Gamers\"><img src=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501573019_Feiner1--rgov-66x44.png\" alt=\"Touchscreen Application for Use By Disabled Gamers\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our project is focused on making Virtual Reality accessible for users with mobility or dexterity impairments. As part of our project, we are implementing a touchscreen application for controlling difficult interactions in a first-person shooter game.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Touchscreen Application for Use By Disabled Gamers</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501096781_Street_Camera_Navigation_2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501096781_Street_Camera_Navigation_2--rgov-800width.png\" title=\"Towards Street Camera-based Outdoor Navigation for Blind Pedestrians\"><img src=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501096781_Street_Camera_Navigation_2--rgov-66x44.png\" alt=\"Towards Street Camera-based Outdoor Navigation for Blind Pedestrians\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Overview of the street camera-based navigation system. The system provides real-time auditory feedback to help BLV users avoid obstacles, know exactly when to cross the street, and understand the overall layout of the environment.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Towards Street Camera-based Outdoor Navigation for Blind Pedestrians</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501463934_Hands_On_Glove--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501463934_Hands_On_Glove--rgov-800width.png\" title=\"Hands-On: Using Gestures to Control Descriptions of a Virtual Environment for People with Visual Impairments\"><img src=\"/por/images/Reports/POR/2025/2051060/2051060_10733289_1739501463934_Hands_On_Glove--rgov-66x44.png\" alt=\"Hands-On: Using Gestures to Control Descriptions of a Virtual Environment for People with Visual Impairments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In this demonstration, we present a haptic glove that PVI can use to request object descriptions in VR with their hands through familiar hand gestures. The gloves use retractable strings to track the user's finger movements.</div>\n<div class=\"imageCredit\">Project team</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Shiri&nbsp;Azenkot\n<div class=\"imageTitle\">Hands-On: Using Gestures to Control Descriptions of a Virtual Environment for People with Visual Impairments</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nAugmented and virtual reality (XR) technologies have become popular consumer products. However, at present, these technologiesdevices, development and authoring tools, and contentpose barriers for people with disabilities. How can you describe a 3D virtual environment to a person with a visual impairment? How should you present captions in 3D to a person who is hard-of-hearing? How can a person with a motor control disability navigate a 3D world?\r\n\n\nOver the course of this Research Experience for Undergraduates program, Cornell Tech and Columbia University partnered to train over 20 aspiring undergraduate students to be future leaders in XR accessibility. They received training in both emerging extended reality technologies as well as core accessibility principles, and were mentored by scholars and practitioners with experience researching and implementing new accessibility features. These students made meaningful contributions to projects exploring these scientific frontiers, exploring topics like gesture-based feedback for virtual environments, accessible sports broadcasting, neurodiverse-friendly social VR, and AI guides for supporting blind people in virtual environments.\r\n\n\nThis research experience supported NSFs mission of broadening participation by supporting students from under-represented groups, especially students with disabilities, in their continuing research endeavors. In addition, the resulting research conducted by students will benefit people with disabilities around the world. As XR becomes an increasingly popular consumer platform, the research these students have conducted will help ensure that it provides an equal user experience to people regardless of their disability status.\r\n\n\n\t\t\t\t\tLast Modified: 02/13/2025\n\n\t\t\t\t\tSubmitted by: ShiriAzenkot\n"
 }
}