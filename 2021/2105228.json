{
 "awd_id": "2105228",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "Neural mechanisms of relational perception",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": "7032927376",
 "po_email": "jwmirand@nsf.gov",
 "po_sign_block_name": "Josie Welkom Miranda",
 "awd_eff_date": "2021-07-15",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 138000.0,
 "awd_amount": 71875.0,
 "awd_min_amd_letter_date": "2021-07-11",
 "awd_max_amd_letter_date": "2022-07-21",
 "awd_abstract_narration": "This award was provided as part of NSF's Social, Behavioral and Economic Sciences Postdoctoral Research Fellowships (SPRF) program. The goal of the SPRF program is to prepare promising, early career doctoral-level scientists for scientific careers in academia, industry or private sector, and government. SPRF awards involve two years of training under the sponsorship of established scientists and encourage Postdoctoral Fellows to perform independent research. NSF seeks to promote the participation of scientists from all segments of the scientific community, including those from underrepresented groups, in its research programs and activities; the postdoctoral period is considered to be an important level of professional development in attaining this goal. Each Postdoctoral Fellow must address important scientific questions that advance their respective disciplinary fields. Under the sponsorship of Drs. Michael F. Bonner, Chaz Firestone, and Barbara Landau at Johns Hopkins University, this postdoctoral fellowship award supports an early career scientist investigating how the human brain represents visual relations. The world is more than a bag of objects: We see not only individual objects and their features (e.g., a fluffy cat or a textured mat) but also how they relate (a cat sitting ON a mat). Relations are a property holding between objects, beyond any properties the objects have on their own. How do we represent such relations? Although relations themselves cast no light onto our eyes, a growing body of work suggests that relations between objects are extracted in rapid and automatic visual processing, much as we automatically perceive an object's shape or color. Despite this, we have surprisingly little understanding of how the human brain represents such relations. For example, does the visual system automatically extract the structure of relations (distinguishing [mat on cat] from [cat on mat])? And might the brain represent relations and the participating objects (e.g., cat, mat, and ON) in an integrated, \"compressed\" manner (much like a computer might compress the contents of a file or image)? The proposed research aims to provide answers to these and other questions, using a set of physical relations (e.g., containment, support, adhesion, and fit) as a case study. By integrating methods from the fields of vision science and cognitive computational neuroscience, this research will advance our understanding of how the human brain extracts relational information from visual scenes. This research also has broad implications for understanding perceptual processing of physical relations, which are an unexplored but important domain in STEM education and crucial for scientific understanding, e.g., about physical mechanics (such as the movement of gas particles in a container).\r\n\r\nThe proposed research combines psychophysical, neuroimaging, and computational modeling approaches to pursue three objectives, aimed at characterizing: (1) what properties of relations are perceived, (2) where relational information is represented in the brain, and (3) how relational structure is computed and represented. We focus on physical relations between objects (e.g., containment [\"on\"] and support [\"in\"]), as such relations are central to many other processes in the mind, including physical understanding (e.g., if the mat moves, will the cat too?). In the first objective, we will use rapid perceptual tasks to measure the influence of relational properties on similarity judgment behavior. In the second objective, we will identify which areas of the brain encode visual relational information, by identifying brain regions in which the participating objects (e.g., cat, mat) are encoded in an integrated, non-linear manner (i.e., where relational representations are not well-approximated by simple weighted sums of the representations of the participating objects). In the third objective, we will test the hypothesis that the brain implements a compositional representation of visual relations, by asking whether a model that explicitly encodes relational structure (e.g., mat as Supporter, cat as Supported) can predict neural patterns for novel relational scenes. The proposed research engages a new frontier in scene representation: how the human brain computes high-level information about the relational structure of the world. It also has direct implications for theories of spatial cognition, language, and intuitive physics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alon",
   "pi_last_name": "Hafri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alon Hafri",
   "pi_email_addr": "",
   "nsf_id": "000814244",
   "pi_start_date": "2021-07-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Barbara",
   "pi_last_name": "Landau",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Barbara Landau",
   "pi_email_addr": "",
   "nsf_id": "000202816",
   "pi_start_date": "2021-07-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chaz",
   "pi_last_name": "Firestone",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chaz Firestone",
   "pi_email_addr": "",
   "nsf_id": "000784505",
   "pi_start_date": "2021-07-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Bonner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Bonner",
   "pi_email_addr": "",
   "nsf_id": "000797665",
   "pi_start_date": "2021-07-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Hafri, Alon",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Baltimore",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "",
  "inst_zip_code": "212182687",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": null,
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182683",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "040Y00",
   "pgm_ele_name": "(SPRF-FR) SBE Postdoctoral Res"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "7137",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 71875.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>When you look out at the world, you'll notice different objects and their features, such as a brown dog, a soft cat, or a textured mat. But beyond the individual objects, you may also appreciate how they relate to one another: a dog chasing a cat, or a cat on a mat. Relations such as these are all around us; indeed, they're a fundamental way that we make sense of the scenes that we experience from one moment to the next. How does the mind represent such relations? Whereas how people recognize objects has been the focus of much work in vision science, how people apprehend visual relations has received much less scientific attention.</p>\n<p><br />In this proposal, we explored how the mind represents visual relations and the visual scenes in which they are embedded. We tested the hypothesis that we automatically perceive many relations &mdash; much as we see more basic visual properties like colors or shapes without effort &mdash; and we sought to characterize this remarkable ability.</p>\n<p><br />First, we investigated how the mind 'builds' representations of relations, such as of a book on a table or a plant on a shelf. Even though it may seem like we 'take in' the visual information from a scene all at once, we tested the possibility that the mind constructs relational representations sequentially. In other words, that it represents one object first, and then the other (in particular. by starting with more physically stable 'reference' objects, like tables and shelves, rather than 'figure' objects, like laptops and plants). Across several studies, we found evidence that this was indeed the case: people were faster to recognize (and had a preference to manually 'compose') scenes starting with reference rather than figure objects.</p>\n<p><br />Second, we made key discoveries about how cognitive systems connect in the mind, using the relational property of 'symmetry' as a test case. Symmetry is ubiquitous in art, physics, and biology, and humans are exquisitely sensitive to visual symmetry. Yet symmetry goes far beyond the visuospatial world. Many words refer to symmetrical concepts (e.g., \"equal\", \"marry\"): if x equals y, it entails that y equals x. Is it just a coincidence that symmetry appears in both language and vision? We suspected not. To test this, we presented images of shapes that were symmetrical and non-symmetrical and asked people to choose from a set of words the one that best describes them. We found that people associated symmetrical images with words that have symmetrical meaning. Thus, we demonstrated that vision and language are connected at a deep level, involving abstract and structured representation of relational properties.</p>\n<p><br />Third, we investigated a special kind of visual relation: that of caused state-changes. When the sun melts ice, a pin pops a balloon, or a flame ignites a log or cigarette, this is a change of state. Representing such changes involves representing a relation: how an object at one moment in time is linked to its past or future \"self\", despite its sometimes dramatically different properties. How does the mind represent state changes? In a series of studies, we found that human memory actively distorts or 'plays forward' such changes (e.g., melting ice): when people observe an object changing, they remember it as more changed (e.g., more melted) than it actually was. Thus, we discovered that the mind relates two states of an object to one another by playing forward the change in memory.</p>\n<p><br />Finally, we investigated how the mind makes sense of our complex visual environment &mdash; not only how we perceive and remember visual relations, but how such processes interact when representing complex visual scenes, more generally. Memory is not like a camera: Rather than recording exactly what one sees, the mind may fill in details that were not actually there. A striking example of this memory distortion is boundary extension, whereby observers mistakenly recall a view that extends beyond what was actually seen. What causes this distortion? By exploiting a photographic technique called 'tilt-shift', we created a visual illusion of 'scale change': we made large-scale scenes appear as if they were 'miniature' scenes (like a diorama). In doing so, we discovered that the spatial scale at which the visual environment is viewed has deep consequences for how it is remembered.</p>\n<p><br />Taken together, our work reveals how the mind constructs relational representations in visual scenes, how relational representations in vision 'connect up' with language, and how visual relations (and complex scenes in general) are represented and distorted in memory. Although this project is centered on how the human mind represents visual relations, there are direct implications of this work for understanding how relations are represented outside of vision, including in language. The findings also have implications for how memory represents and distorts complex visual scenes, including the relations embedded in those scenes.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2022<br>\n\t\t\t\t\tModified by: Alon&nbsp;Hafri</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhen you look out at the world, you'll notice different objects and their features, such as a brown dog, a soft cat, or a textured mat. But beyond the individual objects, you may also appreciate how they relate to one another: a dog chasing a cat, or a cat on a mat. Relations such as these are all around us; indeed, they're a fundamental way that we make sense of the scenes that we experience from one moment to the next. How does the mind represent such relations? Whereas how people recognize objects has been the focus of much work in vision science, how people apprehend visual relations has received much less scientific attention.\n\n\nIn this proposal, we explored how the mind represents visual relations and the visual scenes in which they are embedded. We tested the hypothesis that we automatically perceive many relations &mdash; much as we see more basic visual properties like colors or shapes without effort &mdash; and we sought to characterize this remarkable ability.\n\n\nFirst, we investigated how the mind 'builds' representations of relations, such as of a book on a table or a plant on a shelf. Even though it may seem like we 'take in' the visual information from a scene all at once, we tested the possibility that the mind constructs relational representations sequentially. In other words, that it represents one object first, and then the other (in particular. by starting with more physically stable 'reference' objects, like tables and shelves, rather than 'figure' objects, like laptops and plants). Across several studies, we found evidence that this was indeed the case: people were faster to recognize (and had a preference to manually 'compose') scenes starting with reference rather than figure objects.\n\n\nSecond, we made key discoveries about how cognitive systems connect in the mind, using the relational property of 'symmetry' as a test case. Symmetry is ubiquitous in art, physics, and biology, and humans are exquisitely sensitive to visual symmetry. Yet symmetry goes far beyond the visuospatial world. Many words refer to symmetrical concepts (e.g., \"equal\", \"marry\"): if x equals y, it entails that y equals x. Is it just a coincidence that symmetry appears in both language and vision? We suspected not. To test this, we presented images of shapes that were symmetrical and non-symmetrical and asked people to choose from a set of words the one that best describes them. We found that people associated symmetrical images with words that have symmetrical meaning. Thus, we demonstrated that vision and language are connected at a deep level, involving abstract and structured representation of relational properties.\n\n\nThird, we investigated a special kind of visual relation: that of caused state-changes. When the sun melts ice, a pin pops a balloon, or a flame ignites a log or cigarette, this is a change of state. Representing such changes involves representing a relation: how an object at one moment in time is linked to its past or future \"self\", despite its sometimes dramatically different properties. How does the mind represent state changes? In a series of studies, we found that human memory actively distorts or 'plays forward' such changes (e.g., melting ice): when people observe an object changing, they remember it as more changed (e.g., more melted) than it actually was. Thus, we discovered that the mind relates two states of an object to one another by playing forward the change in memory.\n\n\nFinally, we investigated how the mind makes sense of our complex visual environment &mdash; not only how we perceive and remember visual relations, but how such processes interact when representing complex visual scenes, more generally. Memory is not like a camera: Rather than recording exactly what one sees, the mind may fill in details that were not actually there. A striking example of this memory distortion is boundary extension, whereby observers mistakenly recall a view that extends beyond what was actually seen. What causes this distortion? By exploiting a photographic technique called 'tilt-shift', we created a visual illusion of 'scale change': we made large-scale scenes appear as if they were 'miniature' scenes (like a diorama). In doing so, we discovered that the spatial scale at which the visual environment is viewed has deep consequences for how it is remembered.\n\n\nTaken together, our work reveals how the mind constructs relational representations in visual scenes, how relational representations in vision 'connect up' with language, and how visual relations (and complex scenes in general) are represented and distorted in memory. Although this project is centered on how the human mind represents visual relations, there are direct implications of this work for understanding how relations are represented outside of vision, including in language. The findings also have implications for how memory represents and distorts complex visual scenes, including the relations embedded in those scenes.\n\n\t\t\t\t\tLast Modified: 12/28/2022\n\n\t\t\t\t\tSubmitted by: Alon Hafri"
 }
}