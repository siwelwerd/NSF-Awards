{
 "awd_id": "2134248",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Scalable Linear Algebra and Neural Network Theory",
 "cfda_num": "47.041, 47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 350000.0,
 "awd_amount": 350000.0,
 "awd_min_amd_letter_date": "2021-08-12",
 "awd_max_amd_letter_date": "2023-07-24",
 "awd_abstract_narration": "These projects will use randomized numerical linear algebra building blocks to develop improved methods in stochastic optimization theory and statistical/machine learning theory.  The motivation is that, while machine learning and deep learning methodology has transformed certain applications, such as computer vision and natural language processing, its promised impact on many other areas has yet to be seen.  The reason for this is the flip side of why it has been successful where it has.  In the applications where it has had the most remarkable successes, people have adopted the following strategy: get large quantities of data; train a neural network model using stochastic first order methods; and implement and apply the model in a user-facing industrial application.  There are many well-known limitations with this general approach, ranging from the need for large quantities of data and daunting compute resources to interpretability and robustness issues.  These limitations are particularly apparent when using neural networks for problems such as high-performance computing, fluid mechanics/dynamics, temporal supply chain forecasting problems, biotechnology, etc., where interpretability is paramount.  This work aims to address central technical issues underlying this approach, namely: while linear algebraic techniques are central to the design and use of modern neural network models, current methodology uses linear algebra in relatively superficial ways.  If we have stronger control over the linear algebraic methods, the community will have a more practical theory to guide neural network use in a broad range of applications beyond computer vision and natural language processing.  These methods will enable qualitatively more refined scalable implementations and applications of neural network models in a range of scientific and engineering domains. Broader impacts of these projects include mentoring of grant-supported graduate students and postdoctoral researchers.\r\n\r\nTechnically, the work will focus on three general directions: optimization theory, including convex optimization based neural network and going beyond optimization; scalable linear algebra theory, including randomized linear algebra for neural networks, and sparse randomized linear algebra; and statistics and machine learning theory, including implicit regularization, and learning with limited non-iid data.  More broadly, the goal is to provide a basis for practical theory that can guide practice, in a manner analogous to how linear algebraic and functional analytic methods underlie practical and useful theory in a broad range of scientific/engineering applications. We expect that such a challenging task is possible since many of the recent developments in machine learning theory and neural network practice have parallels in scientific computing, where there is a long history of what may be called scalable linear algebra for physical/engineering theory.  Many of the methods to be developed may be viewed as bridging the interdisciplinary gap between these old ideas and the new challenges we face; and principal investigators have a history of developing interdisciplinary classes, summer schools, workshops related to the topics of the proposed work, and they will continue to do so.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mert",
   "pi_last_name": "Pilanci",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mert Pilanci",
   "pi_email_addr": "pilanci@stanford.edu",
   "nsf_id": "000779542",
   "pi_start_date": "2021-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 Jane Stanford Way",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 116667.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 116667.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 116666.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c7346fc2-7fff-8278-2aff-1c458e80b2a8\">\r\n<p dir=\"ltr\"><span>This project focused on developing advanced methods that use large-scale randomized linear algebra to improve the performance and efficiency of deep learning systems. Our work began with a new theoretical understanding of attention-based transformer networks and adversarial networks, reformulating them as convex weighted low-rank models. This approach not only provided insights into their optimization landscapes but also paved the way for more efficient low-rank approximation techniques.</span></p>\r\n<p dir=\"ltr\"><span>In the subsequent phases of the project, we introduced randomized low-rank decomposition techniques that accelerate training and simplify the adaptation of models to new tasks. Additionally, spectral-based methods enabled more precise and efficient fine-tuning by focusing on the most impactful parameters, significantly reducing the complexity of fine-tuning. These methods collectively lower computational requirements while maintaining state-of-the-art performance across diverse applications. To maximize the impact of this work, we released open-source software and comprehensive documentation, making it accessible for researchers and practitioners to integrate these advanced numerical linear algebra techniques into their deep learning workflows.</span></p>\r\n<p dir=\"ltr\"><span>Overall, this research provides foundational tools for improving the efficiency, reliability, and scalability of modern machine learning. By empowering applications ranging from computer vision to natural language processing, it has also offered interdisciplinary training opportunities that equip the next generation of AI innovators with the skills to drive future advancements.</span></p>\r\n</span></p><br>\n<p>\n Last Modified: 01/13/2025<br>\nModified by: Mert&nbsp;Pilanci</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nThis project focused on developing advanced methods that use large-scale randomized linear algebra to improve the performance and efficiency of deep learning systems. Our work began with a new theoretical understanding of attention-based transformer networks and adversarial networks, reformulating them as convex weighted low-rank models. This approach not only provided insights into their optimization landscapes but also paved the way for more efficient low-rank approximation techniques.\r\n\n\nIn the subsequent phases of the project, we introduced randomized low-rank decomposition techniques that accelerate training and simplify the adaptation of models to new tasks. Additionally, spectral-based methods enabled more precise and efficient fine-tuning by focusing on the most impactful parameters, significantly reducing the complexity of fine-tuning. These methods collectively lower computational requirements while maintaining state-of-the-art performance across diverse applications. To maximize the impact of this work, we released open-source software and comprehensive documentation, making it accessible for researchers and practitioners to integrate these advanced numerical linear algebra techniques into their deep learning workflows.\r\n\n\nOverall, this research provides foundational tools for improving the efficiency, reliability, and scalability of modern machine learning. By empowering applications ranging from computer vision to natural language processing, it has also offered interdisciplinary training opportunities that equip the next generation of AI innovators with the skills to drive future advancements.\r\n\t\t\t\t\tLast Modified: 01/13/2025\n\n\t\t\t\t\tSubmitted by: MertPilanci\n"
 }
}