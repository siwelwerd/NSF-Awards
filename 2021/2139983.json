{
 "awd_id": "2139983",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Developing data and evaluation methods to assess the generality and robustness of AI systems for abstraction and analogy-making",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2021-09-01",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 199661.0,
 "awd_amount": 215661.0,
 "awd_min_amd_letter_date": "2021-08-18",
 "awd_max_amd_letter_date": "2023-05-16",
 "awd_abstract_narration": "The ability of humans to make conceptual abstractions and analogies is at the root of many of our most important cognitive capabilities, such as learning new concepts from small numbers of examples, flexibly adapting our prior knowledge and experience to new situations, and communicating our knowledge to others. While AI has made dramatic progress over the last decade in areas such as vision, natural language processing, and robotics, current AI systems almost entirely lack the ability to form humanlike abstractions and analogies.  The lack of such abilities is in part responsible for the lack of robustness in current AI systems, as well as their difficulties with extrapolating what they have learned to diverse situations.  While there have been many efforts in past AI research on this topic, each individual effort has generally focused on a specific problem domain, without careful evaluation of the AI system\u2019s robustness within its domain or its generality across different domains.  In this project we will promote progress in AI by creating a web-based platform that offers a diverse set of abstraction and analogy-making challenges for the research community as well as new evaluation methods that test for generality and robustness within and across different challenge domains.  We will use our platform to evaluate selected existing AI approaches and to measure human performance on our challenges in order to compare with AI systems\u2019 performance.  Our work will contribute to the AI research community by spurring new approaches and evaluation methods for abstraction and analogy-making in machines, and will contribute more broadly via the development of methods for robust and generalizable AI systems.  \r\n\r\n Our specific research plan is to (1) curate an initial suite of idealized challenge domains inspired by Hofstadter\u2019s letter-string analogies, Raven\u2019s progressive matrices, Bongard problems, and Chollet\u2019s Abstraction and Reasoning Corpus;  (2) develop evaluation methods along dimensions such as robustness to variations on a particular concept, generality across domains, and scalability to more complex instances of a problem; (3) evaluate selected AI methods for abstraction and analogy using our evaluation methods; and (4) measure human benchmarks on our challenge suite using paid participants on the Amazon Mechanical Turk platform.  At the end of the project period, we will have demonstrated the utility and promise of our challenge problems and evaluations, and will have gained insight into their limitations. This work will set the stage for future efforts on expanding our challenge suite, improving our evaluation metrics, and developing and evaluating novel AI approaches to abstraction and analogy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Melanie",
   "pi_last_name": "Mitchell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Melanie Mitchell",
   "pi_email_addr": "mm@santafe.edu",
   "nsf_id": "000461779",
   "pi_start_date": "2021-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Santa Fe Institute",
  "inst_street_address": "1399 HYDE PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA FE",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5059462727",
  "inst_zip_code": "875018943",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "NM03",
  "org_lgl_bus_name": "SANTA FE INSTITUTE OF SCIENCE",
  "org_prnt_uei_num": "",
  "org_uei_num": "M8SBQ7NVNAH4"
 },
 "perf_inst": {
  "perf_inst_name": "Santa Fe Institute",
  "perf_str_addr": "1399 Hyde Park Road",
  "perf_city_name": "Santa Fe",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "875018943",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "NM03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 199661.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project we developed new methods for evaluating generalization and robustness in AI systems that perform abstraction and analogy-making.&nbsp; Our benchmarks and methods build on three domains that have been central for testing such systems: Raven's Progressive Matrices, the Abstraction and Reasoning Corpus, and letter-string analogies.</p>\n<p><strong>Ravens Progressive Matrices:</strong> Many studies have shown that neural networks can be trained to perform as well or better than humans on the RAVEN benchmark, a large set of simplified Raven's-style problems developed in 2019.&nbsp; But how general and robust are these systems? We proposed evaluating such systems on concept-based benchmarks---ones that assess generality and robustness on particular abstract concepts. We created a novel set of evaluation problems in the RAVEN domain, each of which is a variation on a particular abstract concept, such as \"Sameness\" or \"Numerical Progression\".&nbsp; We used these problems to evaluate two state-of-the-art systems that matched or exceeded human accuracy on the original RAVEN benchmark.&nbsp; We found that the accuracy of these systems dropped dramatically on our problems, indicating that, while these systems perform well on the original RAVEN problems, they lack robust, general understanding of the concepts that the benchmark supposedly tested.</p>\n<p><strong>Abstraction and Reasoning Corpus:</strong> The Abstraction and Reasoning Corpus (ARC) was developed by Chollet in 2019 to evaluate few-shot abstract reasoning on simple visual puzzles that capture important spatial and semantic concepts. &nbsp;Similar to our work on RAVEN, our group created a new set of puzzles in the ARC domain that are designed (1) to be easy for humans and (2) to provide a concept-based evaluation by including systematic variations on specific concepts.&nbsp; We used our \"ConceptARC\" benchmark to evaluate two state-of-the-art programs for solving ARC, as well as text-only and multimodal versions of OpenAI's GPT-4, a sophisticated pre-trained large language model.&nbsp; We also performed a study evaluating humans on this benchmark.&nbsp; We found that there remains a large gap between the high performance of humans on these puzzles compared with any of the AI systems we tested.</p>\n<p><strong>Letter-String Analogies:</strong> In the 1980s, Hofstadter created the letter-string analogies domain as an idealized \"microworld\" for studying creative analogy-making.&nbsp; In 2023, Webb, Holyoak, and Lu showed that OpenAI's GPT-3 performed as well as humans on a large set of letter-string analogy problems. Our group evaluated the robustness of GPT-3 (and of later GPT models) in solving such problems, by introducing two types of \"counterfactual\" letter-string problems: one type that used a counterfactual alphabet with permuted letters, and a second type that used counterfactual alphabets consisting of non-alphabetic symbols. We showed that while human performance remained high on such counterfactual versions of analogy problems, the accuracy of GPT models decreased dramatically, indicating that these systems may not be forming and reasoning with abstract concepts the way humans do.</p>\n<p><strong>Intellectual Merit:</strong> Imbuing machines with abilities for abstraction and analogy-making is an essential step toward more general and robust artificial intelligence. By developing novel methods and benchmarks for assessing such abilities, this project has helped evaluate where AI systems stand with respect to humans in these crucial cognitive capacities, and has helped set the stage for more thorough assessments of such systems.</p>\n<p><strong>Broader Impacts:</strong> An essential part of our project was the training and professional development of young scientists.&nbsp; One postdoctoral fellow, two post-undergraduate research interns, and two undergraduate students were involved in our research.&nbsp; We also placed emphasis on the public dissemination of our work, through multiple papers, public lectures, and the availability of all of our benchmark problems and code on the web.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 06/24/2024<br>\nModified by: Melanie&nbsp;Mitchell</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project we developed new methods for evaluating generalization and robustness in AI systems that perform abstraction and analogy-making. Our benchmarks and methods build on three domains that have been central for testing such systems: Raven's Progressive Matrices, the Abstraction and Reasoning Corpus, and letter-string analogies.\n\n\nRavens Progressive Matrices: Many studies have shown that neural networks can be trained to perform as well or better than humans on the RAVEN benchmark, a large set of simplified Raven's-style problems developed in 2019. But how general and robust are these systems? We proposed evaluating such systems on concept-based benchmarks---ones that assess generality and robustness on particular abstract concepts. We created a novel set of evaluation problems in the RAVEN domain, each of which is a variation on a particular abstract concept, such as \"Sameness\" or \"Numerical Progression\". We used these problems to evaluate two state-of-the-art systems that matched or exceeded human accuracy on the original RAVEN benchmark. We found that the accuracy of these systems dropped dramatically on our problems, indicating that, while these systems perform well on the original RAVEN problems, they lack robust, general understanding of the concepts that the benchmark supposedly tested.\n\n\nAbstraction and Reasoning Corpus: The Abstraction and Reasoning Corpus (ARC) was developed by Chollet in 2019 to evaluate few-shot abstract reasoning on simple visual puzzles that capture important spatial and semantic concepts. Similar to our work on RAVEN, our group created a new set of puzzles in the ARC domain that are designed (1) to be easy for humans and (2) to provide a concept-based evaluation by including systematic variations on specific concepts. We used our \"ConceptARC\" benchmark to evaluate two state-of-the-art programs for solving ARC, as well as text-only and multimodal versions of OpenAI's GPT-4, a sophisticated pre-trained large language model. We also performed a study evaluating humans on this benchmark. We found that there remains a large gap between the high performance of humans on these puzzles compared with any of the AI systems we tested.\n\n\nLetter-String Analogies: In the 1980s, Hofstadter created the letter-string analogies domain as an idealized \"microworld\" for studying creative analogy-making. In 2023, Webb, Holyoak, and Lu showed that OpenAI's GPT-3 performed as well as humans on a large set of letter-string analogy problems. Our group evaluated the robustness of GPT-3 (and of later GPT models) in solving such problems, by introducing two types of \"counterfactual\" letter-string problems: one type that used a counterfactual alphabet with permuted letters, and a second type that used counterfactual alphabets consisting of non-alphabetic symbols. We showed that while human performance remained high on such counterfactual versions of analogy problems, the accuracy of GPT models decreased dramatically, indicating that these systems may not be forming and reasoning with abstract concepts the way humans do.\n\n\nIntellectual Merit: Imbuing machines with abilities for abstraction and analogy-making is an essential step toward more general and robust artificial intelligence. By developing novel methods and benchmarks for assessing such abilities, this project has helped evaluate where AI systems stand with respect to humans in these crucial cognitive capacities, and has helped set the stage for more thorough assessments of such systems.\n\n\nBroader Impacts: An essential part of our project was the training and professional development of young scientists. One postdoctoral fellow, two post-undergraduate research interns, and two undergraduate students were involved in our research. We also placed emphasis on the public dissemination of our work, through multiple papers, public lectures, and the availability of all of our benchmark problems and code on the web.\n\n\n\t\t\t\t\tLast Modified: 06/24/2024\n\n\t\t\t\t\tSubmitted by: MelanieMitchell\n"
 }
}