{
 "awd_id": "2106152",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: A Bias-Aware Approach to Modeling Users in Interactive Information Retrieval",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922706",
 "po_email": "ccaragea@nsf.gov",
 "po_sign_block_name": "Cornelia Caragea",
 "awd_eff_date": "2021-06-15",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 174959.0,
 "awd_amount": 182959.0,
 "awd_min_amd_letter_date": "2021-05-26",
 "awd_max_amd_letter_date": "2022-05-24",
 "awd_abstract_narration": "People often act intuitively and are subject to systematic biases when making decisions under uncertainty due to their inability to calculate all the possible consequences of their choices - a fundamental cognitive phenomenon called \"bounded rationality\". Without proactive information supports, these decisions could be driven by misleading information, cognitive biases and heuristics and may result in significant deviations from desired outcomes: Health information seekers may easily trust medical misinformation that confirms their existing expectations. Students often heavily rely on top ranked results and stop at short satisficing answers, rather than exploring more credible and informative Web pages. Online shoppers tend to quickly accept immediate mediocre recommendations after encountering several bad quality products (with low reference levels), without examining all available options. By investigating users\u2019 systematic biases, this project aims to break new grounds for information retrieval (IR) research and address fundamental bottlenecks in the development of bias-aware search systems. The outcomes of this project can help people better leverage the power of information through 1) incorporating the knowledge about their biases into search algorithms, and 2) proactively capturing bias-related search problems and promoting informed, unbiased decision-making.\r\n\r\n\r\nThe project seeks to study users\u2019 systematic biases and leverage the learned knowledge in improving the explanatory and predicative power of IR models. The technical aims of the project include: (1) understanding the relationships between search interactions and users\u2019 systematic biases; (2) building bias-aware prediction models of search interactions; (3) developing a scalable and potentially transformative approach to modeling users and their decision-making processes under biases in interactive IR. To achieve these goals, the investigator will conduct a series of user studies and experiments. First, the research team will carry out controlled lab studies to examine the associations between users\u2019 search interactions and several major systematic biases that have been widely confirmed by behavioral experiments, including reference dependence, framing effect, and loss aversion. Then, the team will extract new features and create bias-aware models for predicting users\u2019 search behavior, experience, and problems. Finally, this project will apply deep neural networks in developing more fine-grained bias-aware models based on large scale test collections and search logs, and evaluate the performance of modified models in a wider range of search scenarios. The proposed models can provide a more solid behavioral and psychological basis for supporting the simulations of search interactions. Such simulations, properly constructed, could address major challenges in the design of boundedly-rational formal models and bias-aware intelligent systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jiqun",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jiqun Liu",
   "pi_email_addr": "jiqunliu@ou.edu",
   "nsf_id": "000841792",
   "pi_start_date": "2021-05-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Oklahoma Norman Campus",
  "inst_street_address": "660 PARRINGTON OVAL RM 301",
  "inst_street_address_2": "",
  "inst_city_name": "NORMAN",
  "inst_state_code": "OK",
  "inst_state_name": "Oklahoma",
  "inst_phone_num": "4053254757",
  "inst_zip_code": "730193003",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OK04",
  "org_lgl_bus_name": "UNIVERSITY OF OKLAHOMA",
  "org_prnt_uei_num": "",
  "org_uei_num": "EVTSTTLCEWS5"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Oklahoma",
  "perf_str_addr": "210 Stephenson Parkway",
  "perf_city_name": "Norman",
  "perf_st_code": "OK",
  "perf_st_name": "Oklahoma",
  "perf_zip_code": "730199705",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OK04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 174959.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored the critical role that human cognitive biases play in interactions with intelligent systems, such as search engines and recommender systems. Cognitive biases&mdash;such as confirmation bias, loss aversion, decoy effect, and reference dependence&mdash;can heavily influence how users search for, evaluate, interact with and use information. These biases often lead to skewed judgments and decision-making, making it harder for users to arrive at well-informed conclusions and desired outcomes. The project aimed to address this issue 1) by investigating how biases manifest in user behavior and 2) by developing methods to detect and mitigate their effects in interactive search systems.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual Merit</strong></p>\n<p>One of the core achievements in this project was uncovering how deeply user biases influence digital interactions. For instance, in online search scenarios, users often gravitate toward information that confirms their preexisting expectations and beliefs (confirmation bias), which can narrow their perspectives and prevent balanced decision-making. Similarly, loss aversion&mdash;where users are more focused on avoiding losses than achieving gains&mdash;can distort their search strategies and in-situ evaluations on presented options, leading them to prioritize familiar or less risky options, even when better choices are available.</p>\n<p>Through a series of user studies and controlled experiments, the project provided empirical data on how cognitive biases affect both search outcomes and user satisfaction. For example, participants were often found to avoid certain search results or recommendations because of the way options were framed, even when those options were more relevant and credible. This finding highlights how subtle interface design choices can inadvertently reinforce user biases. By mapping these biases onto user interaction data, we gained new insights into how users make decisions when faced with large sets of information. To counteract these issues, the project developed bias-aware user models, which are designed to better understand and predict how users will respond to information and make search decisions based on their inherent biases. These models help intelligent systems like commercial search engines and recommendation platforms provide more balanced results that not only align with user preferences but also gently correct for biases, and offer a more diverse and informative range of options. For instance, a bias-aware search engine might adjust the ranking of search results to subtly present perspectives that challenge the user&rsquo;s preconceptions, and promote a broader and more diverse understanding of the topic at hand.</p>\n<p>The project also contributed to advancing evaluation metrics that account for these biases, which allow researchers and developers to assess the effectiveness of intelligent systems, especially on how well they mitigate bias in search result judgments and promote fair exposure of different perspectives. These bias-aware evaluation metrics provide a more comprehensive framework for evaluating systems that interact with human cognition, moving beyond traditional metrics (e.g. click-through rates) to measure more meaningful engagement with information.</p>\n<p>&nbsp;</p>\n<p><strong>Broader Impacts</strong></p>\n<p>The findings of this project have important implications for creating fairer, more equitable and responsible digital systems. For example, in healthcare, biased search interactions could lead to users prioritizing less reliable health advice simply because it confirms their existing beliefs. The bias-aware models developed in this project could help mitigate these effects by promoting more balanced information, potentially guiding users toward better health outcomes. Similarly, in education, students using biased search engines may overlook important learning materials that challenge their views, limiting their academic growth. A bias-mitigating system could instead encourage exposure to a wider range of viewpoints, fostering deeper learning and critical thinking.</p>\n<p>Beyond specific sectors, the project introduced practical tools and methods that can be applied to future system design and assessment. By incorporating the project&rsquo;s findings, developers of search engines, recommendation systems, and other information retrieval tools can create interfaces that help users make more informed, less biased decisions. For example, recommendation systems in online shopping platforms could be improved to prevent users from making hasty purchases based on short-term emotional responses, instead encouraging more deliberate and informed choices. Moreover, the project&rsquo;s findings are already informing educational initiatives. Materials from the project, including datasets and experimental findings, have been integrated into course modules that offer students new perspectives on how cognitive biases influence interactions with personalized information systems. These modules not only enhance students' understanding of user behavior in real-world systems but also equip them with the knowledge and skills needed to develop next-generation, bias-aware interactive systems themselves.</p>\n<p>In summary, this project advances our understanding of how human biases shape interactions with intelligent systems and offers practical solutions for assessing and reducing these biases in digital environments, especially in the era of AI. The insights gained and tools developed will contribute to building more equitable, transparent, and user-centered information systems across a wide range of sectors, ensuring that users are better supported in making informed, unbiased decisions.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/23/2024<br>\nModified by: Jiqun&nbsp;Liu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project explored the critical role that human cognitive biases play in interactions with intelligent systems, such as search engines and recommender systems. Cognitive biasessuch as confirmation bias, loss aversion, decoy effect, and reference dependencecan heavily influence how users search for, evaluate, interact with and use information. These biases often lead to skewed judgments and decision-making, making it harder for users to arrive at well-informed conclusions and desired outcomes. The project aimed to address this issue 1) by investigating how biases manifest in user behavior and 2) by developing methods to detect and mitigate their effects in interactive search systems.\n\n\n\n\n\nIntellectual Merit\n\n\nOne of the core achievements in this project was uncovering how deeply user biases influence digital interactions. For instance, in online search scenarios, users often gravitate toward information that confirms their preexisting expectations and beliefs (confirmation bias), which can narrow their perspectives and prevent balanced decision-making. Similarly, loss aversionwhere users are more focused on avoiding losses than achieving gainscan distort their search strategies and in-situ evaluations on presented options, leading them to prioritize familiar or less risky options, even when better choices are available.\n\n\nThrough a series of user studies and controlled experiments, the project provided empirical data on how cognitive biases affect both search outcomes and user satisfaction. For example, participants were often found to avoid certain search results or recommendations because of the way options were framed, even when those options were more relevant and credible. This finding highlights how subtle interface design choices can inadvertently reinforce user biases. By mapping these biases onto user interaction data, we gained new insights into how users make decisions when faced with large sets of information. To counteract these issues, the project developed bias-aware user models, which are designed to better understand and predict how users will respond to information and make search decisions based on their inherent biases. These models help intelligent systems like commercial search engines and recommendation platforms provide more balanced results that not only align with user preferences but also gently correct for biases, and offer a more diverse and informative range of options. For instance, a bias-aware search engine might adjust the ranking of search results to subtly present perspectives that challenge the users preconceptions, and promote a broader and more diverse understanding of the topic at hand.\n\n\nThe project also contributed to advancing evaluation metrics that account for these biases, which allow researchers and developers to assess the effectiveness of intelligent systems, especially on how well they mitigate bias in search result judgments and promote fair exposure of different perspectives. These bias-aware evaluation metrics provide a more comprehensive framework for evaluating systems that interact with human cognition, moving beyond traditional metrics (e.g. click-through rates) to measure more meaningful engagement with information.\n\n\n\n\n\nBroader Impacts\n\n\nThe findings of this project have important implications for creating fairer, more equitable and responsible digital systems. For example, in healthcare, biased search interactions could lead to users prioritizing less reliable health advice simply because it confirms their existing beliefs. The bias-aware models developed in this project could help mitigate these effects by promoting more balanced information, potentially guiding users toward better health outcomes. Similarly, in education, students using biased search engines may overlook important learning materials that challenge their views, limiting their academic growth. A bias-mitigating system could instead encourage exposure to a wider range of viewpoints, fostering deeper learning and critical thinking.\n\n\nBeyond specific sectors, the project introduced practical tools and methods that can be applied to future system design and assessment. By incorporating the projects findings, developers of search engines, recommendation systems, and other information retrieval tools can create interfaces that help users make more informed, less biased decisions. For example, recommendation systems in online shopping platforms could be improved to prevent users from making hasty purchases based on short-term emotional responses, instead encouraging more deliberate and informed choices. Moreover, the projects findings are already informing educational initiatives. Materials from the project, including datasets and experimental findings, have been integrated into course modules that offer students new perspectives on how cognitive biases influence interactions with personalized information systems. These modules not only enhance students' understanding of user behavior in real-world systems but also equip them with the knowledge and skills needed to develop next-generation, bias-aware interactive systems themselves.\n\n\nIn summary, this project advances our understanding of how human biases shape interactions with intelligent systems and offers practical solutions for assessing and reducing these biases in digital environments, especially in the era of AI. The insights gained and tools developed will contribute to building more equitable, transparent, and user-centered information systems across a wide range of sectors, ensuring that users are better supported in making informed, unbiased decisions.\n\n\n\t\t\t\t\tLast Modified: 09/23/2024\n\n\t\t\t\t\tSubmitted by: JiqunLiu\n"
 }
}