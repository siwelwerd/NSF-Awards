{
 "awd_id": "2120750",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Validating and Communiciating Model-Based Approaches for Data Visualization Ability Assessment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 238848.0,
 "awd_amount": 121869.0,
 "awd_min_amd_letter_date": "2021-05-11",
 "awd_max_amd_letter_date": "2021-05-11",
 "awd_abstract_narration": "People are encountering graphs, charts, and other visual representations of data now more than ever before. Yet creators of these visualizations currently must reason with sparse and conflicting evidence on how well people can read the visualizations they publish.  Current guidelines do not take into account the possibility that different people have different strengths and weaknesses when interpreting visual data. This project will use studies of visualization effectiveness to inform our understanding of the abilities and biases of viewers, both individually and collectively.  To do this, the project team will use a combination of experiments, statistical modeling, and interview studies to both challenge long-standing assumptions about visualization effectiveness, and to lay a foundation for future experiments that account for differences in visualization reading ability. The work will also support a broader educational goal of using robust statistical modeling techniques in experimentation, through course modules that can be integrated into existing data visualization courses, and through outreach activities that allow individuals to see how well they perform visualization tasks compared to others who have taken the experiments.\r\n\r\nThis work seeks to answer three primary research questions. The first is to determine the extent to which individuals differ in their ability to perform basic tasks with data visualizations, through large-scale crowdsourced experiments that use transparent statistical methodologies to establish individual differences in data visualization performance. The second question evaluates the relationship between low-level visualization performance and higher-level assessments such as visualization literacy and cognitive abilities, recruiting both expert and novice populations to evaluate the extent to which these hypothesized measures of visualization literacy correlate with each other. The third question determines how alternative ways of presenting visualization experiment results shape the design recommendations researchers and designers draw from them, through a comparative evaluation of longstanding ways of presenting visualization experiment results, and by designing new ways of presenting results that may lead to more mature interpretation of experiment results by broader visualization community. The work will provide new perspectives on visualization literacy by augmenting chart reading experiments with novel measures of visualization ability, and by studying how creators currently make use of existing visualization design guidelines in their design process.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Kay",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Kay",
   "pi_email_addr": "mjskay@northwestern.edu",
   "nsf_id": "000734010",
   "pi_start_date": "2021-05-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "750 N. Lake Shore Drive",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606114579",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "IL05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 121869.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project examined various aspects of people's basic ability to read and understand <em>data&nbsp;visualizations</em>&nbsp;(i.e.&nbsp;visual depictions of data, such as bar charts, pie charts, and scatterplots), a set of skills known collectively as <em>visualization literacy</em>.</p>\n<p>Re-examining longstanding results on visualization understanding, we found substantial variation in people's ability to read basic chart types, such as bar charts and pie charts. While typical design recommendations suggest that chart reading ability is very similar from person to person, attributing the effectiveness of charts largely to their basic perceptual properties, we found more variation: some people are much better at reading certain chart types than would be expected based on prior work. These results suggest a need for more nuanced design recommendations when trying to determine the best chart to use to display any particular dataset. They also suggest that training people in how to better read common chart types may in some cases be more impactful than picking different chart designs.</p>\n<p>We also examined people's skills at reading unfamiliar and misleading charts. We found that students in unversity-level visualization classes develop an important ability to <em>deconstruct</em>&nbsp;an unfamiliar visualization into its constituent parts in order to decipher its message --- a skill that could be more explicitly taught to improve visualization understanding amongst the general public. We also developed a new test for measuring people's susceptibility to being misled by poorly-constructed charts, which can be used to assess this ability amongst the general public. Looking forward, we hope this work can form a basis for future work that aims at measuring and improving people's ability to interpret visualizations they encounter in the wild --- at work, on social media, or in the news --- particularly when these visualizations may be unfamiliar or poorly made.</p><br>\n<p>\n Last Modified: 05/07/2024<br>\nModified by: Matthew&nbsp;Kay</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project examined various aspects of people's basic ability to read and understand datavisualizations(i.e.visual depictions of data, such as bar charts, pie charts, and scatterplots), a set of skills known collectively as visualization literacy.\n\n\nRe-examining longstanding results on visualization understanding, we found substantial variation in people's ability to read basic chart types, such as bar charts and pie charts. While typical design recommendations suggest that chart reading ability is very similar from person to person, attributing the effectiveness of charts largely to their basic perceptual properties, we found more variation: some people are much better at reading certain chart types than would be expected based on prior work. These results suggest a need for more nuanced design recommendations when trying to determine the best chart to use to display any particular dataset. They also suggest that training people in how to better read common chart types may in some cases be more impactful than picking different chart designs.\n\n\nWe also examined people's skills at reading unfamiliar and misleading charts. We found that students in unversity-level visualization classes develop an important ability to deconstructan unfamiliar visualization into its constituent parts in order to decipher its message --- a skill that could be more explicitly taught to improve visualization understanding amongst the general public. We also developed a new test for measuring people's susceptibility to being misled by poorly-constructed charts, which can be used to assess this ability amongst the general public. Looking forward, we hope this work can form a basis for future work that aims at measuring and improving people's ability to interpret visualizations they encounter in the wild --- at work, on social media, or in the news --- particularly when these visualizations may be unfamiliar or poorly made.\t\t\t\t\tLast Modified: 05/07/2024\n\n\t\t\t\t\tSubmitted by: MatthewKay\n"
 }
}