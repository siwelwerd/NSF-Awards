{
 "awd_id": "2041221",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research: Compositional Linguistic Generalization in Human and Machine Learning",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927920",
 "po_email": "jvaldesk@nsf.gov",
 "po_sign_block_name": "Jorge Valdes Kroff",
 "awd_eff_date": "2021-01-15",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 12771.0,
 "awd_amount": 12771.0,
 "awd_min_amd_letter_date": "2020-12-21",
 "awd_max_amd_letter_date": "2021-05-20",
 "awd_abstract_narration": "Compositionality, the principle that the meaning of a complex expression is built from the meanings of its parts, is central to human language. This property enables us to produce and comprehend novel expressions that we have never encountered before, by composing the parts that we already know. For example, an English speaker who is told what the meaning of the sentence 'The blick saw the cat' is (let's say 'blick' is a black duck), can easily generalize their understanding to a novel sentence 'The cat saw the blick' without explicitly being told what it means. The goal of this project is to take a step towards elucidating the mechanism underlying generalization facilitated by the principle of compositionality. This goal will be undertaken through a combination of human and machine learning studies. This project will encourage methodological transfer between human and machine learning research, as well as promoting collaboration between scholars in Linguistics and researchers working on language in industrial labs. This research has potential implications for Artificial Intelligence (AI)\u2014computational models with better generalization capacity can help address shortcomings of existing models, such as robustness and data efficiency.\r\n\r\nContemporary neural models\u2014a family of models that is based on massive parallel computation inspired by biological neural circuits, and has greatly advanced the progress in AI\u2014only achieve partial success in compositional generalization. In particular, prior work has shown that neural models struggle with generalizations that require novel composition of known structures (structural generalization). An instance of a structural generalization is generalizing a modifier only seen in object position to subject position. For example, if a model can assign the correct meaning to 'the girl saw a cat on the mat', can it also assign correct meaning to 'the girl on the mat saw a cat'? Limited structural generalization in neural models motivates the two main research questions of this project. First, what are the capabilities and limitations of structural generalization in human learners? Second, what can be learned from the inner workings of neural models that achieve partial compositional success, and what revisions to these models would facilitate more human-like generalization? The first question will be explored through an artificial language learning study with human subjects (native English speakers), and the second question, via a computational modeling study with neural networks. This research will advance our understanding of the sufficient conditions for human-like compositional generalization to arise in neural networks. Furthermore, the human experiments will fill an important gap in the literature by testing structural generalization in a controlled experiment\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Smolensky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Paul Smolensky",
   "pi_email_addr": "smolensky@jhu.edu",
   "nsf_id": "000221798",
   "pi_start_date": "2020-12-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Geraldine",
   "pi_last_name": "Legendre",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Geraldine Legendre",
   "pi_email_addr": "legendre@jhu.edu",
   "nsf_id": "000370840",
   "pi_start_date": "2020-12-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tal",
   "pi_last_name": "Linzen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tal Linzen",
   "pi_email_addr": "linzen@nyu.edu",
   "nsf_id": "000753386",
   "pi_start_date": "2020-12-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Najoung",
   "pi_last_name": "Kim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Najoung Kim",
   "pi_email_addr": "najoungk@gmail.com",
   "nsf_id": "000828589",
   "pi_start_date": "2020-12-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N Charles St",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182685",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "837400",
   "pgm_ele_name": "DDRI Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 12771.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored the following two research questions: (1) What modifications to neural network learners facilitate compositional generalization? and (2) Do human learners generalize to novel structures, in particular generalize modification under very constrained exposure conditions?</p>\n<p>Regarding the first research question, we found that additionally training the neural network learners on the glossing task (word-by-word translation) significantly improved compositional generalization. Importantly, an analysis of the network outputs revealed that this task helped networks produce more faithful input-output mappings. However, we did not see substantial improvements in cases that require generalization to novel structures. Through additional investigations into structural generalization, we found that the only kind of models that could handle novel structures were those explicitly augmented with structural information (e.g., through a parser), pointing towards the necessity of stronger structural biases. The models that failed to generalize structurally included recent large language models trained on billions of tokens of training data, highlighting an important weakness in their capacity.</p>\n<p>Regarding the second research question, we conducted artificial language learning studies on human participants (adult native speakers of English) to test whether they generalized modification to novel syntactic positions---a type of generalization that neural network learners especially struggled with. Our experiments showed that even under extremely limited exposure conditions (seeing a modified structure in one syntactic position), the participants were willing to generalize modification to different syntactic positions not observed during training, exhibiting a different inductive bias from the neural network learners we tested.</p>\n<p>Our findings hold significance for multiple disciplines, and have been disseminated in the form of publications and invited talks in various communities spanning Linguistics, Computer Science, Cognitive Science, Philosophy, and Data Science, and also received media coverage from Allen Institute for AI. All datasets developed as a part of this project have been released publicly, enabling them to function as benchmarks for future model development. Furthermore, this project enabled the training of the student Co-PI and a visiting student from the University of Paris, as well as facilitating international collaboration.&nbsp;</p><br>\n<p>\n Last Modified: 04/30/2024<br>\nModified by: Najoung&nbsp;Kim</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project explored the following two research questions: (1) What modifications to neural network learners facilitate compositional generalization? and (2) Do human learners generalize to novel structures, in particular generalize modification under very constrained exposure conditions?\n\n\nRegarding the first research question, we found that additionally training the neural network learners on the glossing task (word-by-word translation) significantly improved compositional generalization. Importantly, an analysis of the network outputs revealed that this task helped networks produce more faithful input-output mappings. However, we did not see substantial improvements in cases that require generalization to novel structures. Through additional investigations into structural generalization, we found that the only kind of models that could handle novel structures were those explicitly augmented with structural information (e.g., through a parser), pointing towards the necessity of stronger structural biases. The models that failed to generalize structurally included recent large language models trained on billions of tokens of training data, highlighting an important weakness in their capacity.\n\n\nRegarding the second research question, we conducted artificial language learning studies on human participants (adult native speakers of English) to test whether they generalized modification to novel syntactic positions---a type of generalization that neural network learners especially struggled with. Our experiments showed that even under extremely limited exposure conditions (seeing a modified structure in one syntactic position), the participants were willing to generalize modification to different syntactic positions not observed during training, exhibiting a different inductive bias from the neural network learners we tested.\n\n\nOur findings hold significance for multiple disciplines, and have been disseminated in the form of publications and invited talks in various communities spanning Linguistics, Computer Science, Cognitive Science, Philosophy, and Data Science, and also received media coverage from Allen Institute for AI. All datasets developed as a part of this project have been released publicly, enabling them to function as benchmarks for future model development. Furthermore, this project enabled the training of the student Co-PI and a visiting student from the University of Paris, as well as facilitating international collaboration.\t\t\t\t\tLast Modified: 04/30/2024\n\n\t\t\t\t\tSubmitted by: NajoungKim\n"
 }
}