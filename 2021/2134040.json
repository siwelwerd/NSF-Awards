{
 "awd_id": "2134040",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Foundations of Deep Learning: Theory, Robustness, and the Brain\u200b",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927212",
 "po_email": "jmead@nsf.gov",
 "po_sign_block_name": "Jodi Mead",
 "awd_eff_date": "2021-12-01",
 "awd_exp_date": "2024-11-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2021-09-15",
 "awd_max_amd_letter_date": "2021-09-15",
 "awd_abstract_narration": "A truly comprehensive theory of machine learning has the potential of informing science and engineering in the same profound way Maxwell\u2019s equations did. It was the development of that theory by Maxwell that truly unleashed the potential of electricity, leading to radio, radars, computers, and the Internet. In an analogy, deep learning (DL) has found over the past decade many applications, so far without a comprehensive theory. An eventual theory of learning that explains why and how deep networks work and what their limitations are may thus enable the development of even more powerful learning approaches \u2013 especially if the goal of reconnecting DL to brain research bears fruit. In the long  term, the ability to develop and build better intelligent machines will be essential to any technology-based economy. After all, even in its current \u2013 still highly imperfect \u2013state, DL is impacting or about to impact just about every aspect of our society and life. The investigators also plan to complement their theoretical research with the educational goal of training a diverse population of young researchers from mathematics, computer science, statistics, electrical engineering, and computational neuroscience in the field of machine learning and of its theoretical underpinnings.\r\n\r\nThe investigators propose to join forces in a multi-pronged and collaborative assault on the profound mysteries of DL, informed by  the sum of their experience, expertise, ideas, and insight. The research goals are threefold: to develop a sound foundational/mathematical understanding of DL; in doing so to advance the foundational understanding of learning more generally; and to advance the practice of DL by addressing its above-mentioned  weaknesses. Of six foundational thrusts, the first two focus on the standard decomposition of the prediction error in approximation and sample (or estimation) error. Their goal is to extend classical results in  approximation theory and theory of learnability to DL. These two are then supported by a research project that is specific to deep learning: analysis of the dynamics of gradient descent in training a network. The  fourth theme is about robustness against adversaries and shifts, a powerful test for theories which is also important for practical deployment of learning systems. The fifth thrust is about developing the theory of control through DL, as well as exploring dynamical systems aspects of deep reinforcement learning. The final topic connects research on DL to its origins - and possibly its future: networks of neurons in the brain. The proposed research also promises to advance the foundations of learning theory. Success in this project will result in sharper mathematical techniques for machine learning and comprehensive foundations of machine learning robustness, broadly construed. It will also ultimately enable development of learning algorithms that transcend deep learning and guide the way towards creating more intelligent machines, and shed new light on our own intelligence.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Elad",
   "pi_last_name": "Hazan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elad Hazan",
   "pi_email_addr": "ehazan@cs.princeton.edu",
   "nsf_id": "000674538",
   "pi_start_date": "2021-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2fe935cb-7fff-660e-4266-499d16af5843\"> </span></p>\r\n<p><span id=\"docs-internal-guid-baac4a6b-7fff-edc0-b8e4-25be38447622\"> </span></p>\r\n<p dir=\"ltr\"><span>The research goals of this project are threefold: to develop a sound foundational/mathematical understanding of Deep Learning; in doing so to advance the foundational understanding of learning more generally; and to advance the practice of Deep Learning by addressing its&nbsp; weaknesses. This PI is involved in two of six thrusts:&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>1. Study of the dynamics of optimization</span></p>\r\n<p dir=\"ltr\"><span>2. Advancement of a theory of deep reinforcement learning: deep control and learning dynamics in games</span></p>\r\n<p dir=\"ltr\"><span>The main outcome of this project is a new theoretically-motivated neural architecture for sequence prediction called </span><a href=\"https://sites.google.com/view/gbrainprinceton/projects/spectral-transformers\"><span>spectral transformers</span></a><span>.</span></p>\r\n<p dir=\"ltr\"><span>This area of research studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models and hybrid transformer models based on learning linear dynamical systems with the </span><a href=\"https://arxiv.org/abs/1711.00946\"><span>spectral filtering algorithm</span></a><span> by the PI. This gives rise to a novel sequence prediction architecture we call spectral transformers. Spectral transformers have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems, robotics data sequences,&nbsp; long-range prediction, and language modeling tasks. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.</span></p>\r\n<p dir=\"ltr\"><span>Recent theoretical advancements in this line of work include provable length generalization guarantees, and near-linear time generation/inference.</span></p>\r\n<p><span>For publications, experiments and more materials see </span><a href=\"https://sites.google.com/view/gbrainprinceton/projects/spectral-transformers\"><span>this webpage</span></a><span>.</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/24/2025<br>\nModified by: Elad&nbsp;Hazan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\n \r\n\n\nThe research goals of this project are threefold: to develop a sound foundational/mathematical understanding of Deep Learning; in doing so to advance the foundational understanding of learning more generally; and to advance the practice of Deep Learning by addressing its weaknesses. This PI is involved in two of six thrusts:\r\n\n\n1. Study of the dynamics of optimization\r\n\n\n2. Advancement of a theory of deep reinforcement learning: deep control and learning dynamics in games\r\n\n\nThe main outcome of this project is a new theoretically-motivated neural architecture for sequence prediction called spectral transformers.\r\n\n\nThis area of research studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models and hybrid transformer models based on learning linear dynamical systems with the spectral filtering algorithm by the PI. This gives rise to a novel sequence prediction architecture we call spectral transformers. Spectral transformers have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems, robotics data sequences, long-range prediction, and language modeling tasks. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\r\n\n\nRecent theoretical advancements in this line of work include provable length generalization guarantees, and near-linear time generation/inference.\r\n\n\nFor publications, experiments and more materials see this webpage.\r\n\n\n\t\t\t\t\tLast Modified: 01/24/2025\n\n\t\t\t\t\tSubmitted by: EladHazan\n"
 }
}