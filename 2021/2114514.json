{
 "awd_id": "2114514",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SHF: Small: Towards Robust Deep Learning Computing on GPUs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2021-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 182308.0,
 "awd_amount": 198308.0,
 "awd_min_amd_letter_date": "2021-08-30",
 "awd_max_amd_letter_date": "2022-04-07",
 "awd_abstract_narration": "Graphics processing units (GPU) have become one of the most promising computing engines in many application domains such as scientific simulations and deep learning. With the massive parallel processing power provided by GPUs, most of the state-of-the-art server and edge systems employ GPUs as the core computing engines for deep-learning model training and inference. As the performance of deep learning models becomes one of the most important delimiters that determines market revenue of the model creators and the convenience of daily lives of model consumers, it is critical to enforce reliable and robust deep-learning computation. This project aims to explore the challenges and opportunities to address the reliability and privacy implications of GPU computing as a deep-learning accelerator and design lightweight protection schemes.\r\n\r\nThe technical aims of this project are divided into three thrusts. The first thrust explores and evaluates possible vulnerabilities and their impact on GPU-based deep-learning computing. The second thrust tackles the vulnerabilities at the compute-unit level by redesigning GPU building blocks, such as new scheduling algorithms and activation acceleration logic. The third thrust explores selective integrity protection mechanisms in communication channels and memory subsystems to transfer data between the CPU and GPU without imposing significant performance overhead. The proposed solutions will mitigate architectural and system vulnerabilities in GPU-based deep learning computing, which will enable the deep learning algorithm developers to focus more on performance improvement and technological advancement, and the consumers to use deep learning-based cognitive products without privacy concerns. The findings of this research will be integrated into undergraduate and graduate courses as well as various outreach activities on K-12 education, and publicly shared through open-source repositories.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hyeran",
   "pi_last_name": "Jeon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hyeran Jeon",
   "pi_email_addr": "hjeon7@ucmerced.edu",
   "nsf_id": "000695788",
   "pi_start_date": "2021-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California - Merced",
  "inst_street_address": "5200 N LAKE RD",
  "inst_street_address_2": "",
  "inst_city_name": "MERCED",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2092012039",
  "inst_zip_code": "953435001",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "CA13",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, MERCED",
  "org_prnt_uei_num": "",
  "org_uei_num": "FFM7VPAG8P92"
 },
 "perf_inst": {
  "perf_inst_name": "University of California - Merced",
  "perf_str_addr": "5200 N. Lake Road",
  "perf_city_name": "Merced",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "953435000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "CA13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 182308.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>As workloads become larger and more complex, robust computing becomes more challenging. This project explored vulnerabilities and performance bottlenecks of GPU computing. GPUs become one of the most important computing engines for emerging workloads, such as deep learning. To support the increasing memory consumption of such emerging workloads, GPU platforms have also evolved to integrate multiple GPUs within the same processor package. This project tackled the robustness of both single and multi-GPU systems for computing deep learning models and large GPU workloads as follows.</span></p>\r\n<p>&nbsp;</p>\r\n<p><span>Intellectual contributions:</span></p>\r\n<p><span>(1)&nbsp;&nbsp;&nbsp;&nbsp; While single GPU systems have been extensively studied, we observed that the unique training methodology of emerging deep learning workloads, especially transformer models, enables&nbsp;</span><span>to design</span><span>&nbsp;a&nbsp;</span><span>new model-extraction attack. Our proposed Decepticon (IISWC?23) discovered that the weight values are highly similar between pre-trained transformer models and their downstream fine-tuned models, regardless of the downstream tasks. With the observation, Decepticon shows that architectural secrets of black-box fine-tuned transformer models can be extracted, from the model architecture information to the individual weight values. With the extracted cloned models, Decepticon also demonstrates that highly accurate adversarial attacks can be designed. Decepticon also suggested several mitigation ideas such as randomizing libraries and kernels.</span></p>\r\n<p><span>(2)&nbsp;&nbsp;&nbsp;&nbsp; The project expanded the scope of research to multi-GPU systems. In our GPGPU?23 paper, we characterized multi-GPU computing performance and observed that the current system settings hinder scalable computing. In our follow-up paper, Barre Chord (ISCA?24), we observed that the address translation overhead is one of the critical reasons. The Barre Chord tackles the limited bandwidth of page table walkers by fundamentally reducing the translation loads. By mapping pages in a more systemic way across multi-chip-module GPUs, Barre Chord coalesces address translations that are mapped on the same local physical addresses across multiple GPU chiplets. Then, Barre Chord demonstrates that the coalesced pages can be translated without expensive page table walks but with simple computations.</span></p>\r\n<p><span>(3)&nbsp;&nbsp;&nbsp;&nbsp; Our findings have been published in IISWC'23, WOCC'23, GPGPU'23, NOPE'22, and ISCA'24. A book chapter was also published by Springer.</span></p>\r\n<p>&nbsp;</p>\r\n<p><span>Broader Impacts:</span></p>\r\n<p><span>(1)&nbsp;&nbsp;&nbsp;&nbsp; The project?s findings&nbsp;</span><span>provide guidance for</span><span>&nbsp;designing a more robust and scalable GPU platform by considering potential information leakage and performance bottlenecks in the interactions between emerging workloads and GPU architectures.</span></p>\r\n<p><span>(2)&nbsp;&nbsp;&nbsp;&nbsp; The project?s outcomes have been integrated into undergraduate and graduate courses, which helped motivate multiple undergraduate students to pursue post-graduate degrees in computer science.</span></p>\r\n<p><span>(3)&nbsp;&nbsp;&nbsp;&nbsp; Two PhD students have been supported by this project. Two undergraduate students with diverse backgrounds have&nbsp;</span><span>been also</span><span>&nbsp;supported through REU.&nbsp;</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/16/2025<br>\nModified by: Hyeran&nbsp;Jeon</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs workloads become larger and more complex, robust computing becomes more challenging. This project explored vulnerabilities and performance bottlenecks of GPU computing. GPUs become one of the most important computing engines for emerging workloads, such as deep learning. To support the increasing memory consumption of such emerging workloads, GPU platforms have also evolved to integrate multiple GPUs within the same processor package. This project tackled the robustness of both single and multi-GPU systems for computing deep learning models and large GPU workloads as follows.\r\n\n\n\r\n\n\nIntellectual contributions:\r\n\n\n(1) While single GPU systems have been extensively studied, we observed that the unique training methodology of emerging deep learning workloads, especially transformer models, enablesto designanew model-extraction attack. Our proposed Decepticon (IISWC?23) discovered that the weight values are highly similar between pre-trained transformer models and their downstream fine-tuned models, regardless of the downstream tasks. With the observation, Decepticon shows that architectural secrets of black-box fine-tuned transformer models can be extracted, from the model architecture information to the individual weight values. With the extracted cloned models, Decepticon also demonstrates that highly accurate adversarial attacks can be designed. Decepticon also suggested several mitigation ideas such as randomizing libraries and kernels.\r\n\n\n(2) The project expanded the scope of research to multi-GPU systems. In our GPGPU?23 paper, we characterized multi-GPU computing performance and observed that the current system settings hinder scalable computing. In our follow-up paper, Barre Chord (ISCA?24), we observed that the address translation overhead is one of the critical reasons. The Barre Chord tackles the limited bandwidth of page table walkers by fundamentally reducing the translation loads. By mapping pages in a more systemic way across multi-chip-module GPUs, Barre Chord coalesces address translations that are mapped on the same local physical addresses across multiple GPU chiplets. Then, Barre Chord demonstrates that the coalesced pages can be translated without expensive page table walks but with simple computations.\r\n\n\n(3) Our findings have been published in IISWC'23, WOCC'23, GPGPU'23, NOPE'22, and ISCA'24. A book chapter was also published by Springer.\r\n\n\n\r\n\n\nBroader Impacts:\r\n\n\n(1) The project?s findingsprovide guidance fordesigning a more robust and scalable GPU platform by considering potential information leakage and performance bottlenecks in the interactions between emerging workloads and GPU architectures.\r\n\n\n(2) The project?s outcomes have been integrated into undergraduate and graduate courses, which helped motivate multiple undergraduate students to pursue post-graduate degrees in computer science.\r\n\n\n(3) Two PhD students have been supported by this project. Two undergraduate students with diverse backgrounds havebeen alsosupported through REU.\r\n\n\n\t\t\t\t\tLast Modified: 01/16/2025\n\n\t\t\t\t\tSubmitted by: HyeranJeon\n"
 }
}