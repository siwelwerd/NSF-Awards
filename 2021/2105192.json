{
 "awd_id": "2105192",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: HCC: Modeling computer-mediated task-oriented dialogues with multi-modality information theoretic approaches",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2021-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 166937.0,
 "awd_amount": 166937.0,
 "awd_min_amd_letter_date": "2021-04-16",
 "awd_max_amd_letter_date": "2021-04-16",
 "awd_abstract_narration": "Effective real-time remote communication is challenging because unlike face-to-face conversations, information is exchanged indirectly through computer screens and speakers.  In these situations, rich non-verbal information such as body poses, hand gestures, and facial expressions is often blocked or neglected. In order to improve the quality of existing remote communication systems, this project will develop an in-depth quantitative understanding of the kinds of information non-verbal aspects of communication might add to the conversation. The research team will address this issue by developing novel measures of non-verbal communication content based on state-of-the-art machine learning and computer vision techniques. The team will also develop software toolkits that can support both behavioral scientists who study conversation and designers of future computer-mediated communication systems. \r\n\r\nThe research team proposes a series of studies that use information theoretic models to distill information from the non-verbal communication channels, including facial expressions, upper-body gestures, and whole-body poses, and synthesize them into the ultimate goal of predicting the success of computer-mediated collaborative tasks.  As part of the work the team will develop several novel analysis methods.  These include discretizing and symbolizing non-verbal information through joint representation learning and aggregation, using neural sequential models to estimate the amount of information in parallel discretized series, and using the temporal-spectral patterns in all communication channels to predict the task success. A multimodal information incorporated corpus of task-oriented dialogues will also be collected as part of the work.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yang",
   "pi_last_name": "Xu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yang Xu",
   "pi_email_addr": "yxu4@sdsu.edu",
   "nsf_id": "000791008",
   "pi_start_date": "2021-04-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "San Diego State University Foundation",
  "inst_street_address": "5250 CAMPANILE DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAN DIEGO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6195945731",
  "inst_zip_code": "921821901",
  "inst_country_name": "United States",
  "cong_dist_code": "51",
  "st_cong_dist_code": "CA51",
  "org_lgl_bus_name": "SAN DIEGO STATE UNIVERSITY FOUNDATION",
  "org_prnt_uei_num": "H59JKGFZKHL7",
  "org_uei_num": "H59JKGFZKHL7"
 },
 "perf_inst": {
  "perf_inst_name": "San Diego State University",
  "perf_str_addr": "5500 Campanile Drive",
  "perf_city_name": "San  Diego",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "921827720",
  "perf_ctry_code": "US",
  "perf_cong_dist": "51",
  "perf_st_cong_dist": "CA51",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "102Z",
   "pgm_ref_txt": "COVID-Disproportionate Impcts Inst-Indiv"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 166937.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main outcomes of the project are as follows.</p>\n<p>1)&nbsp;&nbsp;&nbsp;&nbsp; The PI&rsquo;s team has developed an algorithm for encoding gestures and eye-gaze signals that comes with speech and conversation, which has been considered as a difficult task in the field of computational psycholinguistics. The algorithm overcomes the difficulty in encoding non-verbal signals due to the irregularity, sparsity, and noisy-distribution nature of these signals, and can run effectively on video data containing a sole speaker with a fixed camera perspective. The mathematics behind the algorithm is easy to understand. The resulting encoded gesture/eye-gaze tags can directly capture the positions and movements of hands/eyes in space, and thus are easy to interpret. The algorithm runs at decent time and memory efficiency and can meet most analysis needs without using advanced acceleration techniques such as GPUs.</p>\n<p>2)&nbsp;&nbsp;&nbsp;&nbsp; Have developed a complete training (in the sense of machine learning) procedure that is able to extract the &ldquo;meanings&rdquo; of non-verbal signals in language. The procedure is based on the common next-word-prediction paradigm in language modeling task, and incorporates non-verbal tokens as additional information. The non-verbal tokens are obtained using the encoding algorithm described in bullet 1), and they are assigned with vector representations in embedding space. Their representations are fused with those from words, and fed as input to the neural network models. Both classical long short-term memory (LSTM) and modern transformer architectures are implemented. The training procedure has GPU acceleration enabled, and can efficiently learn the representations for non-verbal tokens. Lastly, the learned representations can serve as the basis for interpret the meanings of certain gesture/eye-gaze signals. For example, in order to interpret the meaning of a gesture with two hands clasping together, we can first tokenize it and then obtain its representation vector. Next, we look for the most nearby word vectors in the semantic space, and use the aggregated neighboring word vectors to interpret the meaning of this gesture.</p>\n<p>3)&nbsp;&nbsp;&nbsp;&nbsp; Provided new findings that enrich current theories of human communication. The PI&rsquo;s team has published three research papers surrounding the topic of interpreting the communicative functions of non-verbal signals in natural language, such as hand gestures and eye-gaze. The first paper (COLING 2022) concludes that gestures are used rationally in the same way words are used in human language, by showing that the information content encoded in gestures conform to the principle of &ldquo;entropy rate constancy&rdquo; (ERC), which is a golden-standard to test the efficiency of a communication system. The second paper (ACL 2023) demonstrates that using gestures can indeed facilitate verbal communication, by showing that language models have better performance in predicting next word when the inputs contain properly represented gesture tokens as opposed to not. The third paper (ACL 2024, <a href=\"https://aclanthology.org/2024.findings-acl.210/\">https://aclanthology.org/2024.findings-acl.210/</a>) extends the investigation from gestures to eye-gaze in dialogue and the main finding is that eye-gaze also conforms to ERC principle. Altogether, the findings of the three works provide new insights about human communication, and specifically highlights the innate human inclination of maintaining efficiency in communication, no matter forms, verbal or non-verbal.</p>\n<p>4)&nbsp;&nbsp;&nbsp;&nbsp; Provided datasets and codebase that can be re-used in future work and shared to the research community. The datasets contain sources of video data analyzed in the published works listed above, containing speakers of two languages, English and Chinese. The codebase covers the complete procedure of data collection, pre-processing, and preliminary analysis, and is well documented.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/06/2024<br>\nModified by: Yang&nbsp;Xu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe main outcomes of the project are as follows.\n\n\n1) The PIs team has developed an algorithm for encoding gestures and eye-gaze signals that comes with speech and conversation, which has been considered as a difficult task in the field of computational psycholinguistics. The algorithm overcomes the difficulty in encoding non-verbal signals due to the irregularity, sparsity, and noisy-distribution nature of these signals, and can run effectively on video data containing a sole speaker with a fixed camera perspective. The mathematics behind the algorithm is easy to understand. The resulting encoded gesture/eye-gaze tags can directly capture the positions and movements of hands/eyes in space, and thus are easy to interpret. The algorithm runs at decent time and memory efficiency and can meet most analysis needs without using advanced acceleration techniques such as GPUs.\n\n\n2) Have developed a complete training (in the sense of machine learning) procedure that is able to extract the meanings of non-verbal signals in language. The procedure is based on the common next-word-prediction paradigm in language modeling task, and incorporates non-verbal tokens as additional information. The non-verbal tokens are obtained using the encoding algorithm described in bullet 1), and they are assigned with vector representations in embedding space. Their representations are fused with those from words, and fed as input to the neural network models. Both classical long short-term memory (LSTM) and modern transformer architectures are implemented. The training procedure has GPU acceleration enabled, and can efficiently learn the representations for non-verbal tokens. Lastly, the learned representations can serve as the basis for interpret the meanings of certain gesture/eye-gaze signals. For example, in order to interpret the meaning of a gesture with two hands clasping together, we can first tokenize it and then obtain its representation vector. Next, we look for the most nearby word vectors in the semantic space, and use the aggregated neighboring word vectors to interpret the meaning of this gesture.\n\n\n3) Provided new findings that enrich current theories of human communication. The PIs team has published three research papers surrounding the topic of interpreting the communicative functions of non-verbal signals in natural language, such as hand gestures and eye-gaze. The first paper (COLING 2022) concludes that gestures are used rationally in the same way words are used in human language, by showing that the information content encoded in gestures conform to the principle of entropy rate constancy (ERC), which is a golden-standard to test the efficiency of a communication system. The second paper (ACL 2023) demonstrates that using gestures can indeed facilitate verbal communication, by showing that language models have better performance in predicting next word when the inputs contain properly represented gesture tokens as opposed to not. The third paper (ACL 2024, https://aclanthology.org/2024.findings-acl.210/) extends the investigation from gestures to eye-gaze in dialogue and the main finding is that eye-gaze also conforms to ERC principle. Altogether, the findings of the three works provide new insights about human communication, and specifically highlights the innate human inclination of maintaining efficiency in communication, no matter forms, verbal or non-verbal.\n\n\n4) Provided datasets and codebase that can be re-used in future work and shared to the research community. The datasets contain sources of video data analyzed in the published works listed above, containing speakers of two languages, English and Chinese. The codebase covers the complete procedure of data collection, pre-processing, and preliminary analysis, and is well documented.\n\n\n\t\t\t\t\tLast Modified: 09/06/2024\n\n\t\t\t\t\tSubmitted by: YangXu\n"
 }
}