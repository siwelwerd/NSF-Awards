{
 "awd_id": "2007714",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Compression for Learning over networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 523925.0,
 "awd_amount": 523925.0,
 "awd_min_amd_letter_date": "2020-07-30",
 "awd_max_amd_letter_date": "2020-07-30",
 "awd_abstract_narration": "Data compression is a core component of all communication protocols, as it can translate to bandwidth savings, energy efficiency and low delay operations. In the traditional setup, an information source compresses its messages so that they can be communicated efficiently with the goal of ensuring accurate reconstruction at the destination. This project seeks to design compression schemes that are specifically tailored to Machine Learning applications: If the transmitted messages support a given learning task (e.g., classification or learning), the desired compression schemes should provide better support for the learning task instead of focusing on reconstruction accuracy. This approach to compression could potentially yield significant benefits in terms of communication efficiency, while simultaneously promoting the successful implementation of Machine Learning algorithms. By improving communication efficiency, such schemes are expected to  contribute to the successful implementation  of distributed machine learning algorithms over networks.\r\n\r\nTraditionally, compression schemes are evaluated using rate-distortion trade-offs; this project is interested in rate-accuracy trade-offs, where accuracy captures the effect that quantization may have on a specific machine learning task. There is particular interest in information-theoretic lower bounds and trade-offs, and in explicit compression for the following two questions: (1) How to compress for model training, when we need to use distributed communication constrained nodes to learn a model, fast and efficiently; and (2) How to compress for communication during inference. The project will derive bounds and algorithms for distributed compression of features coming from composite distributions that will be used for a machine learning task, such as classification.  This work will advance the state of the art, and  build new connections between the areas of data compression and distributed machine learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christina",
   "pi_last_name": "Fragouli",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christina Fragouli",
   "pi_email_addr": "christina.fragouli@ucla.edu",
   "nsf_id": "000637237",
   "pi_start_date": "2020-07-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Suhas",
   "pi_last_name": "Diggavi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suhas Diggavi",
   "pi_email_addr": "suhas@ee.ucla.edu",
   "nsf_id": "000550026",
   "pi_start_date": "2020-07-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 523925.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Traditionally, compression schemes are evaluated using rate-distortion&nbsp;</span><br /><span>trade-offs; this project is interested in rate-accuracy trade-offs,&nbsp;</span><br /><span>where accuracy captures the effect that quantization may have on a&nbsp;</span><br /><span>specific machine learning task. There is particular interest in&nbsp;</span><br /><span>information-theoretic lower bounds and trade-offs, and in explicit&nbsp;</span><br /><span>compression for the following two questions: (1) How to compress for&nbsp;</span><br /><span>model training, when we need to use distributed communication&nbsp;</span><br /><span>constrained nodes to learn a model, fast and efficiently; and (2) How to&nbsp;</span><br /><span>compress for communication during inference. The project will derive&nbsp;</span><br /><span>bounds and algorithms for distributed compression of features coming&nbsp;</span><br /><span>from composite distributions that will be used for a machine learning&nbsp;</span><br /><span>task, such as classification. The goals of this project is to create new&nbsp;</span><br /><span>foundational theory and algorithms which will advance the state of the&nbsp;</span><br /><span>art, and build new connections between the areas of data compression and&nbsp;</span><br /><span>distributed machine learning. If successful, the project will not only&nbsp;</span><br /><span>build new theory, but also could have impact in practical applications&nbsp;</span><br /><span>of learning over communication constrained networks. The project was&nbsp;</span><br /><span>arranged in three thrusts, with Thrust 1 focused on compression for&nbsp;</span><br /><span>distributed model training, Thrust 2 exploring compression for inference&nbsp;</span><br /><span>time&nbsp; and Thrust 3 that combined both training and inference time&nbsp;</span><br /><span>compression. We believe that we had significant results in the project&nbsp;</span><br /><span>towards these goals.</span><br /><br /><span>One measure of academic impact of the project is the number of&nbsp;</span><br /><span>publications during the duration of the project; 45 publications&nbsp;</span><br /><span>resulted from this project (that acknowledged support of this grant) in&nbsp;</span><br /><span>top tier conference and journals.&nbsp; Papers acknowledging support from&nbsp;</span><br /><span>this project have garnered nearly 500 citations according to Google&nbsp;</span><br /><span>Scholar. It also supported (in part) the PhD work of 7 graduate&nbsp;</span><br /><span>students, one postdoctoral researchers and several undergraduate (REU)&nbsp;</span><br /><span>students. The four graduated PhD students have gone on to careers in&nbsp;</span><br /><span>industrial research labs. Another measure of impact is the recognition&nbsp;</span><br /><span>of the papers and PIs through this project. For example, one of papers&nbsp;</span><br /><span>supported (in part) by this project was recognized through&nbsp; the&nbsp;</span><br /><span>prestigious 2021 ACM Conference on Computer and Communications Security&nbsp;</span><br /><span>(CCS) best paper award. One of the students supported by this award&nbsp;</span><br /><span>received the UCLA PhD dissertation award. It also resulted in faculty&nbsp;</span><br /><span>research awards on related topics for the PIs from Amazon and Meta. The&nbsp;</span><br /><span>research in this project also fostered a collaboration with industrial&nbsp;</span><br /><span>research labs (Google) through joint publications. The work was also&nbsp;</span><br /><span>disseminated through several invited lectures in various industrial and&nbsp;</span><br /><span>academics settings by the PIs, and through organization of workshops and&nbsp;</span><br /><span>conferences.</span><br /><br /><span>This project developed several fundamental ideas compression for&nbsp;</span><br /><span>distributed learning systems, which were discovered and published during&nbsp;</span><br /><span>the course of this project. The details are in the publications and were&nbsp;</span><br /><span>also summarized in annual&nbsp;</span><span class=\"il\">reports</span><span>&nbsp;of the project. Among the several&nbsp;</span><br /><span>works (including over 25 journal and journal-equivalent conference&nbsp;</span><br /><span>papers) we would like to highlight the following: (i) Gradient&nbsp;</span><br /><span>compression for decentralized networks: we proposed new algorithms that&nbsp;</span><br /><span>used error-compensation, event-triggering and sparsification to enable&nbsp;</span><br /><span>schemes whose convergence was equivalent to vanilla stochastic&nbsp;</span><br /><span>optimization with orders of magnitude reduction in communications; this&nbsp;</span><br /><span>work was published in several communities including Control (Trans. Aut.&nbsp;</span><br /><span>Control, Automatica, CDC), information theory (JSAIT, ISIT) etc. (ii)&nbsp;</span><br /><span>Compression for contextual multi-arm bandits: In this line of work we&nbsp;</span><br /><span>initated new compression methods for multi-arm bandits and also showed&nbsp;</span><br /><span>fundamental connections between contextual MAB and linear bandit&nbsp;</span><br /><span>problems. These works were published both in learning venues (NeurIPS,&nbsp;</span><br /><span>AISTATS, COLT) as well as information theory venues (JSAIT, ISIT)&nbsp; (iii)&nbsp;</span><br /><span>Compression for interactive distributed inference: we developed new&nbsp;</span><br /><span>schemes and information theoretic bounds by introducing interaction in&nbsp;</span><br /><span>distributed inference, demonstrating a new way to break dependencies in&nbsp;</span><br /><span>observations (iv) Information theory and algorithms for personalized&nbsp;</span><br /><span>learning: we developed a new statistical framework for personalized&nbsp;</span><br /><span>learning and through that developed both information theoretic bounds as&nbsp;</span><br /><span>well as algorithms. These were published in learning venues (e.g.,&nbsp;</span><br /><span>NeurIPS, ICLR) as well as information theory venues (ISIT). These are&nbsp;</span><br /><span>only a few of the many results from the project, which also included a&nbsp;</span><br /><span>review paper on compression for distributed learning and co-editing a&nbsp;</span><br /><span>special issue on the topic in IEEE Journal of Selected Areas in&nbsp;</span><br /><span>Communications. The project also involved out reach and training of&nbsp;</span><br /><span>undergraduate students, including several female students.</span></p>\r\n<p><span><br /></span></p><br>\n<p>\n Last Modified: 01/19/2025<br>\nModified by: Christina&nbsp;Fragouli</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTraditionally, compression schemes are evaluated using rate-distortion\ntrade-offs; this project is interested in rate-accuracy trade-offs,\nwhere accuracy captures the effect that quantization may have on a\nspecific machine learning task. There is particular interest in\ninformation-theoretic lower bounds and trade-offs, and in explicit\ncompression for the following two questions: (1) How to compress for\nmodel training, when we need to use distributed communication\nconstrained nodes to learn a model, fast and efficiently; and (2) How to\ncompress for communication during inference. The project will derive\nbounds and algorithms for distributed compression of features coming\nfrom composite distributions that will be used for a machine learning\ntask, such as classification. The goals of this project is to create new\nfoundational theory and algorithms which will advance the state of the\nart, and build new connections between the areas of data compression and\ndistributed machine learning. If successful, the project will not only\nbuild new theory, but also could have impact in practical applications\nof learning over communication constrained networks. The project was\narranged in three thrusts, with Thrust 1 focused on compression for\ndistributed model training, Thrust 2 exploring compression for inference\ntime and Thrust 3 that combined both training and inference time\ncompression. We believe that we had significant results in the project\ntowards these goals.\n\nOne measure of academic impact of the project is the number of\npublications during the duration of the project; 45 publications\nresulted from this project (that acknowledged support of this grant) in\ntop tier conference and journals. Papers acknowledging support from\nthis project have garnered nearly 500 citations according to Google\nScholar. It also supported (in part) the PhD work of 7 graduate\nstudents, one postdoctoral researchers and several undergraduate (REU)\nstudents. The four graduated PhD students have gone on to careers in\nindustrial research labs. Another measure of impact is the recognition\nof the papers and PIs through this project. For example, one of papers\nsupported (in part) by this project was recognized through the\nprestigious 2021 ACM Conference on Computer and Communications Security\n(CCS) best paper award. One of the students supported by this award\nreceived the UCLA PhD dissertation award. It also resulted in faculty\nresearch awards on related topics for the PIs from Amazon and Meta. The\nresearch in this project also fostered a collaboration with industrial\nresearch labs (Google) through joint publications. The work was also\ndisseminated through several invited lectures in various industrial and\nacademics settings by the PIs, and through organization of workshops and\nconferences.\n\nThis project developed several fundamental ideas compression for\ndistributed learning systems, which were discovered and published during\nthe course of this project. The details are in the publications and were\nalso summarized in annualreportsof the project. Among the several\nworks (including over 25 journal and journal-equivalent conference\npapers) we would like to highlight the following: (i) Gradient\ncompression for decentralized networks: we proposed new algorithms that\nused error-compensation, event-triggering and sparsification to enable\nschemes whose convergence was equivalent to vanilla stochastic\noptimization with orders of magnitude reduction in communications; this\nwork was published in several communities including Control (Trans. Aut.\nControl, Automatica, CDC), information theory (JSAIT, ISIT) etc. (ii)\nCompression for contextual multi-arm bandits: In this line of work we\ninitated new compression methods for multi-arm bandits and also showed\nfundamental connections between contextual MAB and linear bandit\nproblems. These works were published both in learning venues (NeurIPS,\nAISTATS, COLT) as well as information theory venues (JSAIT, ISIT) (iii)\nCompression for interactive distributed inference: we developed new\nschemes and information theoretic bounds by introducing interaction in\ndistributed inference, demonstrating a new way to break dependencies in\nobservations (iv) Information theory and algorithms for personalized\nlearning: we developed a new statistical framework for personalized\nlearning and through that developed both information theoretic bounds as\nwell as algorithms. These were published in learning venues (e.g.,\nNeurIPS, ICLR) as well as information theory venues (ISIT). These are\nonly a few of the many results from the project, which also included a\nreview paper on compression for distributed learning and co-editing a\nspecial issue on the topic in IEEE Journal of Selected Areas in\nCommunications. The project also involved out reach and training of\nundergraduate students, including several female students.\r\n\n\n\n\t\t\t\t\tLast Modified: 01/19/2025\n\n\t\t\t\t\tSubmitted by: ChristinaFragouli\n"
 }
}