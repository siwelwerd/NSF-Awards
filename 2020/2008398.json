{
 "awd_id": "2008398",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF:Small:Software and Hardware Optimizations for Learning over Graphs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2020-08-12",
 "awd_max_amd_letter_date": "2020-08-12",
 "awd_abstract_narration": "Many emerging high-performance applications from various domains of national importance (e.g., astrophysics, computational chemistry, drug discovery, and nuclear physics)  employ large graph structures (with millions of nodes and edges connecting them) to represent their data, and perform different types of  analytics on them, with the goal of understanding and extracting useful information from this massive data.  While this graph-based analytics is fast becoming a fundamental piece of high-performance computing, existing analytics methods on graphs do not scale with increased hardware resources, mostly employ conventional machine learning/graph analysis strategies,  and do not take full advantage of emerging heterogeneous compute and storage elements. As a result, the time-to-insight from large-scale graph-based data increases significantly, thereby slowing down scientific discoveries. Motivated by this observation, this NSF-funded project explores, from a holistic viewpoint, Graph Neural Networks (GNNs), a type of Neural Network which directly operates on graph structures, as a main tool to optimize various high-performance applications that benefit from machine learning and data analytics. This project has the ultimate goal of making the transitioning from existing machine learning mechanisms to GNNs smooth and effective in a variety of application domains. By facilitating more efficient and cost-effective use of hardware resources that are provided by custom clusters, supercomputers and cloud systems, this project is also expected to reduce the barrier to entry to the GNN world  for a broad population of researchers, practitioners, and machine learning companies. The educational and outreach components of this research include 1) undergraduate student involvement via vertically integrated research projects; 2) a new graduate course on GNNs; 3) participation of Science-U program (a summer science camp for K-12) at Penn State, and 4) summer workshops for high school girls and high school teachers.\r\n\r\nMore specifically, this project: 1) explores the theoretical foundations of GNNs with the goal of identifying the roots of convergence and scalability problems, which are critical to address when employing them in high-performance applications; 2) investigates an architecture-agnostic programming  language support for GNN computations, focusing in particular on developer productivity,  language expressiveness, and ease of extensibility (to ensure that it inter-operates with existing programming paradigms, models ,and tools); 3) explores compiler support for automatically optimizing and mapping GNN applications onto emerging hardware platforms (including multicore CPUs, GPUs, and FPGAs as well as their ensembles); 4) develops custom architecture support for GNN computations, with the goal of exceeding the performance of current programmable hardware  options; 5) carries out an end-to-end experimental evaluation of  GNN-based applications to identify the aspects that require further attention; and finally 6) develops a GNN-based benchmark suite that can be used on a wide variety of hardware platforms. These six components collectively form a multi-layer ecosystem tuned to understand and optimize high-performance, large-scale applications that can benefit from graph-driven learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mehrdad",
   "pi_last_name": "Mahdavi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mehrdad Mahdavi",
   "pi_email_addr": "mzm616@psu.edu",
   "nsf_id": "000789106",
   "pi_start_date": "2020-08-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mahmut",
   "pi_last_name": "Kandemir",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Mahmut T Kandemir",
   "pi_email_addr": "mtk2@psu.edu",
   "nsf_id": "000163936",
   "pi_start_date": "2020-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "The Pennsylvania State University",
  "perf_str_addr": "W365 Westgate Bldg",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"paragraph\"><span class=\"normaltextrun\"><span>The aim of this<span>&nbsp;</span></span></span><span class=\"normaltextrun\"><span>SHF small project, titled, &ldquo;Software and Hardware Optimizations for Learning over Graphs&rdquo;</span></span><span class=\"normaltextrun\"><span><span>&nbsp;</span>was to address critical challenges in large-scale machine learning over graph data with a focus on jointly addressing hardware and software challenges, particularly for domains including computational chemistry, drug discovery, social network mining, and smart transportation. These fields often rely on massive graph-structured data to gain insights and make predictions. Graph Neural Networks (GNNs), especially Graph Convolutional Networks (GCNs), have emerged as powerful tools for learning from graph data, but scaling them efficiently has been a significant hurdle. Moreover, traditional learning algorithms for graph data struggle to fully exploit the potential of emerging hardware resources and technologies, leading to slower insights and delayed scientific progress. </span></span></p>\r\n<p class=\"paragraph\"><span class=\"normaltextrun\">This project&rsquo;s overarching goal was to create scalable, efficient optimization algorithms for training GNNs by integrating theory, computer architecture, and compiler design. The key objectives include: i) theoretical insights for GNN scalability: We delved into the theoretical foundations of GNNs to understand the reasons behind convergence, generalization, and scalability issues, ensuring their successful application in large-scale, high-performance contexts and, inspired by these insights, developed efficient hardware-aware sampling methods, scalable and distributed algorithms for training GNN models, simple model architectures for learning from evolving graphs; ii) compiler support for hardware optimization: We explored how compilers can optimize and map GNN computations onto a wide range of emerging hardware platforms, including multicore CPUs, GPUs, and FPGAs, for maximum efficiency; iii) end-to-end evaluation: A comprehensive experimental evaluation carried out to identify areas where GNN applications can be further optimized and refined in applications such as drug discovery; and iv) GNN benchmark suite: We&nbsp; developed a robust suite of benchmarks for GNN-based applications, enabling performance testing across a wide variety of hardware platforms in different application domains.</span><span class=\"eop\">&nbsp;</span></p>\r\n<p class=\"paragraph\"><span>&nbsp;</span><span class=\"normaltextrun\">By focusing on these areas, the project aimed to create a comprehensive ecosystem for optimizing large-scale, high-performance applications that use graph-driven learning. This work will not only advance the theoretical and computational understanding of GNNs but also pave the way for new hardware, software, and compiler tools that can accelerate the time-to-insight for critical applications in science and technology.</span><span class=\"eop\">&nbsp;</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/24/2025<br>\nModified by: Mehrdad&nbsp;Mahdavi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe aim of thisSHF small project, titled, Software and Hardware Optimizations for Learning over Graphswas to address critical challenges in large-scale machine learning over graph data with a focus on jointly addressing hardware and software challenges, particularly for domains including computational chemistry, drug discovery, social network mining, and smart transportation. These fields often rely on massive graph-structured data to gain insights and make predictions. Graph Neural Networks (GNNs), especially Graph Convolutional Networks (GCNs), have emerged as powerful tools for learning from graph data, but scaling them efficiently has been a significant hurdle. Moreover, traditional learning algorithms for graph data struggle to fully exploit the potential of emerging hardware resources and technologies, leading to slower insights and delayed scientific progress. \r\n\n\nThis projects overarching goal was to create scalable, efficient optimization algorithms for training GNNs by integrating theory, computer architecture, and compiler design. The key objectives include: i) theoretical insights for GNN scalability: We delved into the theoretical foundations of GNNs to understand the reasons behind convergence, generalization, and scalability issues, ensuring their successful application in large-scale, high-performance contexts and, inspired by these insights, developed efficient hardware-aware sampling methods, scalable and distributed algorithms for training GNN models, simple model architectures for learning from evolving graphs; ii) compiler support for hardware optimization: We explored how compilers can optimize and map GNN computations onto a wide range of emerging hardware platforms, including multicore CPUs, GPUs, and FPGAs, for maximum efficiency; iii) end-to-end evaluation: A comprehensive experimental evaluation carried out to identify areas where GNN applications can be further optimized and refined in applications such as drug discovery; and iv) GNN benchmark suite: We developed a robust suite of benchmarks for GNN-based applications, enabling performance testing across a wide variety of hardware platforms in different application domains.\r\n\n\nBy focusing on these areas, the project aimed to create a comprehensive ecosystem for optimizing large-scale, high-performance applications that use graph-driven learning. This work will not only advance the theoretical and computational understanding of GNNs but also pave the way for new hardware, software, and compiler tools that can accelerate the time-to-insight for critical applications in science and technology.\r\n\n\n\t\t\t\t\tLast Modified: 02/24/2025\n\n\t\t\t\t\tSubmitted by: MehrdadMahdavi\n"
 }
}