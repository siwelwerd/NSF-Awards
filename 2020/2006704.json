{
 "awd_id": "2006704",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FET: Small: Smart Probabilistic Computation with Limited Resources",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2020-06-15",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 492087.0,
 "awd_amount": 492087.0,
 "awd_min_amd_letter_date": "2020-06-02",
 "awd_max_amd_letter_date": "2020-06-02",
 "awd_abstract_narration": "The overall goal of this project is to understand how to build smarter, more compact, and more environmentally friendly computing devices that can cope efficiently with uncertainty. Many current computer-based machines, for example, mobile phones and advanced biomedical devices, depend on remote data centers (the so-called cloud) to provide brain-like computational services that employ artificial intelligence and are probabilistic in nature. These centers consume enormous amounts of high-cost resources such as computer hardware, computing time, and electrical energy.  Embedding their functions directly in low-cost, reliable and environmentally friendly ways in end-user devices like smartphones is a key research challenge with broad implications. One way of doing this is the stochastic computing (SC) approach explored in this project. The SC circuits are inherently probabilistic and are orders-of-magnitude smaller and more error-tolerant than conventional circuits.  Thus, they provide novel ways to create tiny, low-cost and reliable systems. Besides its potential for reducing the cost of, and need for, large systems like data centers, SC has recently been shown suitable for extremely resource-restricted biomedical applications such as retinal implants to restore vision to the blind.  The project will also enable graduate, undergraduate and underrepresented students to be trained in computing techniques at the forefront of modern computing technologies, thus contributing to the future workforce.\r\n\r\nTraditional computer science is mostly devoted to non-probabilistic (deterministic) methods that, although far easier to understand than probabilistic, are less well-suited to the applications of interest cited above. Stochastic computing is one of the most promising probabilistic techniques available. It can be concisely defined as computing with probabilities encoded in randomized streams of 0\u2019s and 1\u2019s. It is readily implementable with standard electronic technology; for example, a single logic gate can perform multiplication, even when the input data is very noisy. The project will investigate the theoretical underpinnings of SC and its application to severely resource-restricted systems. Among the questions to be addressed are: What are the computational fundamental limits on SC\u2019s accuracy? How can SC\u2019s unique progressive precision and randomness properties be exploited?  How can system architects efficiently balance accuracy, performance, energy needs, and overall system cost? Prototype systems will be designed and evaluated via mathematical analysis, simulation and emulation using field programmable gate arrays. These include designs incorporating neural networks, as well as smart cameras, medical implants, and portable automatic speech-recognition systems. The research is also expected to provide useful insights into other probabilistic technologies such as quantum computing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Hayes",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "John P Hayes",
   "pi_email_addr": "jhayes@eecs.umich.edu",
   "nsf_id": "000311318",
   "pi_start_date": "2020-06-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan",
  "perf_str_addr": "2260 Hayward Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "089Y00",
   "pgm_ele_name": "FET-Fndtns of Emerging Tech"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 492087.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">Advanced computing technologies like smartphones and neural networks (NNs) &nbsp;have the potential to transform the world in revolutionary ways. However, they often consume excessive&nbsp; amounts of scarce &nbsp;resources like chip space and electrical energy. The focus of this NSF project is an unconventional computing technique called stochastic computing (SC) which greatly reduces&nbsp; resource needs &nbsp;by replacing conventional binary numbers with random bit-streams called stochastic numbers (SNs). The key advantage of SC is its ability to perform SN operations using tiny low-resource circuits. For example, the number 0.5 can be represented by a randomly chosen 8-bit SN such as 01010101 or 1001110 in which the presence of a 1 has probability 0.50. The product 0.50 x 0.50 = 0.25 can then be computed as AND(01010101, 1001110) = 00010100 = 0.25, implying that a single AND gate can serve as an multiplier. If, however,&nbsp; two SNs have&nbsp; correlated bit patterns, their AND-combination only approximates their exact numerical product. SC&rsquo;s drawbacks include complex and poorly understood trade-offs among such parameters as approximation accuracy, correlation impact, energy usage, and hardware implementation costs. This project shed new light on some of these trade-offs. The project supported the thesis research of two PhD students&nbsp; and an undergraduate student at the University of Michigan.&nbsp;</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">The project&rsquo;s overall aims are to strengthen the theoretical foundations of SC, and to explore SC&rsquo;s potential for building high-quality low-cost smart systems. Because of an insufficient &nbsp;theoretical understanding of SC, designers have long relied on ad-hoc design methods followed by simulation to analyze &nbsp;SC systems. However, simulation often demands huge computational resources and can produce uncertain, misleading, or inexplicable results. To address this problem, we developed a comprehensive new error analysis methodology for SC built around Bayesian statistics which account for application-dependencies in SC. For example, we observed that an SC circuit&rsquo;s input values, e.g., the distribution of pixel intensities in the case of an image classifier, have a significant impact on accuracy measurements. Our methodology &nbsp;introduces several useful concepts new to SC, and points to ways to improve or even replace simulation-based performance estimates. We significantly expanded our understanding of several other key aspects of SC theory, including correlation management, and error tolerance. We discovered new uses for correlation for the efficient implementation of arithmetic operations, stochastic number generation, image filtering,&nbsp; and&nbsp; multi-layer SC circuits. For example, we found that the two most common types of SN generators, despite their functional similarity, differ significantly in their correlation behavior. This discovery enabled us to define a&nbsp; new family of&nbsp; stochastic adders called parallel accumulative adders which support very flexible accuracy-energy trade-offs and are well-suited to NN design, It also led to a highly effective method for large-scale stochastic addition we call CeMux (correlation enhanced multiplexer). We believe&nbsp; that CeMux is the best method developed so far in terms of accuracy and accuracy-area trade-offs&nbsp; for building the large weighted-adder networks central to many NN types.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">The project explored some useful applications of our SC ideas in the field of biomedical equipment &nbsp;Electrocardiograms ( ECG) &nbsp;are used to monitor a&nbsp; patient&rsquo;s cardiovascular health and&nbsp; detect conditions such as arrhythmia. Noise is removed from an ECG signal by means of filters that place large demands on an ECG monitor&rsquo;s limited computational resources. In a case study, we demonstrated that CeMux is a very promising way for SC to improve&nbsp; &nbsp;ECG filtering. We also considered the application of SC-based filtering methods to hearing aid design.&nbsp; In particular, we demonstrated that SC is well-suited to designing low-cost filterbanks, an important part of a hearing aid. We showed that an SC filterbank can &nbsp;achieves the same accuracy and latency as a conventional non-SC design, but with an exceptionally large 70% reduction in chip area. The power consumption of our proposed SC filterbank is 38-96% that of the conventional &nbsp;design. The project also explored the benefits and costs of applying &nbsp;SC to the design and analysis of compact, resource-limited&nbsp; NNs of the LSTM (long short term memory) type. We considered a design space spanned by fully binary (non-stochastic), fully stochastic, and several hybrid (mixed) LSTM architectures. &nbsp;Using standard classification benchmarks, we found &nbsp;that area and power can be reduced up to 47% and 86% respectively with little or no impact on classification accuracy. We explored the introduction of &nbsp;wavelet transforms into LSTMs to improve their performance. We&nbsp; devised a regularized training procedure that&nbsp; reduces the computation time of SC-based NNs by a factor of four without sacrificing accuracy or hardware costs. These findings suggest that SC can be successfully applied to more powerful convolutional NNs than previously thought</p><br>\n<p>\n Last Modified: 07/09/2024<br>\nModified by: John&nbsp;P&nbsp;Hayes</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAdvanced computing technologies like smartphones and neural networks (NNs) have the potential to transform the world in revolutionary ways. However, they often consume excessive amounts of scarce resources like chip space and electrical energy. The focus of this NSF project is an unconventional computing technique called stochastic computing (SC) which greatly reduces resource needs by replacing conventional binary numbers with random bit-streams called stochastic numbers (SNs). The key advantage of SC is its ability to perform SN operations using tiny low-resource circuits. For example, the number 0.5 can be represented by a randomly chosen 8-bit SN such as 01010101 or 1001110 in which the presence of a 1 has probability 0.50. The product 0.50 x 0.50 = 0.25 can then be computed as AND(01010101, 1001110) = 00010100 = 0.25, implying that a single AND gate can serve as an multiplier. If, however, two SNs have correlated bit patterns, their AND-combination only approximates their exact numerical product. SCs drawbacks include complex and poorly understood trade-offs among such parameters as approximation accuracy, correlation impact, energy usage, and hardware implementation costs. This project shed new light on some of these trade-offs. The project supported the thesis research of two PhD students and an undergraduate student at the University of Michigan.\n\n\n\n\n\nThe projects overall aims are to strengthen the theoretical foundations of SC, and to explore SCs potential for building high-quality low-cost smart systems. Because of an insufficient theoretical understanding of SC, designers have long relied on ad-hoc design methods followed by simulation to analyze SC systems. However, simulation often demands huge computational resources and can produce uncertain, misleading, or inexplicable results. To address this problem, we developed a comprehensive new error analysis methodology for SC built around Bayesian statistics which account for application-dependencies in SC. For example, we observed that an SC circuits input values, e.g., the distribution of pixel intensities in the case of an image classifier, have a significant impact on accuracy measurements. Our methodology introduces several useful concepts new to SC, and points to ways to improve or even replace simulation-based performance estimates. We significantly expanded our understanding of several other key aspects of SC theory, including correlation management, and error tolerance. We discovered new uses for correlation for the efficient implementation of arithmetic operations, stochastic number generation, image filtering, and multi-layer SC circuits. For example, we found that the two most common types of SN generators, despite their functional similarity, differ significantly in their correlation behavior. This discovery enabled us to define a new family of stochastic adders called parallel accumulative adders which support very flexible accuracy-energy trade-offs and are well-suited to NN design, It also led to a highly effective method for large-scale stochastic addition we call CeMux (correlation enhanced multiplexer). We believe that CeMux is the best method developed so far in terms of accuracy and accuracy-area trade-offs for building the large weighted-adder networks central to many NN types.\n\n\n\n\n\nThe project explored some useful applications of our SC ideas in the field of biomedical equipment Electrocardiograms ( ECG) are used to monitor a patients cardiovascular health and detect conditions such as arrhythmia. Noise is removed from an ECG signal by means of filters that place large demands on an ECG monitors limited computational resources. In a case study, we demonstrated that CeMux is a very promising way for SC to improve ECG filtering. We also considered the application of SC-based filtering methods to hearing aid design. In particular, we demonstrated that SC is well-suited to designing low-cost filterbanks, an important part of a hearing aid. We showed that an SC filterbank can achieves the same accuracy and latency as a conventional non-SC design, but with an exceptionally large 70% reduction in chip area. The power consumption of our proposed SC filterbank is 38-96% that of the conventional design. The project also explored the benefits and costs of applying SC to the design and analysis of compact, resource-limited NNs of the LSTM (long short term memory) type. We considered a design space spanned by fully binary (non-stochastic), fully stochastic, and several hybrid (mixed) LSTM architectures. Using standard classification benchmarks, we found that area and power can be reduced up to 47% and 86% respectively with little or no impact on classification accuracy. We explored the introduction of wavelet transforms into LSTMs to improve their performance. We devised a regularized training procedure that reduces the computation time of SC-based NNs by a factor of four without sacrificing accuracy or hardware costs. These findings suggest that SC can be successfully applied to more powerful convolutional NNs than previously thought\t\t\t\t\tLast Modified: 07/09/2024\n\n\t\t\t\t\tSubmitted by: JohnPHayes\n"
 }
}