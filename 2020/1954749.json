{
 "awd_id": "1954749",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: SHF: Medium: TensorNN: An Algorithm and Hardware Co-design Framework for On-device Deep Neural Network Learning using Low-rank Tensors",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 416000.0,
 "awd_min_amd_letter_date": "2020-04-29",
 "awd_max_amd_letter_date": "2022-04-14",
 "awd_abstract_narration": "Deep neural network (DNN) is an important Artificial Intelligence (AI) technique and it has recently gained widespread applications in numerous fields such as image recognition, machine translation, autonomous vehicles and healthcare diagnosis. Conventional DNNs are implemented using cloud computing, where a large amount of computing resource is available in a centrally-pooled manner. In order to achieve stronger data privacy, less response time and relaxed data transmission burden, deploying DNN functionality in a distributed manner at the edges of the network has become a very attractive proposition. However, DNN-learning on mobile devices that are at the edge of the network is very challenging due to conflicting requirements of large time and energy consumption, and limited on-device resources. In order to address this challenge, this project leverages low-rank tensors as a powerful mathematical tool for representing and compressing tensor-format data, to form a new family of ultra-low cost deep neural networks. This brings an order-of-magnitude reduction in time and energy consumption for deep neural network learning. Investigations in many areas of BigData research will benefit as well. This project involves graduate and undergraduate students, especially from underrepresented groups, through summer research experiences, and senior design projects to broaden the participation of computing. The outcomes of this project will be disseminated to the community in the format of technical publications, talks and tutorials in both academic institutions and industry.\r\n\r\nIn order to remove the barriers of realizing real-time energy-efficient DNN-learning on the resource and energy-constrained embedded devices, this project considers innovations at three levels: 1) at theory level, it develops a novel redundancy-free matrix-vector multiplication scheme to reduce computational cost, including a new online update scheme for low-rank tensors to enable fast compressed data update; 2) at algorithm level, it develops low-rank tensor-based forward and backward propagation schemes to support low-cost accelerated inference and training, including catastrophic forgetting-resilient training scheme and training-aware compression scheme to improve the learning robustness and memory efficiency; and 3) at hardware design level, it proposes efficient hardware architecture that fully utilize the benefits provided by low-rank tensors to achieve improved hardware performance for on-device DNN inference and learning. Finally, the efficacy of the proposed research will be validated and evaluated, via software implementations on different DNN models in different target applications. A field-programmable gate array (FPGA)-based hardware prototype will also be developed.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Keshab",
   "pi_last_name": "Parhi",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Keshab K Parhi",
   "pi_email_addr": "parhi@umn.edu",
   "nsf_id": "000208606",
   "pi_start_date": "2020-04-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "200 Union St SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550172",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 264421.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 135579.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major goals of this project include model compression in neural networks and applications of model compression in biological and biomedical applications. Model compression involves use of low-rank tensor decomposition. Model compression can reduce energy consumption in deep learning applications. Model compression in biological brain networks addresses modeling of brain graphs using sparse representations.</p>\n<p>Major activities included model compression in various types of neural networks, model compression in brain networks of healthy humans and humans with neuro-psychiatric disorders such as epilepsy and major depressive disorder (MDD), subjects with preFrontal Cortex lesion, and acceleration of neural networks. Other activities included optimization of quantum circuits for error control codes.</p>\n<p>Specific objectives included reducing number of parameters in neural networks, reducing model parameters to improve accuracy of classifying subjects with neuro-psychiatric disorders vs. healthy controls, reducing latency and energy consumption in training neural networks, and in reducing the number of quantum gates in quantum error control code decoders.</p>\n<p>The intellectual merits are summarized below:</p>\n<p>1. In a review paper in IEEE Circuits and Systems Magazine, we summarized approaches to model reduction by tensor decomposition in convolutional neural networks, long short-term memory, and transformers.</p>\n<p>2. In the context of brain analysis, we designed classifiers that require less number of parameters based on brain network connectivity. Specifically, our contributions are seven-fold. (a) For healthy humans, a classifier was designed for gender classification from resting-state and task functional magnetic resonance imaging (fMRI). (b) A new graph entropy approach was developed and applied to classify adolescents with major depressive disorder (MDD) vs. healthy controls. (c) To analyze causality of time-series in the frequency domain, a new approach referred as frequency-domain convergent cross-mapping (FDCCM) was developed. This method was applied to classify subjects with Parkinson's using electroencephalogram (EEG). (d) A sequence transformer was designed to predict seizures among subjects with epilepsy. (e) using effective connectivity of brain networks, seizure onset zones were identified successfully with higher accuracy compared to prior approaches. (f) Brain connectivity approaches were used to decode cognition among humans. (g) Effective connectivity metrics were used to classify subjects with preFrontal cortex lesion via working mnemory task. These results were published in five journal papers (IEEE TBME, IEEE JBHI, IEEE TNSRE, JNE, and IEEE Access), and five conference papers published in IEEE EMBS, IEEE NER, IEEE ISBI, and Asilomar Conference on Signals, Systems, and Computers.</p>\n<p>3. Accelerator architectures were developed for training convolutional neural networks, graph neural networks, and graph attention networks. Gradient interleaving was introduced to reduce the memory access for fetching the gradients from memory. Closed-form expressions were derived for backpropagation equations to train graph attention networks. A review paper on brain-inspired computing was published in IEEE OJCAS. Approaches to quantization in FFT architectures were investigated. The goal was to study the effect of signal and data quantization in signal too quantization ratio and power consumption. These results were published in four journal papers (IEEE TCAS-I, IEEE TCAD, IEEE OJCAS, and Springer JSPS), and a conference paper at ISCAS.</p>\n<p>4. Approaches to reduction of number of quantum gates were proposed for quantum stabilizer codes. These results were published in IEEE Circuits and Systems Magazine and IEEE TCAS-I.</p>\n<p>The broader impacts are summarized below.</p>\n<p>1. This grant has led to 11 journal and 11 conference papers.</p>\n<p>2. The project has trained one post-doctoral fellow, four Ph.D. students (including three graduated students), one MSECE student, and five NSF REU students. Three of the REU students have authored IEEE conference papers.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/01/2024<br>\nModified by: Keshab&nbsp;K&nbsp;Parhi</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe major goals of this project include model compression in neural networks and applications of model compression in biological and biomedical applications. Model compression involves use of low-rank tensor decomposition. Model compression can reduce energy consumption in deep learning applications. Model compression in biological brain networks addresses modeling of brain graphs using sparse representations.\n\n\nMajor activities included model compression in various types of neural networks, model compression in brain networks of healthy humans and humans with neuro-psychiatric disorders such as epilepsy and major depressive disorder (MDD), subjects with preFrontal Cortex lesion, and acceleration of neural networks. Other activities included optimization of quantum circuits for error control codes.\n\n\nSpecific objectives included reducing number of parameters in neural networks, reducing model parameters to improve accuracy of classifying subjects with neuro-psychiatric disorders vs. healthy controls, reducing latency and energy consumption in training neural networks, and in reducing the number of quantum gates in quantum error control code decoders.\n\n\nThe intellectual merits are summarized below:\n\n\n1. In a review paper in IEEE Circuits and Systems Magazine, we summarized approaches to model reduction by tensor decomposition in convolutional neural networks, long short-term memory, and transformers.\n\n\n2. In the context of brain analysis, we designed classifiers that require less number of parameters based on brain network connectivity. Specifically, our contributions are seven-fold. (a) For healthy humans, a classifier was designed for gender classification from resting-state and task functional magnetic resonance imaging (fMRI). (b) A new graph entropy approach was developed and applied to classify adolescents with major depressive disorder (MDD) vs. healthy controls. (c) To analyze causality of time-series in the frequency domain, a new approach referred as frequency-domain convergent cross-mapping (FDCCM) was developed. This method was applied to classify subjects with Parkinson's using electroencephalogram (EEG). (d) A sequence transformer was designed to predict seizures among subjects with epilepsy. (e) using effective connectivity of brain networks, seizure onset zones were identified successfully with higher accuracy compared to prior approaches. (f) Brain connectivity approaches were used to decode cognition among humans. (g) Effective connectivity metrics were used to classify subjects with preFrontal cortex lesion via working mnemory task. These results were published in five journal papers (IEEE TBME, IEEE JBHI, IEEE TNSRE, JNE, and IEEE Access), and five conference papers published in IEEE EMBS, IEEE NER, IEEE ISBI, and Asilomar Conference on Signals, Systems, and Computers.\n\n\n3. Accelerator architectures were developed for training convolutional neural networks, graph neural networks, and graph attention networks. Gradient interleaving was introduced to reduce the memory access for fetching the gradients from memory. Closed-form expressions were derived for backpropagation equations to train graph attention networks. A review paper on brain-inspired computing was published in IEEE OJCAS. Approaches to quantization in FFT architectures were investigated. The goal was to study the effect of signal and data quantization in signal too quantization ratio and power consumption. These results were published in four journal papers (IEEE TCAS-I, IEEE TCAD, IEEE OJCAS, and Springer JSPS), and a conference paper at ISCAS.\n\n\n4. Approaches to reduction of number of quantum gates were proposed for quantum stabilizer codes. These results were published in IEEE Circuits and Systems Magazine and IEEE TCAS-I.\n\n\nThe broader impacts are summarized below.\n\n\n1. This grant has led to 11 journal and 11 conference papers.\n\n\n2. The project has trained one post-doctoral fellow, four Ph.D. students (including three graduated students), one MSECE student, and five NSF REU students. Three of the REU students have authored IEEE conference papers.\n\n\n\t\t\t\t\tLast Modified: 09/01/2024\n\n\t\t\t\t\tSubmitted by: KeshabKParhi\n"
 }
}