{
 "awd_id": "2008412",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FET: Small: Ferroelectric Transistor based Spiking Neural Networks with Adaptive Learning for Edge AI: from Devices to Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2020-06-15",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 499998.0,
 "awd_amount": 499998.0,
 "awd_min_amd_letter_date": "2020-06-03",
 "awd_max_amd_letter_date": "2020-06-03",
 "awd_abstract_narration": "Incorporating artificial intelligence (AI) in electronic systems has been widely recognized as one of the key enablers for several emerging applications. However, the energy efficiency and the learning capabilities of state-of-the-art AI systems is far from that achievable by human brains. This research undertakes a cross-layer exploration spanning novel devices, low-power neural networks, and new learning schemes. The exploration will exploit Ferroelectric Field Effect Transistor (FeFET) technology with intrinsic neuro-mimetic features to achieve energy-efficient neural hardware and adaptable learning. The low-power hardware solutions and adaptive-learning algorithms have the potential to impact critical applications such as computer-aided diagnosis, robotics, speech/face recognition, and data classification, thereby directly benefiting areas such as healthcare, defense, security etc. Moreover, power savings should translate to longer battery life for edge devices and energy-efficient data processing for applications like wearable health-monitoring platforms. The project will leverage outreach programs at Purdue University and develop a summer Research Experiences for Undergraduates (REU) program to involve undergraduates and minority students in the project. The broad nature of this project will provide opportunity for undergraduate students to get introduced to the field of AI based on emerging technologies.\r\n\r\nSpiking Neural Networks (SNNs), due to their self-learning capabilities, show promise in introducing adaptability in learning for AI systems, but suffer from low accuracy. Improving SNN accuracy and performance not only necessitates novel learning mechanisms that support adaptable lifelong learning, but also an intrinsically suitable technology for low-power scalable hardware. To address this critical need, this project will carry out a comprehensive devices-to-algorithms exploration of multi-domain FeFET based SNNs. The main objectives include (a) design of low-power neurons and synapses using FeFETs, and (b) development of adaptive and sequential learning algorithms utilizing the unique attributes of the neuro-mimetic devices. To enable bio-plausible features in FeFETs, physics-based device optimization will be carried out to utilize the multi-domain effects and domain dynamics of ferroelectrics. To facilitate cross-layer exploration, a devices-to-systems simulation framework will be developed capturing the rich dynamics of the neurons and synapses, their interactions in an SNN, and the impact of new learning algorithms on system performance/accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sumeet",
   "pi_last_name": "Gupta",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Sumeet K Gupta",
   "pi_email_addr": "guptask@purdue.edu",
   "nsf_id": "000675326",
   "pi_start_date": "2020-06-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kaushik",
   "pi_last_name": "Roy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kaushik Roy",
   "pi_email_addr": "kaushik@ecn.purdue.edu",
   "nsf_id": "000482731",
   "pi_start_date": "2020-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "465 Northwestern Ave",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072035",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "089Y00",
   "pgm_ele_name": "FET-Fndtns of Emerging Tech"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 499998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The advancements in neural computing have led to unprecedented performance of artificial intelligence (AI) platforms leading to super-human accuracies for several tasks. However, this success bears a concomitant cost of enormous energy requirements to process AI workloads. To counter the issues associated with AI hardware design, especially for resource-constrained edge devices, innovations at the hardware and algorithm levels are needed that can achieve quantum improvements in the energy efficiency, while sustaining acceptable levels of accuracy. At the hardware level, novel technologies, circuits and architectures are needed that can push the boundaries of the state-of-the-art solutions by enhancing the computing robustness, energy efficiency, and scalability. To that end, emerging non-volatile memory technologies have shown a large potential to circumvent the limitations of standard transistors, albeit with their own design issues. Furthermore, merging data storage and processing (as opposed to having separate memory and processing modules, as in standard computing architectures) has attracted a lot of attention for AI accelerator design. However, memory-compute fusion leads to other design challenges, which need to be tackled to harness of the full potential of this technique. &nbsp;Such hardware-level advancements need to be complemented by training schemes that can achieve systemic improvements in energy efficiency. &nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>&nbsp;This project addressed this need by conducting a comprehensive exploration of hafnium zirconium oxide (HZO)-based ferroelectric technology for the design of energy efficient artificial intelligence (AI) accelerators. The approach included developing cross-layer design solutions and conducting a comprehensive design space exploration spanning experimental characterization, simulation-based design and analysis of ferroelectric transistors and capacitors, circuit/architecture designs enabling energy efficient computations and hardware-aware training to counter the effects of device-circuit non-idealities. The team fabricated HZO-based devices and performed extensive characterizations to understand their characteristics, including device-to-device variability. Techniques based on interface optimization were established to lower the programming voltage (thereby, enhancing the energy efficiency). &nbsp;The experimental studies were coupled with simulation-based analyses. In particular, the static and dynamic characteristics of HZO-based ferroelectric transistors and capacitors were analyzed to understand the underlying physical mechanisms and establish the device optimization strategies to cater to the need of target neural accelerators. To achieve this, this project developed 3D phase-field models for HZO-based devices accounting for polycrystallinity of HZO, multi-domain effects, anisotropic elastic interactions, and dynamic behavior. Based on these models, the team unraveled several key features and design strategies for HZO-based transistors and capacitors, including the effect of HZO thickness on the device behavior, the dependence of HZO characteristics on the attributes of applied voltage pulses, device-to-device variations and techniques to reduce them, and others. The team also analyzed the unique dynamic characteristics which show deviation from the first-order circuits used in standard spiking neural networks. Based on the device analysis, several hardware-algorithm co-design techniques were established to enhance the accuracy and energy efficiency of the neural accelerators. These techniques included (i) designing computing-in-memory arrays and macros based on the co-optimization of HZO thickness, number of bits stored per device, memory array size and peripheral circuits (ii) incorporating adaptability in HZO-based spiking neurons, (iii) designing compact and low power oscillatory neurons and their coupled networks and (iv) developing technology-aware training methodologies to exploit their unique features (such as non-first-order behavior of HZO neurons to reduce the energy consumption of spiking neural networks) or to counter their limitations (such as low distinguishability of HZO-based capacitors). The proposed designs for the computing-in-memory macros and oscillatory/spiking neurons established several key cross-layer design insights for ferroelectric-based neural accelerators and led to a significant enhancement in computation robustness and energy efficiency. This, coupled with the hardware-aware training, showed an improvement in system accuracy and/or reduction in power consumption.&nbsp; &nbsp;</p>\n<p>&nbsp;The enhancement in energy efficiency of neural accelerators achieved via the proposed design solutions can be immensely beneficial for a broad range of edge applications that rely on efficient data processing. These include computer aided diagnosis, speech/face recognition, data classification etc. Such systems will directly impact critical areas such as healthcare, security and defense. In addition, this project has led to the participation and training of multiple graduate students (including a female student), who, through this research, got an in-depth knowledge on ferroelectric transistors and capacitors, and their applications in neural computing. This will prepare them for their future career and leadership positions in engineering and technology. Furthermore, the research outcomes of this project were utilized to enhance the graduate curriculum by integrating them in a course on Advanced VLSI Design offered by the Elmore Family School of Electrical and Computer Engineering at Purdue University. Outreach activities such as organization of workshops were conducted to provide exposure to emerging technologies and their role in future computing to a broader class of engineers and researchers. The circuit-compatible models for HZO-devices developed in this project were shared with other researchers to enable their exploration of this promising technology.&nbsp;&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/22/2024<br>\nModified by: Sumeet&nbsp;K&nbsp;Gupta</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe advancements in neural computing have led to unprecedented performance of artificial intelligence (AI) platforms leading to super-human accuracies for several tasks. However, this success bears a concomitant cost of enormous energy requirements to process AI workloads. To counter the issues associated with AI hardware design, especially for resource-constrained edge devices, innovations at the hardware and algorithm levels are needed that can achieve quantum improvements in the energy efficiency, while sustaining acceptable levels of accuracy. At the hardware level, novel technologies, circuits and architectures are needed that can push the boundaries of the state-of-the-art solutions by enhancing the computing robustness, energy efficiency, and scalability. To that end, emerging non-volatile memory technologies have shown a large potential to circumvent the limitations of standard transistors, albeit with their own design issues. Furthermore, merging data storage and processing (as opposed to having separate memory and processing modules, as in standard computing architectures) has attracted a lot of attention for AI accelerator design. However, memory-compute fusion leads to other design challenges, which need to be tackled to harness of the full potential of this technique. Such hardware-level advancements need to be complemented by training schemes that can achieve systemic improvements in energy efficiency. \n\n\nThis project addressed this need by conducting a comprehensive exploration of hafnium zirconium oxide (HZO)-based ferroelectric technology for the design of energy efficient artificial intelligence (AI) accelerators. The approach included developing cross-layer design solutions and conducting a comprehensive design space exploration spanning experimental characterization, simulation-based design and analysis of ferroelectric transistors and capacitors, circuit/architecture designs enabling energy efficient computations and hardware-aware training to counter the effects of device-circuit non-idealities. The team fabricated HZO-based devices and performed extensive characterizations to understand their characteristics, including device-to-device variability. Techniques based on interface optimization were established to lower the programming voltage (thereby, enhancing the energy efficiency). The experimental studies were coupled with simulation-based analyses. In particular, the static and dynamic characteristics of HZO-based ferroelectric transistors and capacitors were analyzed to understand the underlying physical mechanisms and establish the device optimization strategies to cater to the need of target neural accelerators. To achieve this, this project developed 3D phase-field models for HZO-based devices accounting for polycrystallinity of HZO, multi-domain effects, anisotropic elastic interactions, and dynamic behavior. Based on these models, the team unraveled several key features and design strategies for HZO-based transistors and capacitors, including the effect of HZO thickness on the device behavior, the dependence of HZO characteristics on the attributes of applied voltage pulses, device-to-device variations and techniques to reduce them, and others. The team also analyzed the unique dynamic characteristics which show deviation from the first-order circuits used in standard spiking neural networks. Based on the device analysis, several hardware-algorithm co-design techniques were established to enhance the accuracy and energy efficiency of the neural accelerators. These techniques included (i) designing computing-in-memory arrays and macros based on the co-optimization of HZO thickness, number of bits stored per device, memory array size and peripheral circuits (ii) incorporating adaptability in HZO-based spiking neurons, (iii) designing compact and low power oscillatory neurons and their coupled networks and (iv) developing technology-aware training methodologies to exploit their unique features (such as non-first-order behavior of HZO neurons to reduce the energy consumption of spiking neural networks) or to counter their limitations (such as low distinguishability of HZO-based capacitors). The proposed designs for the computing-in-memory macros and oscillatory/spiking neurons established several key cross-layer design insights for ferroelectric-based neural accelerators and led to a significant enhancement in computation robustness and energy efficiency. This, coupled with the hardware-aware training, showed an improvement in system accuracy and/or reduction in power consumption. \n\n\nThe enhancement in energy efficiency of neural accelerators achieved via the proposed design solutions can be immensely beneficial for a broad range of edge applications that rely on efficient data processing. These include computer aided diagnosis, speech/face recognition, data classification etc. Such systems will directly impact critical areas such as healthcare, security and defense. In addition, this project has led to the participation and training of multiple graduate students (including a female student), who, through this research, got an in-depth knowledge on ferroelectric transistors and capacitors, and their applications in neural computing. This will prepare them for their future career and leadership positions in engineering and technology. Furthermore, the research outcomes of this project were utilized to enhance the graduate curriculum by integrating them in a course on Advanced VLSI Design offered by the Elmore Family School of Electrical and Computer Engineering at Purdue University. Outreach activities such as organization of workshops were conducted to provide exposure to emerging technologies and their role in future computing to a broader class of engineers and researchers. The circuit-compatible models for HZO-devices developed in this project were shared with other researchers to enable their exploration of this promising technology.\n\n\n\t\t\t\t\tLast Modified: 07/22/2024\n\n\t\t\t\t\tSubmitted by: SumeetKGupta\n"
 }
}