{
 "awd_id": "2045167",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Random Neural Networks",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924885",
 "po_email": "tbartosz@nsf.gov",
 "po_sign_block_name": "Tomek Bartoszynski",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 142798.0,
 "awd_amount": 142798.0,
 "awd_min_amd_letter_date": "2020-08-17",
 "awd_max_amd_letter_date": "2020-10-14",
 "awd_abstract_narration": "Neural networks are algorithms that in the past several years have achieved state of the art in a variety of important machine learning tasks, ranging from computer vision (e.g. self-driving cars) to natural language processing (e.g. Echo, Alex, Google Translate, etc) and reinforcement learning (e.g. AlphaGo and AlphaStar). Despite these impressive successes, it is not clear why neural nets work so well. In this project, the PI will use tools from probability to develop our theoretical understanding of neural networks. The goal is to give us a deep understanding of why neural nets are so efficient at overcoming challenges in optimization and high-dimensional data analysis. These theoretical insights will, in turn, inform the intuition of engineers for building the next generation of neural net-based machine learning systems. \r\n\r\nMathematically, the study of neural networks is a cross between approximation theory and optimization, touching on topics from random matrix theory, Gaussian processes, hyperplane arrangements, tensor decompositions, and optimal transport, to name a few. The PI will focus specifically on (i) the stability of gradient-based optimization of neural networks to both the linear statistics and spectral asymptotics of random matrix ensembles given by products of many random matrices in the regime where both the number of terms in the product and the sizes of the matrices simultaneously group, and (ii) computing the correlation functions of neural networks at initialization (e.g. with random weights and biases). Questions of type (i) give quantitative information on the numerical stability of neural network architectures at initialization. Questions of type (ii), in contrast, aim at principles for data-driven architecture selection.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Boris",
   "pi_last_name": "Hanin",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Boris L Hanin",
   "pi_email_addr": "bhanin@princeton.edu",
   "nsf_id": "000653806",
   "pi_start_date": "2020-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "Off of Research &Proj. Admin. PO Box 36",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126300",
   "pgm_ele_name": "PROBABILITY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 142798.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Intellectual Merit</strong>. A neural network is a model with many parameters. Such models have brought dramatic progress in tasks ranging from language (e.g. ChatGPT) to vision (e.g. self-driving cars). In practice, the typical use of a neural network is for fitting training data (e.g. examples of text on the internet). This is done by first randomly selecting the parameters of a network and then gradually adjusting the parameters to fit the observed data. As its title suggests, this project focused on analyzing neural networks with randomly chosen parameters (i.e. at the start of training).</p>\n<p>The key questions the PI sought to answer are:</p>\n<p>1. What kinds of functions can neural networks of a specific form -- called an architecture -- compute if one is allowed to choose any setting of network parameters?<br />2. What kinds of functions can neural networks of a specific architecture typically compute when one chooses the network parameters at random?<br />3. How can one design a way to choose neural network parameters at random so that neural network training (i.e. adjusting parameters to fit data) proceeds as quickly as possible?</p>\n<p>To answer these and related questions the PI wrote 9 research articles under the auspices of this grant. The key conceptual takeaways from these articles were the following:</p>\n<p>1. There is a huge gap between the functions that a neural network with a given architecture an compute in principle and those that it typically computes. So an explanation of why neural networks work so well cannot be based on the former.<br />2. The structure of the function computed at the start of training by a neural network with L layers of width N is best summarized through the ratio L / N. Specifically, if L / N is too big, then training becomes very difficult. Further, if L / N is too small, then the network is easy to train but can only learn very simple functions. This helps to explain why L / N is typically small but positive in practice.</p>\n<p><strong>Broader Impacts.</strong> The main broader impacts of this project are related to education. The PI taught 5 graduate deep learning theory courses (two at Texas A&amp;M and three at Princeton) which covered material related to this project. The A&amp;M course resulted in 4 journal articles written by groups of students and the Princeton courses resulted in at least 3 such articles.</p>\n<p>Further, one of the articles \"Deep ReLU Networks Preserve Expected Length\" is joint with an undergraduate. Moreover, the PI advised 15 or so senior thesis projects at Princeton, and two of them (one on the nodal geometry of ReLU networks and one on exploding and vanishing gradients in convolutional networks) were based on projects developed under this grant.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/01/2023<br>\n\t\t\t\t\tModified by: Boris&nbsp;L&nbsp;Hanin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIntellectual Merit. A neural network is a model with many parameters. Such models have brought dramatic progress in tasks ranging from language (e.g. ChatGPT) to vision (e.g. self-driving cars). In practice, the typical use of a neural network is for fitting training data (e.g. examples of text on the internet). This is done by first randomly selecting the parameters of a network and then gradually adjusting the parameters to fit the observed data. As its title suggests, this project focused on analyzing neural networks with randomly chosen parameters (i.e. at the start of training).\n\nThe key questions the PI sought to answer are:\n\n1. What kinds of functions can neural networks of a specific form -- called an architecture -- compute if one is allowed to choose any setting of network parameters?\n2. What kinds of functions can neural networks of a specific architecture typically compute when one chooses the network parameters at random?\n3. How can one design a way to choose neural network parameters at random so that neural network training (i.e. adjusting parameters to fit data) proceeds as quickly as possible?\n\nTo answer these and related questions the PI wrote 9 research articles under the auspices of this grant. The key conceptual takeaways from these articles were the following:\n\n1. There is a huge gap between the functions that a neural network with a given architecture an compute in principle and those that it typically computes. So an explanation of why neural networks work so well cannot be based on the former.\n2. The structure of the function computed at the start of training by a neural network with L layers of width N is best summarized through the ratio L / N. Specifically, if L / N is too big, then training becomes very difficult. Further, if L / N is too small, then the network is easy to train but can only learn very simple functions. This helps to explain why L / N is typically small but positive in practice.\n\nBroader Impacts. The main broader impacts of this project are related to education. The PI taught 5 graduate deep learning theory courses (two at Texas A&amp;M and three at Princeton) which covered material related to this project. The A&amp;M course resulted in 4 journal articles written by groups of students and the Princeton courses resulted in at least 3 such articles.\n\nFurther, one of the articles \"Deep ReLU Networks Preserve Expected Length\" is joint with an undergraduate. Moreover, the PI advised 15 or so senior thesis projects at Princeton, and two of them (one on the nodal geometry of ReLU networks and one on exploding and vanishing gradients in convolutional networks) were based on projects developed under this grant.\n\n\t\t\t\t\tLast Modified: 10/01/2023\n\n\t\t\t\t\tSubmitted by: Boris L Hanin"
 }
}