{
 "awd_id": "2002348",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Collaboratively Perceiving, Comprehending, and Projecting into the Future: Supporting Team Situational Awareness with Adaptive Collaborative Tactons",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2019-08-12",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 15734.0,
 "awd_amount": 15734.0,
 "awd_min_amd_letter_date": "2019-10-31",
 "awd_max_amd_letter_date": "2019-10-31",
 "awd_abstract_narration": "Data overload, especially in the visual channel, and associated breakdowns in monitoring already represent a major challenge in data-rich environments.  One promising means of overcoming data overload is through the introduction of multimodal displays which distribute information across various sensory channels (including vision, audition, and touch).  In recent years, touch has received more attention as a means to offload the overburdened visual and auditory channels, but much remains to be discovered in this modality.  Tactons, or tactile icons/displays, are structured, abstract messages that can be used to communicate information in the absence of vision.  However, the effectiveness of tactons may be compromised if their design does not take into account that complex systems depend on the coordinated activities of a team.  The PI's goal in this project is to establish a research program that will explore adaptive collaborative tactons as a means to support situational awareness, that is the ability of a team to perceive and comprehend information from the environment and predict future events in real time.  Project outcomes will contribute to a deeper understanding of perception and attention between and across sensory channels for individuals and teams, and to how multimodal interfaces can support teamwork in data-rich domains.\r\n\r\nThe work will integrate three disparate topics within human factors: multimodal interfaces, situational awareness, and adaptive systems.   The PI will create methods to design tactons that take into account both context and the types of information needed by a team, by leveraging the multimodal aspects to develop quantitative and qualitative models and algorithms using physiological measures (in particular, eye tracking data).  These in turn will inform the functionality of adaptive tactons that support collaboration by adjusting the presentation of information in response to various sensed parameters and conditions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sara",
   "pi_last_name": "Riggs",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Sara L Riggs",
   "pi_email_addr": "sriggs@virginia.edu",
   "nsf_id": "000694670",
   "pi_start_date": "2019-10-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "P.O.BOX 400195",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044195",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 15734.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Communicating information using the tactile modality is a promising means to address data overload faced in complex, data-rich work domains. However, it is important to ensure that the tactile parameters which represent information are intuitive and can support situational awareness, i.e., the ability of a team to perceive and comprehend information from the environment and predict future events in real time.&nbsp;Currently there is no consensus on which tactile parameters should be used to present different types of information and to an even lesser degree, how tactile information can support situational awareness.</p>\n<p>The major goals of this project include the following:</p>\n<ol>\n<li>Develop methods to design tactons, i.e., tactile icons/displays, that can be used to communicate information to a team.</li>\n<li>Design tactons that can improve situational awareness amongst team members.</li>\n<li>Develop quantitative and qualitative models and algorithms using eye tracking technology to assess and support team situational awareness.</li>\n</ol>\n<p>For Goal #1, a series of studies were conducted to understand how to best present information using the sensing of touch. A usability study evaluated tactons within the complex work domain of anesthesia monitoring in the operating room. We found that anesthesia providers who participated in our studies were effectively able to respond to our tactons while performing other tasks. Overall, the findings contribute to a better understanding of how to design tactile displays and demonstrated the effectiveness of end-user feedback in developing intuitive alerts.</p>\n<p>For Goals #2 and #3, studies were conducted to see how pairs of individuals worked together in another complex work domain, Unmanned Aerial Vehicle control for search and rescue missions. Individual differences such as personality traits and video game experience were examined to determine whether it affected team situational awareness, i.e., the ability to anticipate and adapt to changes in the work environment as a team. Eye tracking technology was also used to understand the participants eye movement behavior; this data was mathematically modeled. The findings suggest technology should account for individual differences to support collaboration. The findings also show that eye tracking technology can provide unique insights to explain performance between individuals.</p>\n<p>Over 12 undergraduate students from multiple disciplines&mdash;computer science, computer, electrical, industrial, systems engineering as well as psychology&mdash;were supported by this project and an NSF REU supplement. One URM PhD student was funded under this grant, and she was ultimately awarded an NSF GRFP based on the initial data collected as part of this grant. The findings from this work were also presented at various academic venues domestically and internationally (e.g., conferences, seminars, courses) and to the public (e.g., the Society of Women high school visitations to the Riggs Lab). The preliminary findings resulted in several follow up research questions which were proposed as part of the PI&rsquo;s NSF CAREER award.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Sara&nbsp;Riggs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCommunicating information using the tactile modality is a promising means to address data overload faced in complex, data-rich work domains. However, it is important to ensure that the tactile parameters which represent information are intuitive and can support situational awareness, i.e., the ability of a team to perceive and comprehend information from the environment and predict future events in real time. Currently there is no consensus on which tactile parameters should be used to present different types of information and to an even lesser degree, how tactile information can support situational awareness.\n\nThe major goals of this project include the following:\n\nDevelop methods to design tactons, i.e., tactile icons/displays, that can be used to communicate information to a team.\nDesign tactons that can improve situational awareness amongst team members.\nDevelop quantitative and qualitative models and algorithms using eye tracking technology to assess and support team situational awareness.\n\n\nFor Goal #1, a series of studies were conducted to understand how to best present information using the sensing of touch. A usability study evaluated tactons within the complex work domain of anesthesia monitoring in the operating room. We found that anesthesia providers who participated in our studies were effectively able to respond to our tactons while performing other tasks. Overall, the findings contribute to a better understanding of how to design tactile displays and demonstrated the effectiveness of end-user feedback in developing intuitive alerts.\n\nFor Goals #2 and #3, studies were conducted to see how pairs of individuals worked together in another complex work domain, Unmanned Aerial Vehicle control for search and rescue missions. Individual differences such as personality traits and video game experience were examined to determine whether it affected team situational awareness, i.e., the ability to anticipate and adapt to changes in the work environment as a team. Eye tracking technology was also used to understand the participants eye movement behavior; this data was mathematically modeled. The findings suggest technology should account for individual differences to support collaboration. The findings also show that eye tracking technology can provide unique insights to explain performance between individuals.\n\nOver 12 undergraduate students from multiple disciplines&mdash;computer science, computer, electrical, industrial, systems engineering as well as psychology&mdash;were supported by this project and an NSF REU supplement. One URM PhD student was funded under this grant, and she was ultimately awarded an NSF GRFP based on the initial data collected as part of this grant. The findings from this work were also presented at various academic venues domestically and internationally (e.g., conferences, seminars, courses) and to the public (e.g., the Society of Women high school visitations to the Riggs Lab). The preliminary findings resulted in several follow up research questions which were proposed as part of the PI\u2019s NSF CAREER award.\n\n \n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Sara Riggs"
 }
}