{
 "awd_id": "1941808",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Learning Neurosymbolic 3D Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922533",
 "po_email": "hshen@nsf.gov",
 "po_sign_block_name": "Han-Wei Shen",
 "awd_eff_date": "2020-04-01",
 "awd_exp_date": "2025-03-31",
 "tot_intn_awd_amt": 549999.0,
 "awd_amount": 565999.0,
 "awd_min_amd_letter_date": "2020-03-16",
 "awd_max_amd_letter_date": "2024-05-14",
 "awd_abstract_narration": "High-quality 3D models are increasingly in demand, driven by numerous industries and by the need for synthetic training data to scale up autonomous vision systems. But creating such models is a laborious and time-consuming process requiring years of training, so current practice will be insufficient to satisfy future data demands. One way forward is through generative models of 3D objects, that is to have machines learn to synthesize high-quality objects, a nice vision which has yet to be realized. Existing 3D generative models fall into one of two broad categories, each with limitations. Symbolic generative models such as shape grammars can enable non-experts to generate high-quality geometry but have severely limited expressiveness, while neural generative models are flexible and can in theory learn to express any shape but they are inscrutable and produce flawed geometry.  This project will explore a new class of generative shape model that combines the best of both worlds: neuro-symbolic 3D models. The main insight is to use a symbolic program to model the logical part structure of a 3D object (e.g., the legs of a chair are connected to its seat), and then to use neural networks to refine this structure into high-quality geometry. Such a representation supports synthesis of new objects, reconstruction of objects from real-world sensor input, and high-level editing of object structure and geometry. It also supports modeling of higher-order object properties, including kinematics and physics.  To enable massive-scale generation of synthetic 3D training data for computer vision and robotics, a neuro-symbolic version of the widely used ShapeNet dataset will be implemented and released. To help democratize 3D content creation, the project will collaborate with Unity Technologies to integrate neuro-symbolic 3D models into their popular 3D graphics engine. Project outcomes will also include an open-source, pedagogical deep learning framework to educate a new generation of researchers with the multidisciplinary skillset needed for neuro-symbolic modeling, in concert with activities (e.g., piloting new integrated visual computing curricula via summer schools and hosting visiting student researchers from historically under-represented groups) designed to improve student mastery of neural network fundamentals.\r\n\r\nThe recognition-by-components theory of vision posits that people recognize objects by first understanding their fundamental parts and then using a secondary process to handle objects that are not distinguishable by these parts alone. Neuro-symbolic 3D models operationalize this theory for object synthesis via two algorithmic phases. The first phase is a new procedural representation called a hierarchical part graph program that is a human-readable computer program which, when executed, constructs a graph of connected object parts at multiple levels of detail wherein the bottom level of detail consists of parametric primitives such as cuboids and cylinders. While suggestive of shape, these graphs do not capture the full variety of geometry found in real-world objects. Thus, the second phase of the model is a new neural adaptive subdivision procedure which converts the low-fidelity parts into high-fidelity surface geometry. This decomposition is a natural fit for the common case of human-made objects, but it can also be extended to organic objects. The hypothesis is that this approach to 3D object generation will be able to efficiently synthesize and reconstruct a variety of high-quality objects in a unified, easily-editable representation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Ritchie",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Ritchie",
   "pi_email_addr": "daniel_ritchie@brown.edu",
   "nsf_id": "000737205",
   "pi_start_date": "2020-03-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "Office of Sponsored Projects",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129093",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 118990.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 106362.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 223330.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 117317.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Neural networks have become popular for generative visual modeling (including for 3D objects) because they can be trained on large datasets to produce impressive visual results. However, they are difficult to control, as it is not clear how to adjust the parameters of a neural network to change the outputs it produces.&nbsp;<br />The aim of this award is to develop new methods for representing and generating 3D objects using \"neurosymbolic\" models: techniques that combine deep neural networks with human-readable computer code. Human-readable programs for generating 3D objects, referred to in computer graphics as \"procedural models,\" have gained wide adoption for their ability to expose intuitive parameters that make it easy to change the output shape to explore design variations. By combining neural and procedural models, we seek to create a new family of 3D object modeling methods that can learn from large datasets but also provide human-readable (and editable) code.<br /><br /><strong>Intellectual Merit:</strong><br />Our research introduced a new approach to 3D object modeling: learning neural networks that generate 3D object programs. We have demonstrated how to train such networks from large datasets of 3D objects that come with programs. Since such datasets are rare and difficult to construct, this award also funded research into how to infer programs for datasets of 3D objects that do not already have them. Since these programs usually need to refer to meaningful parts of objects, we also investigated learning-based methods for decomposing 3D objects into fine-grained parts that require less training data than previous approaches. For programs to be maximally usable, they should be written using subroutines that organize the 3D object into meaningful substructures. Thus, another major thread of our research investigated how to automatically discover such subroutines from datasets of 3D objects. Our research efforts have been highly interdisciplinary, bringing together ideas from 3D shape analysis and geometry processing, machine learning, and program synthesis.<br /><br /><strong>Broader Impacts:</strong><br />Many industries and applications are increasingly demanding high-quality 3D object models: from industrial and product design, to interior design and architecture, to mixed reality and gaming. Our neurosymbolic models are designed to make the creation of such 3D assets easier, more accessible, and more reliable. Another application which has a high demand for virtual 3D object models is the creation of synthetic training data for computer vision and robotics. Methods developed in our lab have already been used in such applications (e.g. generating variations on 3D objects that robots must learn to interact with). In addition, the methods we have developed for discovering useful 3D object modeling subroutines and the methods we have developed for inferring programs for 3D objects are general enough to be applicable to other program synthesis problems, making them potentially impactful for any domain in which it is desirable to automatically find useful programs for representing or processing data. Finally, PI Ritchie has open-sourced a pedagogical deep learning framework that he developed as part of this award's education plan.</p><br>\n<p>\n Last Modified: 04/20/2025<br>\nModified by: Daniel&nbsp;Ritchie</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191318854_sc_qual_chair--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191318854_sc_qual_chair--rgov-800width.png\" title=\"Subroutine discovery\"><img src=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191318854_sc_qual_chair--rgov-66x44.png\" alt=\"Subroutine discovery\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our ShapeCoder system can automatically discover useful subroutines (or abstractions) from a dataset of part-segmented 3D shapes. When run on chairs, ShapeCoder discovers an abstraction that explains a common layout of a complete chair, with only five free parameters.</div>\n<div class=\"imageCredit\">Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie\n<div class=\"imageTitle\">Subroutine discovery</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191685961_pipe--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191685961_pipe--rgov-800width.png\" title=\"Learning fine-grained 3D part shape segmentation\"><img src=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191685961_pipe--rgov-66x44.png\" alt=\"Learning fine-grained 3D part shape segmentation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Neurally-Guided Shape Parser (NGSP) learns to assign fine-grained semantic labels (rightmost) to shape regions (leftmost). A guide network generates a set of proposed label assignments. The label assignments are sent through likelihood modules that evaluate the global coherence of each proposal.</div>\n<div class=\"imageCredit\">Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie\n<div class=\"imageTitle\">Learning fine-grained 3D part shape segmentation</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191089016_teaser--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191089016_teaser--rgov-800width.png\" title=\"Learning to generate 3D shape programs\"><img src=\"/por/images/Reports/POR/2025/1941808/1941808_10658150_1745191089016_teaser--rgov-66x44.png\" alt=\"Learning to generate 3D shape programs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We present a deep generative model which learns to write novel programs in ShapeAssembly, a domain-specific language for modeling 3D shape structures. Executing a ShapeAssembly program produces a shape composed of a hierarchical connected assembly of part proxies cuboids. Our method develops a well-</div>\n<div class=\"imageCredit\">Daniel Ritchie</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Ritchie\n<div class=\"imageTitle\">Learning to generate 3D shape programs</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nNeural networks have become popular for generative visual modeling (including for 3D objects) because they can be trained on large datasets to produce impressive visual results. However, they are difficult to control, as it is not clear how to adjust the parameters of a neural network to change the outputs it produces.\nThe aim of this award is to develop new methods for representing and generating 3D objects using \"neurosymbolic\" models: techniques that combine deep neural networks with human-readable computer code. Human-readable programs for generating 3D objects, referred to in computer graphics as \"procedural models,\" have gained wide adoption for their ability to expose intuitive parameters that make it easy to change the output shape to explore design variations. By combining neural and procedural models, we seek to create a new family of 3D object modeling methods that can learn from large datasets but also provide human-readable (and editable) code.\n\nIntellectual Merit:\nOur research introduced a new approach to 3D object modeling: learning neural networks that generate 3D object programs. We have demonstrated how to train such networks from large datasets of 3D objects that come with programs. Since such datasets are rare and difficult to construct, this award also funded research into how to infer programs for datasets of 3D objects that do not already have them. Since these programs usually need to refer to meaningful parts of objects, we also investigated learning-based methods for decomposing 3D objects into fine-grained parts that require less training data than previous approaches. For programs to be maximally usable, they should be written using subroutines that organize the 3D object into meaningful substructures. Thus, another major thread of our research investigated how to automatically discover such subroutines from datasets of 3D objects. Our research efforts have been highly interdisciplinary, bringing together ideas from 3D shape analysis and geometry processing, machine learning, and program synthesis.\n\nBroader Impacts:\nMany industries and applications are increasingly demanding high-quality 3D object models: from industrial and product design, to interior design and architecture, to mixed reality and gaming. Our neurosymbolic models are designed to make the creation of such 3D assets easier, more accessible, and more reliable. Another application which has a high demand for virtual 3D object models is the creation of synthetic training data for computer vision and robotics. Methods developed in our lab have already been used in such applications (e.g. generating variations on 3D objects that robots must learn to interact with). In addition, the methods we have developed for discovering useful 3D object modeling subroutines and the methods we have developed for inferring programs for 3D objects are general enough to be applicable to other program synthesis problems, making them potentially impactful for any domain in which it is desirable to automatically find useful programs for representing or processing data. Finally, PI Ritchie has open-sourced a pedagogical deep learning framework that he developed as part of this award's education plan.\t\t\t\t\tLast Modified: 04/20/2025\n\n\t\t\t\t\tSubmitted by: DanielRitchie\n"
 }
}