{
 "awd_id": "2007278",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Accelerating Machine Learning via Randomized Automatic Differentiation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2020-09-03",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Machine learning is having a tremendous impact on our society and economy, but it depends critically on the ability to efficiently fit a model to data.  The technique of automatic differentiation takes software code written to build such a model and automatically performs the calculus derivations necessary to fit it to data. Automatic differentiation tools have been at the heart of the resurgence of neural networks for tackling problems ranging from drug discovery to self-driving cars.  This project revisits core assumptions in the way that automatic differentiation works, and identifies new ways that it can take advantage of randomness to find better machine learning models, faster.  This research will lead to new tools that expand the frontier of what machine learning systems are possible.\r\n\r\nThe project will develop new techniques for automatic differentiation when it will be used as part of a stochastic optimization procedure, as is commonly done in training deep neural networks.  Rather than exact Jacobian accumulation on the linearized computational graph, this project proposes techniques for selecting random subgraphs such that the Jacobian is preserved in expectation but much less memory and computation is required.  Beyond randomization of the central Jacobian accumulation problem, the project will also explore how randomization can enable new approaches to implicit differentiation as used in PDE-constrained optimization and related problems. Additionally, the project will develop techniques for finding good, approximate near-optimal dynamic programming schedules for the linearized computational graph.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ryan",
   "pi_last_name": "Adams",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Ryan P Adams",
   "pi_email_addr": "rpa@princeton.edu",
   "nsf_id": "000623159",
   "pi_start_date": "2020-09-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>One of the biggest factors in the ongoing \"AI revolution\" is the development of tools for \"automatic differentiation\". &nbsp;These tools make it possible to train giant neural networks with millions or billions of parameters, by using algorithms to automatically perform the calculus that would otherwise have to be done by hand. &nbsp;Although these algorithms are in widespread use, costing billions of dollars and consuming significant energy resources, relatively little work has been done to revisit their foundations and look for opportunities to improve their efficiency. &nbsp;In this project, we developed new methods for automatic differentation that take advantage of randomness to make them more efficient in terms of computation and memory. &nbsp;These new methods are just as capable as what has previously been done, but can make better use of resources. &nbsp;On some problems, they use orders of magnitude less memory while giving the same performance.</p>\r\n<p>This work has also led to new ideas that do not just make automatic differentiation faster, but that also make it possible to use it on problems for which it previously was impossible. &nbsp;In particular, many kinds of machine learning approaches to physical systems---electromagnetics, optics, mechanics---have geometric structures that prevent conventional methods from working. &nbsp;Specifically, they involve performing integration of volumes and surfaces with hard boundaries. &nbsp;Solutions to these problems require reasoning about how the boundaries should move, but this is something that has not been possible to do automatically. &nbsp;Our new method of Fiber Monte Carlo solves this problem, and we are already seeing multiple new domains where it is applicable, making it possible to use machine learning to, e.g., design new photonic circuits or architectural structures.</p>\r\n<p>We have put these new methods to work in several collaborations with scientists in other disciplines, making it possible to explore completely new ideas that would have been unthinkable even a few years ago. &nbsp;In particular, we have explored new ideas for mechanical meta-materials that can have interesting nonlinear properties which cannoth be achieved by materials found in nature.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/21/2025<br>\nModified by: Ryan&nbsp;P&nbsp;Adams</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nOne of the biggest factors in the ongoing \"AI revolution\" is the development of tools for \"automatic differentiation\". These tools make it possible to train giant neural networks with millions or billions of parameters, by using algorithms to automatically perform the calculus that would otherwise have to be done by hand. Although these algorithms are in widespread use, costing billions of dollars and consuming significant energy resources, relatively little work has been done to revisit their foundations and look for opportunities to improve their efficiency. In this project, we developed new methods for automatic differentation that take advantage of randomness to make them more efficient in terms of computation and memory. These new methods are just as capable as what has previously been done, but can make better use of resources. On some problems, they use orders of magnitude less memory while giving the same performance.\r\n\n\nThis work has also led to new ideas that do not just make automatic differentiation faster, but that also make it possible to use it on problems for which it previously was impossible. In particular, many kinds of machine learning approaches to physical systems---electromagnetics, optics, mechanics---have geometric structures that prevent conventional methods from working. Specifically, they involve performing integration of volumes and surfaces with hard boundaries. Solutions to these problems require reasoning about how the boundaries should move, but this is something that has not been possible to do automatically. Our new method of Fiber Monte Carlo solves this problem, and we are already seeing multiple new domains where it is applicable, making it possible to use machine learning to, e.g., design new photonic circuits or architectural structures.\r\n\n\nWe have put these new methods to work in several collaborations with scientists in other disciplines, making it possible to explore completely new ideas that would have been unthinkable even a few years ago. In particular, we have explored new ideas for mechanical meta-materials that can have interesting nonlinear properties which cannoth be achieved by materials found in nature.\r\n\n\n\t\t\t\t\tLast Modified: 01/21/2025\n\n\t\t\t\t\tSubmitted by: RyanPAdams\n"
 }
}