{
 "awd_id": "2028930",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: PPoSS: Planning: Performance Scalability, Trust, and Reproducibility: A Community Roadmap to Robust Science  in High-throughput Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 70000.0,
 "awd_amount": 70000.0,
 "awd_min_amd_letter_date": "2020-08-21",
 "awd_max_amd_letter_date": "2021-08-23",
 "awd_abstract_narration": "This project is focused on a critical issue in computational science. As scientists in all fields increasingly rely on high-throughput applications (which combine multiple components into increasingly complex multi-modal workflows on heterogeneous systems), the increasing complexities of those applications hinder the scientists\u2019 ability to generate robust results.  The project recruits a cross-disciplinary community working together to define, design, implement, and use a set of solutions for robust science.  In so doing, the community defines a roadmap that enables high-throughput applications to withstand and overcome adverse conditions such as heterogeneous, unreliable architectures at all scales including extreme scale, rigorous testing under uncertainties, unexplainable algorithms (e.g., in machine learning), and black-box methods. The project\u2019s novelties are its comprehensive, cross-disciplinary study of high-throughput applications for robust scientific discovery from hardware and systems all the way to policies and practices.\r\n\r\nThrough three virtual mini-workshops called virtual world cafes, this project engages a community of scientists at campuses (through the Computing Alliance of Hispanic-Serving Institutions [CAHSI], the Coalition for Academic Scientific Computing [CASC], and the Southern California Earthquake Center [SCEC]), at national laboratories, and in industry. The scientists participate in  defining scalability, trust, and reproductivity in an initial set of high-throughput applications; identifying a set of experimental practices that support the in-concert successful progress of these applications\u2019 workflows; advancing towards a vision of general hardware and software solutions for robust science by evaluating the generality and transferability of experimental practices and by identifying any missing parts; and defining a research agenda for the next-generation workflows.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ewa",
   "pi_last_name": "Deelman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ewa Deelman",
   "pi_email_addr": "deelman@isi.edu",
   "nsf_id": "000119337",
   "pi_start_date": "2020-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Rafael",
   "pi_last_name": "Ferreira da Silva",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rafael Ferreira da Silva",
   "pi_email_addr": "rafsilva@isi.edu",
   "nsf_id": "000663889",
   "pi_start_date": "2020-08-21",
   "pi_end_date": "2021-08-23"
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "4676 Admiralty Way, Ste 1001",
  "perf_city_name": "Marina del Rey",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "902926611",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "042Y00",
   "pgm_ele_name": "PPoSS-PP of Scalable Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 70000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ef90c5ec-7fff-1ac6-ae7b-c806d9a56382\">\n<p dir=\"ltr\"><span>High-throughput applications measure their performance in computational throughput using distributed resources and are vital for scientific discovery. These applications are also increasingly complex, combining multiple components: data generation; data collection and merging; data pre-processing and feature extraction; data analysis and modeling; and data verification, validation, and visualization. Adverse conditions, such as heterogeneous, unreliable architectures at all scales, including extreme scale, testing under uncertainty, black-box methods, and unexplainable algorithms, hinder the ability of scientists to generate robust science.&nbsp; Robust science uses research methods that are scalable, reproducible, and trustworthy for generalizable solutions.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>There are three essential requirements to achieve robust science in high-throughput applications:&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>Performance scalability: </span><span>High-throughput applications must meet hardware and software performance expectations when executed, despite heterogeneous resources and large-scale systems. Performance scalability can be enhanced by using consistent metrics and methods to measure computing experiments and deploying rigorous scheduling and resource provisioning models to map tasks to available infrastructure efficiently.</span></p>\n<p dir=\"ltr\"><span>&nbsp;&nbsp;&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>Reproducibility:</span><span> Scientists must be able to draw the same scientific conclusions using the knowledge encapsulated in the original computational experiment. In the ICERM report, this was referred to as confirmable research. Reproducibility can be accomplished by verifying and leveraging others&rsquo; findings, supporting and exploring alternative methods, and explaining algorithms.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>Trustworthiness: </span><span>Scientists must trust the technology, people, and organizations delivering their scientific discoveries.&nbsp; Trust can be accomplished by providing software and data security solutions while supplying the necessary attributes for confidence in the scientist&rsquo;s results and results from others.</span></p>\n<br />\n<p dir=\"ltr\"><span>These three requirements are the driving factors in any roadmap to pursuing robust science. Specifically, scientists should target performance scalability (both spatial and temporal) as a metric of success to meet the scalability requirements; correctness (by overcoming data corruption, faulty software, and system failures) as metrics of success to meet the trust requirements; and modeling accuracy at a range (e.g., through verification and validation) as a metric of success to meet the reproducibility requirements. These findings are the outcome of two virtual mini-workshops in February and May of 2021 called Virtual World Cafes (VWC) based on the world cafe method. The two VWC engaged application communities to share needs and recommendations through structured conversational processes. Participants were distributed across several breakout sessions in an online meeting, switching sessions periodically and getting introduced to the previous discussion at their new session by a session lead.</span></p>\n<br />\n<p dir=\"ltr\"><span>Overall, a successful roadmap to robust science for high-throughput applications builds on increasingly complex multi-modal workflows. The first step to success in delivering scientific discovery for these applications is to establish a vibrant next-generation community that works together to define, design, implement, and use robust solutions. The second step is to build those solutions to span five critical areas: architecture; systems; high-performance computing; programming models and compilers; and algorithms and theory. The last step comprises combining these areas into an integrated continuum through AI orchestrations, policies, and practices accessible to the newly created communities.&nbsp;</span></p>\n<br /><br /><br /><br /><br /><br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/30/2023<br>\n\t\t\t\t\tModified by: Ewa&nbsp;Deelman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nHigh-throughput applications measure their performance in computational throughput using distributed resources and are vital for scientific discovery. These applications are also increasingly complex, combining multiple components: data generation; data collection and merging; data pre-processing and feature extraction; data analysis and modeling; and data verification, validation, and visualization. Adverse conditions, such as heterogeneous, unreliable architectures at all scales, including extreme scale, testing under uncertainty, black-box methods, and unexplainable algorithms, hinder the ability of scientists to generate robust science.  Robust science uses research methods that are scalable, reproducible, and trustworthy for generalizable solutions. \n\n\nThere are three essential requirements to achieve robust science in high-throughput applications: \n\n\nPerformance scalability: High-throughput applications must meet hardware and software performance expectations when executed, despite heterogeneous resources and large-scale systems. Performance scalability can be enhanced by using consistent metrics and methods to measure computing experiments and deploying rigorous scheduling and resource provisioning models to map tasks to available infrastructure efficiently.\n    \nReproducibility: Scientists must be able to draw the same scientific conclusions using the knowledge encapsulated in the original computational experiment. In the ICERM report, this was referred to as confirmable research. Reproducibility can be accomplished by verifying and leveraging others\u2019 findings, supporting and exploring alternative methods, and explaining algorithms. \n\n\nTrustworthiness: Scientists must trust the technology, people, and organizations delivering their scientific discoveries.  Trust can be accomplished by providing software and data security solutions while supplying the necessary attributes for confidence in the scientist\u2019s results and results from others.\n\n\nThese three requirements are the driving factors in any roadmap to pursuing robust science. Specifically, scientists should target performance scalability (both spatial and temporal) as a metric of success to meet the scalability requirements; correctness (by overcoming data corruption, faulty software, and system failures) as metrics of success to meet the trust requirements; and modeling accuracy at a range (e.g., through verification and validation) as a metric of success to meet the reproducibility requirements. These findings are the outcome of two virtual mini-workshops in February and May of 2021 called Virtual World Cafes (VWC) based on the world cafe method. The two VWC engaged application communities to share needs and recommendations through structured conversational processes. Participants were distributed across several breakout sessions in an online meeting, switching sessions periodically and getting introduced to the previous discussion at their new session by a session lead.\n\n\nOverall, a successful roadmap to robust science for high-throughput applications builds on increasingly complex multi-modal workflows. The first step to success in delivering scientific discovery for these applications is to establish a vibrant next-generation community that works together to define, design, implement, and use robust solutions. The second step is to build those solutions to span five critical areas: architecture; systems; high-performance computing; programming models and compilers; and algorithms and theory. The last step comprises combining these areas into an integrated continuum through AI orchestrations, policies, and practices accessible to the newly created communities. \n\n\n\n\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 01/30/2023\n\n\t\t\t\t\tSubmitted by: Ewa Deelman"
 }
}