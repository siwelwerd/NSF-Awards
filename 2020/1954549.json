{
 "awd_id": "1954549",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: SHF: Medium: TensorNN: An Algorithm and Hardware Co-design Framework for On-device Deep Neural Network Learning using Low-rank Tensors",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2020-04-29",
 "awd_max_amd_letter_date": "2022-08-25",
 "awd_abstract_narration": "Deep neural network (DNN) is an important Artificial Intelligence (AI) technique and it has recently gained widespread applications in numerous fields such as image recognition, machine translation, autonomous vehicles and healthcare diagnosis. Conventional DNNs are implemented using cloud computing, where a large amount of computing resource is available in a centrally-pooled manner. In order to achieve stronger data privacy, less response time and relaxed data transmission burden, deploying DNN functionality in a distributed manner at the edges of the network has become a very attractive proposition. However, DNN-learning on mobile devices that are at the edge of the network is very challenging due to conflicting requirements of large time and energy consumption, and limited on-device resources. In order to address this challenge, this project leverages low-rank tensors as a powerful mathematical tool for representing and compressing tensor-format data, to form a new family of ultra-low cost deep neural networks. This brings an order-of-magnitude reduction in time and energy consumption for deep neural network learning. Investigations in many areas of BigData research will benefit as well. This project involves graduate and undergraduate students, especially from underrepresented groups, through summer research experiences, and senior design projects to broaden the participation of computing. The outcomes of this project will be disseminated to the community in the format of technical publications, talks and tutorials in both academic institutions and industry.\r\n\r\nIn order to remove the barriers of realizing real-time energy-efficient DNN-learning on the resource and energy-constrained embedded devices, this project considers innovations at three levels: 1) at theory level, it develops a novel redundancy-free matrix-vector multiplication scheme to reduce computational cost, including a new online update scheme for low-rank tensors to enable fast compressed data update; 2) at algorithm level, it develops low-rank tensor-based forward and backward propagation schemes to support low-cost accelerated inference and training, including catastrophic forgetting-resilient training scheme and training-aware compression scheme to improve the learning robustness and memory efficiency; and 3) at hardware design level, it proposes efficient hardware architecture that fully utilize the benefits provided by low-rank tensors to achieve improved hardware performance for on-device DNN inference and learning. Finally, the efficacy of the proposed research will be validated and evaluated, via software implementations on different DNN models in different target applications. A field-programmable gate array (FPGA)-based hardware prototype will also be developed.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaodong",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaodong Wang",
   "pi_email_addr": "wangx@ee.columbia.edu",
   "nsf_id": "000286898",
   "pi_start_date": "2020-04-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 263075.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 136925.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we have developed fundamental limits, novel optimization formulations, and provably efficient algorithms to extract information from multi-way (i.e., tensor) datasets, and to facilitate efficient real-world machine learning tasks.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual merits:</strong> We have made significant technical contributions in the following three areas. &nbsp;(1) <em>Fundamental limits for tensor inverse problems:</em> We have investigated the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries, for finite completability of a low-rank tensor given some components of its rank. To find the deterministic necessary and sufficient conditions, we have developed an algebraic geometric analysis of the underlying manifold, which allows us to incorporate multiple rank components in the proposed analysis in contrast with the conventional geometric approaches on the Grassmannian manifold. This analysis characterizes the algebraic independence of a set of polynomials defined based on the sampling pattern, which is closely related to the finite completability of the sampled tensor. Probabilistic conditions are then studied and a lower bound on the sampling probability is given, which guarantees that the proposed deterministic conditions on the sampling patterns for finite completability hold with high probability. Furthermore, using the proposed geometric approach for finite completability, we have developed a sufficient condition on the sampling pattern that ensures there exists exactly one completion of the sampled tensor. (2)<em> Communication-free distributed deep learning via spectral tensor layers:</em> We have proposed a novel spectral tensor layer framework for communication-free distributed deep learning. The overall architecture is as follows: first, we represent the data in tensor form (instead of vector form) and replace the matrix product in conventional neural networks with the tensor product, which in effect imposes certain transformed-induced structure on the original weight matrices, e.g., a block-circulant structure; then, we apply a linear transform along a certain dimension to split the original dataset into multiple spectral sub-datasets; as a result, the proposed spectral tensor network consists of parallel branches where each branch is a conventional neural network trained on a spectral sub dataset with zero communication overhead. The parallel branches are directly ensembled (i.e., the weighted sum of their outputs) to generate an overall network with substantially stronger generalization capability than that of each branch. Moreover, the proposed method enjoys a byproduct of decentralization gain in terms of memory and computation, compared with traditional networks. (3<em>) A curriculum learning approach to large-scale optimization:</em> We have investigated<em> </em>neural networks&rsquo; ability to approximate the solution map of certain classes of non-convex optimization problems. The model is trained in an unsupervised manner to map a given problem instance to a near-optimal solution. Training is offline so that online optimization requires only feedforward computation, the complexity of which is orders of magnitude less than state-of-the-art optimization algorithms. To obtain a near-optimal solution mapping, either of two curriculum learning strategies is required: The reward curriculum employs a sequence of learning objectives of increasing complexity. The subspace curriculum employs a sequence of training data distributions restricting the data to linear subspaces of increasing dimension.</p>\n<p>&nbsp;</p>\n<p><strong>Broader impacts:</strong> We have involved a number of undergraduates in summer research. One female Ph.D student (Jiaai Liu) and one female post doc (Ye Hu) have worked on this project. We have also interacted with an established K-12 outreach program, the Center for Technology, Innovation &amp; Community Engagement (CTICE), that recruits students from Harlem in New York City (in which the student population is&gt; 85%composed of under-represented groups) and prepares them for challenging STEM careers. CTICE enrolls students of a broad range of ages (starting at the first grade) and provides innovative hands-on classroom and field programs to excite and engage students in various science and technology projects<em>.</em></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/26/2024<br>\nModified by: Xiaodong&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1954549/1954549_10666996_1727397757021_Screenshot_2024_09_26_at_6.53.02__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1954549/1954549_10666996_1727397757021_Screenshot_2024_09_26_at_6.53.02__8239_PM--rgov-800width.png\" title=\"Spectral tensor NN\"><img src=\"/por/images/Reports/POR/2024/1954549/1954549_10666996_1727397757021_Screenshot_2024_09_26_at_6.53.02__8239_PM--rgov-66x44.png\" alt=\"Spectral tensor NN\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Structure of our spectral tensor neural network for communication-free distributed deep learning.</div>\n<div class=\"imageCredit\">Xiaodong Wang</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Xiaodong&nbsp;Wang\n<div class=\"imageTitle\">Spectral tensor NN</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, we have developed fundamental limits, novel optimization formulations, and provably efficient algorithms to extract information from multi-way (i.e., tensor) datasets, and to facilitate efficient real-world machine learning tasks.\n\n\n\n\n\nIntellectual merits: We have made significant technical contributions in the following three areas. (1) Fundamental limits for tensor inverse problems: We have investigated the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries, for finite completability of a low-rank tensor given some components of its rank. To find the deterministic necessary and sufficient conditions, we have developed an algebraic geometric analysis of the underlying manifold, which allows us to incorporate multiple rank components in the proposed analysis in contrast with the conventional geometric approaches on the Grassmannian manifold. This analysis characterizes the algebraic independence of a set of polynomials defined based on the sampling pattern, which is closely related to the finite completability of the sampled tensor. Probabilistic conditions are then studied and a lower bound on the sampling probability is given, which guarantees that the proposed deterministic conditions on the sampling patterns for finite completability hold with high probability. Furthermore, using the proposed geometric approach for finite completability, we have developed a sufficient condition on the sampling pattern that ensures there exists exactly one completion of the sampled tensor. (2) Communication-free distributed deep learning via spectral tensor layers: We have proposed a novel spectral tensor layer framework for communication-free distributed deep learning. The overall architecture is as follows: first, we represent the data in tensor form (instead of vector form) and replace the matrix product in conventional neural networks with the tensor product, which in effect imposes certain transformed-induced structure on the original weight matrices, e.g., a block-circulant structure; then, we apply a linear transform along a certain dimension to split the original dataset into multiple spectral sub-datasets; as a result, the proposed spectral tensor network consists of parallel branches where each branch is a conventional neural network trained on a spectral sub dataset with zero communication overhead. The parallel branches are directly ensembled (i.e., the weighted sum of their outputs) to generate an overall network with substantially stronger generalization capability than that of each branch. Moreover, the proposed method enjoys a byproduct of decentralization gain in terms of memory and computation, compared with traditional networks. (3) A curriculum learning approach to large-scale optimization: We have investigated neural networks ability to approximate the solution map of certain classes of non-convex optimization problems. The model is trained in an unsupervised manner to map a given problem instance to a near-optimal solution. Training is offline so that online optimization requires only feedforward computation, the complexity of which is orders of magnitude less than state-of-the-art optimization algorithms. To obtain a near-optimal solution mapping, either of two curriculum learning strategies is required: The reward curriculum employs a sequence of learning objectives of increasing complexity. The subspace curriculum employs a sequence of training data distributions restricting the data to linear subspaces of increasing dimension.\n\n\n\n\n\nBroader impacts: We have involved a number of undergraduates in summer research. One female Ph.D student (Jiaai Liu) and one female post doc (Ye Hu) have worked on this project. We have also interacted with an established K-12 outreach program, the Center for Technology, Innovation & Community Engagement (CTICE), that recruits students from Harlem in New York City (in which the student population is 85%composed of under-represented groups) and prepares them for challenging STEM careers. CTICE enrolls students of a broad range of ages (starting at the first grade) and provides innovative hands-on classroom and field programs to excite and engage students in various science and technology projects.\n\n\n\t\t\t\t\tLast Modified: 09/26/2024\n\n\t\t\t\t\tSubmitted by: XiaodongWang\n"
 }
}