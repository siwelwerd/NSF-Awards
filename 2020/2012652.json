{
 "awd_id": "2012652",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Deep Neural Networks for Structured Data: Regression, Distribution Estimation, and Optimal Transport",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 342394.0,
 "awd_amount": 342394.0,
 "awd_min_amd_letter_date": "2020-05-26",
 "awd_max_amd_letter_date": "2020-05-26",
 "awd_abstract_narration": "In the past decade, deep learning has made astonishing breakthroughs in real-world applications, including for example, computer vision, natural language processing, speech recognition, healthcare, and robotics. Deep learning uses multiple layers of linear transformations followed by nonlinear activations to represent abstractions in the data.  It is a common belief that deep neural networks are good at learning various geometric structures hidden in data sets, such as rich local regularities, global symmetries, or repetitive patterns. However, little theory has been established to explain the power of deep neural networks for analyzing complex data sets containing geometric structures. This project will develop the theoretical and computational foundations to better understand how deep neural networks exploit geometric structures in data sets and achieve outstanding performance. The results will provide new insights on developing new deep learning models and methodologies.\r\n\r\nThis project focuses on three sets of related but distinct problems. The first set focuses on efficient approximation of functions on low-dimensional manifolds using deep neural networks. Existing theories show that deep learning estimators converge to the true function extremely slowly in high dimensions. When the function is supported on a low dimensional manifold, the PIs plan to prove a fast convergence rate depending on the intrinsic dimension of the manifold. This project will make contributions in function approximation theory, error analysis in statistical regression and classification, and adaptive theory of deep learning. The second set of problems concerns estimation of probability distributions supported on a low-dimensional manifold by deep generative models. These models utilize two neural networks to minimize the Integral Probability Metric (IPM) between the estimator and the data distribution, over the class of distributions generated by a deep generator network. The function class in IPM is realized by a deep discriminator network. This project will design proper network architectures of the generator and the discriminator, and prove performance guarantees of deep generative models. The third set of problems will focus on efficiently computing the optimal transport between two probability distributions using deep neural networks. After reformulating the optimal transport problem as a min-max optimization problem parametrized by two neural networks, the PIs propose a primal dual stochastic gradient descent algorithm to solve it.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wenjing",
   "pi_last_name": "Liao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wenjing Liao",
   "pi_email_addr": "wliao60@gatech.edu",
   "nsf_id": "000756125",
   "pi_start_date": "2020-05-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tuo",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tuo Zhao",
   "pi_email_addr": "tzhao80@gatech.edu",
   "nsf_id": "000754962",
   "pi_start_date": "2020-05-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 342394.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p id=\"docs-internal-guid-079c262a-7fff-6333-7d30-645ec38644fd\" style=\"line-height: 1.38; text-align: justify; margin-top: 14pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project developed theoretical and computational foundations to understand how deep neural networks exploit low-dimensional geometric structures of data and achieve outstanding performances.&nbsp;</span></p>\r\n<p style=\"line-height: 1.38; text-align: justify; margin-top: 14pt; margin-bottom: 12pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Mathematical theories were developed to show that deep neural networks are adaptive to low-dimensional geometric structures in data sets. Specifically, the PIs studied four sets of fundamental problems in machine learning and computational mathematics: function approximation, distribution estimation, optimal transport and causal inference. On function approximation, an efficient universal approximation theory was established for Holder or Besov functions on a low-dimensional manifold. Three network architectures were constructed for such function approximation: a feedforward ReLU network, a convolutional residual network and a transformer neural network. On distribution estimation, the sample complexity of Generative Adversarial Networks (GANs) was analyzed for the estimation of distributions with a Holder density. On optimal transport, a scalable and efficient computational tool was developed to compute large scale optimal transport using deep generative models, and statistical guarantees of generative models were established using optimal transport theory. On causal inference, statistical guarantees of policy learning in causal inference using neural networks were established when the covariates are sampled on a low-dimensional manifold.</span></p>\r\n<p style=\"line-height: 1.6199999999999999; text-align: justify; margin-top: 8pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project resulted in 18 articles, including 5 journal publications, 2 journal submissions, 10 proceedings in top machine learning conferences (NeurIPS 2019, 2022, 2024, ICML 2021, 2022,2023 and ICLR 2022), and 1 preprint. These papers were co-authored with the PI&rsquo;s students and postdocs.</span></p>\r\n<p style=\"line-height: 1.6199999999999999; text-align: justify; margin-top: 8pt; margin-bottom: 8pt;\" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial,sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">This project included a strong educational component. It supported 5 PhD students and 1 postdoc. These young researchers are successful in their career. One student co-advised by the PIs graduated in 2022 and is currently a tenure-track assistant professor at Northwestern University. The postdoc is currently a tenure-track assistant professor at Hong Kong Baptist University. The PIs have been actively organizing and participating in research activities for graduate, undergraduate and high school students. The PIs also demonstrated strong commitment to service through their active engagement in academic, professional, and community-oriented activities.</span></p><br>\n<p>\n Last Modified: 12/30/2024<br>\nModified by: Wenjing&nbsp;Liao</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project developed theoretical and computational foundations to understand how deep neural networks exploit low-dimensional geometric structures of data and achieve outstanding performances.\r\n\n\nMathematical theories were developed to show that deep neural networks are adaptive to low-dimensional geometric structures in data sets. Specifically, the PIs studied four sets of fundamental problems in machine learning and computational mathematics: function approximation, distribution estimation, optimal transport and causal inference. On function approximation, an efficient universal approximation theory was established for Holder or Besov functions on a low-dimensional manifold. Three network architectures were constructed for such function approximation: a feedforward ReLU network, a convolutional residual network and a transformer neural network. On distribution estimation, the sample complexity of Generative Adversarial Networks (GANs) was analyzed for the estimation of distributions with a Holder density. On optimal transport, a scalable and efficient computational tool was developed to compute large scale optimal transport using deep generative models, and statistical guarantees of generative models were established using optimal transport theory. On causal inference, statistical guarantees of policy learning in causal inference using neural networks were established when the covariates are sampled on a low-dimensional manifold.\r\n\n\nThis project resulted in 18 articles, including 5 journal publications, 2 journal submissions, 10 proceedings in top machine learning conferences (NeurIPS 2019, 2022, 2024, ICML 2021, 2022,2023 and ICLR 2022), and 1 preprint. These papers were co-authored with the PIs students and postdocs.\r\n\n\nThis project included a strong educational component. It supported 5 PhD students and 1 postdoc. These young researchers are successful in their career. One student co-advised by the PIs graduated in 2022 and is currently a tenure-track assistant professor at Northwestern University. The postdoc is currently a tenure-track assistant professor at Hong Kong Baptist University. The PIs have been actively organizing and participating in research activities for graduate, undergraduate and high school students. The PIs also demonstrated strong commitment to service through their active engagement in academic, professional, and community-oriented activities.\t\t\t\t\tLast Modified: 12/30/2024\n\n\t\t\t\t\tSubmitted by: WenjingLiao\n"
 }
}