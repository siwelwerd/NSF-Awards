{
 "awd_id": "2011463",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Causal Inference: Identification, Learning, and Decision-Making",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 341925.0,
 "awd_amount": 341925.0,
 "awd_min_amd_letter_date": "2019-12-17",
 "awd_max_amd_letter_date": "2019-12-17",
 "awd_abstract_narration": "Understanding the causal mechanisms underlying an observed phenomenon is one of the primary goals of science. The realization that statistical associations in themselves are insufficient for elucidating those mechanisms has led researchers to enrich traditional statistical analysis with techniques based on \"causal inference\". Most of the recent advances in the field, however, operate under overly optimistic assumptions, which are often not met in practical, large-scale situations. This project seeks to develop a sound and general causal inference theory to cover those situations. The goal is to design a framework for decision-making of intelligent systems, including (1) learning a causal representation of the data-generating environment (learning), (2) performing efficient inference leveraging the learned model (planning/inference), and (3) using the new inferred representation, based on (1) and (2), to decide how to act next (decision-making). The new finding will benefit investigators in every area of the empirical sciences, including artificial intelligence, machine learning, statistics, economics, and the health and social sciences. The research is expected to fundamentally change the practice of data science in areas where the standard causal assumptions are violated (i.e., missing data, selection bias, and confounding bias). The work on decision-making is expected to pave the way toward the design of an \"automated scientist\", i.e., a program that combines both observational and experimental data, conducts its own experiments, and decides on the best choices of actions and policies. The project also helps to disseminate the principles of causal inference throughout the sciences by (1) engaging in the establishment of new \"data science\" curriculum where causal inference plays a central role, and (2) developing new educational materials for students and the general public explaining the practice of causal inference (e.g., book). Furthermore, the project supports the causal inference community by fostering a number of educational initiatives such as forums, workshops, and the creation of new incentives for the development of educational material (e.g., a \"Causality Education Award\").\r\n\r\nMaking claims about the existence of causal connections (structural learning), the magnitude of causal effects (identification), and designing optimal interventions (decision-making) are some of the most important tasks found throughout data-driven fields. This project studies identification, learning, and decision-making settings where (1) data are missing not at random, (2) non-parametric estimation is not feasible, and (3) aggregated behavior does not translate into guidance for individual-level decision-making. Specifically, the project considers the problem when measurements are systematically distorted (missing data), which has received an enormous amount of attention in the statistical literature, but has not essentially been investigated in the context of causal inference when data are missing not at random. The project further aims to leverage the special properties of linear models, the most common first approximation to non-parametric causal inference, to elucidate causal relationships in data, and to facilitate sensitivity analysis in such models.  Finally, the project considers the fundamental problem on how causal and counterfactual knowledge can speed-up experimentation and support principled decision-making. The goal is to develop a complete algorithmic theory to determine when a particular causal effect can be learned from data and how to incorporate causal knowledge learned (possibly by experimentation) so that it can be amortized over new environmental conditions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Elias",
   "pi_last_name": "Bareinboim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elias Bareinboim",
   "pi_email_addr": "eb@cs.columbia.edu",
   "nsf_id": "000717905",
   "pi_start_date": "2019-12-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "500 W 120th Street",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 341925.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Inferring causal effects from large data collections is a fundamental challenge found throughout the data sciences. There is a growing literature trying to understand the conditions under which causal conclusions can be drawn from a combination of observational and experimental datasets. This project focuses on three tightly-connected topics of causality research, namely, 1) identification of causal distributions, 2) learning a qualitative description of the underlying causal mechanisms (i.e., constructing a causal explanation), and making decisions based on uncertain and only partially specified causally knowledge about the underlying environment. The results developed in this project have appeared in dozens of papers in the leading artificial intelligence and machine learning venues and were disseminated to talks, lectures, workshops, and tutorials. We list below some of these accomplishments.&nbsp;</p>\n<p><br /><span>A) Identification</span><br /><span>Causal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. We investigated the generalization of this problem in many different and pervasive dimensions.</span><br /><span>A-1 Partial-observability: Most of the results of this literature implicitly assume that every variable modeled in the graph is measured in the available distributions. However, in practice, the data collections of the different studies considered do not measure the same variables consistently. We studied a sufficient graphical criterion for determining whether the effects are identifiable from partially-observed distributions. Building on these graphical properties, we developed an algorithm that returns a formula for a causal effect regarding the available distributions.</span><br /><span>A-2 Stochastic Interventions: Some of the most prominent results in causal inference have been developed in the context of atomic interventions. In practice, many real-world settings require more complex types of interventions that cannot be represented by a simple atomic intervention. We investigated and developed machinery that covers a general class of interventions covering some non-trivial kinds of policies. Specifically, we introduced a new set of inference rules (akin to the celebrated Pearl's do-calculus) that can be used to derive claims about general interventions, which we call sigma calculus.</span><br /><span>A-3 Markov equivalence: We studied identification when the causal graph is not given but can be partially inferred from given data. We derived an algorithm to identify conditional causal effects, particularly useful for evaluating conditional plans, policies, and more individualized treatments.</span><br /><span>A-4 Weighting-Based Estimators: There is a gap between causal effect identification and estimation. We extended weighting-based methods to more general settings, and developed novel machinery for estimating causal effects using the weighting-based method as a building block. These estimators were shown to be extremely effective in simulated studies.</span><br /><span>A-5 Linear Structural Causal Models: We investigated graphical conditions to allow efficient identification in arbitrary linear structural causal models. In particular, we developed a method to efficiently find unconditioned instrumental subsets, which are generalizations of instrumental variables that can be used to tame the complexity of many canonical algorithms found in the literature. We introduced a new, efficient, and most general identification criterion called Instrumental Cutsets (IC).</span><br /><br /><span>B. Learning</span><br /><span>The challenge of learning the causal structure underlying a particular phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation. We introduced a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions. We provided a formal graphical characterization of this equivalence. We further extended the FCI algorithm, originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting.</span><br /><br /><span>C. Decision-Making</span><br /><span>A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine patients' treatment assignment based on evolving treatments and covariates' history. These regimes are particularly useful for managing chronic disorders and are arguably one of the critical aspects of developing more personalized decision-making systems.</span><br /><span>C-1: We investigated the online reinforcement learning (RL) problem for selecting optimal DTRs provided that observational data is available. We developed the first adaptive algorithm that achieves near-optimal regret in DTRs in online settings without access to historical data. We further derived informative bounds on the system dynamics of the underlying DTR from confounded, observational data.</span><br /><span>C-2: We also showed that if the causal diagram of the underlying environment is provided, one could achieve regret that is exponentially smaller than traditional bounds. In particular, we developed two online algorithms that satisfy such regret bounds by exploiting the DTR's causal structure.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/09/2020<br>\n\t\t\t\t\tModified by: Elias&nbsp;Bareinboim</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nInferring causal effects from large data collections is a fundamental challenge found throughout the data sciences. There is a growing literature trying to understand the conditions under which causal conclusions can be drawn from a combination of observational and experimental datasets. This project focuses on three tightly-connected topics of causality research, namely, 1) identification of causal distributions, 2) learning a qualitative description of the underlying causal mechanisms (i.e., constructing a causal explanation), and making decisions based on uncertain and only partially specified causally knowledge about the underlying environment. The results developed in this project have appeared in dozens of papers in the leading artificial intelligence and machine learning venues and were disseminated to talks, lectures, workshops, and tutorials. We list below some of these accomplishments. \n\n\nA) Identification\nCausal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. We investigated the generalization of this problem in many different and pervasive dimensions.\nA-1 Partial-observability: Most of the results of this literature implicitly assume that every variable modeled in the graph is measured in the available distributions. However, in practice, the data collections of the different studies considered do not measure the same variables consistently. We studied a sufficient graphical criterion for determining whether the effects are identifiable from partially-observed distributions. Building on these graphical properties, we developed an algorithm that returns a formula for a causal effect regarding the available distributions.\nA-2 Stochastic Interventions: Some of the most prominent results in causal inference have been developed in the context of atomic interventions. In practice, many real-world settings require more complex types of interventions that cannot be represented by a simple atomic intervention. We investigated and developed machinery that covers a general class of interventions covering some non-trivial kinds of policies. Specifically, we introduced a new set of inference rules (akin to the celebrated Pearl's do-calculus) that can be used to derive claims about general interventions, which we call sigma calculus.\nA-3 Markov equivalence: We studied identification when the causal graph is not given but can be partially inferred from given data. We derived an algorithm to identify conditional causal effects, particularly useful for evaluating conditional plans, policies, and more individualized treatments.\nA-4 Weighting-Based Estimators: There is a gap between causal effect identification and estimation. We extended weighting-based methods to more general settings, and developed novel machinery for estimating causal effects using the weighting-based method as a building block. These estimators were shown to be extremely effective in simulated studies.\nA-5 Linear Structural Causal Models: We investigated graphical conditions to allow efficient identification in arbitrary linear structural causal models. In particular, we developed a method to efficiently find unconditioned instrumental subsets, which are generalizations of instrumental variables that can be used to tame the complexity of many canonical algorithms found in the literature. We introduced a new, efficient, and most general identification criterion called Instrumental Cutsets (IC).\n\nB. Learning\nThe challenge of learning the causal structure underlying a particular phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation. We introduced a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions. We provided a formal graphical characterization of this equivalence. We further extended the FCI algorithm, originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting.\n\nC. Decision-Making\nA dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine patients' treatment assignment based on evolving treatments and covariates' history. These regimes are particularly useful for managing chronic disorders and are arguably one of the critical aspects of developing more personalized decision-making systems.\nC-1: We investigated the online reinforcement learning (RL) problem for selecting optimal DTRs provided that observational data is available. We developed the first adaptive algorithm that achieves near-optimal regret in DTRs in online settings without access to historical data. We further derived informative bounds on the system dynamics of the underlying DTR from confounded, observational data.\nC-2: We also showed that if the causal diagram of the underlying environment is provided, one could achieve regret that is exponentially smaller than traditional bounds. In particular, we developed two online algorithms that satisfy such regret bounds by exploiting the DTR's causal structure.\n\n\t\t\t\t\tLast Modified: 09/09/2020\n\n\t\t\t\t\tSubmitted by: Elias Bareinboim"
 }
}