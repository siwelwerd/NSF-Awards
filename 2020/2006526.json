{
 "awd_id": "2006526",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: AF: Small: Parallel Reinforcement Learning with Communication and Adaptivity Constraints",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922095",
 "po_email": "kwimmer@nsf.gov",
 "po_sign_block_name": "Karl Wimmer",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 257745.0,
 "awd_amount": 257745.0,
 "awd_min_amd_letter_date": "2020-08-05",
 "awd_max_amd_letter_date": "2020-08-05",
 "awd_abstract_narration": "Reinforcement learning has witnessed great research advancement in recent years and achieved successes in many practical applications.  However, reinforcement-learning algorithms also have the reputation for being data- and computation-hungry for large-scale applications.  This project will address this issue by studying the important question of how to make reinforcement-learning algorithms scalable via introducing multiple learning agents and allowing them to collect data and learn optimal strategies collaboratively.  The outcomes of this project will have impacts on numerous areas where reinforcement learning is used at a scale, e.g., multi-phase clinical trials, training autonomous-driving algorithms, crowdsourcing tasks, pricing, and assortment optimization for stores at different locations.  The research products will be disseminated via talks at academic conferences and workshops, universities, industrial labs, and online media, and will also be integrated in two courses on the forefront of reinforcement learning and big-data algorithms.\r\n\r\nMore technically, this project will study how to address the fundamental constraints on communication and adaptivity for the learning agents.  In particular, this project will investigate a handful of collaborative learning models, including full communication, synchronized communication, synchronized communication with limited adaptivity, and asynchronized communication, and study the following general questions: (1) what is the fundamental advantage of allowing adaptivity in the parallel learning model; (2) are there inherent differences on the degree of parallelism between model-based and model-free reinforcement learning; (3) what is the impact of asynchronized communication; and (4) is it possible to communication-efficiently parallelize general algorithmic techniques in reinforcement learning?  The team of researchers will address these questions by studying a set of core problems, including best arm(s) identification and regret minimization in multi-armed bandits, contextual bandits, finite-state Markov decision process (MDP) learning, reinforcement learning with function approximates, and coordinated exploration in MDPs.  Through studying these questions, this project will bring new techniques, perspectives, and insight to communication-efficient parallel reinforcement learning.  This project will also have a significant impact on a number of related research areas such as control theory, operations research, information theory and communication complexity, and multi-agent systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuan",
   "pi_last_name": "Zhou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuan Zhou",
   "pi_email_addr": "yuanz@illinois.edu",
   "nsf_id": "000752744",
   "pi_start_date": "2020-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "Board of Trustees of the University of Illinois",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 257745.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span><span>This project has successfully developed optimal solutions to several key problems in online learning, specifically under communication and adaptivity constraints. Notable achievements include identifying the best set of multiple arms in collaborative multi-agent settings, minimizing regret in linearly parameterized bandits while adhering to adaptivity constraints, and deriving the optimal regret bounds for tabular model-free reinforcement learning. These contributions address fundamental challenges in online decision-making processes where balancing exploration (gathering new information) and exploitation (utilizing current knowledge) is critical, especially when adaptivity -- the ability to update decisions based on incoming information -- is limited.</span></span></p>\r\n<p><span><span><span>By systematically analyzing and resolving these problems, this project advances the theoretical understanding of the inherent trade-offs among exploration, exploitation, and adaptivity, which are central to online learning frameworks. The insights gained have significant implications for a variety of real-world applications. For instance, in clinical trials, the results from the earlier phases can inform the decisions of the next phase where the experimental design has to be carefully designed to gather as much information as possible within a limited number of phases. Similarly, in dynamic pricing, inventory control and assortment optimization, the ability to learn, optimize decision while making few adjustments due to operational constraints is crucial. Overall, this project broadens both the theoretical and practical scope of online learning in adaptivity-constrained environments.</span><br /></span></span></p><br>\n<p>\n Last Modified: 01/14/2025<br>\nModified by: Yuan&nbsp;Zhou</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project has successfully developed optimal solutions to several key problems in online learning, specifically under communication and adaptivity constraints. Notable achievements include identifying the best set of multiple arms in collaborative multi-agent settings, minimizing regret in linearly parameterized bandits while adhering to adaptivity constraints, and deriving the optimal regret bounds for tabular model-free reinforcement learning. These contributions address fundamental challenges in online decision-making processes where balancing exploration (gathering new information) and exploitation (utilizing current knowledge) is critical, especially when adaptivity -- the ability to update decisions based on incoming information -- is limited.\r\n\n\nBy systematically analyzing and resolving these problems, this project advances the theoretical understanding of the inherent trade-offs among exploration, exploitation, and adaptivity, which are central to online learning frameworks. The insights gained have significant implications for a variety of real-world applications. For instance, in clinical trials, the results from the earlier phases can inform the decisions of the next phase where the experimental design has to be carefully designed to gather as much information as possible within a limited number of phases. Similarly, in dynamic pricing, inventory control and assortment optimization, the ability to learn, optimize decision while making few adjustments due to operational constraints is crucial. Overall, this project broadens both the theoretical and practical scope of online learning in adaptivity-constrained environments.\n\t\t\t\t\tLast Modified: 01/14/2025\n\n\t\t\t\t\tSubmitted by: YuanZhou\n"
 }
}