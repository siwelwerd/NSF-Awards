{
 "awd_id": "2008590",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Adaptive rendering and display for emerging immersive experiences",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922533",
 "po_email": "hshen@nsf.gov",
 "po_sign_block_name": "Han-Wei Shen",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 497177.0,
 "awd_amount": 497177.0,
 "awd_min_amd_letter_date": "2020-08-14",
 "awd_max_amd_letter_date": "2020-10-19",
 "awd_abstract_narration": "The fundamental architecture for rendering information on TVs and monitors has changed very little: the information is still sent as pixels, scanned row by row into a sequence of images called frames. At the same time, an increasingly important class of immersive applications is emerging: socially rich videoconferencing supports the nonverbal communication; in-situ learning has remote students using augmented reality to see lessons in context, such as biology in the garden; teleoperation allows doctors to deliver healthcare at distance. Unfortunately, traditional display architecture cannot deliver crucial features these applications require, including life-size, zero-delay remote windows; and multi-viewer, glasses-free experiences. This project investigates transformative, adaptive display architectures that might support these features. By designing an abstract \u201cideal display\u201d using an API (Application Programming Interface), the research team will enable exploration of a diverse range of underlying display architectures for supporting emerging applications. In this process, it will convene workshops to build the multidisciplinary research relationships needed to meet these goals and begin addressing the dearth of women and minorities in computing with a special effort in the university's computer graphics class. \r\n\r\nTo rekindle innovation in displays, the project will focus on five non-traditional, adaptive display techniques: frameless rendering, selective updates, local storage and processing, knowledge of the viewer, and high-level representations. Frameless rendering forms images with pixels representing different moments in time. This is particularly effective at reducing delay. Selective updates replace part of the image, rather than all of it, useful when images are too large for a full change. Local storage and processing allow older samples to be stored on the display, then combined with new samples just in time to make imagery with less delay and bandwidth. Higher level primitives like gradients and edges reduce bandwidth by succinctly describing local pixel distributions. Finally, knowledge of the viewer allows displays to render information only where and when it is needed, for example by using eye tracking. The project will apply these techniques toward three goals. First, to define an open API for the ideal perceptual display, which saturates all of the visual sense. This effort will drive future research by identifying underserved perceptual sensitivities, clarifying engineering tradeoffs in actual displays, and increasing the market for those displays. The second and third goals are to prototype two adaptive approximations of the ideal perceptual display on hybrid display-FPGA (Field Programmable Gate Arrays) systems, to test and inform our API. A frameless, foveated display will reduce detail in the periphery to reduce delay, particularly for large displays. A foveated light field display will vary color by view angle to support glasses-free stereo and multi-user viewing. The project will evaluate both prototypes with perceptual comparisons of them to more traditional displays.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Watson",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin A Watson",
   "pi_email_addr": "bwatson@ncsu.edu",
   "nsf_id": "000460838",
   "pi_start_date": "2020-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957214",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 497177.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project sought to improve displays for immersive experiences, such as gaming and video conferencing. It used an interdisciplinary approach combining computer graphics and human computer interaction to reexamine how displays represent time, and to integrate viewers more tightly with displays.</p>\r\n<p>The project had four primary research outcomes: rapid point rendering for 3D light field display, improved 3D display quality with real-time predistorted rendering, low-latency scanning for gaming and interactive displays, and gaze-preserving video conferencing.</p>\r\n<p>&nbsp;</p>\r\n<p>RAPID POINT RENDERING FOR 3D LIGHT FIELD DISPLAY</p>\r\n<p>Modern 3D displays require creating multiple views of the same scene (two per viewer). Interactively making large numbers of views is challenging for current graphics hardware. This project met this challenge with a new technique that makes creation of many views five times faster (Figure 1). It achieved this by transforming the scene from its original triangular format into points, and then using those points to create many views in parallel.</p>\r\n<p>&nbsp;</p>\r\n<p>IMPROVED 3D DISPLAY QUALITY WITH REAL-TIME PREDISTORTED RENDERING</p>\r\n<p>Most glasses-free 3D displays today have disturbing visual artifacts, with viewers seeing parts of the wrong views. These artifacts result from poor mathematical modeling of the lenses on 3D screens, which allow a viewer&rsquo;s eyes to see distinct views of the same scene without wearing glasses. This project significantly reduced the artifacts by developing a more accurate model that it evaluates in real time, taking into account the tracked position of the viewer&rsquo;s eyes (Figure 2). The resulting system can work with most commercially available 3D displays, is easily integrated into gaming engines, and is provisionally patented.</p>\r\n<p>&nbsp;</p>\r\n<p>LOW-LATENCY SCANNING FOR GAMING DISPLAYS</p>\r\n<p>In gaming, the player that reacts first most often wins. For this reason, gamers seek to reduce delay, the time it takes for their mouse click to change what is on the display. One source of delay is the top-to-bottom display scan, which puts the image&rsquo;s uppermost pixels onto the HDMI cable first, and the same pixels onto the top of the screen first. For gamers and in fact any users, there is nothing special about these pixels. This project researched random and other scans that more quickly update display areas important to players. Moreover, when players reduce delay by turning off computer-display synchronization, our scans drastically decrease the resulting visual artifacts (Figure 3). With less delay and better imagery, players react more quickly, and win more often. Various implementations of these scans are provisionally patented.</p>\r\n<p>&nbsp;</p>\r\n<p>GAZE-PRESERVING VIDEO CONFERENCING</p>\r\n<p>Video conferencing has become a fixture in our work lives, but is a poor substitute for an in-person meeting. One reason is uncertainty about who is speaking to whom, which causes interruptions, confusion and fatigue. A major cause of this is lack of support for gaze. In person, we look at one another through our own eyes, allowing us to see who is looking at us and who isn't. But during a video conference, we look at each person through a single shared camera, creating uncertainty about the conversation&rsquo;s flow. This project developed a new, inexpensive technique supporting gaze in video conferencing. We are currently refining and testing this technique, with an eye toward commercialization.</p>\r\n<p>&nbsp;</p>\r\n<p>These outcomes were widely disseminated in academic conferences, including both computer graphics and display venues, where some were recognized as distinguished contributions. Three provisional patents have already resulted. The project developed lasting relationships with academic collaborators in Japan and Korea, where much of the display industry is concentrated, including a one year visiting Korean scholar. In addition, the project had beneficial effects on education and training, including mentoring of seven doctoral students, as well as several masters and undergraduate students. Most of the doctoral students attended academic conferences, furthering their academic development. In graduate and undergraduate teaching, the project supported a course component featuring the research of women and minorities.&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 04/22/2025<br>\nModified by: Benjamin&nbsp;A&nbsp;Watson</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341183405_lfdpr_figure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341183405_lfdpr_figure--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341183405_lfdpr_figure--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1: Results generated by our point rendering for 3D light field display. Left column, gold standard (GStd); middle column, our technique (LFDPR); right column, the  previous standard (MVR). These are viewed from the left (top row), center (middle row), and right (bottom row).</div>\n<div class=\"imageCredit\">Ajinkya Gavane</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Benjamin&nbsp;A&nbsp;Watson\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341461559_scanning_figure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341461559_scanning_figure--rgov-800width.jpg\" title=\"Figure 3\"><img src=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341461559_scanning_figure--rgov-66x44.jpg\" alt=\"Figure 3\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3: Results from our low-latency scanning for gaming. Both images show the display while synchronization is off to reduce delay. Left, the standard top-to-bottom scan, with all new information at the bottom. Right, our new random scan, with new information distributed throughout the image.</div>\n<div class=\"imageCredit\">Aaron Fulmer</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Benjamin&nbsp;A&nbsp;Watson\n<div class=\"imageTitle\">Figure 3</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341277347_lfdpre_figure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341277347_lfdpre_figure--rgov-800width.jpg\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2025/2008590/2008590_10697339_1745341277347_lfdpre_figure--rgov-66x44.jpg\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2: Results generated by our real-time predistorted rendering for 3D display. Left, our technique, right the existing standard.</div>\n<div class=\"imageCredit\">Tianyu Wu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Benjamin&nbsp;A&nbsp;Watson\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project sought to improve displays for immersive experiences, such as gaming and video conferencing. It used an interdisciplinary approach combining computer graphics and human computer interaction to reexamine how displays represent time, and to integrate viewers more tightly with displays.\r\n\n\nThe project had four primary research outcomes: rapid point rendering for 3D light field display, improved 3D display quality with real-time predistorted rendering, low-latency scanning for gaming and interactive displays, and gaze-preserving video conferencing.\r\n\n\n\r\n\n\nRAPID POINT RENDERING FOR 3D LIGHT FIELD DISPLAY\r\n\n\nModern 3D displays require creating multiple views of the same scene (two per viewer). Interactively making large numbers of views is challenging for current graphics hardware. This project met this challenge with a new technique that makes creation of many views five times faster (Figure 1). It achieved this by transforming the scene from its original triangular format into points, and then using those points to create many views in parallel.\r\n\n\n\r\n\n\nIMPROVED 3D DISPLAY QUALITY WITH REAL-TIME PREDISTORTED RENDERING\r\n\n\nMost glasses-free 3D displays today have disturbing visual artifacts, with viewers seeing parts of the wrong views. These artifacts result from poor mathematical modeling of the lenses on 3D screens, which allow a viewers eyes to see distinct views of the same scene without wearing glasses. This project significantly reduced the artifacts by developing a more accurate model that it evaluates in real time, taking into account the tracked position of the viewers eyes (Figure 2). The resulting system can work with most commercially available 3D displays, is easily integrated into gaming engines, and is provisionally patented.\r\n\n\n\r\n\n\nLOW-LATENCY SCANNING FOR GAMING DISPLAYS\r\n\n\nIn gaming, the player that reacts first most often wins. For this reason, gamers seek to reduce delay, the time it takes for their mouse click to change what is on the display. One source of delay is the top-to-bottom display scan, which puts the images uppermost pixels onto the HDMI cable first, and the same pixels onto the top of the screen first. For gamers and in fact any users, there is nothing special about these pixels. This project researched random and other scans that more quickly update display areas important to players. Moreover, when players reduce delay by turning off computer-display synchronization, our scans drastically decrease the resulting visual artifacts (Figure 3). With less delay and better imagery, players react more quickly, and win more often. Various implementations of these scans are provisionally patented.\r\n\n\n\r\n\n\nGAZE-PRESERVING VIDEO CONFERENCING\r\n\n\nVideo conferencing has become a fixture in our work lives, but is a poor substitute for an in-person meeting. One reason is uncertainty about who is speaking to whom, which causes interruptions, confusion and fatigue. A major cause of this is lack of support for gaze. In person, we look at one another through our own eyes, allowing us to see who is looking at us and who isn't. But during a video conference, we look at each person through a single shared camera, creating uncertainty about the conversations flow. This project developed a new, inexpensive technique supporting gaze in video conferencing. We are currently refining and testing this technique, with an eye toward commercialization.\r\n\n\n\r\n\n\nThese outcomes were widely disseminated in academic conferences, including both computer graphics and display venues, where some were recognized as distinguished contributions. Three provisional patents have already resulted. The project developed lasting relationships with academic collaborators in Japan and Korea, where much of the display industry is concentrated, including a one year visiting Korean scholar. In addition, the project had beneficial effects on education and training, including mentoring of seven doctoral students, as well as several masters and undergraduate students. Most of the doctoral students attended academic conferences, furthering their academic development. In graduate and undergraduate teaching, the project supported a course component featuring the research of women and minorities.\r\n\n\n\t\t\t\t\tLast Modified: 04/22/2025\n\n\t\t\t\t\tSubmitted by: BenjaminAWatson\n"
 }
}