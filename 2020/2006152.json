{
 "awd_id": "2006152",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Capturing Multisensory Interactions in Cutaneous Displays",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922971",
 "po_email": "sroberts@nsf.gov",
 "po_sign_block_name": "Scott Robertson",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2025-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2020-09-02",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "There has been growing interest in understanding how tactile displays that stimulate the skin can be used to expand the avenues available for communication for users of mobile devices and wearable technology. At present, such devices rely heavily on vision to communicate.  Tactile features are increasingly being implemented in smart watches and Android devices where they provide greater opportunities for communication beyond simple alerts and notifications. These technological developments are supported by an expanding body of research on how wearable devices such as rings, watches, bracelets and earpieces can be designed to function as tactile or thermal displays. The appeal of these devices is that they are always in direct contact with the skin, the cues are private and the sites are readily accessible if a user needs to make a response. Although much of the research on incorporating tactile cues in smart devices has focused on employing vibration to communicate, other sensory cues from the skin such as pressure, temperature and skin stretch have been explored. This work has demonstrated the feasibility of incorporating tactile and thermal feedback in a small device, but has typically focused on comparing single inputs rather than on understanding how these various signals can be combined to create a richer sensory experience that also expands the bandwidth for communication.  By selectively combining vibration, pressure, skin stretch and thermal cues in a display it is anticipated that the information provided to a user can be augmented and potentially offload the overworked visual and auditory systems. The advantages of using tactile-thermal signals for communication include the large area of skin available to present information and the effectiveness of such stimulation in capturing our attention. The application of this work extends beyond assisting users of mobile devices and wearable technologies to providing those with visual or hearing impairments who make use of the sense of touch to compensate for their sensory loss with additional possibilities for communication.\r\n\r\nThere have been few studies that have systematically explored the benefits and challenges of combining thermal and tactile feedback in a single display. Much of the previous work with thermo-tactile displays has evaluated these channels as independent sources of information, rather than as concurrent, synergistic inputs. The contribution of thermal displays to multi-sensory systems has not been investigated, although they show promise in that thermal inputs are known to influence tactile sensory processing and so the possibility exists that the dimensionality of information can be enhanced in some situations by the addition of thermal cues. This project is focused on understanding how cutaneous signals generated by tactile-thermal interfaces can be designed to communicate efficiently with users, in contexts in which we know tactile communication is effective. A framework that details how different tactile and thermal inputs can be combined or used in isolation in displays should increase the interface designers' ability to choose among modalities and assign functions and types of information to the channel best suited for their presentation. Such displays will be mounted at different sites on the body depending on the particular application and tasks that are being performed. User performance will be evaluated by measuring the discriminability of thermo-tactile cues, the information transfer associated with changing the dimensionality of the cues, their identification with concurrent workload, and the preference of users for particular cue characteristics (i.e., preferred body location such as hand, forearm, thigh, etc).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lynette",
   "pi_last_name": "Jones",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Lynette A Jones",
   "pi_email_addr": "LJones@MIT.edu",
   "nsf_id": "000092937",
   "pi_start_date": "2020-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": null
}