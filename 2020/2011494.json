{
 "awd_id": "2011494",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Zero-Order and Stochastic  Methods for Large-Scale Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-07-22",
 "awd_max_amd_letter_date": "2020-07-22",
 "awd_abstract_narration": "The promise of artificial intelligence (AI) has been a topic of both public and private interest for decades. It has recently blossomed thanks to the rapidly evolving and expanding field of machine learning, which has produced impressive results in perceptual tasks and has emerged as the core technology of modern AI. The intelligent systems that have been borne out of machine learning - such as search engines, recommendation platforms, and speech and image recognition software - have become an indispensable part of modern society. Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on the world's increasingly powerful computing platforms and the availability of very large data sets. One of the pillars of machine learning is optimization, which, in this context, involves the numerical computation of parameters for a system designed to make decisions based on yet unseen data. That is, based on currently available data, these parameters are chosen to be optimal with respect to a given learning problem. The central role that optimization plays in machine learning has inspired great numbers in various research communities to tackle even more challenging machine learning problems, and to design new optimization methods that are more widely applicable. This project is devoted to the development of a new generation of optimization methods that will help advance the field of machine learning by reducing computing time and allowing for the formulation of larger and more complex models. This will help AI expand into many domains such as medicine, robotics and logistics. This project provides research training opportunities for graduate students.\r\n\r\nIn technical terms, the goal of this proposal is to develop new algorithms for stochastic optimization problems, such as those arising in machine learning, statistics and black-box simulations. It consists of two interrelated projects encompassing algorithm design, convergence analysis, and numerical testing on realistic applications. The first project deals with constrained optimization problems in which the objective function is stochastic and the constraints are deterministic. The proposed methods use varying sample sizes to gradually reduce the variance in the gradient approximation; they have been studied in the context of unconstrained optimization, but their extension to the constrained setting is not simple because the projections or proximal operators used to enforce the constraints introduce discontinuities. The second project studies zero-order methods for the solution of noisy unconstrained optimization problems. Unlike derivative-free methods that construct quadratic models of the objective function using interpolation techniques, the proposed methods invest significant effort in computing a good approximation to the gradient of the noisy function and delegate the construction of a quadratic model to quasi-Newton updating. The two projects are interrelated and when combined will yield algorithms that scale into the millions of variables and parallelize easily. Their efficiency will be demonstrated in the solution of problems arising in reinforcement learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jorge",
   "pi_last_name": "Nocedal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jorge Nocedal",
   "pi_email_addr": "j-nocedal@northwestern.edu",
   "nsf_id": "000375872",
   "pi_start_date": "2020-07-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2145 Sheridan Rd",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602080834",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Artificial intelligence has long captured both public and private interest, but recent advancements in machine learning have transformed it into a fundamental discipline.&nbsp; Rooted in statistics and powered by advanced numerical algorithms, machine learning combines powerful computing platforms and vast datasets. A key component of this field is <strong>optimization</strong>, which involves the numerical computation of parameters to make optimal decisions based on data. The critical role of optimization in machine learning has spurred research into more efficient and broadly applicable methods. This project focused on creating the next generation of optimization techniques to reduce computing time and enable the development of larger, more complex models, thereby expanding AI's reach into areas like medicine, robotics, and logistics.</p>\n<p>Deterministic optimization is a well-developed field that served as the starting point in the investigation. The proposal focused on the development of new algorithms for stochastic optimization problems, relevant to machine learning, statistics, and black-box simulations. It comprised two interrelated projects comprising algorithm design, convergence analysis, and numerical testing on realistic applications.</p>\n<p>The first project addressed constrained optimization problems where the objective function is stochastic, and the constraints are deterministic. The proposed methods employed varying sample sizes to reduce the variance in gradient approximation. While these methods had been studied in unconstrained optimization, extending them to constrained settings posed major challenges due to discontinuities introduced by projections or proximal operators. Numerical experiments demonstrated the efficiency and robustness of the new methods on problems with nonlinear constraints.</p>\n<p>The second project explored zero-order methods for noisy unconstrained optimization problems. Unlike traditional derivative-free methods that build quadratic models using interpolation, the proposed methods focused on accurately approximating the gradient of the noisy function, with the quadratic model constructed via quasi-Newton updating. This investigation led to a surprising finding, namely that there are a few design principles that can be employed to transform a classical algorithm into one involving noise, including the zero-order setting. The discovery of these design principles may prove to be the most important contribution of this project, in the long term.</p>\n<p>The two projects were designed to complement each other, ultimately producing scalable algorithms capable of handling millions of variables and parallelizing efficiently. One can foresee the impact of the new algorithms in a variety of engineering and scientific applications.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/11/2024<br>\nModified by: Jorge&nbsp;Nocedal</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nArtificial intelligence has long captured both public and private interest, but recent advancements in machine learning have transformed it into a fundamental discipline. Rooted in statistics and powered by advanced numerical algorithms, machine learning combines powerful computing platforms and vast datasets. A key component of this field is optimization, which involves the numerical computation of parameters to make optimal decisions based on data. The critical role of optimization in machine learning has spurred research into more efficient and broadly applicable methods. This project focused on creating the next generation of optimization techniques to reduce computing time and enable the development of larger, more complex models, thereby expanding AI's reach into areas like medicine, robotics, and logistics.\n\n\nDeterministic optimization is a well-developed field that served as the starting point in the investigation. The proposal focused on the development of new algorithms for stochastic optimization problems, relevant to machine learning, statistics, and black-box simulations. It comprised two interrelated projects comprising algorithm design, convergence analysis, and numerical testing on realistic applications.\n\n\nThe first project addressed constrained optimization problems where the objective function is stochastic, and the constraints are deterministic. The proposed methods employed varying sample sizes to reduce the variance in gradient approximation. While these methods had been studied in unconstrained optimization, extending them to constrained settings posed major challenges due to discontinuities introduced by projections or proximal operators. Numerical experiments demonstrated the efficiency and robustness of the new methods on problems with nonlinear constraints.\n\n\nThe second project explored zero-order methods for noisy unconstrained optimization problems. Unlike traditional derivative-free methods that build quadratic models using interpolation, the proposed methods focused on accurately approximating the gradient of the noisy function, with the quadratic model constructed via quasi-Newton updating. This investigation led to a surprising finding, namely that there are a few design principles that can be employed to transform a classical algorithm into one involving noise, including the zero-order setting. The discovery of these design principles may prove to be the most important contribution of this project, in the long term.\n\n\nThe two projects were designed to complement each other, ultimately producing scalable algorithms capable of handling millions of variables and parallelizing efficiently. One can foresee the impact of the new algorithms in a variety of engineering and scientific applications.\n\n\n\t\t\t\t\tLast Modified: 08/11/2024\n\n\t\t\t\t\tSubmitted by: JorgeNocedal\n"
 }
}