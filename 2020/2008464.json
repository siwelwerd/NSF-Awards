{
 "awd_id": "2008464",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Towards Photorealistic Augmented and Virtual Reality Displays",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922533",
 "po_email": "hshen@nsf.gov",
 "po_sign_block_name": "Han-Wei Shen",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 516000.0,
 "awd_min_amd_letter_date": "2020-07-22",
 "awd_max_amd_letter_date": "2021-04-22",
 "awd_abstract_narration": "The holy grail for a 3D display is to produce a scene that, to our eyes, is indistinguishable from reality. Despite significant advances in display technology, simultaneous deception of all perceptual depth cues is still beyond the reach of most displays. As a consequence, existing 3D displays, especially those used in Augmented / Virtual Reality (AR/VR) applications, tend to cause discomfort due to subtle differences between the real and the virtual world. This project aims to develop novel 3D photorealistic display designs that ensure that the light field incident on the eye mimics reality at a significantly higher level of fidelity as compared to existing designs. The development of such a technology stands to impact a broad range of disciplines and applications that go beyond AR/VR systems and applications, such as ophthalmology, and testing of optical systems. Further, as the visual realism of displayed content gets better, they stand to replace standard 2D monitors that are ubiquitous in work and home environments today. These photorealistic displays will have the potential to make AR/VR systems more accessible for visually impaired people. The education and outreach components of this project disseminates display research and demos in middle and high schools in the greater Pittsburgh region via lab visits, and hands-on workshops.\r\n\r\nTo achieve the goal of photorealistic AR/VR displays, this project explores differ approaches for modeling of the virtual world and focuses on the development of three distinct approaches that progressively increase realism at the cost of complexity: (i) simple models in form of depth and texture decomposition; (ii) light field representations that provide a dense sampling of the ray space; (iii) coherent wavefronts using Fourier optics. All the three approaches aim to produce light fields that are dense in spatial and angular resolution even though the underlying physical mechanisms rely on different models for light transport. The project will develop techniques for generating content that is easily adapted to the requirements of the display. The project also explores and characterizes the fundamental properties and features in terms of achievable spatial and angular resolutions, the precision in depiction of occlusion, the size of the eye-box, and finally their limitations.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aswin",
   "pi_last_name": "Sankaranarayanan",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Aswin C Sankaranarayanan",
   "pi_email_addr": "saswin@andrew.cmu.edu",
   "nsf_id": "000623495",
   "pi_start_date": "2020-07-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "O'Toole",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew O'Toole",
   "pi_email_addr": "motoole2@andrew.cmu.edu",
   "nsf_id": "000788069",
   "pi_start_date": "2020-07-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><strong>Motivation and Goals</strong></p>\r\n<p class=\"p2\">Modern display technology strives toward the ultimate goal of creating virtual scenes indistinguishable from reality. While recent advances have been significant, current 3D displays&mdash;particularly in AR/VR applications&mdash;still struggle to simultaneously satisfy all perceptual depth cues, often resulting in user discomfort due to subtle disparities between real and virtual environments. This project advances the state-of-the-art in 3D photorealistic displays by developing novel designs that more faithfully reproduce real-world light fields.</p>\r\n<p class=\"p2\">&nbsp;</p>\r\n<p class=\"p1\"><strong>Intellectual Merit</strong></p>\r\n<p class=\"p2\">&nbsp;Our research has yielded significant advances in both the theory and implementation of 3D display technologies. The project's primary innovation is the Split-Lohmann computational lens, a breakthrough technology that transforms conventional 2D displays into 3D ones. This novel design employs cubic phase plates and phase spatial light modulators in an arrangement that enables independent focal depth control for each display pixel. The system generates 3D scenes in real-time from RGB texture images and corresponding depth maps. Using the Split-Lohmann technology, we developed a 3D multifocal near-eye display leveraging its spatial selective focusing capabilities. We also created an all-in-focus imager that maintains high spatial and temporal resolution while optically focusing entire scenes.</p>\r\n<p class=\"p2\">Holographic displays provide one of the most promising avenues for realizing 3D displays in a compact form factor. One of the challenges with this class of displays has been the poor field of view associated with today&rsquo;s digital holographic techniques, a limitation associated with the currently available phase modulators used to create the holograpm. Our work here has looked at the theory of new holographic displays, where we showed that adding a microlenslet array to the phase modulator allows for improved immersion. Our work also proposed a design where using a cascade of modulators we are able to significantly scale up the field of view of a holographic display.</p>\r\n<p class=\"p2\">&nbsp;We have also made substantial progress in reality capture technologies for 3D display content generation. Our work in neural rendering techniques has enabled complete 3D model reconstruction from single kaleidoscopic photographs. We enhanced depth map recovery using time-of-flight sensors and ray consistency principles for dynamic scene reconstruction. Additionally, we developed wide field-of-view scanning technology for capturing iridescent reflectance, enabling precise measurement of structural coloration and thin film reflectance.</p>\r\n<p class=\"p2\">&nbsp;</p>\r\n<p class=\"p1\"><strong>Broader Impacts</strong></p>\r\n<p class=\"p2\">The project has achieved significant educational and scientific impact through multiple channels. Our research has been widely disseminated through premier computer graphics and vision conferences, with the Split-Lohmann work receiving recognition as a SIGGRAPH 2023 Best Paper. We have conducted public demonstrations of binocular 3D display prototypes at various conferences and to industry representatives in our lab.</p>\r\n<p class=\"p2\">On the educational front, we created an undergraduate course titled \"Introduction to Extended Reality Systems,\" which incorporates project research into the curriculum. Our K-12 educational outreach efforts culminated in the development of a hands-on workshop for middle-school students. This workshop explores human depth perception principles, examines 3D display technology fundamentals, and engages students in interactive display building exercises. This comprehensive curriculum and outreach program ensures the project's innovations reach audiences across academic levels while fostering interest in advanced display technologies.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/24/2024<br>\nModified by: Aswin&nbsp;C&nbsp;Sankaranarayanan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nMotivation and Goals\r\n\n\nModern display technology strives toward the ultimate goal of creating virtual scenes indistinguishable from reality. While recent advances have been significant, current 3D displaysparticularly in AR/VR applicationsstill struggle to simultaneously satisfy all perceptual depth cues, often resulting in user discomfort due to subtle disparities between real and virtual environments. This project advances the state-of-the-art in 3D photorealistic displays by developing novel designs that more faithfully reproduce real-world light fields.\r\n\n\n\r\n\n\nIntellectual Merit\r\n\n\nOur research has yielded significant advances in both the theory and implementation of 3D display technologies. The project's primary innovation is the Split-Lohmann computational lens, a breakthrough technology that transforms conventional 2D displays into 3D ones. This novel design employs cubic phase plates and phase spatial light modulators in an arrangement that enables independent focal depth control for each display pixel. The system generates 3D scenes in real-time from RGB texture images and corresponding depth maps. Using the Split-Lohmann technology, we developed a 3D multifocal near-eye display leveraging its spatial selective focusing capabilities. We also created an all-in-focus imager that maintains high spatial and temporal resolution while optically focusing entire scenes.\r\n\n\nHolographic displays provide one of the most promising avenues for realizing 3D displays in a compact form factor. One of the challenges with this class of displays has been the poor field of view associated with todays digital holographic techniques, a limitation associated with the currently available phase modulators used to create the holograpm. Our work here has looked at the theory of new holographic displays, where we showed that adding a microlenslet array to the phase modulator allows for improved immersion. Our work also proposed a design where using a cascade of modulators we are able to significantly scale up the field of view of a holographic display.\r\n\n\nWe have also made substantial progress in reality capture technologies for 3D display content generation. Our work in neural rendering techniques has enabled complete 3D model reconstruction from single kaleidoscopic photographs. We enhanced depth map recovery using time-of-flight sensors and ray consistency principles for dynamic scene reconstruction. Additionally, we developed wide field-of-view scanning technology for capturing iridescent reflectance, enabling precise measurement of structural coloration and thin film reflectance.\r\n\n\n\r\n\n\nBroader Impacts\r\n\n\nThe project has achieved significant educational and scientific impact through multiple channels. Our research has been widely disseminated through premier computer graphics and vision conferences, with the Split-Lohmann work receiving recognition as a SIGGRAPH 2023 Best Paper. We have conducted public demonstrations of binocular 3D display prototypes at various conferences and to industry representatives in our lab.\r\n\n\nOn the educational front, we created an undergraduate course titled \"Introduction to Extended Reality Systems,\" which incorporates project research into the curriculum. Our K-12 educational outreach efforts culminated in the development of a hands-on workshop for middle-school students. This workshop explores human depth perception principles, examines 3D display technology fundamentals, and engages students in interactive display building exercises. This comprehensive curriculum and outreach program ensures the project's innovations reach audiences across academic levels while fostering interest in advanced display technologies.\r\n\n\n\t\t\t\t\tLast Modified: 11/24/2024\n\n\t\t\t\t\tSubmitted by: AswinCSankaranarayanan\n"
 }
}