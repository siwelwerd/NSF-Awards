{
 "awd_id": "1955587",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: CIF: Medium: Learning and Inference in High-Dimensional Models: Rigorous Analysis and Applications",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 449883.0,
 "awd_amount": 449883.0,
 "awd_min_amd_letter_date": "2020-06-23",
 "awd_max_amd_letter_date": "2022-07-21",
 "awd_abstract_narration": "A key feature of contemporary signal processing and machine learning problems is their massive scale. Signal processing tasks routinely involve images and videos with millions of pixels, and modern deep-learning methods often involve millions of tunable parameters. Although recent methods, particularly deep learning, have had tremendous practical success in the high-dimensional setting, they are difficult to explain from a theoretical perspective. This project seeks to develop mathematical tools to better understand such estimation and learning problems along the following directions:  How does one tractably formulate precise, high-dimensional analyses of contemporary problems; what do those analyses say about the information-theoretic limits of estimation and learning; and how can these limits be approached by practical algorithms? To achieve broader impacts, the project includes dissemination in workshops, coordination with the growing machine learning industry and the development of a new module on data science to be provided to high school teachers.\r\n\r\nThe project builds on the powerful approximate message passing (AMP) framework, an estimation methodology that offers the potential for a rigorous analytic understanding of modern, high-dimensional problems. Since its origin as a method for understanding linear inverse problems in compressed sensing, AMP has had tremendous success in a wide range of estimation and learning tasks. This project aims to extend the AMP framework to contemporary, large-scale learning tasks. The project is organized into three main thrusts: 1) inference with structured bilinear models, 2) learning of multi-layer neural networks, and 3) analysis with Fourier and convolutional operators.  In each thrust, the project will develop fundamental mathematical theory and validate the theory on key applications, particular in image processing and statistical learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Schniter",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Philip Schniter",
   "pi_email_addr": "schniter@ece.osu.edu",
   "nsf_id": "000297419",
   "pi_start_date": "2020-06-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "2015 Neil Ave",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101240",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7797",
   "pgm_ref_txt": "COMM & INFORMATION FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 98447.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 173408.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 178028.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project targeted fundamental questions about high-dimensional estimation and learning: How we can tractably formulate precise, high-dimensional analyses of contemporary problems, what those analyses say about the performance limits of estimation, and how we can approach those limits with practical algorithms. Our main tool is Vector Approximate Message Passing (VAMP)---a particular incarnation of Expectation Consistent (EC) Approximation, which offers the potential for a rigorous analytic understanding of modern, high-dimensional estimation problems.</p>\n<p>The VAMP algorithm was originally proposed to solve linear regression problems, where the goal is to estimate a vector \"x\" with iid entries from measurements \"y=A*x+noise\", where \"A\" is a known random measurement matrix and the noise is white and Gaussian. &nbsp;Since the iid assumption on \"x\" is a bit too simplistic for many practical applications, in one thrust of this project, we modeled \"x\" as the matrix-valued output of a deep neural network driven by white Gaussian noise, like the decoder of a variational autoencoder (VAE), and proposed the Multi-Layer Matrix VAMP (ML-Mat-VAMP) algorithm to generate MAP or MMSE estimates of \"x\". &nbsp;By modeling \"A\" and the decoder's linear layers as rotationally invariant random matrices, we proved that the performance of ML-Mat-VAMP can be exactly predicted in the high-dimensional limit using a state-evolution formalism. &nbsp;Furthermore, we empirically demonstrated that the state evolution yields good predictions even at moderate dimensions. &nbsp;Applications of ML-Mat-VAMP include, e.g., multi-task and mixed regression problems, sketched clustering, and inferring the input of a two-layer neural network. &nbsp;</p>\n<p>In some practical applications of linear regression, however, \"x\" and \"A\" are structured in a way that all known variants of VAMP fail. &nbsp;For example, in imaging applications, both \"x\" and \"A\" may exhibit significant Fourier-domain structure, in which case VAMP algorithms can give unpredictable results or even diverge. &nbsp;Thus, in another thrust of this project, we proposed improved damping strategies for VAMP that slow the algorithm in an effort to maintain consistency with its (damped) state evolution. &nbsp;We demonstrated that these damping strategies work well for many structures on \"A\" and \"x\", although admittedly not all.</p>\n<p>To improve performance in the important special case of subsampled Fourier \"A\", which arises in MRI and other applications, another thrust of this project leveraged an orthogonal wavelet transform \"W\" to write \"A*x\" as \"B*c\" for \"B=A*W'\" and \"c=W*x\". The advantage here is that the Fourier-wavelet matrix \"B\" is approximately block-diagonal, with blocks that behave like rotationally invariant random matrices. &nbsp;We then designed a VAMP-style algorithm to separately track the variance in each wavelet subband of \"c\" and demonstrated state-of-the-art performance in MRI. &nbsp;Because non-uniform noise variance across wavelet subbands implies correlated noise across pixels, part of the challenge was to develop a convolutional denoising network that handles correlated noise with on-the-fly changes in correlation statistics.</p>\n<p>Inspired by how existing VAMP/EC algorithms use deep neural networks (DNNs) to supply sophisticated prior knowledge of \"x\" in VAMP's conditional-mean stage, another thrust of this project proposed a \"deepEC\" framework that also uses DNNs to predict VAMP's conditional-variance terms, which are analytically intractable when \"A\" is not a high-dimensional rotationally invariant random matrix. &nbsp;Inspired by diffusion methods, we further improved this deepEC algorithm using a stochastic damping approach that injects a decaying amount of noise over the iterations to help satisfy certain white-Gaussian error assumptions without perturbing the fixed point. &nbsp;We then extended this deepEC from the linear regression setting \"y=A*x+noise\" to the generalized linear model (GLM) setting, where the measurements \"y\" are modeled as realizations of a conditional distribution \"p(y|z)\" driven by the (hidden) transform outputs \"z=A*x\". &nbsp;By appropriately choosing p(y|z), the GLM covers phase-retrieval (PR), dequantization, image recovery from photon counts, logistic regression, and many other tasks. &nbsp;We then applied our deepEC to the challenging problem of Fourier-based phase retrieval, and demonstrated performance that surpasse state-of-the-art plug-and-play and diffusion methods.</p>\n<p>Apart from VAMP-related work, this project also supported several other explorations of high-dimensional inference. &nbsp;For example, we co-authored the first ever overview of sketched clustering for the IEEE Signal Processing Magazine, which is now highly cited. &nbsp;We also investigated deep-network approaches to convex regularization for image recovery, as well as denoisers that perform near-optimally for a wide range of unknown noise levels.</p><br>\n<p>\n Last Modified: 09/04/2024<br>\nModified by: Philip&nbsp;Schniter</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project targeted fundamental questions about high-dimensional estimation and learning: How we can tractably formulate precise, high-dimensional analyses of contemporary problems, what those analyses say about the performance limits of estimation, and how we can approach those limits with practical algorithms. Our main tool is Vector Approximate Message Passing (VAMP)---a particular incarnation of Expectation Consistent (EC) Approximation, which offers the potential for a rigorous analytic understanding of modern, high-dimensional estimation problems.\n\n\nThe VAMP algorithm was originally proposed to solve linear regression problems, where the goal is to estimate a vector \"x\" with iid entries from measurements \"y=A*x+noise\", where \"A\" is a known random measurement matrix and the noise is white and Gaussian. Since the iid assumption on \"x\" is a bit too simplistic for many practical applications, in one thrust of this project, we modeled \"x\" as the matrix-valued output of a deep neural network driven by white Gaussian noise, like the decoder of a variational autoencoder (VAE), and proposed the Multi-Layer Matrix VAMP (ML-Mat-VAMP) algorithm to generate MAP or MMSE estimates of \"x\". By modeling \"A\" and the decoder's linear layers as rotationally invariant random matrices, we proved that the performance of ML-Mat-VAMP can be exactly predicted in the high-dimensional limit using a state-evolution formalism. Furthermore, we empirically demonstrated that the state evolution yields good predictions even at moderate dimensions. Applications of ML-Mat-VAMP include, e.g., multi-task and mixed regression problems, sketched clustering, and inferring the input of a two-layer neural network. \n\n\nIn some practical applications of linear regression, however, \"x\" and \"A\" are structured in a way that all known variants of VAMP fail. For example, in imaging applications, both \"x\" and \"A\" may exhibit significant Fourier-domain structure, in which case VAMP algorithms can give unpredictable results or even diverge. Thus, in another thrust of this project, we proposed improved damping strategies for VAMP that slow the algorithm in an effort to maintain consistency with its (damped) state evolution. We demonstrated that these damping strategies work well for many structures on \"A\" and \"x\", although admittedly not all.\n\n\nTo improve performance in the important special case of subsampled Fourier \"A\", which arises in MRI and other applications, another thrust of this project leveraged an orthogonal wavelet transform \"W\" to write \"A*x\" as \"B*c\" for \"B=A*W'\" and \"c=W*x\". The advantage here is that the Fourier-wavelet matrix \"B\" is approximately block-diagonal, with blocks that behave like rotationally invariant random matrices. We then designed a VAMP-style algorithm to separately track the variance in each wavelet subband of \"c\" and demonstrated state-of-the-art performance in MRI. Because non-uniform noise variance across wavelet subbands implies correlated noise across pixels, part of the challenge was to develop a convolutional denoising network that handles correlated noise with on-the-fly changes in correlation statistics.\n\n\nInspired by how existing VAMP/EC algorithms use deep neural networks (DNNs) to supply sophisticated prior knowledge of \"x\" in VAMP's conditional-mean stage, another thrust of this project proposed a \"deepEC\" framework that also uses DNNs to predict VAMP's conditional-variance terms, which are analytically intractable when \"A\" is not a high-dimensional rotationally invariant random matrix. Inspired by diffusion methods, we further improved this deepEC algorithm using a stochastic damping approach that injects a decaying amount of noise over the iterations to help satisfy certain white-Gaussian error assumptions without perturbing the fixed point. We then extended this deepEC from the linear regression setting \"y=A*x+noise\" to the generalized linear model (GLM) setting, where the measurements \"y\" are modeled as realizations of a conditional distribution \"p(y|z)\" driven by the (hidden) transform outputs \"z=A*x\". By appropriately choosing p(y|z), the GLM covers phase-retrieval (PR), dequantization, image recovery from photon counts, logistic regression, and many other tasks. We then applied our deepEC to the challenging problem of Fourier-based phase retrieval, and demonstrated performance that surpasse state-of-the-art plug-and-play and diffusion methods.\n\n\nApart from VAMP-related work, this project also supported several other explorations of high-dimensional inference. For example, we co-authored the first ever overview of sketched clustering for the IEEE Signal Processing Magazine, which is now highly cited. We also investigated deep-network approaches to convex regularization for image recovery, as well as denoisers that perform near-optimally for a wide range of unknown noise levels.\t\t\t\t\tLast Modified: 09/04/2024\n\n\t\t\t\t\tSubmitted by: PhilipSchniter\n"
 }
}