{
 "awd_id": "2014861",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Analyzing Dependent Extremes via Joint Quantile Regression",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 149999.0,
 "awd_amount": 149999.0,
 "awd_min_amd_letter_date": "2020-06-20",
 "awd_max_amd_letter_date": "2020-06-20",
 "awd_abstract_narration": "What is common between Hurricane Harvey and the Great Recession? Both are examples of ordinary and loosely connected processes attaining extraordinary levels in a synchronized manner. Accurately predicting the size and frequency of synchronized extreme outcomes is crucial to robust risk management. But this task remains a difficult challenge to data science that has been mostly built around the notions of average behavior and independence. A case in point is regression analyses where one studies how a group of outcomes are simultaneously influenced by a common set of predictors. Current statistical methods can either address inter-dependency between multiple outcomes, or model transitions from ordinary to extraordinary levels of a single outcome. But nothing satisfactory exists to handle both. This research fills this gap with new data analysis tools based on quantile driven regression analysis. The new tools are specialized for analyzing data recorded over space and time or within network clusters. They are subjected to detailed mathematical scrutiny for accuracy and reliability. Applications to finance and environmental sciences are carried out to assess the usefulness of the new tools in scientific investigation and policy making. The project integrates statistical research with software development and graduate education. \r\n\r\nQuantiles are simply percentiles expressed in terms of a level varying from zero through one, as opposed to a percentage point. The quantiles of a variable give direct access to its smooth transitions between ordinary and extraordinary levels. In standard quantile regression, one estimates the effects of predictors at any given quantile level of an outcome. Such estimation is easy to carry out when observation units are mutually independent; there is no need of a detailed probabilistic model for the predictor-outcome relation. But data with known dependency structures present a far more complex challenge; accurate estimation requires adjusting for intrinsic noise correlation between units in close proximity. Such a task remains beyond the scope of ordinary quantile regression methods due to their model-free nature and their focus on single quantiles in isolation. In contrast, joint quantile regression makes this task feasible by incorporating a full probabilistic model for the outcome and enabling a joint estimation at all quantile levels at once. The research investigates the use of copula modeling to address noise correlation in joint quantile regression, focusing greatly on appropriate customization of the copula formulation for each data type. A rigorous asymptotic analysis is carried out toward statistical guarantees of the resulting tools. Quantitative and visualization-based diagnostic tools will be developed for model assessment and selection. All tools will be incorporated in the R package \u201cqrjoint\u201d available through The Comprehensive R Archive Network.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Surya",
   "pi_last_name": "Tokdar",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Surya T Tokdar",
   "pi_email_addr": "surya.tokdar@duke.edu",
   "nsf_id": "000548769",
   "pi_start_date": "2020-06-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St, Suite 710",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 149999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Extreme events, whether be it heavy rainfall or large wildfires, pose risk to life and property. Statistical analysis of such events is challenging. On the one hand, extreme events are rare and atypical. On the other hand, statistical methods are largely built for analyzing the average or the typical outcome. Extreme value theory, a branch of statistics, offers concepts and tools to target extreme outcomes, but its practice leaves many gaps. Current approaches often resort to isolating large outcomes from historical records, so that inference on the extreme is not overwhelmed by the more numerous typical values. Such thresholding throws away potentially valuable information, especially when records are correlated. Also, identifying an appropriate threshold can be more art than science.&nbsp;</p>\n<p>This award has supported research on statistical theory and methods that target analysis of extreme outcomes without thresholding; attempting to extract maximum possible information out of limited data. Our results show that it is feasible to combine ideas from extreme value theory with modeling and computational tools from Bayesian statistics to create new methodology which judiciously utilizes information from more typical but extreme-adjacent records to improve estimation and prediction of extreme outcomes. Our mathematical analysis shows that this new methodology is adaptive: it makes optimal use of the data to determine how deep into the typical range one needs to reach to improve estimation of the extreme realizations. The improvement is sometimes dramatic! Thresholding methods are prone to producing excessively wide ranges of estimated values, sometimes extending orders of magnitude above and beyond the truth. In contrast, the new methodology produces provably good bounds that are also tight enough to be useful in practice.&nbsp;</p>\n<p>A second important outcome of this award has been the development of a regression analysis framework which can investigate how the association between a response variable and a given set of predictors varies between typical and atypical levels. This type of regression analysis falls under the label of quantile regression. However, existing methods could carry out only a localized analysis at a given percentile level of the response and are reliable mostly in the typical range. New research tools developed under this award allow the synthesis of extreme value theory and Bayesian estimation to be brought over into the field of quantile regression and provide important customizations for analyzing correlated records commonly encountered in environmental sciences.&nbsp;</p>\n<p>The award has supported one doctoral student in their dissertation research and professional development. It has also supported creation of research projects for MS students aspiring to go into doctoral programs in statistics. Research findings were disseminated widely via publication of peer reviewed journal articles, presentation at prestigious domestic and international academic conferences, and creation and release of publicly available software packages to carry out statistical analyses on large data sets using the new methodology developed under this award.&nbsp;&nbsp;The research publications have attracted attention from researchers in environmental sciences and have helped foster new collaborations.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/15/2024<br>\nModified by: Surya&nbsp;T&nbsp;Tokdar</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nExtreme events, whether be it heavy rainfall or large wildfires, pose risk to life and property. Statistical analysis of such events is challenging. On the one hand, extreme events are rare and atypical. On the other hand, statistical methods are largely built for analyzing the average or the typical outcome. Extreme value theory, a branch of statistics, offers concepts and tools to target extreme outcomes, but its practice leaves many gaps. Current approaches often resort to isolating large outcomes from historical records, so that inference on the extreme is not overwhelmed by the more numerous typical values. Such thresholding throws away potentially valuable information, especially when records are correlated. Also, identifying an appropriate threshold can be more art than science.\n\n\nThis award has supported research on statistical theory and methods that target analysis of extreme outcomes without thresholding; attempting to extract maximum possible information out of limited data. Our results show that it is feasible to combine ideas from extreme value theory with modeling and computational tools from Bayesian statistics to create new methodology which judiciously utilizes information from more typical but extreme-adjacent records to improve estimation and prediction of extreme outcomes. Our mathematical analysis shows that this new methodology is adaptive: it makes optimal use of the data to determine how deep into the typical range one needs to reach to improve estimation of the extreme realizations. The improvement is sometimes dramatic! Thresholding methods are prone to producing excessively wide ranges of estimated values, sometimes extending orders of magnitude above and beyond the truth. In contrast, the new methodology produces provably good bounds that are also tight enough to be useful in practice.\n\n\nA second important outcome of this award has been the development of a regression analysis framework which can investigate how the association between a response variable and a given set of predictors varies between typical and atypical levels. This type of regression analysis falls under the label of quantile regression. However, existing methods could carry out only a localized analysis at a given percentile level of the response and are reliable mostly in the typical range. New research tools developed under this award allow the synthesis of extreme value theory and Bayesian estimation to be brought over into the field of quantile regression and provide important customizations for analyzing correlated records commonly encountered in environmental sciences.\n\n\nThe award has supported one doctoral student in their dissertation research and professional development. It has also supported creation of research projects for MS students aspiring to go into doctoral programs in statistics. Research findings were disseminated widely via publication of peer reviewed journal articles, presentation at prestigious domestic and international academic conferences, and creation and release of publicly available software packages to carry out statistical analyses on large data sets using the new methodology developed under this award.The research publications have attracted attention from researchers in environmental sciences and have helped foster new collaborations.\n\n\n\t\t\t\t\tLast Modified: 02/15/2024\n\n\t\t\t\t\tSubmitted by: SuryaTTokdar\n"
 }
}