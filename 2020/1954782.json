{
 "awd_id": "1954782",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: RI: Medium: Introspective Perception and Planning for Long-Term Autonomy",
 "cfda_num": "47.070, 47.079",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 613269.0,
 "awd_min_amd_letter_date": "2020-06-01",
 "awd_max_amd_letter_date": "2022-07-05",
 "awd_abstract_narration": "Building and deploying autonomous service robots in real human environments has been a long-standing challenge in artificial intelligence and robotics.  Such robots can assist humans in everyday activities and offer a transformational impact on society.  In order to successfully realize their benefits, however, robots must be cognizant of their limitations, and when uncertain about an action, they must be able to ask for human assistance.  When robots are deployed in novel environments, developers cannot fully foresee what errors the robots may make, what may be the root causes of such errors, how human confidence in the robots\u2019 abilities may change as a result of such errors, and how well the robots may learn to autonomously overcome errors and reduce the reliance on human assistance.  This project develops a comprehensive solution to these challenges by introducing competence-aware autonomy, enabling robots to learn what aspects of the environment, the situation, and the task lead to varying levels of success.  When asking for human assistance, a competence-aware robot can offer evidence to explain its level of confidence.  When left to act autonomously, a competence-aware robot can hypothesize contingency actions to help reduce its own uncertainty and to remain autonomous with high confidence.  Consequently, the project transforms the ability of researchers and practitioners to deploy robots in unstructured environments where limited knowledge is available prior to deployment.  This enables workers with limited robotics expertise to deploy robots more safely in unstructured environments and teach them over time to be progressively independent.  The project team will also develop new course materials at UT Austin and UMass Amherst, mentor undergraduate student researchers with special attention to underrepresented groups, perform outreach activities to introduce programming with robots to grade school students, develop conference workshops and tutorials on integrated perception and planning research, and strengthen collaborations between academia and industry.\r\n\r\nThe project addresses the need to build competency-aware systems by introducing approaches to satisfy six core properties of introspective perception and planning. They include: 1) an approach to autonomously supervise the training of introspective perception by relying on different types of consistency metrics; 2) an approach to learn to identify causal factors of perception errors by considering both local and global cues in sensed data; 3) an approach to analyze sequences of actions and observations from logs to learn the impact of actions on introspective perception; 4) an introspective planning approach that is cognizant of different levels of autonomy, each associated with certain restrictions on autonomous operation; 5) an introspective planning approach that is cognizant of the cost of different forms of human assistance and can learn to minimize the reliance on humans over time; and 6) an introspective planning approach that can learn from human feedback about negative side effects and can attempt to explain them and mitigate their impact.  The project identifies key patterns of interaction between these different components to enable a robot to autonomously learn to plan around its limitations and minimize the reliance on humans.  The team conducts a comprehensive evaluation consisting of individual part-based testing in high-fidelity simulation, and extensive real-world deployments of service mobile robots across the University of Texas at Austin and University of Massachusetts Amherst campuses, while performing key challenge tasks that directly support the facilities of both universities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shlomo",
   "pi_last_name": "Zilberstein",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shlomo Zilberstein",
   "pi_email_addr": "shlomo@cs.umass.edu",
   "nsf_id": "000460242",
   "pi_start_date": "2020-06-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "140 Governors Drive",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "054Y00",
   "pgm_ele_name": "GVF - Global Venture Fund"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 195547.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 198466.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 219256.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Building and deploying autonomous service robots in real human environments has been a long-standing challenge in artificial intelligence and robotics. Such robots can assist humans in everyday activities and offer a transformational impact on society. However, to successfully realize their benefits, robots must be cognizant of their limitations, and when uncertain about an action, they must be able to ask for human assistance. This project produced comprehensive solutions to these challenges by introducing competence-aware autonomy, which enables robots to learn what aspects of the environment, the situation, and the task lead to varying levels of success. When asking for human assistance, a competence-aware robot can offer evidence to explain its level of confidence. When left to act autonomously, a competence-aware robot can hypothesize contingency actions to help reduce its own uncertainty and to remain autonomous with high confidence. Consequently, the project improved the ability of researchers and practitioners to deploy robots in unstructured environments where limited knowledge is available prior to deployment.&nbsp;</span></p>\n<p>The project produced a comprehensive approach to building competency-aware systems through the following specific outcomes: (1) Developed a novel introspective planning model--competence-aware systems (CAS)--that enables a semi-autonomous system to reason about its own competence and allowed distinct level of autonomy by leveraging human feedback or assistance; (2) Developed techniques for learning to improve autonomous behavior from human feedback and reduce the reliance on such feedback over time; (3) Developed techniques to mitigate negative side effects (NSE) during the deployment of an autonomous system using a combination of human feedback and autonomous exploration; and (4) Developed competence-aware path planning via introspective perception, that can reason about plan execution failures due to errors in perception, without requiring a priori enumeration of failure sources or requiring location-specific failure statistics.</p>\n<p><span>Together with collaborators and industry partners, we evaluated these methods rigorously on several platforms, including mobile robots deployed on campus and an experimental Autonomous Vehicle developed by Nissan North America.&nbsp;</span>Overall, these results established a foundation for competence-aware systems in AI and robotics. Additional outcomes include training and mentoring of graduate and undergraduate students with special attention to underrepresented groups (four graduate students obtained their doctoral degrees while working on the project, including one female student who is currently an assistant professor; several undergraduate students who worked on the project enrolled in doctoral programs, including one student who received a prestigious NSF Graduate Research Fellowship).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/11/2024<br>\nModified by: Shlomo&nbsp;Zilberstein</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nBuilding and deploying autonomous service robots in real human environments has been a long-standing challenge in artificial intelligence and robotics. Such robots can assist humans in everyday activities and offer a transformational impact on society. However, to successfully realize their benefits, robots must be cognizant of their limitations, and when uncertain about an action, they must be able to ask for human assistance. This project produced comprehensive solutions to these challenges by introducing competence-aware autonomy, which enables robots to learn what aspects of the environment, the situation, and the task lead to varying levels of success. When asking for human assistance, a competence-aware robot can offer evidence to explain its level of confidence. When left to act autonomously, a competence-aware robot can hypothesize contingency actions to help reduce its own uncertainty and to remain autonomous with high confidence. Consequently, the project improved the ability of researchers and practitioners to deploy robots in unstructured environments where limited knowledge is available prior to deployment.\n\n\nThe project produced a comprehensive approach to building competency-aware systems through the following specific outcomes: (1) Developed a novel introspective planning model--competence-aware systems (CAS)--that enables a semi-autonomous system to reason about its own competence and allowed distinct level of autonomy by leveraging human feedback or assistance; (2) Developed techniques for learning to improve autonomous behavior from human feedback and reduce the reliance on such feedback over time; (3) Developed techniques to mitigate negative side effects (NSE) during the deployment of an autonomous system using a combination of human feedback and autonomous exploration; and (4) Developed competence-aware path planning via introspective perception, that can reason about plan execution failures due to errors in perception, without requiring a priori enumeration of failure sources or requiring location-specific failure statistics.\n\n\nTogether with collaborators and industry partners, we evaluated these methods rigorously on several platforms, including mobile robots deployed on campus and an experimental Autonomous Vehicle developed by Nissan North America.Overall, these results established a foundation for competence-aware systems in AI and robotics. Additional outcomes include training and mentoring of graduate and undergraduate students with special attention to underrepresented groups (four graduate students obtained their doctoral degrees while working on the project, including one female student who is currently an assistant professor; several undergraduate students who worked on the project enrolled in doctoral programs, including one student who received a prestigious NSF Graduate Research Fellowship).\n\n\n\n\n\n\t\t\t\t\tLast Modified: 10/11/2024\n\n\t\t\t\t\tSubmitted by: ShlomoZilberstein\n"
 }
}