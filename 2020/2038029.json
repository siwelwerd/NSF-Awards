{
 "awd_id": "2038029",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC-EDU: Privacy Enhancing Techniques and Innovations for AI-Cybersecurity Cross Training",
 "cfda_num": "47.076",
 "org_code": "11010000",
 "po_phone": "7032922677",
 "po_email": "liyang@nsf.gov",
 "po_sign_block_name": "Li Yang",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2020-07-28",
 "awd_max_amd_letter_date": "2020-07-28",
 "awd_abstract_narration": "Artificial intelligence (AI) is being rapidly deployed in many security-critical applications. This has fueled the use of AI to improve cybersecurity via speed of reasoning and reaction (AI for cybersecurity). At the same time, the widespread use of AI introduces new adversarial threats to AI systems and highlights a need for robustness and resilience guarantees for AI (cybersecurity for AI), while ensuring fairness of and trust in AI algorithmic decision making. Not surprisingly, privacy-enhancing technologies and innovations are critical to mitigating the adverse effects of intentional exploitation and protecting AI systems. However, resources for AI-cybersecurity cross-training are limited, and even fewer programs integrate topics, techniques and research innovations pertaining to privacy in their basic curricula covering AI or cybersecurity. To bridge this cross-training gap and to advance AI-cybersecurity education, this project will create a pilot program on privacy-enhancing AI-cybersecurity cross-training, which will provide a transformative learning experience for students. The results of this project will provide students with the AI-cybersecurity knowledge and skills  that will enable them to enter the workforce and contribute to the creation of a secure and trustworthy AI-cybersecurity environment that simultaneously supports AI safety, AI privacy and AI fairness for all. \r\n\r\nThe intellectual merit of this project stems from the development of a first-of-its-kind research and teaching methodology that will provide effective AI-cybersecurity cross-training in the context of privacy. This will include developing a privacy foundation virtual laboratory (vLab) and three advanced topic vLabs, each representing a unique educational innovation for AI-cybersecurity cross-training. The AI for Security vLab will enable students to learn that privacy is a critical system property for all AI-enabled cybersecurity systems and applications. The Security of AI vLab will assist students in learning that privacy is an important safety guarantee against a variety of privacy leakage risks. The AI Fairness and Trust vLab will empower students to learn that privacy is an essential measure of trust and fairness of AI systems by ensuring the right to privacy and AI ethics for all. By participating in these vLabs, students will learn to use risk assessment tools to understand new vulnerabilities to attack of AI models and to design risk-mitigation tools to protect AI model learning and reasoning against security or privacy violations and algorithmic biases.\r\n\r\nThis project is supported by a special initiative of the Secure and Trustworthy Cyberspace (SaTC) program to foster new, previously unexplored, collaborations between the fields of cybersecurity, artificial intelligence, and education. The SaTC program aligns with the Federal Cybersecurity Research and Development Strategic Plan and the National Privacy Research Strategy to protect and preserve the growing social and economic benefits of cyber systems while ensuring security and privacy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DGE",
 "org_div_long_name": "Division Of Graduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ling",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ling Liu",
   "pi_email_addr": "lingliu@cc.gatech.edu",
   "nsf_id": "000435549",
   "pi_start_date": "2020-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "093Z",
   "pgm_ref_txt": "AI Education/Workforce Develop"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0420",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002021DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Artificial Intelligence (AI) is being rapidly deployed in many security-critical applications. This has fueled the use of AI to improve Cybersecurity in speed of reasoning and reaction (AI for security), and at the same time, wide spread use of AI introduces a variety of new adversarial threats inherent AI systems and calls for developing robustness guarantees for AI (Security for AI).&nbsp;&nbsp;We argue that privacy enhancing technologies and innovations are at the center of these three facets for mitigating adverse effect of intentional exploitations and protecting privately trained models and their sensitive training data. However, very limited curriculum or hands-on learning resources are available for AI-Cybersecurity cross training, and even fewer programs in computer science, data science and engineering integrate privacy enhancing fundamentals, techniques and research innovations in their basic curriculum covering AI or Cybersecurity.</p>\n<p>To bridge such cross training gap, this project takes a principled approach to create a privacy-enhancing AI-Cybersecurity cross training research pilot program. We develop the risk assessment tools to identify and categorize the types of risks that threaten the privacy, utility and trust of AI models, and the risk mitigation tools to manage and detect risks, regulate access to sensitive data, and secure the AI models trained over sensitive or proprietary data. These tools are packaged in AI Privacy vLab and AI Security vLab as educational vehicles to provide hand-on experiences and enable students to gain an in-depth understanding of AI privacy risks and AI Security threats, to learn how to design, measure, evaluate both the robustness of AI and the effectiveness of risk mitigation strategies.&nbsp;</p>\n<p>This project has designed a first-of-its-kind research-teaching alliance method &nbsp;towards effective AI-Cybersecurity cross training through the AI privacy lens. We develop a privacy foundation virtual laboratory to experiment a unique educational innovation to AI-Cybersecurity cross-training.&nbsp;&nbsp;The&nbsp;AI for Security vLab&nbsp;has enabled students to learn that privacy is a critical system property for all AI-enabled cybersecurity systems and applications, given the high sensitivity of cybersecurity-related training data and high confidentiality of AI models trained for cybersecurity applications.&nbsp;</p>\n<p>Our NSF SaTC EDU funded AI-Cybersecurity cross training project took a radically different and yet methodical approach to AI-Cybersecurity education. It creates a new way of providing AI-Cybersecurity cross training program by synergizing AI-Privacy and AI-Security for training the next generation of AI-Cybersecurity workforce. We conjecture that the findings could help laying the groundwork for training and preparing students with the knowledge and skills required to develop and deploy next generation of secure and trustworthy AI-Cybersecurity systems.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/26/2023<br>\nModified by: Ling&nbsp;Liu</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700775879150_1.Screenshot_2023_11_23_at_4.39.10__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700775879150_1.Screenshot_2023_11_23_at_4.39.10__8239_PM--rgov-800width.png\" title=\"Project Overview\"><img src=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700775879150_1.Screenshot_2023_11_23_at_4.39.10__8239_PM--rgov-66x44.png\" alt=\"Project Overview\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">AI-CyberSecurity Cross Training Project</div>\n<div class=\"imageCredit\">getty images, source:nature, Yolo-github</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Ling&nbsp;Liu\n<div class=\"imageTitle\">Project Overview</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776709807_6.Screenshot_2023_11_23_at_4.49.37__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776709807_6.Screenshot_2023_11_23_at_4.49.37__8239_PM--rgov-800width.png\" title=\"Defense against Data Poisoning\"><img src=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776709807_6.Screenshot_2023_11_23_at_4.49.37__8239_PM--rgov-66x44.png\" alt=\"Defense against Data Poisoning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Model Trojan Attacks and Proactive Countermeasure</div>\n<div class=\"imageCredit\">authors</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ling&nbsp;Liu\n<div class=\"imageTitle\">Defense against Data Poisoning</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776590715_5.Screenshot_2023_11_23_at_4.50.23__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776590715_5.Screenshot_2023_11_23_at_4.50.23__8239_PM--rgov-800width.png\" title=\"AI security Lab Example Modules\"><img src=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776590715_5.Screenshot_2023_11_23_at_4.50.23__8239_PM--rgov-66x44.png\" alt=\"AI security Lab Example Modules\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Model Trojan Attacks and Reactive Countermeasure</div>\n<div class=\"imageCredit\">authors</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ling&nbsp;Liu\n<div class=\"imageTitle\">AI security Lab Example Modules</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776326246_2.Screenshot_2023_11_23_at_4.47.01__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776326246_2.Screenshot_2023_11_23_at_4.47.01__8239_PM--rgov-800width.png\" title=\"AI Privacy vLab\"><img src=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776326246_2.Screenshot_2023_11_23_at_4.47.01__8239_PM--rgov-66x44.png\" alt=\"AI Privacy vLab\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">AI Privacy + AI Security vLabs</div>\n<div class=\"imageCredit\">Authors</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ling&nbsp;Liu\n<div class=\"imageTitle\">AI Privacy vLab</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776429356_3.Screenshot_2023_11_23_at_4.46.23__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776429356_3.Screenshot_2023_11_23_at_4.46.23__8239_PM--rgov-800width.png\" title=\"AI Security vLab API\"><img src=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776429356_3.Screenshot_2023_11_23_at_4.46.23__8239_PM--rgov-66x44.png\" alt=\"AI Security vLab API\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">AI-CyberSecurity cross-training vLabs and modules</div>\n<div class=\"imageCredit\">Authors</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ling&nbsp;Liu\n<div class=\"imageTitle\">AI Security vLab API</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776505853_4.Screenshot_2023_11_23_at_4.48.08__8239_PM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776505853_4.Screenshot_2023_11_23_at_4.48.08__8239_PM--rgov-800width.png\" title=\"Ai Privacy Attack and defense\"><img src=\"/por/images/Reports/POR/2023/2038029/2038029_10690504_1700776505853_4.Screenshot_2023_11_23_at_4.48.08__8239_PM--rgov-66x44.png\" alt=\"Ai Privacy Attack and defense\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">AI Privacy Lab example modules</div>\n<div class=\"imageCredit\">authors</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Ling&nbsp;Liu\n<div class=\"imageTitle\">Ai Privacy Attack and defense</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nArtificial Intelligence (AI) is being rapidly deployed in many security-critical applications. This has fueled the use of AI to improve Cybersecurity in speed of reasoning and reaction (AI for security), and at the same time, wide spread use of AI introduces a variety of new adversarial threats inherent AI systems and calls for developing robustness guarantees for AI (Security for AI).We argue that privacy enhancing technologies and innovations are at the center of these three facets for mitigating adverse effect of intentional exploitations and protecting privately trained models and their sensitive training data. However, very limited curriculum or hands-on learning resources are available for AI-Cybersecurity cross training, and even fewer programs in computer science, data science and engineering integrate privacy enhancing fundamentals, techniques and research innovations in their basic curriculum covering AI or Cybersecurity.\n\n\nTo bridge such cross training gap, this project takes a principled approach to create a privacy-enhancing AI-Cybersecurity cross training research pilot program. We develop the risk assessment tools to identify and categorize the types of risks that threaten the privacy, utility and trust of AI models, and the risk mitigation tools to manage and detect risks, regulate access to sensitive data, and secure the AI models trained over sensitive or proprietary data. These tools are packaged in AI Privacy vLab and AI Security vLab as educational vehicles to provide hand-on experiences and enable students to gain an in-depth understanding of AI privacy risks and AI Security threats, to learn how to design, measure, evaluate both the robustness of AI and the effectiveness of risk mitigation strategies.\n\n\nThis project has designed a first-of-its-kind research-teaching alliance method towards effective AI-Cybersecurity cross training through the AI privacy lens. We develop a privacy foundation virtual laboratory to experiment a unique educational innovation to AI-Cybersecurity cross-training.TheAI for Security vLabhas enabled students to learn that privacy is a critical system property for all AI-enabled cybersecurity systems and applications, given the high sensitivity of cybersecurity-related training data and high confidentiality of AI models trained for cybersecurity applications.\n\n\nOur NSF SaTC EDU funded AI-Cybersecurity cross training project took a radically different and yet methodical approach to AI-Cybersecurity education. It creates a new way of providing AI-Cybersecurity cross training program by synergizing AI-Privacy and AI-Security for training the next generation of AI-Cybersecurity workforce. We conjecture that the findings could help laying the groundwork for training and preparing students with the knowledge and skills required to develop and deploy next generation of secure and trustworthy AI-Cybersecurity systems.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/26/2023\n\n\t\t\t\t\tSubmitted by: LingLiu\n"
 }
}