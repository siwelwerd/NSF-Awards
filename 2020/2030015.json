{
 "awd_id": "2030015",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID: Time-Sensitive Human Forest and Model Forecasts for COVID-19 Vaccine and Treatment Trials",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927263",
 "po_email": "roconnor@nsf.gov",
 "po_sign_block_name": "Robert O'Connor",
 "awd_eff_date": "2020-08-15",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-08-21",
 "awd_max_amd_letter_date": "2020-10-14",
 "awd_abstract_narration": "Accurate, time-specific predictions are important for planning and decision making during fast-moving pandemics. In particular, whether an effective COVID-19 vaccine will be available in 9, 12, or 18 months is an issue of vital national interest. The main objective of this project is to compare the accuracy of a new method for crowd-based forecasting of time-specific outcomes\u2013such as clinical trial transitions of COVID-19 treatments and vaccines\u2013to that of new machine learning models. The research will examine the relative strengths of crowd and modeling methods and explore combinations of the two in predicting clinical trial results. A forecasting tournament is the project\u2019s main method for human data collection. It starts in 2020 and continues until 2021. People with interest in forecasting and clinical trials are encouraged to sign up for participation, independently of their background. Study participants complete surveys and forecasting training, and will then have the opportunity to make probabilistic forecasts on specific trial events over several months, with regular accuracy feedback. To broaden the impacts of this work, the research team disseminates the aggregate forecasts about clinical trial phase transition of COVID-19 treatments and vaccines through public health information channels. These forecasts, combined with predictive training and accuracy feedback provided to study participants, may aid the coordination of public health and clinical development efforts to overcome the pandemic.\r\n \r\nThe primary research goal of the project is to improve the predictive performance of crowd-based methods, machine models and ensembles of the two. Psychologists have shown that taking the outside view, by examining a prediction problem in context of historical reference classes, improves accuracy. The crowd-based  approach, referred to as human forest, combines reference class forecasting and collective intelligence approaches to produce data-driven estimates from a group of forecasters. The time-specific human forest variant employs a survival analysis approach, enabling forecasters to construct reference classes  and obtain unbiased historical estimates in the presence of missing data. On the decision science front, the research goals include testing the effects of interfaces featuring historical estimates; understanding the psychology of reference class selection; examining time-scope sensitivity in judgmental forecasting; and assessing the relative importance of subject matter expertise versus general predictive competence. On the machine-modeling front, the research goals include integrating survival-type models into machine learning and improving their performance using bi-level optimization to choose hyper-parameters. The results are released as soon as they become available.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sauleh",
   "pi_last_name": "Siddiqui",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sauleh Siddiqui",
   "pi_email_addr": "sauleh@american.edu",
   "nsf_id": "000625966",
   "pi_start_date": "2020-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Pavel",
   "pi_last_name": "Atanasov",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Pavel D Atanasov",
   "pi_email_addr": "pavel@pytho.io",
   "nsf_id": "000794863",
   "pi_start_date": "2020-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Regina",
   "pi_last_name": "Joseph",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Regina Joseph",
   "pi_email_addr": "rj@pytho.io",
   "nsf_id": "000796296",
   "pi_start_date": "2020-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "American University",
  "inst_street_address": "4400 MASSACHUSETTS AVE NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2028853440",
  "inst_zip_code": "200168003",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "AMERICAN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "H4VNDUN2VWU5"
 },
 "perf_inst": {
  "perf_inst_name": "American University",
  "perf_str_addr": "4400 Massachusetts Avenue, NW",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200160001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132100",
   "pgm_ele_name": "Decision, Risk & Mgmt Sci"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "096Z",
   "pgm_ref_txt": "COVID-19 Research"
  },
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ab52f94b-7fff-0c50-1f8a-e34c4ffe0084\"> </span></p>\n<p><span id=\"docs-internal-guid-ab52f94b-7fff-0c50-1f8a-e34c4ffe0084\"> </span></p>\n<p dir=\"ltr\"><span>Clinical trials determine if therapies and vaccines can reach patients. </span><span>Better predictions about the outcomes of clinical trials can help in public health planning. As observed in the COVID-19 pandemic, predicting when therapies and vaccines may become available can be almost as important as estimating the eventual outcome.</span></p>\n<p dir=\"ltr\"><span>But making such predictions is difficult. Researchers have developed machine learning algorithms to estimate the probability of transition through clinical trial phases toward eventual approval. Previous research in other high-impact areas has shown that machine learning models are not always the most accurate. Human crowds, groups of forecasters with interest in the topic, have shown relatively high accuracy in predicting outcomes in broad domains like geopolitics and economics.</span></p>\n<p dir=\"ltr\"><span>Our study was designed to answer two related questions. First, are machine models or human crowds better at predicting the timely progression of clinical trials? Second, does Human Forest, a new method for reference class prediction, help human forecasters achieve better accuracy than those using basic methods?</span></p>\n<p dir=\"ltr\"><span>To address these questions, we ran two 6-month forecasting tournaments that took place between September 15, 2020 and February 20, 2021 (Tournament 1, T1); between September 15, 2021 and February 20, 2022 (Tournament 2, T2). The human and machine model teams answered 48 questions in common (28 in T1; 20 in T2). The questions were designed to provide a level playing field between the human and machine model teams. They asked about the probability of trial progression for COVID-19 and other infectious disease vaccines and treatments. Research questions and methods were pre-registered for both T1 and T2.</span></p>\n<p dir=\"ltr\"><span>The machine model team developed a time-specific random survival forest model (RSF) to predict the phase transition of clinical trials. This machine model prediction was compared to the Human Forest prediction described above. The RSF model is an extension of a random forest model that predicts overall phase transition (yes/no). With the time-specific model, the project developed a machine model to predict phase transition (yes/no) by a certain date. The model was trained using data from clinicaltrials.gov and BioMedtracker.&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>The human crowdsourcing workflow involved several elements. First, over 800 volunteers were recruited for participation. Approximately 40% of them had educational or professional background in the life sciences, or previous forecasting tournament experience. Psychometric tests assessed forecaster profiles, e.g., numeracy, cognitive reflection, general and life science knowledge.</span></p>\n<p dir=\"ltr\"><span>Second, training interventions prepared for the tournaments. A 25-minute animated training film (Full Training), and a shorter, limited version of the training film (Basic Training) were provided to randomly assigned cohort segments. Cohort segments assigned to a Control Polls condition received basic information about clinical trial phases and transition base rates. Human Forest participants viewed a 10-minute clip about using the new web platform.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Third, a core project objective was developing and assessing the Human Forest method. Built to enable users to define custom reference classes, the platform was conceived as an information scaffold designed to ingest data corpora (in this case, the same dataset used by the machine model team) and automatically calculate the time-specific base rate of phase transition of the user-generated reference classes. For example, a forecaster may consider a predictively useful reference class to include all trials for antiviral drugs in Phase II, and obtain the base rate of transition from Phase II to III for a given period (e.g., next 4 months). Another forecaster may select a smaller reference class with only trials initiated in the past 5 years. Each forecaster can choose to manually update their estimate away from the base rates, incorporating any additional information. Each forecaster's reference class selection can be represented as a classification tree. The statistical combination of classification trees and adjusted estimates across forecasters thus forms the aggregate Human Forest estimate.</span></p>\n<p dir=\"ltr\"><span>Forecasters across all conditions and therapeutic areach submitted over 60 forecasters registering over 1,000 forecasts across all questions in each tournament. Results show that all human forecasting methods generated lower Brier scores than the RSF model, denoting higher accuracy. Relative to RSF models, human crowdsourcing methods produced Brier score improvements between 36% and 52%, i.e., between one third and one half smaller accuracy errors. Human Forest and Control Polls conditions exhibited similar aggregate accuracy. However, individual forecasters using the Human Forest method exhibited significantly better accuracy than their Control Polls peers. This Human Forest advantage was larger than any individual differences among forecasters, e.g., due to life-science or forecasting experience.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The project funded research for two graduate students and one undergraduate student. The results were presented at selective international conferences and seminars. Two manuscripts are under preparation for submission to peer-reviewed publications.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/23/2022<br>\n\t\t\t\t\tModified by: Sauleh&nbsp;Siddiqui</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\n \nClinical trials determine if therapies and vaccines can reach patients. Better predictions about the outcomes of clinical trials can help in public health planning. As observed in the COVID-19 pandemic, predicting when therapies and vaccines may become available can be almost as important as estimating the eventual outcome.\nBut making such predictions is difficult. Researchers have developed machine learning algorithms to estimate the probability of transition through clinical trial phases toward eventual approval. Previous research in other high-impact areas has shown that machine learning models are not always the most accurate. Human crowds, groups of forecasters with interest in the topic, have shown relatively high accuracy in predicting outcomes in broad domains like geopolitics and economics.\nOur study was designed to answer two related questions. First, are machine models or human crowds better at predicting the timely progression of clinical trials? Second, does Human Forest, a new method for reference class prediction, help human forecasters achieve better accuracy than those using basic methods?\nTo address these questions, we ran two 6-month forecasting tournaments that took place between September 15, 2020 and February 20, 2021 (Tournament 1, T1); between September 15, 2021 and February 20, 2022 (Tournament 2, T2). The human and machine model teams answered 48 questions in common (28 in T1; 20 in T2). The questions were designed to provide a level playing field between the human and machine model teams. They asked about the probability of trial progression for COVID-19 and other infectious disease vaccines and treatments. Research questions and methods were pre-registered for both T1 and T2.\nThe machine model team developed a time-specific random survival forest model (RSF) to predict the phase transition of clinical trials. This machine model prediction was compared to the Human Forest prediction described above. The RSF model is an extension of a random forest model that predicts overall phase transition (yes/no). With the time-specific model, the project developed a machine model to predict phase transition (yes/no) by a certain date. The model was trained using data from clinicaltrials.gov and BioMedtracker.  \nThe human crowdsourcing workflow involved several elements. First, over 800 volunteers were recruited for participation. Approximately 40% of them had educational or professional background in the life sciences, or previous forecasting tournament experience. Psychometric tests assessed forecaster profiles, e.g., numeracy, cognitive reflection, general and life science knowledge.\nSecond, training interventions prepared for the tournaments. A 25-minute animated training film (Full Training), and a shorter, limited version of the training film (Basic Training) were provided to randomly assigned cohort segments. Cohort segments assigned to a Control Polls condition received basic information about clinical trial phases and transition base rates. Human Forest participants viewed a 10-minute clip about using the new web platform. \nThird, a core project objective was developing and assessing the Human Forest method. Built to enable users to define custom reference classes, the platform was conceived as an information scaffold designed to ingest data corpora (in this case, the same dataset used by the machine model team) and automatically calculate the time-specific base rate of phase transition of the user-generated reference classes. For example, a forecaster may consider a predictively useful reference class to include all trials for antiviral drugs in Phase II, and obtain the base rate of transition from Phase II to III for a given period (e.g., next 4 months). Another forecaster may select a smaller reference class with only trials initiated in the past 5 years. Each forecaster can choose to manually update their estimate away from the base rates, incorporating any additional information. Each forecaster's reference class selection can be represented as a classification tree. The statistical combination of classification trees and adjusted estimates across forecasters thus forms the aggregate Human Forest estimate.\nForecasters across all conditions and therapeutic areach submitted over 60 forecasters registering over 1,000 forecasts across all questions in each tournament. Results show that all human forecasting methods generated lower Brier scores than the RSF model, denoting higher accuracy. Relative to RSF models, human crowdsourcing methods produced Brier score improvements between 36% and 52%, i.e., between one third and one half smaller accuracy errors. Human Forest and Control Polls conditions exhibited similar aggregate accuracy. However, individual forecasters using the Human Forest method exhibited significantly better accuracy than their Control Polls peers. This Human Forest advantage was larger than any individual differences among forecasters, e.g., due to life-science or forecasting experience. \nThe project funded research for two graduate students and one undergraduate student. The results were presented at selective international conferences and seminars. Two manuscripts are under preparation for submission to peer-reviewed publications.\n\n\t\t\t\t\tLast Modified: 08/23/2022\n\n\t\t\t\t\tSubmitted by: Sauleh Siddiqui"
 }
}