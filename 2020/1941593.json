{
 "awd_id": "1941593",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Interpretable neural network models of morphological realization",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032924770",
 "po_email": "rtheodor@nsf.gov",
 "po_sign_block_name": "Rachel M. Theodore",
 "awd_eff_date": "2020-03-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 273415.0,
 "awd_amount": 273415.0,
 "awd_min_amd_letter_date": "2020-02-03",
 "awd_max_amd_letter_date": "2020-02-03",
 "awd_abstract_narration": "Human languages have complex systems for expressing meaning and grammatical information with morphology, the elements which make up words (e.g., rules of prefixing, suffixing, and copying). The project will develop a novel computational model that is both highly interpretable and capable of learning a broad range of morphological rules from examples. The model will be evaluated on its ability to induce morphological systems from large naturalistic data sets containing many rules and exceptions and to match human performance in generalizing morphological rules from small amounts of evidence in controlled experiments. By incorporating insights from linguistics and other areas of cognitive science, the model will provide a bridge between the studies of human and artificial intelligence. In virtue of its modular and transparent design, the model will shed light on the uniquely human capacity to learn and extend linguistic patterns. The project will provide interdisciplinary training opportunities in linguistics, cognitive psychology, artificial intelligence, and data science for students at many levels and from diverse backgrounds.\r\n\r\nA large body of research in linguistics has identified the general properties of morphological systems and the restricted ways in which they vary across languages, while recent advances in artificial intelligence have given rise to computational models that can learn morphology with minimal supervision. These two approaches, however, have been developed largely in isolation from one another. The project will develop a novel kind of deep neural network that is understandable and interpretable, in the sense that its representations and operations are continuous versions of the discrete symbolic components of linguistic theory, and that can induce a broad range of morphological realization rules from positive evidence with domain-general learning algorithms (e.g., stochastic gradient descent). The model will be evaluated on large naturalistic data sets that are common in natural language processing and on results from new miniature artificial grammar experiments on infixation, intercalation, and reduplication. The project aims to demonstrate that deep neural networks can make transformative contributions to the study of language structure and acquisition when they have a modular design that mirrors the fundamental representations and operations of symbolic linguistic theory.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Colin",
   "pi_last_name": "Wilson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Colin Wilson",
   "pi_email_addr": "wilson@cogsci.jhu.edu",
   "nsf_id": "000567973",
   "pi_start_date": "2020-02-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N Charles Street",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182686",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 273415.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A central property of human language is that words can be built up from more basic units, known as morphemes, that carry their own meaning or grammatical function. Roots, prefixes, and suffixes are the most common types of morpheme (as in the English word <em>re</em><em>-</em><em>build-ing</em>), but many others are found across languages: infixes (e.g., Chamorro <em>tr-um-isti</em>&nbsp;'become sad'), circumfixes (German <em>ge-spiel-t</em>&nbsp;'played'), interfixes (Modern Hebrew <em>g-a-d-a-l</em>&nbsp;'it grew'), ablauts (e.g., English <em>sing</em> ~ <em>sang</em>), reduplicants (e.g., Amele <em>bolob-bolob '</em>many traps' and <em>bi-bilen</em><em>&nbsp;'</em>as he stood'), and others. The study of morphemes has a long and rich tradition in descriptive and theoretical linguistics; more recently there has also been extensive computational and psychological research on this aspect of language. The project summarized here exploited advances in artificial intelligence to develop a novel neural network model of morphology that unifies traditional and computational approaches, that is robust, interpretable, and explainable by design, and that performs comparably to humans in challenging low-data or few-shot experiments on morphology learning and generalization. Several key components of this model have been implemented in trainable neural layers that represent morphemes as distributed patterns of activity, that locate morphemes as prefixes, suffixes, infixes, or in other positions relative to the stem of the word, that selectively copy and locate parts of a root in reduplication, and that apply structured modifications to morphemes depending on their local context (for example, English <em>in-tolerable</em> vs. <em>im-possible</em>). Purely symbolic models were also implemented for the purpose of comparing their performance and data efficiency against state-of-the-art neural computation, and data resources in Javanese, Hindi-Urdu, and other languages were annotated with information that is relevant for morphological patterning. Several novel behavioral experiments investigated human ability to abstract and generalize morphemes that are unfamiliar in their native languages, such as infixation and partial reduplication, from a small number of spoken and written examples, and to apply native morphemes to familiar and nonce roots in a speeded production paradigm. Taken together, the findings of this project could transform scientific and technological approaches to morphology by focusing future research on theoretically-motivated, data-efficient, interpretable and high performing neural models that approach human-level cognition and performance in this domain. Some specific implications are that the neural model, like human experimental participants, can rapidly readily learn the spoken forms of specific morphemes, which vary extensively across languages, as well as their positions, which belong to a restricted set of possibilities, from brief and limited exposure. In addition to these impacts, the project provided significant training in statistical and corpus analysis of spoken and written language data for many undergraduates and graduate research assistants, the findings have been disseminated to the scientific community in the form of several publications, and substantial bodies of code have been developed to provide a foundation for further computational modeling.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/24/2024<br>\nModified by: Colin&nbsp;Wilson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nA central property of human language is that words can be built up from more basic units, known as morphemes, that carry their own meaning or grammatical function. Roots, prefixes, and suffixes are the most common types of morpheme (as in the English word re-build-ing), but many others are found across languages: infixes (e.g., Chamorro tr-um-isti'become sad'), circumfixes (German ge-spiel-t'played'), interfixes (Modern Hebrew g-a-d-a-l'it grew'), ablauts (e.g., English sing ~ sang), reduplicants (e.g., Amele bolob-bolob 'many traps' and bi-bilen'as he stood'), and others. The study of morphemes has a long and rich tradition in descriptive and theoretical linguistics; more recently there has also been extensive computational and psychological research on this aspect of language. The project summarized here exploited advances in artificial intelligence to develop a novel neural network model of morphology that unifies traditional and computational approaches, that is robust, interpretable, and explainable by design, and that performs comparably to humans in challenging low-data or few-shot experiments on morphology learning and generalization. Several key components of this model have been implemented in trainable neural layers that represent morphemes as distributed patterns of activity, that locate morphemes as prefixes, suffixes, infixes, or in other positions relative to the stem of the word, that selectively copy and locate parts of a root in reduplication, and that apply structured modifications to morphemes depending on their local context (for example, English in-tolerable vs. im-possible). Purely symbolic models were also implemented for the purpose of comparing their performance and data efficiency against state-of-the-art neural computation, and data resources in Javanese, Hindi-Urdu, and other languages were annotated with information that is relevant for morphological patterning. Several novel behavioral experiments investigated human ability to abstract and generalize morphemes that are unfamiliar in their native languages, such as infixation and partial reduplication, from a small number of spoken and written examples, and to apply native morphemes to familiar and nonce roots in a speeded production paradigm. Taken together, the findings of this project could transform scientific and technological approaches to morphology by focusing future research on theoretically-motivated, data-efficient, interpretable and high performing neural models that approach human-level cognition and performance in this domain. Some specific implications are that the neural model, like human experimental participants, can rapidly readily learn the spoken forms of specific morphemes, which vary extensively across languages, as well as their positions, which belong to a restricted set of possibilities, from brief and limited exposure. In addition to these impacts, the project provided significant training in statistical and corpus analysis of spoken and written language data for many undergraduates and graduate research assistants, the findings have been disseminated to the scientific community in the form of several publications, and substantial bodies of code have been developed to provide a foundation for further computational modeling.\r\n\n\n\t\t\t\t\tLast Modified: 12/24/2024\n\n\t\t\t\t\tSubmitted by: ColinWilson\n"
 }
}