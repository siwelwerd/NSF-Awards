{
 "awd_id": "2007907",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Scrutable and Explainable Information Retrieval with Model Intrinsic and Agnostic Approaches",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 199998.0,
 "awd_amount": 199998.0,
 "awd_min_amd_letter_date": "2020-08-18",
 "awd_max_amd_letter_date": "2020-10-19",
 "awd_abstract_narration": "Information Retrieval (IR) systems are important for people for information access. For example, intelligent search engines are widely used in Web-based services such as web search, product search, and job search. Recently, sophisticated data and complicated black-box models have made modern IR systems less transparent to users. However, as more and more people rely on IR systems to guide their daily life and decision making, there has been growing needs of explainable search results, both for technical communities and the general public, so that they understand why certain search results are provided. Meanwhile, governmental agencies are demanding IR systems to provide not only high-quality results, but also reasonable justifications, so as to enhance the trustworthiness of the systems. This project focuses on developing algorithms and frameworks to improve the scrutability, explainability, and transparency of modern IR systems. It will inspire large-scale academic-industry collaboration, which benefits billions of users by facilitating the development of reliable and explainable information access services. \r\n\r\nThis project will develop general and reusable frameworks for scrutable and explainable IR. Research in this project will be performed on two directions. The first direction aims at new retrieval models for model-intrinsic explanation. This includes developing transparent inference process and decision boundaries for retrieval actions, scrutable functions that support result exploration with user feedback, and traceable information flow to distinguish the contribution of model inputs. The second direction aims at building analytical and simulative framework for model-agnostic explanation. This includes post-hoc explanation systems with external knowledge, and a simulation framework over black-box retrieval models with explainable outputs. Besides model-intrinsic and model-agnostic approaches, this project will also investigate crowd-sourcing tasks and systematic metrics to compare the effectiveness of intrinsic and agnostic explanations. The research outcomes will include multiple public benchmark datasets and evaluation platforms for explainable IR, which will contribute to the research community for sustainable and reproducible future studies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yongfeng",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yongfeng Zhang",
   "pi_email_addr": "yongfeng.zhang@rutgers.edu",
   "nsf_id": "000763753",
   "pi_start_date": "2020-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "33 Knightsbridge Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088543925",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 199998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div>\n<p><span>The objective of this project is to create explainable and user understandable machine learning models and assessment methods for information retrieval, enabling both designers and users of IR systems to grasp the rationale behind specific system decisions. This enhances the system's credibility. The project has produced 26 scholarly papers for various conferences and journals, and its findings have been disseminated across multiple research communities, including Information Retrieval, Recommender Systems, Machine Learning, Data Mining, Natural Language Processing, and the broader AI field.</span></p>\n<p><span>This project facilitates intelligent systems in producing natural language explanations, structural explanations, knowledge-enhanced explanations, and counterfactual explanations, making AI decisions more comprehensible to users with limited or no background in AI or computer science. These explanations, due to their straightforward nature, provide users with greater control over recommender systems by illustrating potential outcomes of different interactions with recommended items. Moreover, the challenge of explanation evaluation in this field has been addressed by this project. It introduced several quantitative methods for assessing the quality of natural language explanations, allowing for reproducible comparison and evaluation of improvements in these methods. The project also devised ways to measure the impact of explanations on enhancing AI systems' trustworthiness, including their controllability and fairness. These advancements offer researchers standard and user-friendly metrics for explanation evaluation.</span></p>\n<p><span>Beyond technical achievements, the project has been actively involved in research dissemination. The Principal Investigators (PIs) hosted workshops at WSDM and SIGIR to promote the significance and research efforts in explainable AI within Information Retrieval. They also provided tutorials on large language models for recommendation, fairness in recommendations, and conversational recommendation at various conferences, such as SIGIR, WSDM, RecSys, IUI, and CIKM, further disseminating the research goals and insights.</span></p>\n<p><span>The project also made contributions in the educational realm. The concept of explainability is crucial for trustworthy and responsible AI. The search and recommendation framework with explainability features, developed through this project, has been incorporated into the PI's course projects. Explainability is also included as a key aspect in the PI's lectures, offering students practical experience in developing explainable AI methods and understanding responsible application of these techniques.</span></p>\n<p><span>Lastly, as search and recommendation systems increasingly influence decision-making on the web and in intelligent systems, this project underscores the necessity of explaining algorithmic decisions to users. The research outcomes initiate discussions on explainable AI in legal, business, and political contexts, benefiting society by enhancing the transparency of AI decisions for everyday users.</span></p>\n</div><br>\n<p>\n Last Modified: 01/28/2024<br>\nModified by: Yongfeng&nbsp;Zhang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\nThe objective of this project is to create explainable and user understandable machine learning models and assessment methods for information retrieval, enabling both designers and users of IR systems to grasp the rationale behind specific system decisions. This enhances the system's credibility. The project has produced 26 scholarly papers for various conferences and journals, and its findings have been disseminated across multiple research communities, including Information Retrieval, Recommender Systems, Machine Learning, Data Mining, Natural Language Processing, and the broader AI field.\n\n\nThis project facilitates intelligent systems in producing natural language explanations, structural explanations, knowledge-enhanced explanations, and counterfactual explanations, making AI decisions more comprehensible to users with limited or no background in AI or computer science. These explanations, due to their straightforward nature, provide users with greater control over recommender systems by illustrating potential outcomes of different interactions with recommended items. Moreover, the challenge of explanation evaluation in this field has been addressed by this project. It introduced several quantitative methods for assessing the quality of natural language explanations, allowing for reproducible comparison and evaluation of improvements in these methods. The project also devised ways to measure the impact of explanations on enhancing AI systems' trustworthiness, including their controllability and fairness. These advancements offer researchers standard and user-friendly metrics for explanation evaluation.\n\n\nBeyond technical achievements, the project has been actively involved in research dissemination. The Principal Investigators (PIs) hosted workshops at WSDM and SIGIR to promote the significance and research efforts in explainable AI within Information Retrieval. They also provided tutorials on large language models for recommendation, fairness in recommendations, and conversational recommendation at various conferences, such as SIGIR, WSDM, RecSys, IUI, and CIKM, further disseminating the research goals and insights.\n\n\nThe project also made contributions in the educational realm. The concept of explainability is crucial for trustworthy and responsible AI. The search and recommendation framework with explainability features, developed through this project, has been incorporated into the PI's course projects. Explainability is also included as a key aspect in the PI's lectures, offering students practical experience in developing explainable AI methods and understanding responsible application of these techniques.\n\n\nLastly, as search and recommendation systems increasingly influence decision-making on the web and in intelligent systems, this project underscores the necessity of explaining algorithmic decisions to users. The research outcomes initiate discussions on explainable AI in legal, business, and political contexts, benefiting society by enhancing the transparency of AI decisions for everyday users.\n\t\t\t\t\tLast Modified: 01/28/2024\n\n\t\t\t\t\tSubmitted by: YongfengZhang\n"
 }
}