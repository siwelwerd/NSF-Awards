{
 "awd_id": "2053269",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Learning with Low-Quality Visual Data: Handling Both Passive and Active Degradations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 77253.0,
 "awd_amount": 77253.0,
 "awd_min_amd_letter_date": "2020-09-16",
 "awd_max_amd_letter_date": "2020-10-14",
 "awd_abstract_narration": "This project is focused on effectively and robustly exploiting low-quality (LQ) visual data for computer vision tasks. While most current computer vision systems are designed for high-quality visual data, collected from \"clear\" environments where subjects are well observable without significant attenuation or alteration, a dependable vision system must reckon with the entire spectrum of degradations from unconstrained environments. With various degradations arising from the visual data acquisition and processing pipeline, the ubiquitous LQ visual data can dramatically deteriorate the model performance in practice. The project outcome can broadly benefit a variety of real-world applications, such as video surveillance, autonomous/assisted driving, robotics and medical image analysis, where LQ visual data has constituted major performance and reliability bottlenecks. \r\n\r\nThis research categorizes common degradations into the two types: \"passive degradations\" that are caused by uncontrollable environment factors (such as bad weather and low light); and \"active degradations\" that are intentionally introduced in a controllable way to meet certain budget requirements (such as lossy compression). The project will mainly addresses two important technical questions: i) how to overcome passive degradations and achieve more robust high-level task performance on LQ video data, using end-to-end deep learning models; and ii) how to properly introduce and control active degradations to generate the desired form of LQ data, that both satisfies certain budget requirements and maintains the target task utility, using deep adversarial learning models. The resulting new techniques are to be verified on application examples such as video recognition, video annotation, video compression, and de-identified video data sharing for recognition purpose.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhangyang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhangyang Wang",
   "pi_email_addr": "atlaswang@utexas.edu",
   "nsf_id": "000746175",
   "pi_start_date": "2020-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "3925 W Braker Lane Ste 3.340",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787595316",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 77253.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span>The overall goal of the proposed research is to effectively and robustly exploit low-quality (LQ) visual data for computer vision tasks. With various degradations arising from the visual data acquisition and processing pipeline, the ubiquitous LQ visual data can dramatically deteriorate the model performance in many classical and emerging applications. Depending on how the degradations are introduced to the visual data, we categorize common degradations into the following two types: (1) Passive degradations: the acquisition of visual data from the real world is often jeopardized by uncontrollable degradations, caused by complex unconstrained environments, such as bad weather and low illumination in the night, or sensor quality limitations; (2)&nbsp; Active degradations: the visual data transmission, storage and analytics are often manually introduced with controllable degradations, in order to meet certain budget requirements. Sometimes, videos are even intentionally captured or processed to be extremely low-resolution, to create privacy-preserving &ldquo;anonymized videos\".&nbsp;</span></p>\n<p class=\"p1\"><span>There are two important questions to be addressed in this proposal: i) how to overcome passive degradations and achieve robust high-level task performance on LQ visual data; and ii) how to properly introduce and control active data degradations to generate LQ data. To address those two challenges, we have accomplished to the following research outcomes during this project:</span></p>\n<p class=\"p1\"><span>(i) Development of novel algorithms for enhancing image or video quality, in the presence of haze, rain, low light, or motion blur. Analyzing their impacts on subsequent visual understanding tasks</span></p>\n<p class=\"p1\"><span>(ii) Development of novel algorithm for generating more effective&nbsp;&ldquo;anonymized videos\" that balance between video utility and privacy protection, using the core technique of adversarial training.</span></p>\n<p class=\"p1\"><span>(iii) Establishment of three public dataset benchmarks on visual understanding from hazy imagery, rainy imagery, and \"anonymized videos\". Those datasets are becoming popular in the research community.</span></p>\n<p><span>The project also has more broad impact outcomes, besides publications/open-source codes/public datasets, including: (1) many invited talks, especially three tutorials in CVPR/ICCV/ECCV; (2) new courses developed in TAMU/UT Austin, plus two invited short courses&nbsp;at University of Sao Paulo, Brazil; (3) two PhD students (including one female) and a few more undergraduates are involved in this research.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/09/2021<br>\n\t\t\t\t\tModified by: Zhangyang&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The overall goal of the proposed research is to effectively and robustly exploit low-quality (LQ) visual data for computer vision tasks. With various degradations arising from the visual data acquisition and processing pipeline, the ubiquitous LQ visual data can dramatically deteriorate the model performance in many classical and emerging applications. Depending on how the degradations are introduced to the visual data, we categorize common degradations into the following two types: (1) Passive degradations: the acquisition of visual data from the real world is often jeopardized by uncontrollable degradations, caused by complex unconstrained environments, such as bad weather and low illumination in the night, or sensor quality limitations; (2)  Active degradations: the visual data transmission, storage and analytics are often manually introduced with controllable degradations, in order to meet certain budget requirements. Sometimes, videos are even intentionally captured or processed to be extremely low-resolution, to create privacy-preserving \"anonymized videos\". \nThere are two important questions to be addressed in this proposal: i) how to overcome passive degradations and achieve robust high-level task performance on LQ visual data; and ii) how to properly introduce and control active data degradations to generate LQ data. To address those two challenges, we have accomplished to the following research outcomes during this project:\n(i) Development of novel algorithms for enhancing image or video quality, in the presence of haze, rain, low light, or motion blur. Analyzing their impacts on subsequent visual understanding tasks\n(ii) Development of novel algorithm for generating more effective \"anonymized videos\" that balance between video utility and privacy protection, using the core technique of adversarial training.\n(iii) Establishment of three public dataset benchmarks on visual understanding from hazy imagery, rainy imagery, and \"anonymized videos\". Those datasets are becoming popular in the research community.\n\nThe project also has more broad impact outcomes, besides publications/open-source codes/public datasets, including: (1) many invited talks, especially three tutorials in CVPR/ICCV/ECCV; (2) new courses developed in TAMU/UT Austin, plus two invited short courses at University of Sao Paulo, Brazil; (3) two PhD students (including one female) and a few more undergraduates are involved in this research.\n\n\t\t\t\t\tLast Modified: 09/09/2021\n\n\t\t\t\t\tSubmitted by: Zhangyang Wang"
 }
}