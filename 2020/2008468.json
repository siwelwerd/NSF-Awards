{
 "awd_id": "2008468",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: Small: Mitigating Network Bottlenecks via Programmability for Distributed Machine Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 496595.0,
 "awd_amount": 496595.0,
 "awd_min_amd_letter_date": "2020-08-17",
 "awd_max_amd_letter_date": "2020-10-19",
 "awd_abstract_narration": "Distributed machine learning (ML) is becoming an important way to allow multiple learning agents to train on separate slices of the same dataset simultaneously and exchange what they have learned with each other periodically over a network. Due to the significant bandwidth gap between network and processor units, the network is likely to become the bottleneck in these types of systems. To mitigate this issue, this project is developing a distributed ML algorithm and network system co-design to adapt training algorithms to make better use of network resources. First a programmable communication subsystem is proposed to accelerate training synchronization. Specifically, a comprehensive study on the impact of network congestion over distributed ML models will be conducted to provide unique insights. The project is also enhancing existing frameworks by integrating in-network control and exploring synchronization schemes that dynamically adjust learning hyper-parameters based on network signals. Next a scheduler that optimizes the utilization of heterogeneous computing resources is proposed. To that end, both deterministic and learning-based scheduling algorithms are being explored and a framework that enables operation-level scheduling for finer-grained control is being developed.  \r\n\r\nThe proposed research investigates in-network control to mitigate network congestion which remains the biggest challenge for High Performance Computing (HPC) processors. It will significantly improve the training efficiency of the existing distributed training frameworks. In addition, the comprehensive and systematic studies will provide insights to the algorithm and system co-design solutions. The developed framework will also help students and researchers in their big data research projects. New courses will be developed based on the outcomes of the proposed work and new curriculum and training sessions on networking and distributed ML will be developed in High School Tech Camps during the summer. Source code, raw data, and simulation results generated in the project will be stored in standard formats and will be published in the public domain. All data will be archived on the departmental servers at Case Western Reserve University (CWRU) for increased availability and reliability.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "An",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "An Wang",
   "pi_email_addr": "axw474@case.edu",
   "nsf_id": "000788143",
   "pi_start_date": "2020-08-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Case Western Reserve University",
  "inst_street_address": "10900 EUCLID AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CLEVELAND",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "2163684510",
  "inst_zip_code": "441064901",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "OH11",
  "org_lgl_bus_name": "CASE WESTERN RESERVE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJMKEF7EJW69"
 },
 "perf_inst": {
  "perf_inst_name": "Case Western Reserve University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "441064901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "OH11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 496595.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"whitespace-pre-wrap break-words\">In today's world, artificial intelligence and machine learning are becoming increasingly important to our daily life, from healthcare diagnostics to autonomous vehicles. Training these AI systems requires enormous amounts of data and computing power, often spread across many computing nodes working together - a process called distributed machine learning. However, due to the large amount of data exchanged during training (often hundreds of gigabytes to terabytes), network congestion between these computers can significantly slow down the training process. Our project developed new ways to make distributed machine learning systems work more efficiently by addressing these network bottlenecks. Specifically, our framework dynamically adjusts how these training nodes share information based on network conditions..</p>\r\n<p class=\"whitespace-pre-wrap break-words\">Key achievements of our project include:</p>\r\n<ul class=\"-mt-1 [li&gt;&amp;]:mt-2 list-disc space-y-2 pl-8\">\r\n</ul>\r\n<p>1.Development of programmable network controls that can monitor and adjust data flow in real-time, using advanced networking technologies like programmable switches and custom protocols to optimize data transmission patterns<br />2. Creation of an intelligent scheduler that coordinates transmission between computers to minimize network congestion, using reinforcement learning based approaches.<br />3. Implementation of adaptive learning techniques that allow the system to automatically optimize its performance by dynamically adjusting training parameters (such as batch sizes and learning rates) based on network conditions.<br />4. Design of a flexible, service-oriented architecture that can scale up or down based on computing needs.</p>\r\n<ul class=\"-mt-1 [li&gt;&amp;]:mt-2 list-disc space-y-2 pl-8\">\r\n</ul>\r\n<p class=\"whitespace-pre-wrap break-words\">Beyond the technical achievements, our project has had several broader impacts:</p>\r\n<ol class=\"-mt-1 [li&gt;&amp;]:mt-2 list-decimal space-y-2 pl-8\"> </ol>\r\n<p>1. Educational Impact: We developed new course materials and hands-on projects that have helped students learn about distributed systems and machine learning. These materials are being used in multiple courses at Case Western Reserve University, including EECS 325/425 Computer Networks and CSDS 428 Computer Networks II with a Special Topic on SDN.</p>\r\n<ol class=\"-mt-1 [li&gt;&amp;]:mt-2 list-decimal space-y-2 pl-8\"> </ol>\r\n<p>2. Diversity in Computing: Through partnerships with CWRU's Women in Tech Initiative and participation in the ACM-W Chapter, we helped introduce more young female and underrepresented students to careers in computing. Our efforts included organizing workshops and mentoring sessions on AI and machine learning systems.</p>\r\n<ol class=\"-mt-1 [li&gt;&amp;]:mt-2 list-decimal space-y-2 pl-8\"> </ol>\r\n<p>3. Open Source Contribution: We've made all our software tools and research findings freely available to the research community through GitHub repositories and published papers, allowing other scientists and engineers to build upon our work. Our framework has been designed with modularity in mind, making it easy for others to extend and customize.</p>\r\n<ol class=\"-mt-1 [li&gt;&amp;]:mt-2 list-decimal space-y-2 pl-8\"> </ol>\r\n<p class=\"whitespace-pre-wrap break-words\">The techniques we developed can help organizations run their AI systems more efficiently, potentially reducing both computing costs and energy consumption. In our experiments, these improvements resulted in up to 50% reduction in training time for large-scale machine learning models. This is particularly important as AI systems become more prevalent in solving complex problems across many fields, from climate science to drug discovery. Our project has laid the groundwork for more efficient distributed computing systems, while also helping to train the next generation of computer scientists and engineers. As artificial intelligence continues to grow in importance, the tools and techniques we've developed will help make these systems more practical and accessible for researchers and organizations worldwide.</p>\r\n<p class=\"whitespace-pre-wrap break-words\">Through our investigation of distributed machine learning systems, we also uncovered several important challenges that extend beyond our initial focus. For example, we discovered that communication bottlenecks pose even more severe challenges in federated learning - a privacy-preserving approach where models are trained across many devices while keeping data local. Additionally, our research revealed that the periodic communication patterns during training could potentially leak sensitive model information, raising important privacy and security considerations. These findings have been published in peer-reviewed venues and have helped shape new research directions in secure and efficient distributed machine learning systems. These discoveries underscore the broader impact of our work, as they contribute not only to system efficiency but also to the development of more secure and privacy-preserving model training methods.</p><br>\n<p>\n Last Modified: 12/17/2024<br>\nModified by: An&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn today's world, artificial intelligence and machine learning are becoming increasingly important to our daily life, from healthcare diagnostics to autonomous vehicles. Training these AI systems requires enormous amounts of data and computing power, often spread across many computing nodes working together - a process called distributed machine learning. However, due to the large amount of data exchanged during training (often hundreds of gigabytes to terabytes), network congestion between these computers can significantly slow down the training process. Our project developed new ways to make distributed machine learning systems work more efficiently by addressing these network bottlenecks. Specifically, our framework dynamically adjusts how these training nodes share information based on network conditions..\r\n\n\nKey achievements of our project include:\r\n\r\n\r\n\n\n1.Development of programmable network controls that can monitor and adjust data flow in real-time, using advanced networking technologies like programmable switches and custom protocols to optimize data transmission patterns\n2. Creation of an intelligent scheduler that coordinates transmission between computers to minimize network congestion, using reinforcement learning based approaches.\n3. Implementation of adaptive learning techniques that allow the system to automatically optimize its performance by dynamically adjusting training parameters (such as batch sizes and learning rates) based on network conditions.\n4. Design of a flexible, service-oriented architecture that can scale up or down based on computing needs.\r\n\r\n\r\n\n\nBeyond the technical achievements, our project has had several broader impacts:\r\n \r\n\n\n1. Educational Impact: We developed new course materials and hands-on projects that have helped students learn about distributed systems and machine learning. These materials are being used in multiple courses at Case Western Reserve University, including EECS 325/425 Computer Networks and CSDS 428 Computer Networks II with a Special Topic on SDN.\r\n \r\n\n\n2. Diversity in Computing: Through partnerships with CWRU's Women in Tech Initiative and participation in the ACM-W Chapter, we helped introduce more young female and underrepresented students to careers in computing. Our efforts included organizing workshops and mentoring sessions on AI and machine learning systems.\r\n \r\n\n\n3. Open Source Contribution: We've made all our software tools and research findings freely available to the research community through GitHub repositories and published papers, allowing other scientists and engineers to build upon our work. Our framework has been designed with modularity in mind, making it easy for others to extend and customize.\r\n \r\n\n\nThe techniques we developed can help organizations run their AI systems more efficiently, potentially reducing both computing costs and energy consumption. In our experiments, these improvements resulted in up to 50% reduction in training time for large-scale machine learning models. This is particularly important as AI systems become more prevalent in solving complex problems across many fields, from climate science to drug discovery. Our project has laid the groundwork for more efficient distributed computing systems, while also helping to train the next generation of computer scientists and engineers. As artificial intelligence continues to grow in importance, the tools and techniques we've developed will help make these systems more practical and accessible for researchers and organizations worldwide.\r\n\n\nThrough our investigation of distributed machine learning systems, we also uncovered several important challenges that extend beyond our initial focus. For example, we discovered that communication bottlenecks pose even more severe challenges in federated learning - a privacy-preserving approach where models are trained across many devices while keeping data local. Additionally, our research revealed that the periodic communication patterns during training could potentially leak sensitive model information, raising important privacy and security considerations. These findings have been published in peer-reviewed venues and have helped shape new research directions in secure and efficient distributed machine learning systems. These discoveries underscore the broader impact of our work, as they contribute not only to system efficiency but also to the development of more secure and privacy-preserving model training methods.\t\t\t\t\tLast Modified: 12/17/2024\n\n\t\t\t\t\tSubmitted by: AnWang\n"
 }
}