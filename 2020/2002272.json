{
 "awd_id": "2002272",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Theory of Optimization Geometry and Algorithms for Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2019-10-15",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 399992.0,
 "awd_amount": 399992.0,
 "awd_min_amd_letter_date": "2019-11-12",
 "awd_max_amd_letter_date": "2019-11-12",
 "awd_abstract_narration": "Deep learning has attracted a significant amount of interest in recent years due to its widespread applicability in computer vision, artificial intelligence and natural language processing, alongside recent strides in autonomous driving. The theoretical underpinnings behind such success, however, remain elusive to a large extent, hindering its further adoption in other applications. This project aims to advance the theoretical foundations of training neural networks in terms of optimization landscape and algorithmic efficacy, which in turn should have a measurable impact on the practice of deep learning by providing guiding principles for network design, algorithm selection, hyperparameter tuning, and adversarial training. This project adopts an interdisciplinary approach fusing ideas from machine learning, optimization, statistical signal processing, high-dimensional statistics, nonparametric statistics, and information theory. This project will likewise develop courses and tutorials on theoretical foundations of large-scale machine learning and provide extensive training opportunities for students at all levels.\r\n\r\nThis project aims to develop a comprehensive theory to characterize the optimization landscape and geometry of loss functions and algorithmic regularizations of major neural network training problems, and explore how the network architecture---including depth, width, and activation functions---affect these properties, thus providing guidelines for the design of algorithms to train these networks more efficiently with theoretical performance guarantees. The project will explore the geometric properties and their impact on the optimization performance in training multi-layer neural networks, auto-encoders, generative adversarial networks, and adversarial training involving non-convex and saddle-point problems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jason Lee",
   "pi_email_addr": "jasondlee88@gmail.com",
   "nsf_id": "000782428",
   "pi_start_date": "2019-11-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University Electrical Engineering",
  "perf_str_addr": "Engineering Quad",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 399992.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"font-size: xx-small;\">The aim of this project is to study the interaction of nonconvex optimization landscapes and geometry in the context of machine learning problems. In practice, gradient-based algorithms have been incredibly successful in attacking high-dimensional nonconvex problems as sen throughout deep learning and artificial intelligence. On the other hand, the theoretical successes&nbsp; of optimization are not well understood and results are scarce. The goal of this project is to systematically understand the role of gradient algorithms, dynamics, and geometry.</span><span style=\"font-size: xx-small;\"><br /></span><span style=\"font-size: xx-small;\">This project led to several major results. First, we were able to characterize that shallow neural networks can learn multindex models at near-optimal sample and time complexity. Furthermore their representations allow for efficient transfer learning and pretraining, confirming the success of foundations models. This led to a significant understanding in what shallow networks are computationally efficiently learnable by developing the limits of statistical query algorithms and the corresponding generative exponent that measures difficulty. Next, these results were extended to deep feedforward networks, allowing for nonlinear feature learning, and transformers to learn associations between tokens.</span><span style=\"font-size: xx-small;\"><br /></span><span style=\"font-size: xx-small;\"><br /></span><span style=\"font-size: xx-small;\">Second, we solved 2 COLT open problems on optimization dynamics. The first is on the convergence rate of gradient descent with non-constant learning rate schedules. The second is on the sample complexity of multi-distribution learning, which has broad applications in fairness, robustness, and distribution shift. Finally, we settled the sample complexity of online reinforcement learning by designing a new variance-reduction algorithm that is optimal. This closes a long line of work over the past decade on the sample and time complexity of reinforcement learning.</span><span style=\"font-size: xx-small;\"><br /></span><span style=\"font-size: xx-small;\">Beyond the state-of-art results, algorithms, and&nbsp; theory from this project, the results have been presented in leading machine learning conferences and journals and presented in a NeurIPS tutorial. This has also given training opportunities for 3 postdoctoral scholars, now faculty in leading universities and several graduate students that are entering the academic job market this year.</span></p><br>\n<p>\n Last Modified: 02/02/2025<br>\nModified by: Jason&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe aim of this project is to study the interaction of nonconvex optimization landscapes and geometry in the context of machine learning problems. In practice, gradient-based algorithms have been incredibly successful in attacking high-dimensional nonconvex problems as sen throughout deep learning and artificial intelligence. On the other hand, the theoretical successes of optimization are not well understood and results are scarce. The goal of this project is to systematically understand the role of gradient algorithms, dynamics, and geometry.\nThis project led to several major results. First, we were able to characterize that shallow neural networks can learn multindex models at near-optimal sample and time complexity. Furthermore their representations allow for efficient transfer learning and pretraining, confirming the success of foundations models. This led to a significant understanding in what shallow networks are computationally efficiently learnable by developing the limits of statistical query algorithms and the corresponding generative exponent that measures difficulty. Next, these results were extended to deep feedforward networks, allowing for nonlinear feature learning, and transformers to learn associations between tokens.\n\nSecond, we solved 2 COLT open problems on optimization dynamics. The first is on the convergence rate of gradient descent with non-constant learning rate schedules. The second is on the sample complexity of multi-distribution learning, which has broad applications in fairness, robustness, and distribution shift. Finally, we settled the sample complexity of online reinforcement learning by designing a new variance-reduction algorithm that is optimal. This closes a long line of work over the past decade on the sample and time complexity of reinforcement learning.\nBeyond the state-of-art results, algorithms, and theory from this project, the results have been presented in leading machine learning conferences and journals and presented in a NeurIPS tutorial. This has also given training opportunities for 3 postdoctoral scholars, now faculty in leading universities and several graduate students that are entering the academic job market this year.\t\t\t\t\tLast Modified: 02/02/2025\n\n\t\t\t\t\tSubmitted by: JasonLee\n"
 }
}