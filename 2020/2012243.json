{
 "awd_id": "2012243",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "An Accelerated Decomposition Framework for Structured Sparse Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2020-07-15",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-07-11",
 "awd_max_amd_letter_date": "2020-07-11",
 "awd_abstract_narration": "There has been an explosion in the availability of data that is collected from countless sources and through various modalities. For instance, medical image archives are increasing by 20-40% each year and over 400 million procedures per year involve at least one medical image.  A goal among engineers and scientists is the design of advanced scientific tools that can use data to aid humanity.  Many such tools already exist but their success is tightly bound to the idea of problem \u201csparsity\u201d.  For example, when predicting whether a patient in an intensive care unit will develop septic shock, only a few medical measurements are truly helpful in making such predictions. Since there are few important measurements compared to the total measurements available to a doctor, the prediction problem can be viewed as \u201csparse\u201d. Despite the success of existing methods for sparse problems, their inadequacy for many modern machine learning and other types of problems has gradually been noticed by researchers. Since covariates often come in groups (e.g., genes that regulate hormone levels), one may wish to select them jointly instead of individually so that the models deployed make practical sense. Similar concerns occur in other important healthcare settings such as in the prediction of Parkinson's disease. This project will design, analyze, implement, and validate a new optimization framework that can handle these more complicated notions of \u201csparsity\u201d beyond the simplest ones currently analyzed in theory and used in practice. This project provides research training opportunities for graduate students.\r\n\r\nThe minimization of a function composed of a loss/data-fitting term and a regularization function is of immense interest throughout science and engineering.  The past decade has witnessed an explosion of interest in problems involving sparsity-promoting regularization such as the L1-norm. Moving past simple L1-norm regularization, researchers are continually realizing the potential benefits of using more intricate regularization functions that promote structured sparsity, such as the group L1-norm and elastic net functions. The proposed project involves the design, analysis, and implementation of new algorithms for solving optimization problems that involve such structure promoting regularization.  The algorithms will be designed to be broadly applicable, scalable, and efficient, and will be shown to possess strong convergence rate guarantees.  The novelty of the proposed algorithmic framework is a carefully defined \"space decomposition with subspace acceleration\" mechanism.  This mechanism adaptively decomposes the search space, and employs subspace steps based on proximal point and reduced-space Newton-type techniques.  The step decomposition aspect of the methodology makes it more scalable and efficient than, say, straightforward first-order methods.  The PIs will also enhance their general approach by designing new innovative strategies that combine domain decomposition and subspace acceleration in such a way that good complexity properties are obtained, accurate solution support estimates can be reliably achieved, and state-of-the-art numerical performance is attained.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Robinson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Robinson",
   "pi_email_addr": "dpr219@lehigh.edu",
   "nsf_id": "000607397",
   "pi_start_date": "2020-07-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Curtis",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Frank E Curtis",
   "pi_email_addr": "fec309@lehigh.edu",
   "nsf_id": "000549599",
   "pi_start_date": "2020-07-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Lehigh University",
  "inst_street_address": "526 BRODHEAD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BETHLEHEM",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "6107583021",
  "inst_zip_code": "180153008",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "PA07",
  "org_lgl_bus_name": "LEHIGH UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E13MDBKHLDB5"
 },
 "perf_inst": {
  "perf_inst_name": "Lehigh University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "180153005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "PA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has led to the design, analysis, implementation, and testing of new state-of-the-art algorithms for solving optimization problems that incorporate regularization into their formulation. Such optimization problems arise when learning statistical models (e.g., training a deep neural network to produce predictions), performing various image-related tasks in computer vision, and understanding biological processes in computational medicine, as examples.&nbsp; Unlike nearly all previous work in the area, this grant has focused on more complicated regularizers that lead to significantly more challenging optimization problems.&nbsp; The algorithmic framework studied during this project has produced many variants that are applicable in many settings.&nbsp; In particular, we have designed variants that can handle objective functions that can be computed exactly, objective functions that can only be computed inexactly (as is quite common in large-scale machine learning applications), regularizers that have closed-form proximal-operator solutions, regularizers whose proximal operator cannot be computed in closed form, problems with no constraints, and problems with deterministic constraints.&nbsp; The methods we developed all have the so-called support-identification property, meaning that the intended solution structure, as determined by the choice of regularizer, will be correctly identified by the iterates of our methods in a finite number of iterations.&nbsp; As part of our analysis, we gave an upper bound on the maximum number of iterations needed by our methods before the correct support is identified by the iterates of our method.&nbsp; These analyses are some of the first of their kind, in the problem context studied. Our numerical tests compared our methods to state-of-the-art algorithms, and convincingly illustrated the merits and superiority of our approach in terms of efficiency, reliability, and ability to produced structured solutions.</p>\n<p>This grant has supported the training of two different Ph.D. students in the design, analysis, and implementation of optimization algorithms. One of those students has recently graduated and accepted a research position working on the design of optimal models using some of the techniques and ideas developed through this grant.&nbsp; In total, six papers have been submitted, five of which have already been accepted for publication.&nbsp;</p><br>\n<p>\n Last Modified: 07/21/2024<br>\nModified by: Daniel&nbsp;Robinson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project has led to the design, analysis, implementation, and testing of new state-of-the-art algorithms for solving optimization problems that incorporate regularization into their formulation. Such optimization problems arise when learning statistical models (e.g., training a deep neural network to produce predictions), performing various image-related tasks in computer vision, and understanding biological processes in computational medicine, as examples. Unlike nearly all previous work in the area, this grant has focused on more complicated regularizers that lead to significantly more challenging optimization problems. The algorithmic framework studied during this project has produced many variants that are applicable in many settings. In particular, we have designed variants that can handle objective functions that can be computed exactly, objective functions that can only be computed inexactly (as is quite common in large-scale machine learning applications), regularizers that have closed-form proximal-operator solutions, regularizers whose proximal operator cannot be computed in closed form, problems with no constraints, and problems with deterministic constraints. The methods we developed all have the so-called support-identification property, meaning that the intended solution structure, as determined by the choice of regularizer, will be correctly identified by the iterates of our methods in a finite number of iterations. As part of our analysis, we gave an upper bound on the maximum number of iterations needed by our methods before the correct support is identified by the iterates of our method. These analyses are some of the first of their kind, in the problem context studied. Our numerical tests compared our methods to state-of-the-art algorithms, and convincingly illustrated the merits and superiority of our approach in terms of efficiency, reliability, and ability to produced structured solutions.\n\n\nThis grant has supported the training of two different Ph.D. students in the design, analysis, and implementation of optimization algorithms. One of those students has recently graduated and accepted a research position working on the design of optimal models using some of the techniques and ideas developed through this grant. In total, six papers have been submitted, five of which have already been accepted for publication.\t\t\t\t\tLast Modified: 07/21/2024\n\n\t\t\t\t\tSubmitted by: DanielRobinson\n"
 }
}