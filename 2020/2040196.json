{
 "awd_id": "2040196",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID: Augmented Intelligence for Accelerating Covid-Related Scientific Discovery",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 199998.0,
 "awd_amount": 199998.0,
 "awd_min_amd_letter_date": "2020-07-28",
 "awd_max_amd_letter_date": "2020-07-28",
 "awd_abstract_narration": "The project will develop new artificial intelligence (AI) methods to augment the productivity of biomedical researchers and accelerate scientific discovery in the context of the COVID-19 pandemic. We will issue weekly updates to our widely-used Cord-19 and SciSight resources, which are a critical resource for researchers studying SARS-CoV-2, having already been downloaded over 100,000 times by other researchers. We will also extend these resources to make them more useful to doctors and researchers in two ways. First, we will automatically generate one-sentence summaries of each paper to speed sensemaking of the rapidly changing literature. Second, we will automatically extract a broad range of entities (such as disease symptoms and research challenges) and relations to improve filtering and search. \r\n\r\nIn order to generate one-sentence summaries of research papers, we will train an abstractive BART model, using two novel techniques: 1) co-training on the auxiliary task of title prediction, and 2) fine-tuning using a set of one-sentence summaries that we will generate by crowd-sourcing edits peer-review comments taken from sites such as OpenReview. We will test our one-sentence summary generation with a combination of automated (Rouge) metrics and user preference. In order to increase the number of entities and relations extracted from research papers, we will bootstrap with data-programming techniques then apply graph-neural-network methods. We will evaluate our progress using a combination of expert-annotated data and held out information from relevant knowledge bases.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Weld",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel S Weld",
   "pi_email_addr": "danw@allenai.org",
   "nsf_id": "000099760",
   "pi_start_date": "2020-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "Box 352350",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "096Z",
   "pgm_ref_txt": "COVID-19 Research"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 199998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed new artificial intelligence powered tools to help accelerate scientific discovery in the context of the Covid-19 pandemic. We focused on two specific tasks that will benefit both scientists and MDs: summarization of research papers and knowledge graph construction from scientific text, as described below.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Extreme Summarization of Scientific Documents</strong></p>\n<p>We introduced the task of &nbsp;`TLDR generation&rsquo; - &nbsp;a new form of extreme summarization, for scientific papers. (The acronym TLDR stands for `too long; didn&rsquo;t read.&rsquo;) TLDR generation involves creating a 15-25 word summary of a research paper that captures the most important elements of the research: why it&rsquo;s important, what is novel, how the research was performed, and how the results were evaluated. Creating a good TLDR is hard, because it involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. After specifying the task of TLDR generation, we created a dataset of example paper/TLDR pairs. We then developed a novel AI system using neural networks and used machine learning to train the network to perform the task. We show that our method performs well and distribute both data and code as open-source for the community.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Extracting a Knowledge Base of Mechanisms from COVID-19 Papers</strong></p>\n<p>The COVID-19 pandemic has led to the publication of hundreds of thousands of research papers on related viruses. This explosion of papers makes it hard or impossible for scientists to read or even skim summaries of all potentially relevant papers. In response, our project has developed methods for building a symbolic representation of the results described in these papers, called a knowledge base. We are not the first to explore methods for creating knowledge bases from the English text of scientific papers, but our methods have improved on past results in several important ways. 1) We introduced a unified schema for mechanisms that generalizes across many types of activities, functions and influences. 2) We construct and distribute an annotated dataset of papers related to COVID-19. &nbsp;3) Using this dataset, we train a text extraction model and apply it to 160K research-paper abstracts from the COVID-19 literature, constructing a knowledge base of 1.5 million mechanism instances. 4) In a study with MDs working to combat COVID-19, our system was rated higher than PubMed search in terms of utility and quality. We made our search engine, dataset and code publicly available.</p>\n<p><strong>&nbsp;</strong></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/12/2022<br>\n\t\t\t\t\tModified by: Daniel&nbsp;S&nbsp;Weld</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed new artificial intelligence powered tools to help accelerate scientific discovery in the context of the Covid-19 pandemic. We focused on two specific tasks that will benefit both scientists and MDs: summarization of research papers and knowledge graph construction from scientific text, as described below. \n\n \n\nExtreme Summarization of Scientific Documents\n\nWe introduced the task of  `TLDR generation\u2019 -  a new form of extreme summarization, for scientific papers. (The acronym TLDR stands for `too long; didn\u2019t read.\u2019) TLDR generation involves creating a 15-25 word summary of a research paper that captures the most important elements of the research: why it\u2019s important, what is novel, how the research was performed, and how the results were evaluated. Creating a good TLDR is hard, because it involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. After specifying the task of TLDR generation, we created a dataset of example paper/TLDR pairs. We then developed a novel AI system using neural networks and used machine learning to train the network to perform the task. We show that our method performs well and distribute both data and code as open-source for the community. \n\n \n\nExtracting a Knowledge Base of Mechanisms from COVID-19 Papers\n\nThe COVID-19 pandemic has led to the publication of hundreds of thousands of research papers on related viruses. This explosion of papers makes it hard or impossible for scientists to read or even skim summaries of all potentially relevant papers. In response, our project has developed methods for building a symbolic representation of the results described in these papers, called a knowledge base. We are not the first to explore methods for creating knowledge bases from the English text of scientific papers, but our methods have improved on past results in several important ways. 1) We introduced a unified schema for mechanisms that generalizes across many types of activities, functions and influences. 2) We construct and distribute an annotated dataset of papers related to COVID-19.  3) Using this dataset, we train a text extraction model and apply it to 160K research-paper abstracts from the COVID-19 literature, constructing a knowledge base of 1.5 million mechanism instances. 4) In a study with MDs working to combat COVID-19, our system was rated higher than PubMed search in terms of utility and quality. We made our search engine, dataset and code publicly available.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/12/2022\n\n\t\t\t\t\tSubmitted by: Daniel S Weld"
 }
}