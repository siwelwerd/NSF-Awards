{
 "awd_id": "2007257",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Articulate+ - A Conversational Interface for Democr atizing Visual Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 333000.0,
 "awd_amount": 333000.0,
 "awd_min_amd_letter_date": "2020-09-15",
 "awd_max_amd_letter_date": "2021-08-31",
 "awd_abstract_narration": "The Internet gives the public, including scientists, access to vast amounts of data. Visualization is a powerful tool to take that data and turn it into a form that is easier to understand, helping people make better decisions. However, creating good visualizations from data is not easy. The Articulate+ Project will work to generate new knowledge about how to empower people with information visualization, by using speech and gesture as inputs, in new ways. A goal is to enable users to benefit from imprecise statements about what they want, rather than needing to give specific commands telling the computer what to do. This project will make it easier for citizens to engage in conversations with the computer about economic, medical data, demographic, transportation data, climate, and sports data; to answers that are more meaningful, and so to democratize data literacy. \r\n\r\nThis research will use speech, gesture, and log data as new input interaction modalities for visual analytics. It will develop new understandings of how the imprecise and vague nature of natural language queries and gestures, as contextualized by work on a specific visual analysis problem, can be modeled as an opportunity for an intelligent software system to learn more about the underlying intent of the users. This model of intent, in turn, will be used to provide contextualized visualizations, which are expected to help those users gain valuable insights from their data. The audio speech data will be used to computationally model overhearing, to help infer users' current contextualized goals. The gesture data will be used to disambiguate expressions that refer to visual elements of a visualization (e.g., \"that pie chart about electricity consumption\"). The project will develop new understandings of how to combine the log of visualization state, the computational model of overhearing, and the gesture data in order to generate new visualizations in support of users\u2019 work on tasks. Evaluations will be performed in both controlled laboratory and naturalistic study settings to determine whether effective semantic parsers can be derived for these specific visual analysis domains, and how this contextual interface affects users\u2019 experience of visualization and discovery.  The integrated annotated datasets from the studies will be made available to the research community satisfying a need for ecologically valid situated language interaction corpora.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Johnson",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew E Johnson",
   "pi_email_addr": "ajohnson@uic.edu",
   "nsf_id": "000332025",
   "pi_start_date": "2020-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Barbara",
   "pi_last_name": "DiEugenio",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Barbara DiEugenio",
   "pi_email_addr": "bdieugen@uic.edu",
   "nsf_id": "000192544",
   "pi_start_date": "2020-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Moira",
   "pi_last_name": "Zellner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Moira Zellner",
   "pi_email_addr": "m.zellner@northeastern.edu",
   "nsf_id": "000072111",
   "pi_start_date": "2020-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Chicago",
  "inst_street_address": "809 S MARSHFIELD AVE M/C 551",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3129962862",
  "inst_zip_code": "606124305",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "W8XEAJDKMXH3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Chicago",
  "perf_str_addr": "809 S Marshfield",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606124305",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 163901.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 169099.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-649d224c-7fff-deda-43bb-d28c1abd289e\"> </span></p>\n<p dir=\"ltr\"><span>The goal of this project is to make it possible for humans to speak to a computer as naturally as they would another human, and have the computer be able to interpret complex data and produce answers in the form of data visualizations such as charts and graphs. This is important because it enables experts such as scientists and engineers, as well as non-experts such as policy makers, decision makers and the general public, to better understand complex data through the use of state-of-the-art visualization techniques (the process of turning data into informative charts), without having to be data visualization experts themselves.</span></p>\n<p dir=\"ltr\">In working to achieve this we made advances in Artificial Intelligence (AI) and data visualization research, especially in how to create natural language processing techniques to translate natural human speech to the language of data visualization and to support collaboration among people.</p>\n<p dir=\"ltr\">The project successfully developed and demonstrated a working prototype (Articulate+) which was able to produce data visualizations by listening to a conversation between two people, and interjecting during the conversation to produce data visualizations that were relevant to the conversation. Our system was tested with 50 participants (in 25 groups of two), who could ask natural questions on COVID data; Articulate+ was able to answer questions by interfacing to a number of authentic datasets and producing relevant charts (such as 'show me the correlation between COVID risk and access to medical care'). This moves us closer to a future where AI systems can routinely aid humans in complex tasks as if the AI were a human assistant- like a co-pilot of an airplane.</p>\n<p dir=\"ltr\">Our work represents a major paradigm shift from conventional AI systems like Amazon's Alexa and Apple's Siri, which can only respond to a user when explicitly commanded to do so using a wake word, like 'Alexa&hellip;' or 'Hey Siri&hellip;'. Our system is able to listen to a conversation, build up a context of what has already been said, and use that context to determine the relevant charts to produce. We discovered a number of benefits to this approach:</p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>By having an AI continuously listening to a conversation, the collected context can be used by our system to correct imprecise, misheard, or incomplete queries from users, enabling the AI to better determine the intent of its users during a conversation.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>The common use of wake words in conventional AI systems, has an unanticipated effect of conditioning users to speak in an unnatural and stilted way when conversing with AI. By eliminating the need for wake words, conversations between users and our system flowed more naturally like in a human to human conversation.&nbsp;</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>Our project also examined how well current state-of-the-art AI systems like ChatGPT can be made to perform data visualization tasks, in particular how its Large Language Models can be improved to produce more accurate data visualizations. We discovered that currently, ChatGPT is able to produce only the most basic chart types.</span></p>\n<p><span>During the course of this project 4 papers were published. One graduate student obtained his MS degree, and is continuing toward a PhD degree in AI, another graduate student finished her MS degree and is continuing toward a PhD in human-computer interaction, </span><span>and yet another graduate&nbsp; student is continuing on with her PhD in natural language processing</span><span>. One graduate student completed a PhD, and another graduated to become an assistant professor. </span><span>Of the 5 students mentioned here, 3 are women.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/21/2023<br>\nModified by: Andrew&nbsp;E&nbsp;Johnson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThe goal of this project is to make it possible for humans to speak to a computer as naturally as they would another human, and have the computer be able to interpret complex data and produce answers in the form of data visualizations such as charts and graphs. This is important because it enables experts such as scientists and engineers, as well as non-experts such as policy makers, decision makers and the general public, to better understand complex data through the use of state-of-the-art visualization techniques (the process of turning data into informative charts), without having to be data visualization experts themselves.\n\n\nIn working to achieve this we made advances in Artificial Intelligence (AI) and data visualization research, especially in how to create natural language processing techniques to translate natural human speech to the language of data visualization and to support collaboration among people.\n\n\nThe project successfully developed and demonstrated a working prototype (Articulate+) which was able to produce data visualizations by listening to a conversation between two people, and interjecting during the conversation to produce data visualizations that were relevant to the conversation. Our system was tested with 50 participants (in 25 groups of two), who could ask natural questions on COVID data; Articulate+ was able to answer questions by interfacing to a number of authentic datasets and producing relevant charts (such as 'show me the correlation between COVID risk and access to medical care'). This moves us closer to a future where AI systems can routinely aid humans in complex tasks as if the AI were a human assistant- like a co-pilot of an airplane.\n\n\nOur work represents a major paradigm shift from conventional AI systems like Amazon's Alexa and Apple's Siri, which can only respond to a user when explicitly commanded to do so using a wake word, like 'Alexa' or 'Hey Siri'. Our system is able to listen to a conversation, build up a context of what has already been said, and use that context to determine the relevant charts to produce. We discovered a number of benefits to this approach:\n\n\n\n\nBy having an AI continuously listening to a conversation, the collected context can be used by our system to correct imprecise, misheard, or incomplete queries from users, enabling the AI to better determine the intent of its users during a conversation.\n\n\n\n\nThe common use of wake words in conventional AI systems, has an unanticipated effect of conditioning users to speak in an unnatural and stilted way when conversing with AI. By eliminating the need for wake words, conversations between users and our system flowed more naturally like in a human to human conversation.\n\n\n\n\nOur project also examined how well current state-of-the-art AI systems like ChatGPT can be made to perform data visualization tasks, in particular how its Large Language Models can be improved to produce more accurate data visualizations. We discovered that currently, ChatGPT is able to produce only the most basic chart types.\n\n\nDuring the course of this project 4 papers were published. One graduate student obtained his MS degree, and is continuing toward a PhD degree in AI, another graduate student finished her MS degree and is continuing toward a PhD in human-computer interaction, and yet another graduate student is continuing on with her PhD in natural language processing. One graduate student completed a PhD, and another graduated to become an assistant professor. Of the 5 students mentioned here, 3 are women.\n\n\n\t\t\t\t\tLast Modified: 12/21/2023\n\n\t\t\t\t\tSubmitted by: AndrewEJohnson\n"
 }
}