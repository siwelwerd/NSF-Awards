{
 "awd_id": "2006885",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Domain-robust object detection through shape and context",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 461846.0,
 "awd_amount": 461846.0,
 "awd_min_amd_letter_date": "2020-08-10",
 "awd_max_amd_letter_date": "2020-08-10",
 "awd_abstract_narration": "Computer vision has made great advancements in object recognition and detection, but performance drops significantly when the data used at training and deployment time are very different. This is problematic because in many situations, it may be infeasible to retrain the models on a large example set in the domain of interest. For example, artificial intelligence (AI) tools may be developed in one country or region, using that region\u2019s training data, and exported to regions with limited resources to collect new data and retrain models. Unfortunately, the visual environment in the user region may be different from the developer region: some vehicles in India look different from common vehicles in the US; houses often feature bricks on the US East Coast but less frequently on the West Coast; environmental factors (e.g., foliage and smog) may cause models to behave differently. Being robust to domain shifts is important for interpretability and trust when computer vision systems are employed in practice. This project leverages the observation that while the pixels of captured objects change when these objects are shown in different domains (e.g., photographs vs paintings), the overall shape of the objects remains the same. Further, the set of objects that co-occur with the object of interest is also relatively consistent across domains. \r\n\r\nThis project develops new visual representations that capture two global cues: shape and context. While numerous domain adaptations and generalization techniques exist, they have overlooked global cues that can potentially be more robust to domain shifts, based on preliminary experiments. The first proposed representation adapts the medial axis transform (MAT) into a hierarchical, learnable, convolutional representation. MAT computes the \"skeleton\" of an object, and a representation is developed using a dense feature map to ensure there is enough information for the convolutional network to capture, as well as to build robustness to small shifts. Second, context is represented through graphs containing functionally or semantically related objects, and ambient cues (such as co-occurring text or speech) to improve the model's ability to recognize objects in novel modalities. Techniques for making weakly-supervised techniques more robust to domain shifts are explored, as a way of capturing non-semantic context. Next, these global representations are combined with standard appearance-based ones and are adapted to novel domains or made domain-invariant through domain generalization techniques. The domain robustness of the resulting representations is tested in a variety of domain shift scenarios, including photorealistic and artistic datasets, different capture conditions, and controllable shift scenarios (e.g., blurring and masking), for both object recognition and detection. Code, any artificially created situations (data), clear protocols for how to train models for existing techniques, and detailed benchmarking results (quantitative and qualitative) will be released to ensure reproducibility.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adriana",
   "pi_last_name": "Kovashka",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adriana Kovashka",
   "pi_email_addr": "kovashka@cs.pitt.edu",
   "nsf_id": "000690095",
   "pi_start_date": "2020-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "4200 Fifth Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152600001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 461846.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed global representations for object detection,  focusing on shape and context. For shape, the project examined a medial  axis transform-like skeleton representation and demonstrated its impact  on images with low amount of texture. It investigated visual-shape  contrastive learning with a distance transform. It also studied  shape-preserving self-supervised techniques. These works were published  in ECCVW 2020, CVPRW 2022, ICMR 2023 and AAAIW 2023.</p>\r\n<p>In our  second focus on language context for object detection, we examined the  role of language as a bridge to domain gaps at the feature level, or at  the semantic level (through objects with distinct properties). We  studied the sensitivity of vision-language methods to attribute context  in captions (and how to boost use of attributes for improved  open-vocabulary detection), the impact of synonyms and noise in  vision-language data (as well as how to detect and filter this noise),  and the unique perceptions of different speakers reflected in captions  provided in different languages. Finally, we studied extensions such as  atypical objects and how language can help reason about those, sports  analysis through a textual rubric, and human-object interaction with  weak vision-language data. These works were published in CVPR 2024,  NAACL 2024 Findings, EMNLP 2024, BMVC 2023, WACV 2024, EACL 2024, CVPRW  2024, and WACV 2025.</p>\r\n<p>Nine graduate students and multiple undergraduate students were supported and mentored through this grant.</p><br>\n<p>\n Last Modified: 11/26/2024<br>\nModified by: Adriana&nbsp;Kovashka</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project developed global representations for object detection,  focusing on shape and context. For shape, the project examined a medial  axis transform-like skeleton representation and demonstrated its impact  on images with low amount of texture. It investigated visual-shape  contrastive learning with a distance transform. It also studied  shape-preserving self-supervised techniques. These works were published  in ECCVW 2020, CVPRW 2022, ICMR 2023 and AAAIW 2023.\r\n\n\nIn our  second focus on language context for object detection, we examined the  role of language as a bridge to domain gaps at the feature level, or at  the semantic level (through objects with distinct properties). We  studied the sensitivity of vision-language methods to attribute context  in captions (and how to boost use of attributes for improved  open-vocabulary detection), the impact of synonyms and noise in  vision-language data (as well as how to detect and filter this noise),  and the unique perceptions of different speakers reflected in captions  provided in different languages. Finally, we studied extensions such as  atypical objects and how language can help reason about those, sports  analysis through a textual rubric, and human-object interaction with  weak vision-language data. These works were published in CVPR 2024,  NAACL 2024 Findings, EMNLP 2024, BMVC 2023, WACV 2024, EACL 2024, CVPRW  2024, and WACV 2025.\r\n\n\nNine graduate students and multiple undergraduate students were supported and mentored through this grant.\t\t\t\t\tLast Modified: 11/26/2024\n\n\t\t\t\t\tSubmitted by: AdrianaKovashka\n"
 }
}