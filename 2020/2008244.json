{
 "awd_id": "2008244",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Content-Aware Mapping of Streaming AI Workloads on Heterogeneous Edge Devices",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 498262.0,
 "awd_amount": 498262.0,
 "awd_min_amd_letter_date": "2020-06-16",
 "awd_max_amd_letter_date": "2020-06-16",
 "awd_abstract_narration": "Humans can seamlessly detect and classify objects from a wide range of complex data sources, and draw inferences to make predictions and decisions. New types of algorithms known as deep neural networks (DNN), are being developed to endow computers with very same capabilities.  The present approach of transferring all the data to a remote datacenter and have the algorithms executed there is not sustainable because the amount of data being generated is growing exponentially, is too slow, and can compromise privacy and security. The aim of this project is to enable the execution of complex DNN algorithms at or near the place of data acquisition. Referred to as \"AI at the edge\", nearly all the leading industries are developing varieties of new \"edge devices\" to be deployed in the field. This project will develop a framework consisting of technology agnostic software tools that will optimally deploy the DNN algorithms on heterogenous networks of edge devices to maximize their performance and energy efficiency. Domains that will benefit from the outcomes of this project, include retail, security, transportation and logistics, factory automation, healthcare etc. The project team will also include graduate and undergraduate students. Strong effort to recruit students from underrepresented groups will be made. The team will also vigorously pursue various avenues for commercialization. \r\n\r\nThe aim of this project is to enable \"AI at the Edge\" using DNN algorithms, which can be trained on any kind of data, in any number of dimensions, and then used to extract valuable information for automated prediction, classification, and decision making. Sophisticated DNN models can involve 100s of layers and tens of millions of parameters.  Because training is computation and memory intensive, it is performed on servers. However, for performing inference at the edge, industry is building hardware accelerators that implement DNNs in silicon, integrating them with their mobile Systems on Chips (SoC)s to be deployed at the edge, each with their own architectures, memory organization and neuromorphic engines.  Furthermore, complex ML applications will be expressed as heterogeneous Networks of Models (NoMs) of DNNs operating on streaming data.  The key challenges to be addressed in this project are to determine how to optimally map NoMs, whose structure keeps changing depending the content of the data, onto a network of heterogeneous edge computing devices. The optimization will involve replicating and pipelining DNN models and deciding on which edge computing device to deploy each instance of a model, all at run-time. Furthermore, this determination will be based on the content of the data stream, the available resources, the characteristics of the communication medium, as well as the present allocation of models to devices. The outcomes of this project will include technology agnostic algorithms and software tools for performing this mapping.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sarma",
   "pi_last_name": "Vrudhula",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sarma Vrudhula",
   "pi_email_addr": "vrudhula@asu.edu",
   "nsf_id": "000124788",
   "pi_start_date": "2020-06-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852816011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 498262.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Edge computing involves processing data near its source, rather than relying on distant centralized data centers. This strategy aims to minimize latency and bandwidth usage by bringing computation and storage closer to where it's needed, like IoT and mobile devices. This approach enhances efficiency by enabling sensors to perform tasks such as data filtering, preprocessing, and real-time analytics locally. By processing data locally, edge computing improves response times, reduces data traffic to centralized servers, and enhances system efficiency. Its applications span autonomous vehicles, smart cities, industrial automation, and augmented reality, where low latency and high reliability are essential.</p>\n<p>&nbsp;The emergence of artificial intelligence and machine learning is now driving the demand to have edge devices execute the complex ML algorithms for object identification, classification, and decision analytics right at the point where the data is captured. These algorithms are expressed as&nbsp;<em>trained</em>&nbsp;deep neural networks (DNN) that first detect specific types or classes of objects (e.g., people, cars, etc) and then use customized DNNs to further process each individual type of object (e.g. identify relevant features of objects). Thus, the computation involves processing a&nbsp;<em>network of models&nbsp;</em>(NoM), each model being a DNN requiring tens of millions of floating-point operations.&nbsp;&nbsp;Typical edge processors do not have the memory or compute power required to process networks of DNNs, let alone do it in real-time.&nbsp;</p>\n<p>&nbsp;<strong>Intellectual Merit:&nbsp;&nbsp;</strong>The project explored two methods for implementing \"AI at the Edge\". In one approach, known as computation offloading (CF), the system comprises a low-power, constrained processor (referred to as U) and a more powerful local server (E), termed a cloudlet, which substitutes the cloud server. The work on CF led to the development of innovative algorithms for real-time distribution of computations between U and E, a capability currently unavailable in commercial systems. These methods take into account key factors such as communication latency and energy between U and E, WiFi reliability, varying bandwidth, and variable execution times of each deep neural network (DNN) on U and E. Two optimization objectives were considered: (1) minimizing U's energy consumption while considering all factors, and (2) extending the first to minimize U's energy consumption under latency constraints. Lightweight algorithms were devised and tested on contemporary systems, where U was a Raspberry Pi 3 and E was an NVIDIA Jetson TX2 processor for energy minimization. Optimal computation distribution reduced energy usage by 30% compared to running computations solely on U and 23% compared to using E alone. When subject to latency constraints, U was a NVIDIA Jetson Nano and E was a Titan XP GPU. The new optimization algorithm significantly reduced energy consumption by 59%, 47%, and 64% respectively compared to scenarios where DNNs were executed solely on E, solely on U, or randomly distributed between them.</p>\n<p>We developed new algorithms to allocate DNN models across various processors within a mobile multi-processor system-on-chip (MpSoC) module, commonly found in smartphones and IoT devices with energy constraints. Modern MpSoCs feature multiple CPU, GPU, DSP cores, and sometimes neuromorphic processors, posing the challenge of optimizing resource utilization across all cores simultaneously. This optimization varies based on input batch sizes and scene complexity. Our solution was demonstrated on a Qualcomm Snapdragon 865 MpSoC, achieving substantial improvements in execution time compared to baseline, CPU-only, GPU-only, and Central Queue schedulers: up to 32%, 6.67X, 5.60X, and 2.17X faster, respectively.</p>\n<p>Current approaches often drop frames when processing deadlines are missed over several frames, leading to poor video quality. We expanded our scheduling and allocation algorithm to balance between end-to-end accuracy and performance trade-offs. This approach significantly enhanced performance, reducing missed deadlines by 11X and 12.3X, and improving average accuracies by approximately 34% and 40% compared to two leading methods. This underscores the necessity for an adaptable scheduler capable of handling dynamic workloads in real-time.</p>\n<p><strong>Broader Impacts:&nbsp;</strong>The work done on this project advanced the state-of-the-art in edge computing and algorithms developed are ready to be incorporated by companies that make mobile MpSoCs. Further, domains that will benefit from the outcomes of this project, include retail, security, transportation and logistics, factory automation, healthcare, and many more. The project helped students gain skill beyond research. It&nbsp;supported two PhD students fulltime and two other PhD students partially. &nbsp;One of them is a female student. These students were given extensive training in writing, preparing presentations and giving oral presentations in different formats, and delivering presentations to industry and academic forums.&nbsp;&nbsp;One PhD student graduated and is now a tenure-track assistant professor. The other is expected to graduate in Fall 2024. In the final year of the project, three undergraduate students (one female) and one female high school student were recruited on this project. The project resulted in the completion of one Bachelor thesis.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/10/2024<br>\nModified by: Sarma&nbsp;Vrudhula</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nEdge computing involves processing data near its source, rather than relying on distant centralized data centers. This strategy aims to minimize latency and bandwidth usage by bringing computation and storage closer to where it's needed, like IoT and mobile devices. This approach enhances efficiency by enabling sensors to perform tasks such as data filtering, preprocessing, and real-time analytics locally. By processing data locally, edge computing improves response times, reduces data traffic to centralized servers, and enhances system efficiency. Its applications span autonomous vehicles, smart cities, industrial automation, and augmented reality, where low latency and high reliability are essential.\n\n\nThe emergence of artificial intelligence and machine learning is now driving the demand to have edge devices execute the complex ML algorithms for object identification, classification, and decision analytics right at the point where the data is captured. These algorithms are expressed astraineddeep neural networks (DNN) that first detect specific types or classes of objects (e.g., people, cars, etc) and then use customized DNNs to further process each individual type of object (e.g. identify relevant features of objects). Thus, the computation involves processing anetwork of models(NoM), each model being a DNN requiring tens of millions of floating-point operations.Typical edge processors do not have the memory or compute power required to process networks of DNNs, let alone do it in real-time.\n\n\nIntellectual Merit:The project explored two methods for implementing \"AI at the Edge\". In one approach, known as computation offloading (CF), the system comprises a low-power, constrained processor (referred to as U) and a more powerful local server (E), termed a cloudlet, which substitutes the cloud server. The work on CF led to the development of innovative algorithms for real-time distribution of computations between U and E, a capability currently unavailable in commercial systems. These methods take into account key factors such as communication latency and energy between U and E, WiFi reliability, varying bandwidth, and variable execution times of each deep neural network (DNN) on U and E. Two optimization objectives were considered: (1) minimizing U's energy consumption while considering all factors, and (2) extending the first to minimize U's energy consumption under latency constraints. Lightweight algorithms were devised and tested on contemporary systems, where U was a Raspberry Pi 3 and E was an NVIDIA Jetson TX2 processor for energy minimization. Optimal computation distribution reduced energy usage by 30% compared to running computations solely on U and 23% compared to using E alone. When subject to latency constraints, U was a NVIDIA Jetson Nano and E was a Titan XP GPU. The new optimization algorithm significantly reduced energy consumption by 59%, 47%, and 64% respectively compared to scenarios where DNNs were executed solely on E, solely on U, or randomly distributed between them.\n\n\nWe developed new algorithms to allocate DNN models across various processors within a mobile multi-processor system-on-chip (MpSoC) module, commonly found in smartphones and IoT devices with energy constraints. Modern MpSoCs feature multiple CPU, GPU, DSP cores, and sometimes neuromorphic processors, posing the challenge of optimizing resource utilization across all cores simultaneously. This optimization varies based on input batch sizes and scene complexity. Our solution was demonstrated on a Qualcomm Snapdragon 865 MpSoC, achieving substantial improvements in execution time compared to baseline, CPU-only, GPU-only, and Central Queue schedulers: up to 32%, 6.67X, 5.60X, and 2.17X faster, respectively.\n\n\nCurrent approaches often drop frames when processing deadlines are missed over several frames, leading to poor video quality. We expanded our scheduling and allocation algorithm to balance between end-to-end accuracy and performance trade-offs. This approach significantly enhanced performance, reducing missed deadlines by 11X and 12.3X, and improving average accuracies by approximately 34% and 40% compared to two leading methods. This underscores the necessity for an adaptable scheduler capable of handling dynamic workloads in real-time.\n\n\nBroader Impacts:The work done on this project advanced the state-of-the-art in edge computing and algorithms developed are ready to be incorporated by companies that make mobile MpSoCs. Further, domains that will benefit from the outcomes of this project, include retail, security, transportation and logistics, factory automation, healthcare, and many more. The project helped students gain skill beyond research. Itsupported two PhD students fulltime and two other PhD students partially. One of them is a female student. These students were given extensive training in writing, preparing presentations and giving oral presentations in different formats, and delivering presentations to industry and academic forums.One PhD student graduated and is now a tenure-track assistant professor. The other is expected to graduate in Fall 2024. In the final year of the project, three undergraduate students (one female) and one female high school student were recruited on this project. The project resulted in the completion of one Bachelor thesis.\n\n\n\t\t\t\t\tLast Modified: 07/10/2024\n\n\t\t\t\t\tSubmitted by: SarmaVrudhula\n"
 }
}