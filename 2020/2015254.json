{
 "awd_id": "2015254",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Programming the Existing and Emerging Memory Systems for Extreme-scale Parallel Performance",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 498674.0,
 "awd_amount": 498674.0,
 "awd_min_amd_letter_date": "2020-01-07",
 "awd_max_amd_letter_date": "2020-08-07",
 "awd_abstract_narration": "High performance computing (HPC) focuses on using numerical model to simulate complex science and engineering phenomena, such as galaxies, weather and climate, molecular interactions, electric power grids, and aircraft in flight.  Over the next decade the goal is to build HPC parallel system capable of extreme-scale performance (one exaflop (1018)operations per second) and processing exabyte (1018) of data. However, one of the biggest challenges of achieving extreme-scale performance is what is known as the hardware memory wall, which is about the growing gap between the speed of computation performed by CPU and the speed of supplying data to the CPU from memory systems (about x100 time slower). The low performance efficiency of modern HPC system (average <60% and could be as low as 5%) manifests the memory wall impact since a huge amount of computation cycles are wasted for waiting for the arrival of input data.  It becomes very critical to create effective software solutions for achieving the computation potential of hardware and for improving the efficiency and usability of the existing and future computing system. Such solutions will significantly benefit a broad range of disciplines that use parallel computers to solve scientific and engineering problems, and accelerate scientific discovery and problem solving to improve quality of life of the society. \r\nThis CAREER project develops innovative software techniques to address the programming and performance challenges of the existing and emerging memory systems: 1) a portable abstract machine model for programming, compiling and executing parallel applications, 2) new programming interface and model for data mapping, movement, and consistency, and 3) machine-aware compilation and data-aware scheduling techniques to realize an asynchronous task flow execution model to hide the latency of data movement. It addresses the memory wall challenge by developing a memory-centric programming paradigm for helping achieve extreme-scale performance of parallel applications with minimum impairment to programmability. For education, the project involves a broader community starting from high school in the area of HPC and computer science.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yonghong",
   "pi_last_name": "Yan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yonghong Yan",
   "pi_email_addr": "yyan7@uncc.edu",
   "nsf_id": "000604781",
   "pi_start_date": "2020-01-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Charlotte",
  "inst_street_address": "9201 UNIVERSITY CITY BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTE",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "7046871888",
  "inst_zip_code": "282230001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NC12",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE",
  "org_prnt_uei_num": "NEYCH3CVBTR6",
  "org_uei_num": "JB33DT84JNA5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Charlotte",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "282230001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NC12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8030.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 125937.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 130146.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 234561.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-6a3c4aa4-7fff-7cff-34b1-10934f849f79\"> </span></p>\n<p dir=\"ltr\">In this five-year project, we have accomplished the following research findings, software development, outreach activities, and mentoring and education tasks. We have used OpenMP programming models as our baseline to conduct the activities for heterogeneous systems.&nbsp;</p>\n<p dir=\"ltr\"><span>Exploring OpenMP extensions to support performance programming and advanced architecture features:&nbsp;</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have explored extensions to OpenMP map clauses for enabling data mapping between storage and GPU memory, thus by-passing host memory.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have explored OpenMP extensions to support data shuffling between threads via registers, thus by-passing DRAM-based data sharing between threads on GPUs.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have implemented and extended OpenMP metadirectives to support runtime adaptation and machine-learning based adaptation of computation.&nbsp;</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>Compiler and runtime implementation for OpenMP:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have developed a standalone OpenMP parser that can be used in different compilers.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have implemented source-to-source compiler support for OpenMP SIMD constructs for both Intel AVX and ARM SVE vector instructions.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have proposed and implemented Unified Parallel Intermediate Representation (UPIR) for parallel programming models.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have explored using knowledge graphs for generating and analyzing program call graphs.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have developed a source-to-source OpenMP compiler to support research based on OpenMP for parallel programming.&nbsp;</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>Evaluation, benchmarks, performance study and tools support</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have developed a CUDA microbenchmark suite to evaluate the different optimization techniques for GPU programming.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have implemented a convolutional neural network using OpenMP on multiple GPUs</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have evaluated multiple thread-based programming models including OepnMP, OpenACC, Cilk, TBB, etc</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have developed cloud-based data race detection services that combine multiple data-race detection tools</span></p>\n</li>\n</ul>\n<p dir=\"ltr\"><span>Software and Tools for Education&nbsp;</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have developed freecompilercamp infrastructure for cloud-based training of compiler development</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have developed an online interactive OpenMP programming book using Jupyter notebook and large-language models.&nbsp;</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Outreach Activities:</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have organized every year between 2017 - 2023 a workshop at SC for memory centric high performance computing. Every year, a copyrighted proceedings has been produced and the workshop attracted audiences from 70-120.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We have participated in outreach activities for introducing high school students to computer systems and parallel computing.&nbsp;</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Mentor and advising</span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>One Ph.D. student funded with this project graduated in 2023, and joined Intel working for OpenMP compiler.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Two Ph.D. students funded with this project has passed proposal defense by 2024</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Two undergraduate students involved in this project moved on to study their master degree.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>One master student involved in this project moved on to study his Ph.D.&nbsp;</span></p>\n</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/20/2024<br>\nModified by: Yonghong&nbsp;Yan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nIn this five-year project, we have accomplished the following research findings, software development, outreach activities, and mentoring and education tasks. We have used OpenMP programming models as our baseline to conduct the activities for heterogeneous systems.\n\n\nExploring OpenMP extensions to support performance programming and advanced architecture features:\n\n\n\n\nWe have explored extensions to OpenMP map clauses for enabling data mapping between storage and GPU memory, thus by-passing host memory.\n\n\n\n\nWe have explored OpenMP extensions to support data shuffling between threads via registers, thus by-passing DRAM-based data sharing between threads on GPUs.\n\n\n\n\nWe have implemented and extended OpenMP metadirectives to support runtime adaptation and machine-learning based adaptation of computation.\n\n\n\n\nCompiler and runtime implementation for OpenMP:\n\n\n\n\nWe have developed a standalone OpenMP parser that can be used in different compilers.\n\n\n\n\nWe have implemented source-to-source compiler support for OpenMP SIMD constructs for both Intel AVX and ARM SVE vector instructions.\n\n\n\n\nWe have proposed and implemented Unified Parallel Intermediate Representation (UPIR) for parallel programming models.\n\n\n\n\nWe have explored using knowledge graphs for generating and analyzing program call graphs.\n\n\n\n\nWe have developed a source-to-source OpenMP compiler to support research based on OpenMP for parallel programming.\n\n\n\n\nEvaluation, benchmarks, performance study and tools support\n\n\n\n\nWe have developed a CUDA microbenchmark suite to evaluate the different optimization techniques for GPU programming.\n\n\n\n\nWe have implemented a convolutional neural network using OpenMP on multiple GPUs\n\n\n\n\nWe have evaluated multiple thread-based programming models including OepnMP, OpenACC, Cilk, TBB, etc\n\n\n\n\nWe have developed cloud-based data race detection services that combine multiple data-race detection tools\n\n\n\n\nSoftware and Tools for Education\n\n\n\n\nWe have developed freecompilercamp infrastructure for cloud-based training of compiler development\n\n\n\n\nWe have developed an online interactive OpenMP programming book using Jupyter notebook and large-language models.\n\n\n\n\n\n\n\nOutreach Activities:\n\n\n\n\nWe have organized every year between 2017 - 2023 a workshop at SC for memory centric high performance computing. Every year, a copyrighted proceedings has been produced and the workshop attracted audiences from 70-120.\n\n\n\n\nWe have participated in outreach activities for introducing high school students to computer systems and parallel computing.\n\n\n\n\n\n\n\nMentor and advising\n\n\n\n\nOne Ph.D. student funded with this project graduated in 2023, and joined Intel working for OpenMP compiler.\n\n\n\n\nTwo Ph.D. students funded with this project has passed proposal defense by 2024\n\n\n\n\nTwo undergraduate students involved in this project moved on to study their master degree.\n\n\n\n\nOne master student involved in this project moved on to study his Ph.D.\n\n\n\n\n\t\t\t\t\tLast Modified: 08/20/2024\n\n\t\t\t\t\tSubmitted by: YonghongYan\n"
 }
}