{
 "awd_id": "1947801",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CIF: New Paradigms in Generalization and Information-Theoretic Analysis of Deep Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032920000",
 "po_email": "ahero@nsf.gov",
 "po_sign_block_name": "Alfred Hero",
 "awd_eff_date": "2020-04-01",
 "awd_exp_date": "2023-03-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2020-02-28",
 "awd_max_amd_letter_date": "2020-02-28",
 "awd_abstract_narration": "Over the past decade, deep learning (DL) has become the method of choice for various machine learning tasks. The realm of DL applications constantly expands, now including autonomous vehicles, robotic-assisted surgery, medical imaging, and many others. A wide societal acceptance of such technologies relies on the ability of humans to understand and trust them. Unfortunately, the exceptional practical effectiveness of DL systems is not coupled with a comprehensive theory to explain how they operate and why they are so successful on real-world data. This state of affairs obstructs a wider deployment of AI for the applications described above. To alleviate this impasse, this project seeks to open the hood of Deep Neural Networks (DNNs) that enable DL and elucidate how information is processed in these systems. Doing so would make the decisions of AI mechanisms more transparent to end users and other stakeholders, thus contributing to their understanding. Via rigorous performance guarantees, this project also aims to characterize the circumstances under which deep learning system are warranted not to fail. These advances will set the stage for the integration of high-performance AI systems in our daily lives, unlocking their invaluable potential impact.\r\n  \r\nThe project tackles key challenges in DL theory via a novel information-theoretic approach. The main objective is to shed light on the process by which DNNs progressively build representations --- from crude and over-redundant representations in shallow layers, to highly-clustered and interpretable ones in deeper layers --- and to give the designer more control over that process. To that end, three synergistic thrusts are pursued. First is developing novel complexity measures of internal representations by quantifying the flow of information through the DNN. Crucially, these measures are designed for efficient computation over layer dimensionalities typical to state-of-the-art networks for computer vision, speech, and text processing. The second thrust focuses on relating the developed complexity measures to the generalization capability of the network via new instance-dependent generalization bounds. The goal here is to provide performance guarantees for a given DNN in terms of efficiently computable figures of merit. Lastly, the developed machinery is further leveraged to construct tools for pruning redundant neurons/layers, visualizing the DNN's operation, and progressing DNN interpretability. Altogether, this research strives to progress the current uncertain trial-and-error process of DNN design towards the domain of deterministic engineering practice.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ziv",
   "pi_last_name": "Goldfeld",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ziv Goldfeld",
   "pi_email_addr": "zgzg1984@gmail.com",
   "nsf_id": "000806301",
   "pi_start_date": "2020-02-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "373 Pine Tree Road",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Research: </strong>The project made significant progress on two main fronts:</p>\n<p>1) Proposed a novel class of <em>sliced</em> information measures that quantify dependence by averaging or maximizing Shannon's mutual information between low-dimensional projections of the high-dimensional random variables. These measures scale very well to high-dimensional settings, where classic information measures become intractable. This enables various applications to statistical inference and machine learning, encompassing independence testing, representation learning (e.g., via a sliced InfoMax principle), generative modeling (e.g., via sliced InfoGAN), information flow analysis in deep neural networks, and many more. The concept of sliced mutual information is receiving increasing attention from both the information theory and machine learning communities, with several follow-up papers on the topic exploring both theoretical aspects and applications of this idea.</p>\n<p>2) Developing formal performance guarantees for neural estimation of information measures (f-divergences, classical mutual information, sliced variants, directed information, etc.). Neural estimation is a powerful technique that can handle high-dimensional data and massive datasets. Such methods are widely employed in practice but, until recently, were lacking meaningful bounds on the estimation error. In a series of works, we have developed a rigorous non-asymptotic theory to quantify the error of such estimators in terms of the dataset and neural network size. This, in turn, certifies estimates obtained from this approach and can also be used for theory-guided dataset and neural network size selection as well as hyperparameter tuning</p>\n<p><strong>Teaching and curriculum development:</strong></p>\n<p>The information theory class at Cornell ECE was modernized and refreshed to now cover timely material at the intersection of information theory and machine learning. This includes information-theoretic generalization bounds, neural estimation of information measures, and applications to generative modeling and representation learning.</p>\n<p><strong>Knowledge dissemination:</strong></p>\n<p>The PI has given multiple talks on the above topics. These include seminars/colloquia as well as invited talks in conferences and workshops (e.g., IBM Research Seminar, Information Theory and Applications Workshop, Fields Institute Workshop on PDE Methods in Data Science and Machine Learning, etc.).</p>\n<p><strong>Training and professional development:&nbsp;</strong>The project has helped support, in part, two graduate students (one of which identifies as a female) and a postdoc. With the PI's help, they studied advanced topics in probability theory, information theory, and mathematical statistics. The PI has also been working with the students and postdoc on their scientific writing skills as part of paper writing. Lastly, all three have delivered talks on the topic in conferences (and, in the postdoc's case, also in job interviews). The PI worked with them on structuring and planning their presentations, as well as on their delivery skills.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/01/2023<br>\n\t\t\t\t\tModified by: Ziv&nbsp;Goldfeld</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nResearch: The project made significant progress on two main fronts:\n\n1) Proposed a novel class of sliced information measures that quantify dependence by averaging or maximizing Shannon's mutual information between low-dimensional projections of the high-dimensional random variables. These measures scale very well to high-dimensional settings, where classic information measures become intractable. This enables various applications to statistical inference and machine learning, encompassing independence testing, representation learning (e.g., via a sliced InfoMax principle), generative modeling (e.g., via sliced InfoGAN), information flow analysis in deep neural networks, and many more. The concept of sliced mutual information is receiving increasing attention from both the information theory and machine learning communities, with several follow-up papers on the topic exploring both theoretical aspects and applications of this idea.\n\n2) Developing formal performance guarantees for neural estimation of information measures (f-divergences, classical mutual information, sliced variants, directed information, etc.). Neural estimation is a powerful technique that can handle high-dimensional data and massive datasets. Such methods are widely employed in practice but, until recently, were lacking meaningful bounds on the estimation error. In a series of works, we have developed a rigorous non-asymptotic theory to quantify the error of such estimators in terms of the dataset and neural network size. This, in turn, certifies estimates obtained from this approach and can also be used for theory-guided dataset and neural network size selection as well as hyperparameter tuning\n\nTeaching and curriculum development:\n\nThe information theory class at Cornell ECE was modernized and refreshed to now cover timely material at the intersection of information theory and machine learning. This includes information-theoretic generalization bounds, neural estimation of information measures, and applications to generative modeling and representation learning.\n\nKnowledge dissemination:\n\nThe PI has given multiple talks on the above topics. These include seminars/colloquia as well as invited talks in conferences and workshops (e.g., IBM Research Seminar, Information Theory and Applications Workshop, Fields Institute Workshop on PDE Methods in Data Science and Machine Learning, etc.).\n\nTraining and professional development: The project has helped support, in part, two graduate students (one of which identifies as a female) and a postdoc. With the PI's help, they studied advanced topics in probability theory, information theory, and mathematical statistics. The PI has also been working with the students and postdoc on their scientific writing skills as part of paper writing. Lastly, all three have delivered talks on the topic in conferences (and, in the postdoc's case, also in job interviews). The PI worked with them on structuring and planning their presentations, as well as on their delivery skills.\n\n\t\t\t\t\tLast Modified: 06/01/2023\n\n\t\t\t\t\tSubmitted by: Ziv Goldfeld"
 }
}