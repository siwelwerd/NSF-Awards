{
 "awd_id": "2032125",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID: Human Sound Localization and Analytics",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927855",
 "po_email": "aabouzei@nsf.gov",
 "po_sign_block_name": "Alhussein Abouzeid",
 "awd_eff_date": "2020-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2020-06-01",
 "awd_max_amd_letter_date": "2023-05-25",
 "awd_abstract_narration": "COVID-19 is spreading at an unprecedented rate resulting in the death of so many people all over the world. Social distancing is so far the most effective method to limit its spread. However, manually enforcing social distancing is not only labor-intensive but also error-prone and even dangerous due to possible physical contact. This project proposes to develop techniques and mobile systems that localize human sound such as cough and voice and alarm a user when someone is within the social distance.  If successful, this work will significantly advance the state-of-the-art in wireless sensing and localization. To maximize the impact, the researchers will collaborate with industry and local community and release software to the public. The research outcome will also be incorporated into the graduate and undergraduate curriculum. \r\n\r\nThe proposed research aims to develop algorithms and systems to localize uncontrolled and unknown human sound. A unique advantage is that it does not require cooperation from other phones and whoever uses it can immediately benefit from it. It exploits the phone mobility as the user moves to enable localization. The multi-resolution analysis will be performed on low-frequency voice signals to further enhance accuracy. When another phone is cooperating, it will further leverage the time of flight between the two phones to improve the performance.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lili",
   "pi_last_name": "Qiu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lili Qiu",
   "pi_email_addr": "lili@cs.utexas.edu",
   "nsf_id": "000112738",
   "pi_start_date": "2020-06-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "UT Austin",
  "perf_str_addr": "2317 Speedway, Stop D9500",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121757",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "158Y00",
   "pgm_ele_name": "COVID-19 Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "096Z",
   "pgm_ref_txt": "COVID-19 Research"
  },
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  }
 ],
 "app_fund": [
  {
   "app_code": "1N20",
   "app_name": "R&RA CARES Act DEFC N",
   "app_symb_id": "040100",
   "fund_code": "010N2021DB",
   "fund_name": "R&RA CARES Act DEFC N",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>COVID-19 is spreading at an unprecedented rate resulting in the death of so many people all over the world. Social distancing is so far the most effective method to limit its spread. However, manually enforcing social distancing is not only labor-intensive but also error-prone and even dangerous due to possible physical contact. The proposed research aims to develop algorithms and systems to localize uncontrolled and unknown human sound. Meanwhile, we will develop applications based on the location of human sound.</p>\r\n<p>&nbsp;</p>\r\n<p>To this end, we first develop&nbsp;a novel indoor voice localization system, MAVL, by retracing multiple propagation paths that the user's sound traverses. MAVL first estimate AoAs of the multiple paths traversed by the voice signals from the user to the microphone array on the smart speaker. The multipath may include the direct path (if available) and the reflected paths. It then estimates the indoor space (e.g., walls, ceilings) by emitting wideband chirps to estimate the AoA and distance to the reflectors in the room (e.g., walls). It further re-traces the propagation paths based on the estimated AoA of the voice signals and the room structure to localize the voice.<br /><br />We further develop FBDepth, the first passive audio-visual depth estimation framework inspired by the flash-to-bang theory. It is based on the difference between the time-of-flight (ToF) of the light and the sound. We formulate sound source depth estimation as an audio-visual event localization task for collision events. To approach decimeter-level depth accuracy, we design a coarse-to-fine pipeline to push the temporary localization accuracy from event-level to millisecond-level by aligning audio-visual correspondence and manipulating optical flow. FBDepth feeds the estimated visual timestamp together with the audio clip and object visual features to regress the source depth. We use a mobile phone to collect 3.6K+ video clips with 24 different objects at up to 60m. FBDepth shows superior performance especially at a long range compared to monocular and stereo methods.<br /><br /></p>\r\n<p>In addition, we recognitize that online meetings have become an indispensable part of our lives during Covid. This trend is likely to continue due to its convenience and broad reach. However, background noise from other family members, roommates, and officemates not only degrades the voice quality but also raises serious privacy issues. In this work, we develop a novel system, called Spatial Aware Multi-task learning-based Separation (SAMS), to extract audio signals from the target user during teleconferencing. Our solution consists of two novel components: (i) generating fine-grained location embeddings from the user&rsquo;s voice and inaudible tracking sound, which contains the user&rsquo;s position and rich multipath information, (ii) developing a source separation neural network using multi-task learning to jointly optimize source separation and location.</p>\r\n<p><br />The algorithms, techniques, and software resulting from this research have deepened our understanding of wireless sensing and provided significant practical value by enabling new applications. In addition, the research outcomes have been disseminated at leading conferences and talks, and will be incorporated into undergraduate and graduate wireless courses as well as outreach activities.</p><br>\n<p>\n Last Modified: 02/20/2025<br>\nModified by: Lili&nbsp;Qiu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nCOVID-19 is spreading at an unprecedented rate resulting in the death of so many people all over the world. Social distancing is so far the most effective method to limit its spread. However, manually enforcing social distancing is not only labor-intensive but also error-prone and even dangerous due to possible physical contact. The proposed research aims to develop algorithms and systems to localize uncontrolled and unknown human sound. Meanwhile, we will develop applications based on the location of human sound.\r\n\n\n\r\n\n\nTo this end, we first developa novel indoor voice localization system, MAVL, by retracing multiple propagation paths that the user's sound traverses. MAVL first estimate AoAs of the multiple paths traversed by the voice signals from the user to the microphone array on the smart speaker. The multipath may include the direct path (if available) and the reflected paths. It then estimates the indoor space (e.g., walls, ceilings) by emitting wideband chirps to estimate the AoA and distance to the reflectors in the room (e.g., walls). It further re-traces the propagation paths based on the estimated AoA of the voice signals and the room structure to localize the voice.\n\nWe further develop FBDepth, the first passive audio-visual depth estimation framework inspired by the flash-to-bang theory. It is based on the difference between the time-of-flight (ToF) of the light and the sound. We formulate sound source depth estimation as an audio-visual event localization task for collision events. To approach decimeter-level depth accuracy, we design a coarse-to-fine pipeline to push the temporary localization accuracy from event-level to millisecond-level by aligning audio-visual correspondence and manipulating optical flow. FBDepth feeds the estimated visual timestamp together with the audio clip and object visual features to regress the source depth. We use a mobile phone to collect 3.6K+ video clips with 24 different objects at up to 60m. FBDepth shows superior performance especially at a long range compared to monocular and stereo methods.\n\n\r\n\n\nIn addition, we recognitize that online meetings have become an indispensable part of our lives during Covid. This trend is likely to continue due to its convenience and broad reach. However, background noise from other family members, roommates, and officemates not only degrades the voice quality but also raises serious privacy issues. In this work, we develop a novel system, called Spatial Aware Multi-task learning-based Separation (SAMS), to extract audio signals from the target user during teleconferencing. Our solution consists of two novel components: (i) generating fine-grained location embeddings from the users voice and inaudible tracking sound, which contains the users position and rich multipath information, (ii) developing a source separation neural network using multi-task learning to jointly optimize source separation and location.\r\n\n\n\nThe algorithms, techniques, and software resulting from this research have deepened our understanding of wireless sensing and provided significant practical value by enabling new applications. In addition, the research outcomes have been disseminated at leading conferences and talks, and will be incorporated into undergraduate and graduate wireless courses as well as outreach activities.\t\t\t\t\tLast Modified: 02/20/2025\n\n\t\t\t\t\tSubmitted by: LiliQiu\n"
 }
}