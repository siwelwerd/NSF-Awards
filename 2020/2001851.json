{
 "awd_id": "2001851",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS:Small: Incorporating and Balancing Stakeholder Values in Algorithm Design",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 532000.0,
 "awd_min_amd_letter_date": "2019-10-31",
 "awd_max_amd_letter_date": "2022-05-24",
 "awd_abstract_narration": "This project will create a general method for value-sensitive algorithm design and develop tools and techniques to help incorporate the tacit values of stakeholders, balance multiple stakeholders' values, and achieve collective goals in the development of an algorithm.  The research community has paid increasing attention to the role of human values in algorithm design and development. For example, fairness-aware machine learning research attempts to translate fairness notions into formal algorithmic constraints and develop algorithms subject to such constraints. Despite the mathematical rigor of these approaches, prior research suggests a disconnect between the current discrimination-aware machine learning research and stakeholders' realities, context, and constraints; this disconnect is likely to undermine practical initiatives. Furthermore, studies have suggested that there are often tensions among a diverse set of values relevant to the design of the algorithm.  A new general method will be developed in the context of Redesigning Wikipedia's Objective Revision Evaluation Service (ORES), a machine learning-based service designed to generate real-time predictions on edit quality and article quality, which will benefit vast numbers of people who consume the Wikipedia content either directly or indirectly through other applications.\r\n\r\nThere are four major goals of this research.  The first is to articulate and demonstrate a general method for creating algorithmic systems that respect and balance stakeholders' values.  The second goal is to create techniques for generating an algorithmic system's value report and explaining the value trade-offs. The third goal to create, deploy, and evaluate social and technical innovations to address fundamental trade-offs between different values.  The final goal is to design and implement improvements to ORES, which will improve a wide variety of applications that rely on ORES, and Wikipedia's content and community as a whole.  For an example of the kinds of problems that must be solved, quality control algorithms that prioritize efficiency in deleting low quality content incur the risk of undermining the motivation of contributors in peer production communities, particularly new contributors who are still learning how to contribute. To date, however, little work has been conducted to create solutions to address tensions and trade-offs between different values in algorithm design.  The research will be performed through multiple studies by stepping through the process for a diverse set of tasks, each of which will allow interaction with multiple stakeholders, who have different (and perhaps conflicting) values.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Haiyi",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Haiyi Zhu",
   "pi_email_addr": "haiyiz@cs.cmu.edu",
   "nsf_id": "000686407",
   "pi_start_date": "2019-10-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-b8388727-7fff-bea8-82d0-d760e4498dc4\"> </span></p>\n<p dir=\"ltr\"><span><span>During the funding period, we actively engaged in significant research activities, focusing on integrating stakeholder values into AI technologies.</span> These values encompass the perspectives and insights of community members, frontline workers, and policymakers. Our aim was to incorporate these values into the design and evaluation of various algorithmic tools, including AI-based decision support and generative AI tools, across diverse domains. These domains included online content moderation, child welfare, and homeless services.</span></p>\n<p dir=\"ltr\"><span>&nbsp;</span>(1) As an example of our work, we conducted interviews with community stakeholders within the English Wikipedia community to gain a deep understanding of their values concerning the Objective Revision Evaluation System (ORES), an AI-based decision-support system extensively used in various Wikipedia applications and contexts. This research earned recognition as a <span style=\"text-decoration: underline;\">Best Paper honorable mention at CHI 2020</span>. We also developed visualization tools to illustrate the trade-offs between community goals related to Wikipedia ORES, which we published at <span style=\"text-decoration: underline;\">DIS 2021</span>. Additionally, we created the Model Card Authoring Toolkit, a resource that supports community members in comprehending, navigating, and making choices regarding a range of machine learning models through deliberation. We conducted workshops to empirically assess the initial effectiveness of our approach in two online communities - English and Dutch Wikipedia. Our findings documented how participants collectively established thresholds for machine learning-based quality prediction systems used in their content moderation applications, and this research was published at <span style=\"text-decoration: underline;\">FAccT 2022</span>.</p>\n<p dir=\"ltr\">(2) Our research extended into the context of child welfare, including studies to understand fairness notions among community stakeholders concerning the use of decision-support tools in child maltreatment (published at <span style=\"text-decoration: underline;\">CHI 2021)</span>. We also explored how child welfare frontline workers address racial disparities in algorithmic decisions (<span style=\"text-decoration: underline;\">CHI 2022</span>) and investigated ways to enhance human-AI partnerships in the design and evaluation of AI decision support systems in child welfare (<span style=\"text-decoration: underline;\">CHI 2022, with a best paper honorable mention)</span>. Moreover, we engaged community stakeholders to envision new futures beyond AI decision support systems in child welfare, aligning more closely with stakeholder needs and values. This work was published at <span style=\"text-decoration: underline;\">FAccT 2022</span>.</p>\n<p dir=\"ltr\">(3) In recent years, AI-based decision support systems (ADS) have seen increased adoption in homeless services. However, we had limited knowledge of stakeholder desires and concerns regarding their use. To address this gap, we aimed to understand impacted stakeholders' perspectives on a deployed ADS that prioritizes scarce housing resources. We employed AI lifecycle comicboarding, an adapted version of the comicboarding method, to elicit stakeholder feedback and design ideas across various components of an AI system's design. <span style=\"text-decoration: underline;\">This research was recognized with the best paper award at CHI 2023.</span></p>\n<p dir=\"ltr\">(4) While there is a growing body of work focusing on improving value alignment in AI-based decision support tools, less work has centered on concerns related to the fundamental justifiability of using these tools. Our work aimed to center validity considerations in discussions about whether and how to build data-driven algorithms in high-stakes domains. To achieve this, we translated key concepts from validity theory to predictive algorithms, using this lens to re-examine common challenges in problem formulation and data issues that could compromise the justifiability of predictive algorithms. Our research demonstrated how these validity considerations could be distilled into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data. <span style=\"text-decoration: underline;\">This research earned the Best Paper Award at SatML 2023.</span></p>\n<p dir=\"ltr\">(5) Large generative AI models (GMs) like GPT and DALL-E are trained to generate content for various purposes. GM content filters are generally designed to filter out potentially harmful content, such as hate speech. However, harm and benefit are value judgments based on how these models are used in specific contexts. We introduce the term \"green teaming\" to describe methods for bypassing GM content filters to design for beneficial use cases. We illustrate green teaming through various examples, including using ChatGPT as a virtual patient for suicide support training, using Codex to intentionally generate buggy solutions to train students on debugging, and examining an Instagram page using Midjourney to generate images of anti-LGBTQ+ politicians in drag. This work was presented at the <span style=\"text-decoration: underline;\">ICML workshop on Challenges in Deployable Generative AI.</span></p>\n<p dir=\"ltr\"><span>The research supported by this NSF award resulted in 26 papers published in top conferences, including best paper awards at CHI 2023, SatML 2023, and CHIWork 2023, along with best paper honorable mentions at CHI 2020 and CHI 2022. We disseminated our research results to the communities of interest through research publications, conference and workshop presentations, and keynote speeches at various conferences and institutions. PI Zhu delivered talks on this topic at events such as the WikiResearch Showcase live stream, The Metagovernance Seminar at the University of Washington, IBM Research, Toyota Research Institute, Stanford University, Microsoft, Seoul National University, and Microsoft Research Asia.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/17/2023<br>\n\t\t\t\t\tModified by: Haiyi&nbsp;Zhu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nDuring the funding period, we actively engaged in significant research activities, focusing on integrating stakeholder values into AI technologies. These values encompass the perspectives and insights of community members, frontline workers, and policymakers. Our aim was to incorporate these values into the design and evaluation of various algorithmic tools, including AI-based decision support and generative AI tools, across diverse domains. These domains included online content moderation, child welfare, and homeless services.\n (1) As an example of our work, we conducted interviews with community stakeholders within the English Wikipedia community to gain a deep understanding of their values concerning the Objective Revision Evaluation System (ORES), an AI-based decision-support system extensively used in various Wikipedia applications and contexts. This research earned recognition as a Best Paper honorable mention at CHI 2020. We also developed visualization tools to illustrate the trade-offs between community goals related to Wikipedia ORES, which we published at DIS 2021. Additionally, we created the Model Card Authoring Toolkit, a resource that supports community members in comprehending, navigating, and making choices regarding a range of machine learning models through deliberation. We conducted workshops to empirically assess the initial effectiveness of our approach in two online communities - English and Dutch Wikipedia. Our findings documented how participants collectively established thresholds for machine learning-based quality prediction systems used in their content moderation applications, and this research was published at FAccT 2022.\n(2) Our research extended into the context of child welfare, including studies to understand fairness notions among community stakeholders concerning the use of decision-support tools in child maltreatment (published at CHI 2021). We also explored how child welfare frontline workers address racial disparities in algorithmic decisions (CHI 2022) and investigated ways to enhance human-AI partnerships in the design and evaluation of AI decision support systems in child welfare (CHI 2022, with a best paper honorable mention). Moreover, we engaged community stakeholders to envision new futures beyond AI decision support systems in child welfare, aligning more closely with stakeholder needs and values. This work was published at FAccT 2022.\n(3) In recent years, AI-based decision support systems (ADS) have seen increased adoption in homeless services. However, we had limited knowledge of stakeholder desires and concerns regarding their use. To address this gap, we aimed to understand impacted stakeholders' perspectives on a deployed ADS that prioritizes scarce housing resources. We employed AI lifecycle comicboarding, an adapted version of the comicboarding method, to elicit stakeholder feedback and design ideas across various components of an AI system's design. This research was recognized with the best paper award at CHI 2023.\n(4) While there is a growing body of work focusing on improving value alignment in AI-based decision support tools, less work has centered on concerns related to the fundamental justifiability of using these tools. Our work aimed to center validity considerations in discussions about whether and how to build data-driven algorithms in high-stakes domains. To achieve this, we translated key concepts from validity theory to predictive algorithms, using this lens to re-examine common challenges in problem formulation and data issues that could compromise the justifiability of predictive algorithms. Our research demonstrated how these validity considerations could be distilled into a series of high-level questions intended to promote and document reflections on the legitimacy of the predictive task and the suitability of the data. This research earned the Best Paper Award at SatML 2023.\n(5) Large generative AI models (GMs) like GPT and DALL-E are trained to generate content for various purposes. GM content filters are generally designed to filter out potentially harmful content, such as hate speech. However, harm and benefit are value judgments based on how these models are used in specific contexts. We introduce the term \"green teaming\" to describe methods for bypassing GM content filters to design for beneficial use cases. We illustrate green teaming through various examples, including using ChatGPT as a virtual patient for suicide support training, using Codex to intentionally generate buggy solutions to train students on debugging, and examining an Instagram page using Midjourney to generate images of anti-LGBTQ+ politicians in drag. This work was presented at the ICML workshop on Challenges in Deployable Generative AI.\nThe research supported by this NSF award resulted in 26 papers published in top conferences, including best paper awards at CHI 2023, SatML 2023, and CHIWork 2023, along with best paper honorable mentions at CHI 2020 and CHI 2022. We disseminated our research results to the communities of interest through research publications, conference and workshop presentations, and keynote speeches at various conferences and institutions. PI Zhu delivered talks on this topic at events such as the WikiResearch Showcase live stream, The Metagovernance Seminar at the University of Washington, IBM Research, Toyota Research Institute, Stanford University, Microsoft, Seoul National University, and Microsoft Research Asia.\n\n \n\n\t\t\t\t\tLast Modified: 10/17/2023\n\n\t\t\t\t\tSubmitted by: Haiyi Zhu"
 }
}