{
 "awd_id": "2019959",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Visuospatial modulation of bimanual touch perception in real and virtual environments",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 379229.0,
 "awd_amount": 379229.0,
 "awd_min_amd_letter_date": "2020-07-29",
 "awd_max_amd_letter_date": "2020-07-29",
 "awd_abstract_narration": "In order to use our hands to manipulate objects, the nervous system must be able to coordinate and interpret a large amount of disparate sensory information about the position of our hands, the arrangement and positioning of the fingers, and what each is contacting. The computations are complicated by the fact that the information from the two hands must be interpreted relative to their spatial position. For example, the left and right index fingers occupy adjacent spaces when the hands are held together but the fingers can occupy vastly different spaces when the arms are spread apart. In these different situations, we can conceivably rely on our sense of vision to help interpret our sense of touch. Vision can provide information about where our hands are in space as well as what objects we touch and manipulate. Despite the obvious importance of vision for interpreting touch information from the hands, the specific ways in which the visual influence occurs remains poorly understood. This research is aimed at investigating the effects of simple and complex visual cues on touch information from the two hands in bimanual tasks. Understanding bimanual touch processing will facilitate effective nonverbal physical communication between people and intelligent machines and it will support the development of highly dexterous robotic systems, advanced neuroprosthetics, and sensorimotor rehabilitation strategies. Establishing how vision influences our perception of our body in real and virtual environments has clear implications for society\u2019s use of immersive and interactive technologies. The project will also promote the participation and education of young women in neuroscience and related STEM fields.  \r\n \r\nThis research program uses rigorous psychophysics, virtual reality technologies, and computational modeling to elucidate the effects of simple and complex visuospatial cues on the perception of bimanual touch. Behavioral experiments will be performed to quantify the effects of simple light flashes on tactile perception and learning. Neural network models will be fitted to the behavioral data to make inferences about how the nervous system mediates visuotactile and spatial interactions and how neural circuits may be modified through learning. The effects of complex visual cues on proprioception and bimanual touch will also be examined by performing psychophysics experiments in virtual reality. Specifically, visual proprioception cues and virtual object information will be altered in a virtual reality design, allowing the investigator to test hypotheses regarding multisensory integration for proprioception and causal inference processing. Collectively, the research program will yield novel insight into how visual information modulates the perception of tactile cues experienced over the two hands.\r\n\r\nCo-funded by the M3X (Engineering) and PAC (Social, Behavioral, and Economic Sciences) Programs\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Yau",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey Yau",
   "pi_email_addr": "jeffrey.yau@bcm.edu",
   "nsf_id": "000700328",
   "pi_start_date": "2020-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Baylor College of Medicine",
  "inst_street_address": "1 BAYLOR PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "HOUSTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7137981297",
  "inst_zip_code": "770303411",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "BAYLOR COLLEGE OF MEDICINE",
  "org_prnt_uei_num": "FXKMA43NTV21",
  "org_uei_num": "FXKMA43NTV21"
 },
 "perf_inst": {
  "perf_inst_name": "Baylor College of Medicine",
  "perf_str_addr": "One Baylor Plaza",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770303411",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "058y00",
   "pgm_ele_name": "M3X - Mind, Machine, and Motor"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 379229.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project supported work aimed at determining the effect of visual cues on bimanual spatial perception. The project was premised on the notion that specialized cortical networks -- which are accessible by visual signals -- support tactile spatial perception over the two hands. Preliminary data provided early evidence that tactile cues on one hand influence the perception of cues on the other hand and that these bimanual interactions could be modulated by non-informative visual cues. Over the course of the project, we discovered that visual cues exert both online and offline influences on bimanual touch. Specifically, uninformative light cues not only systematically biased the detection and localization of tactile cues experienced concurrently on the hands, but the history of crossmodal exposure also resulted in subsequent biases in tactile localization even when no visual cues were presented. These online and offline visuospatial effects were explained as changes in sensitvity and criterion under a signal detection modeling framework. We then expanded our modeling efforts to establish how a neural network could account for the online and offline visuospatial interactions. Model competition established plasticity restricted to only hand representations or only inter-hemispheric inhibition failed to recapitulate the visuospatial effects. Instead, both forms of plasticity were required for the model to reproduce the behavior. This modeling study provides testable predictions for neural plasticity underlying visuospatial learning. The project also supported the development of AR/VR behavioral testing paradigms. Lastly, this project supported the development, refinement, and execution of a neuroscience \"boot camp\" organized for local high school students.&nbsp;&nbsp;</p><br>\n<p>\n Last Modified: 11/06/2024<br>\nModified by: Jeffrey&nbsp;Yau</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project supported work aimed at determining the effect of visual cues on bimanual spatial perception. The project was premised on the notion that specialized cortical networks -- which are accessible by visual signals -- support tactile spatial perception over the two hands. Preliminary data provided early evidence that tactile cues on one hand influence the perception of cues on the other hand and that these bimanual interactions could be modulated by non-informative visual cues. Over the course of the project, we discovered that visual cues exert both online and offline influences on bimanual touch. Specifically, uninformative light cues not only systematically biased the detection and localization of tactile cues experienced concurrently on the hands, but the history of crossmodal exposure also resulted in subsequent biases in tactile localization even when no visual cues were presented. These online and offline visuospatial effects were explained as changes in sensitvity and criterion under a signal detection modeling framework. We then expanded our modeling efforts to establish how a neural network could account for the online and offline visuospatial interactions. Model competition established plasticity restricted to only hand representations or only inter-hemispheric inhibition failed to recapitulate the visuospatial effects. Instead, both forms of plasticity were required for the model to reproduce the behavior. This modeling study provides testable predictions for neural plasticity underlying visuospatial learning. The project also supported the development of AR/VR behavioral testing paradigms. Lastly, this project supported the development, refinement, and execution of a neuroscience \"boot camp\" organized for local high school students.\t\t\t\t\tLast Modified: 11/06/2024\n\n\t\t\t\t\tSubmitted by: JeffreyYau\n"
 }
}