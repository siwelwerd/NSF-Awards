{
 "awd_id": "2025772",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "SBIR Phase II:  Development of a Multimodal Interface for improving independence of Blind and Visually-Impaired people",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032924392",
 "po_email": "amonk@nsf.gov",
 "po_sign_block_name": "Alastair Monk",
 "awd_eff_date": "2020-09-15",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 989095.0,
 "awd_amount": 1169876.0,
 "awd_min_amd_letter_date": "2020-09-11",
 "awd_max_amd_letter_date": "2022-09-19",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project will develop a new technology for blind and visually-impaired (BVI) people to use touchscreen devices and explore quantitative data. Currently there are no commercial solutions for the 285 million BVI individuals and their support networks to access text and graphical materials.  The proposed project will advance software that converts graphical and text information into a form for BVI users to explore and utilize. This will enable greater education and work opportunities as well as greater independence.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase II effort continues development of a novel touchscreen-based information access software offering multisensory representations of both text and graphical information in digital media. Information can be efficiently conveyed with graphical tools such as bar graphs, but BVI users cannot exploit these methods to synthesize data.  This project develops the artificial intelligence (AI) framework to support automatic conversion of visual graphics into multisensory graphics and demonstrates its utility.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Hari Prasath",
   "pi_last_name": "Palani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hari Prasath Palani",
   "pi_email_addr": "ha.palani@northeastern.edu",
   "nsf_id": "000779011",
   "pi_start_date": "2020-09-11",
   "pi_end_date": "2021-10-18"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Pratt",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel R Pratt",
   "pi_email_addr": "daniel.pratt@unarlabs.com",
   "nsf_id": "000865823",
   "pi_start_date": "2021-10-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "UNAR LABS, LLC",
  "inst_street_address": "1486 BROADWAY",
  "inst_street_address_2": "",
  "inst_city_name": "SOUTH PORTLAND",
  "inst_state_code": "ME",
  "inst_state_name": "Maine",
  "inst_phone_num": "2074041587",
  "inst_zip_code": "041062602",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "ME01",
  "org_lgl_bus_name": "UNAR LABS LLC",
  "org_prnt_uei_num": "",
  "org_uei_num": "KKMALRX3T5C3"
 },
 "perf_inst": {
  "perf_inst_name": "UNAR Labs LLC",
  "perf_str_addr": "795 Congress Street",
  "perf_city_name": "Portland",
  "perf_st_code": "ME",
  "perf_st_name": "Maine",
  "perf_zip_code": "041023305",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "ME01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "010E",
   "pgm_ref_txt": "DISABILITY RES & HOMECARE TECH"
  },
  {
   "pgm_ref_code": "169E",
   "pgm_ref_txt": "SBIR Tech Enhan Partner (TECP)"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "8034",
   "pgm_ref_txt": "Hardware Components"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 989095.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 180781.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Graphical materials such as graphs and maps are extremely important for education, employment, independent living, and safe navigation for both sighted and blind/visually-impaired (BVI) individuals. However, BVI individuals face significant challenges in accessing this wealth of information due to its visual nature. Through this Small Business Innovation Research project, UNAR Labs LLC seeks to address this long-standing, unmet need by providing BVI people with real-time and independent access to digital text and graphical materials via commercial smartphones and tablets. To date, there is no approach that has the potential to provide both text and graphical materials in a seamless manner, which is the de-facto experience gained by sighted users. To fill this gap, UNAR Labs LLC has developed an intuitive information access solution called Midlina, which will automatically translate visual information in to multisensory equivalent and deliver it to BVI users via smart touchscreen-based computing devices. The activities of this SBIR were designed to apply findings from the customer interviews and from past NSF-sponsored academic research to build an intuitive system that convey combined textual and graphical information via a multisensory interface and prove that BVI end-users can utilize the system to gain access to otherwise inaccessible digital media materials.</p>\n<p>The Phase II research was driven by two major goals: (1) Develop a comprehensive visual-to-multisensory conversion framework and establish Midlina as the core technology on Information Access Technology (IAT) devices for performing visual-to-multisensory conversion of combined textual and graphical information from digital documents, and (2) Establish system efficacy, usability, commercial viability, and user acceptance of the two Midlina powered applications. Using math worksheets and SAT practice test materials as exemplary use cases, a first-of-its-kind AI framework was developed that can convert visually oriented inaccessible documents (with text, math, and images) into its equivalent accessible tactile formats. To demonstrate the applicability of the model, we developed two applications: (1) a web-app called Kanak - aimed at teachers for converting educational materials into accessible formats, and (2) a mobile app called, Holo - aimed at BVI end users to access the converted educational materials using voiceover and multisensory feedback. Through human studies involving expert teachers, transcribers, and BVI end-users the usability and acceptance of the products were evaluated. Results demonstrated that participants were able to efficiently use the system for converting digital inaccessible documents into accessible formats and read them using voiceover and haptic interactions. Readily available and affordable access to informative materials will promote independence and increase productivity. The outcome of this SBIR effort (i.e., demonstration that Midlina is commercially viable) clearly indicated that Midlina will significantly aid in reducing the detrimental effects experienced by BVI people on their educational, vocational, and social opportunities.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/24/2023<br>\nModified by: Daniel&nbsp;R&nbsp;Pratt</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nGraphical materials such as graphs and maps are extremely important for education, employment, independent living, and safe navigation for both sighted and blind/visually-impaired (BVI) individuals. However, BVI individuals face significant challenges in accessing this wealth of information due to its visual nature. Through this Small Business Innovation Research project, UNAR Labs LLC seeks to address this long-standing, unmet need by providing BVI people with real-time and independent access to digital text and graphical materials via commercial smartphones and tablets. To date, there is no approach that has the potential to provide both text and graphical materials in a seamless manner, which is the de-facto experience gained by sighted users. To fill this gap, UNAR Labs LLC has developed an intuitive information access solution called Midlina, which will automatically translate visual information in to multisensory equivalent and deliver it to BVI users via smart touchscreen-based computing devices. The activities of this SBIR were designed to apply findings from the customer interviews and from past NSF-sponsored academic research to build an intuitive system that convey combined textual and graphical information via a multisensory interface and prove that BVI end-users can utilize the system to gain access to otherwise inaccessible digital media materials.\n\n\nThe Phase II research was driven by two major goals: (1) Develop a comprehensive visual-to-multisensory conversion framework and establish Midlina as the core technology on Information Access Technology (IAT) devices for performing visual-to-multisensory conversion of combined textual and graphical information from digital documents, and (2) Establish system efficacy, usability, commercial viability, and user acceptance of the two Midlina powered applications. Using math worksheets and SAT practice test materials as exemplary use cases, a first-of-its-kind AI framework was developed that can convert visually oriented inaccessible documents (with text, math, and images) into its equivalent accessible tactile formats. To demonstrate the applicability of the model, we developed two applications: (1) a web-app called Kanak - aimed at teachers for converting educational materials into accessible formats, and (2) a mobile app called, Holo - aimed at BVI end users to access the converted educational materials using voiceover and multisensory feedback. Through human studies involving expert teachers, transcribers, and BVI end-users the usability and acceptance of the products were evaluated. Results demonstrated that participants were able to efficiently use the system for converting digital inaccessible documents into accessible formats and read them using voiceover and haptic interactions. Readily available and affordable access to informative materials will promote independence and increase productivity. The outcome of this SBIR effort (i.e., demonstration that Midlina is commercially viable) clearly indicated that Midlina will significantly aid in reducing the detrimental effects experienced by BVI people on their educational, vocational, and social opportunities.\n\n\n\t\t\t\t\tLast Modified: 11/24/2023\n\n\t\t\t\t\tSubmitted by: DanielRPratt\n"
 }
}