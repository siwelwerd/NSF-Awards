{
 "awd_id": "2004364",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research : Elements : Extending the physics reach of LHCb by developing and deploying algorithms for a fully GPU-based  first trigger stage",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032925147",
 "po_email": "dmassey@nsf.gov",
 "po_sign_block_name": "Daniel F. Massey",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 289603.0,
 "awd_amount": 289603.0,
 "awd_min_amd_letter_date": "2020-05-01",
 "awd_max_amd_letter_date": "2020-05-01",
 "awd_abstract_narration": "The development of the Standard Model (SM) of particle physics is a major intellectual achievement. The validity of this model was further confirmed by the discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN. However, the Standard Model leaves open many questions, including why matter dominates over anti-matter in the Universe and the properties of dark matter. Most explanations require new phenomena, which we call Beyond the Standard Model Physics (BSM), and which the LHCb experiment at CERN has been designed to explore. The LHC is the premier High Energy Physics particle accelerator in the world and is currently operating at the CERN laboratory near Geneva Switzerland, one of the foremost facilities for addressing these BSM questions. The LHCb experiment is one of four large experiments at the LHC and is designed to study in detail the decays of hadrons containing b or c quarks. The goal is to identify the existence of new physics beyond the Standard Model by examining the properties of hadrons containing these quarks. The new physics, or new forces, can be manifest by particles, as yet to be discovered, whose presence would modify decay rates and CP violating asymmetries of hadrons containing the b and c quarks, allowing new phenomena to be observed indirectly - or via direct observation of new force-carrying particles. The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, in which both PIs participate, produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project is developing and deploying software that will maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.\r\n\r\nThe LHCb detector is being upgraded for Run 3 (which will start to record data in 2022), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger is analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in Run 3, LHCb plans to process the entire 25 exabytes each year using high-level computing algorithms. The PIs propose running the entire first trigger-processing stage on GPUs, which has zero (likely negative) net cost, and frees up all of the CPU resources for the second processing stage. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which will need to be reoptimized both for Run 3 conditions but also for usage on GPUs. The specific objectives of this proposal are developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, likely using ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances will make it possible to explore many potential explanations for dark matter, e.g., dark photon decays, and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Sokoloff",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Michael D Sokoloff",
   "pi_email_addr": "mike.sokoloff@uc.edu",
   "nsf_id": "000233322",
   "pi_start_date": "2020-05-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Cincinnati Main Campus",
  "inst_street_address": "2600 CLIFTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CINCINNATI",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "5135564358",
  "inst_zip_code": "452202872",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OH01",
  "org_lgl_bus_name": "CINCINNATI UNIV OF",
  "org_prnt_uei_num": "DZ4YCZ3QSPR5",
  "org_uei_num": "DZ4YCZ3QSPR5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Cincinnati Main Campus",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "452210222",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OH01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 289603.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The LHCb experiment at CERN is optimized to collect very large samples of decays of hadrons (strongly interacting particles) that contain charm and bottom quarks. These allow precise measurements of Standard Model Processes and sensitive searches for Physics Beyond the Standard Model. For Run 3, which started collecting data in 2022, the&nbsp;sensor arrays of the &nbsp;experiment produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ another data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project was participating in the development and deployment of software &nbsp;to &nbsp;maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.</p>\n<p>&nbsp;</p>\n<p>A new software framework, called Allen in honor of Frances Allen, has been developed and deployed to execute the first stage trigger fully inside GPUs that are attached to x86 servers, called event builders, that collect the data from the individual detectors and assemble them into events corresponding to all the signals from a single beam crossing. At design luminosity, each beam crossing typically averages&nbsp;&nbsp;about 5 proton-proton collisions. The first level trigger reconstructs data from each of the detectors and selects about 3% of the beam crossings for additional processing by a second stage software trigger that executes in x86 CPUs.</p>\n<p>&nbsp;</p>\n<p>Allen has been deployed and acquires data at 30 MHz, as planned. In 2022, each of 170 Event Builder nodes (each an X86 server) hosted one GPU. In spring 2023, we added a second GPU to each Event Builder node. In spring 2024, we added a third GPU to each. This allows us to more fully reconstruct each event in the first stage trigger, allowing us to identify potential signal events with even greater efficiency and fidelity. As an example, we are now able to reconstruct tracks with hits only in detectors downstream of the experiment's dipole magnet in the first stage trigger.&nbsp;&nbsp;This was not possible during Runs 1 and 2. This increases the trigger efficiency for decays with Kshort and Lambda decays by factors of 2 to 3, and by even more for cascade decays of charged hyperons which subsequently decay to Lambda baryons.</p>\n<p>&nbsp;</p>\n<p>Because the first stage software trigger is able to process 30 MHz of data from all the detectors in Allen, we do not use a hardware trigger to reduce the data flowing into the software trigger. In Runs 1 and 2 we had to reduce the event rate from 30 MHz to 1 MHz using a hardware trigger. As a result, the current trigger system is 2 to 4 times more efficient for a wide variety of decay channels. Additionally, systematic biases and uncertainties associated with the hardware trigger have been eliminated.&nbsp;&nbsp;This makes analysis easier and more robust.</p>\n<p>&nbsp;</p>\n<p>Altogether, the first stage software trigger that runs on GPUs inside the Allen framework allows LHCb to collect more and higher quality data than was possible in earlier runs. The detector was fully deployed in 2024, and the software trigger performed as envisaged. Run 3 will continue to collect data in 2025 and 2026. The quality (and quantity) of data we have acquired this year appears to be excellent, so the collaboration of more than 1000 physicists should produce many interesting and high-quality physics results.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 10/28/2024<br>\nModified by: Michael&nbsp;D&nbsp;Sokoloff</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe LHCb experiment at CERN is optimized to collect very large samples of decays of hadrons (strongly interacting particles) that contain charm and bottom quarks. These allow precise measurements of Standard Model Processes and sensitive searches for Physics Beyond the Standard Model. For Run 3, which started collecting data in 2022, thesensor arrays of the experiment produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ another data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project was participating in the development and deployment of software to maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.\n\n\n\n\n\nA new software framework, called Allen in honor of Frances Allen, has been developed and deployed to execute the first stage trigger fully inside GPUs that are attached to x86 servers, called event builders, that collect the data from the individual detectors and assemble them into events corresponding to all the signals from a single beam crossing. At design luminosity, each beam crossing typically averagesabout 5 proton-proton collisions. The first level trigger reconstructs data from each of the detectors and selects about 3% of the beam crossings for additional processing by a second stage software trigger that executes in x86 CPUs.\n\n\n\n\n\nAllen has been deployed and acquires data at 30 MHz, as planned. In 2022, each of 170 Event Builder nodes (each an X86 server) hosted one GPU. In spring 2023, we added a second GPU to each Event Builder node. In spring 2024, we added a third GPU to each. This allows us to more fully reconstruct each event in the first stage trigger, allowing us to identify potential signal events with even greater efficiency and fidelity. As an example, we are now able to reconstruct tracks with hits only in detectors downstream of the experiment's dipole magnet in the first stage trigger.This was not possible during Runs 1 and 2. This increases the trigger efficiency for decays with Kshort and Lambda decays by factors of 2 to 3, and by even more for cascade decays of charged hyperons which subsequently decay to Lambda baryons.\n\n\n\n\n\nBecause the first stage software trigger is able to process 30 MHz of data from all the detectors in Allen, we do not use a hardware trigger to reduce the data flowing into the software trigger. In Runs 1 and 2 we had to reduce the event rate from 30 MHz to 1 MHz using a hardware trigger. As a result, the current trigger system is 2 to 4 times more efficient for a wide variety of decay channels. Additionally, systematic biases and uncertainties associated with the hardware trigger have been eliminated.This makes analysis easier and more robust.\n\n\n\n\n\nAltogether, the first stage software trigger that runs on GPUs inside the Allen framework allows LHCb to collect more and higher quality data than was possible in earlier runs. The detector was fully deployed in 2024, and the software trigger performed as envisaged. Run 3 will continue to collect data in 2025 and 2026. The quality (and quantity) of data we have acquired this year appears to be excellent, so the collaboration of more than 1000 physicists should produce many interesting and high-quality physics results.\n\n\n\t\t\t\t\tLast Modified: 10/28/2024\n\n\t\t\t\t\tSubmitted by: MichaelDSokoloff\n"
 }
}