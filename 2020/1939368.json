{
 "awd_id": "1939368",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Fairness-Aware Algorithms for Network Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 359842.0,
 "awd_amount": 375842.0,
 "awd_min_amd_letter_date": "2019-12-23",
 "awd_max_amd_letter_date": "2020-06-11",
 "awd_abstract_narration": "As society becomes increasingly reliant on artificial intelligence (AI) technology, there have been growing concerns whether the decisions generated by the AI systems may lead to discriminatory actions against certain protected groups in the population. These concerns have brought increasing scrutiny into the issue of fairness in AI systems and their underlying machine learning algorithms. To overcome this challenge, the overarching goal of this project is to develop fairness-aware algorithms that can maintain the high utility of decisions generated by the AI systems without discriminating against particular subgroups of the population. Specifically, this research will address fundamental issues of fairness in algorithms that utilize network data in their decision making. Successful completion of this project will not only produce novel algorithms for researchers, but also tools that can help practitioners assess the level of inequality present in social networking platforms. The undergraduate and graduate students who participate in the project will be trained to conduct cutting edge research in AI and network science. The investigators will also seek collaborative partnership with research scientists from the industry to apply the developed methods in order to expand their social impact beyond the academic community.\r\n\r\nThis research fills a major gap in current research on fairness in AI, which has primarily focused on independent and identically distributed (i.i.d.) data. There are still questions remain whether the existing methods are effective when applied to network data. In particular, the link structure of the network often contains information about the protected attributes (e.g., gender, race, or sexual orientation), and thus, must be taken into consideration in the design of fairness-aware machine learning algorithms. To address this issue, the objectives of this project are two-fold: (1) To develop metrics for assessing fairness in network learning algorithms and (2) To design, implement, and evaluate network learning algorithms that consider the tradeoff between fairness and utility of the models for various network analysis tasks and applications (including community detection and link prediction). The innovative methods developed in this project will be a step forward towards bridging the gap between current understanding of fairness in i.i.d. data and its application to network analysis.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Pang-Ning",
   "pi_last_name": "Tan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pang-Ning Tan",
   "pi_email_addr": "ptan@cse.msu.edu",
   "nsf_id": "000381916",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Abdol",
   "pi_last_name": "Esfahanian",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Abdol H Esfahanian",
   "pi_email_addr": "esfahanian@cse.msu.edu",
   "nsf_id": "000325530",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "428 S. Shaw Lane",
  "perf_city_name": "East Lansing",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488241226",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "0757",
   "pgm_ref_txt": "COOP PLAN OPs & SERVICES"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 375842.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As artificial intelligence (AI) technology becomes increasingly integrated into different facets of society, there is mounting concern over the possibility of AI-generated decisions to yield discriminatory outcomes against certain groups within the population. These apprehensions have catalyzed a rigorous examination into fairness within AI systems and the possible biases perpetuated by their underlying machine learning algorithms. In light of this pressing challenge, the primary objective of this project is to develop fairness-aware algorithms that preserve the utility of decisions generated by AI systems while mitigating the risk of discrimination against specific subgroups within the population. This project addresses a significant gap in current studies on fairness in AI, which have predominantly focused on independent and identically distributed (i.i.d.) data, by prioritizing the design of fairness-aware methods for network data.</p>\n<p>Our project has made significant progress in enhancing fairness in AI through various strategies. First, we have pioneered the development of novel metrics specifically designed for evaluating fairness within network analysis. As network data encompasses both a link structure and node attributes, our metrics facilitate the evaluation of fairness from the link or node perspectives. We first introduced a dyadic-level fairness metric specifically designed to foster connections between nodes of different types for link prediction problems. In addition, we also proposed a novel node-level metric known as fairness perception that evaluates fairness for each individual by considering the outcomes of their network connections.</p>\n<p>Second, we developed novel machine learning algorithms to tackle the challenges of incorporating fairness into a wide array of network mining tasks, including link prediction, node classification, node ranking, and graph sampling. For link prediction, we observed that existing algorithms often exhibit a link bias that inadvertently contribute to the increasing segregation of a network&mdash;a phenomenon commonly referred to as the filter bubble problem, due to the homophily principle. To mitigate this issue, we presented a novel framework that combines adversarial network learning with supervised link prediction techniques to alleviate the filter bubble problem. For node classification, we identified two significant biases that may impede the performance of current methods, namely, (1) bias favoring the majority class and (2) bias detrimental to the protected group. To address these biases, we proposed an adversarial learning algorithm incorporating a novel cost-sensitive exponential loss function, aimed at mitigating the adverse effects of both biases.</p>\n<p>Another major contribution of this project is in the development of a graph sampling algorithm that balances the tradeoff between maintaining fairness and preserving the topological features of the network. This is accomplished through the utilization of a unified metric called max-min subgraph fairness. A greedy algorithm is also proposed to generate a fair, representative sample from a large network. We also presented theoretical approximation guarantees for the proposed graph sampling algorithm, relying on principles of submodularity and curvature ratios.</p>\n<p>Additionally, we have explored fairness within the context of assessing node influence and its ranking in networks. &nbsp;Specifically, we investigated the use of a deep neural network to simulate influence propagation in a complex network, employing a diffusion process such as the linear threshold model. The neural network model is trained to learn the node representation while capturing both the node influence and the diffusion process dynamics. We then proceeded to tackle the issue of identifying the top-k most influential nodes within a network while avoiding bias towards specific groups. To achieve this, we introduced a novel approach to integrate both fairness and rank utility criteria in a unified optimization deep neural network framework.</p>\n<p>A summary of the outcomes of this project are as follows:</p>\n<p>- 6 published peer-reviewed articles (with several papers currently under review) along with 1 M.S. and 1 Ph.D. dissertation.</p>\n<p>- A project website providing extensive information related to the project, including an online tutorial on network analysis and a fair social influence gaming app targeting K-12 students.</p>\n<p>- A GitHub repository called FawNA (Fairness-aware Network Analysis) was created to disseminate the software and datasets generated from the project.</p>\n<p>- Supported 4 graduate and 3 undergraduate students, including 1 female and 2 minority students from underrepresented groups. Two of the graduate students who completed their degrees have embarked on careers in the industry. One of the undergraduate students who graduated has chosen to advance their academic journey by pursuing a graduate degree in computer science.</p><br>\n<p>\n Last Modified: 03/19/2024<br>\nModified by: Pang-Ning&nbsp;Tan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs artificial intelligence (AI) technology becomes increasingly integrated into different facets of society, there is mounting concern over the possibility of AI-generated decisions to yield discriminatory outcomes against certain groups within the population. These apprehensions have catalyzed a rigorous examination into fairness within AI systems and the possible biases perpetuated by their underlying machine learning algorithms. In light of this pressing challenge, the primary objective of this project is to develop fairness-aware algorithms that preserve the utility of decisions generated by AI systems while mitigating the risk of discrimination against specific subgroups within the population. This project addresses a significant gap in current studies on fairness in AI, which have predominantly focused on independent and identically distributed (i.i.d.) data, by prioritizing the design of fairness-aware methods for network data.\n\n\nOur project has made significant progress in enhancing fairness in AI through various strategies. First, we have pioneered the development of novel metrics specifically designed for evaluating fairness within network analysis. As network data encompasses both a link structure and node attributes, our metrics facilitate the evaluation of fairness from the link or node perspectives. We first introduced a dyadic-level fairness metric specifically designed to foster connections between nodes of different types for link prediction problems. In addition, we also proposed a novel node-level metric known as fairness perception that evaluates fairness for each individual by considering the outcomes of their network connections.\n\n\nSecond, we developed novel machine learning algorithms to tackle the challenges of incorporating fairness into a wide array of network mining tasks, including link prediction, node classification, node ranking, and graph sampling. For link prediction, we observed that existing algorithms often exhibit a link bias that inadvertently contribute to the increasing segregation of a networka phenomenon commonly referred to as the filter bubble problem, due to the homophily principle. To mitigate this issue, we presented a novel framework that combines adversarial network learning with supervised link prediction techniques to alleviate the filter bubble problem. For node classification, we identified two significant biases that may impede the performance of current methods, namely, (1) bias favoring the majority class and (2) bias detrimental to the protected group. To address these biases, we proposed an adversarial learning algorithm incorporating a novel cost-sensitive exponential loss function, aimed at mitigating the adverse effects of both biases.\n\n\nAnother major contribution of this project is in the development of a graph sampling algorithm that balances the tradeoff between maintaining fairness and preserving the topological features of the network. This is accomplished through the utilization of a unified metric called max-min subgraph fairness. A greedy algorithm is also proposed to generate a fair, representative sample from a large network. We also presented theoretical approximation guarantees for the proposed graph sampling algorithm, relying on principles of submodularity and curvature ratios.\n\n\nAdditionally, we have explored fairness within the context of assessing node influence and its ranking in networks. Specifically, we investigated the use of a deep neural network to simulate influence propagation in a complex network, employing a diffusion process such as the linear threshold model. The neural network model is trained to learn the node representation while capturing both the node influence and the diffusion process dynamics. We then proceeded to tackle the issue of identifying the top-k most influential nodes within a network while avoiding bias towards specific groups. To achieve this, we introduced a novel approach to integrate both fairness and rank utility criteria in a unified optimization deep neural network framework.\n\n\nA summary of the outcomes of this project are as follows:\n\n\n- 6 published peer-reviewed articles (with several papers currently under review) along with 1 M.S. and 1 Ph.D. dissertation.\n\n\n- A project website providing extensive information related to the project, including an online tutorial on network analysis and a fair social influence gaming app targeting K-12 students.\n\n\n- A GitHub repository called FawNA (Fairness-aware Network Analysis) was created to disseminate the software and datasets generated from the project.\n\n\n- Supported 4 graduate and 3 undergraduate students, including 1 female and 2 minority students from underrepresented groups. Two of the graduate students who completed their degrees have embarked on careers in the industry. One of the undergraduate students who graduated has chosen to advance their academic journey by pursuing a graduate degree in computer science.\t\t\t\t\tLast Modified: 03/19/2024\n\n\t\t\t\t\tSubmitted by: Pang-NingTan\n"
 }
}