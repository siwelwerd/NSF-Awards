{
 "awd_id": "2016262",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Distributed Shared Persistent Memory",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Alexander Jones",
 "awd_eff_date": "2019-08-13",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 162104.0,
 "awd_amount": 162104.0,
 "awd_min_amd_letter_date": "2020-01-23",
 "awd_max_amd_letter_date": "2020-01-23",
 "awd_abstract_narration": "This project has the potential to revolutionize datacenter systems by developing the first type of system that integrates traditional datacenter memory and storage systems in one layer. The proposed software platform can potentially be used to develop various course and research projects. The investigator will publicly release the developed software to enable a rich set of datacenter applications and to foster more cross-layer research activities.\r\n\r\nNext-generation non-volatile memories (NVMs) provide byte addressability, persistence, and low-latency performance. They are poised to radically alter the landscape of memory and storage technologies and have already inspired a host of research projects. Most previous research on NVMs has focused on using them in a single machine. It is unclear how to best utilize NVMs in distributed, datacenter environments. This project takes a significant step towards the goal of using NVMs in distributed datacenter environments by developing Distributed Shared Persistent Memory (DSPM), a framework that uses a pool of machines with NVMs to form a global, shared, and persistent memory space. Applications can perform traditional memory load and store instructions to access both local and remote data in this global memory space and can at the same time make their data persistent and reliable. Unlike traditional two-layer approaches with a memory and a storage layer, DSPM has just one layer that serves both as distributed memory and as distributed storage. Anticipated deliverables include a new kernel-based, software DSPM system and a set of datacenter applications ported to DSPM.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiying",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiying Zhang",
   "pi_email_addr": "yiying@ucsd.edu",
   "nsf_id": "000705250",
   "pi_start_date": "2020-01-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive 0934",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930621",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 162104.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Persistent memory is a type of hardware that has DRAM-like performance and storage-like capacity and non-volatility. The goal of this project is to explore different design options to support a distributed set of persistent memory that can be shared by multiple applications and multiple servers.</p>\n<p>We first&nbsp;built Hotpot [<em>SoCC'17, NVMW'18</em>], an open-source kernel-level distributed shared persistent memory system that provides low latency, transparent memory accesses, data persistence, data reliability, and high availability. Hotpot includes a basic data store system that splits a global persistent memory space into fix-sized chunks, each of which has a home node and possibly other nodes that access it. It also includes two transaction systems, one suppoting single writer and multiple readers and one supporting multiple writers and multiple readers. Finally, Hotpot supports replication and a set of recovery mechanisms.</p>\n<p>Hotpot is the <em>first</em> system that supports distributed and shared accesses to persistent memory. As such, it allows applications to access large amounts of persistent memory and demonstrates one way to build large-scale persistent memory systems. However, it leaves several other key problems unsolved. First, Hotpot requires each server in a cluster to host both application computation and persistent memory, which means that to deploy persistent memory to an existing data center, we need to find free DDR slots in existing servers or purchase new servers just to host persistent memory. Second, Hotpot employs a coherence protocol that has non-trivial performance cost and demands additional network bandwdith consumption for the coherent traffic.</p>\n<p>To solve the above problems, we proposed a new architecture to easily deploy persistent memory to existing data cetners and to efficiently use it [<em>USENIX&nbsp;ATC'20]</em>. Specifically, our proposal is to attach persistent memory directly to the netowrk as standalone devices with no local processing power. Applications run on separate servers (which we call compute nodes) and store data in the remote persistent memory devices. We call this model \"passive resource disaggregation\". This model has several key benefits to deploying persistent memory. Since no processing power is needed at the persistent-memory devices, the cost of building them is low. Since they are directly attached to the network, deploying persistent memory does not require any changes to existing data-center infrastructures. Furthermore, we lift the coherence-management tasks to the application level. Doing so avoid the high performance overhead caused by automatically provided coherence.</p>\n<p>With the passive resource disaggregation model, we built three types of persistent-memory systems. The first system manages and processes data both at the compute nodes, which then access data in persistent-memory devices over the network. The second system&nbsp;manages and processes data at a centralized global server, which sits between compute nodes and persistent-memory devices. The third system separates the data plane and the control plane, with the former running at compute nodes and the latter running at a global metadata server. Our finding is that the third type has the best overall performance and scalability, and it largely outperforms Hotpot, with low energy cost.</p>\n<p>After building the above distributed persistent-memory systems, we shifted our focus to study potential ways to connect persisten-memory devices to other computing resources. Specifically, we closely inspected four types of high-speed coherent interconnect standards that were proposed by the industry:&nbsp;CXL (Compute Express Link), CCIX (Cache Coherent Interconnect for Accelerators), OpenCAPI (Open Coherent Accelerator Processor Interface), and GenZ.&nbsp;We studied the protocols of these standards, their use cases, performance implications, scalability, reliability, and their pros and cons.&nbsp;Our finding is that GenZ is the most scalable one and has the richest feature support, while CXL is more suitable for a single-server scale that has a CPU-centric view.</p>\n<p>Overall, our above efforts of building distributed persistent-memory systems have laid the foundation of deploying persistent memory in large scale. We have discovered key challenges in building performant, low-cost persistent memory systems, potential pitfalls, and solutions to these challenges. Our works have attracted attention from both academia and industry. With our solutions, applications that require large amounts of data such as machine learning, data analytics, and graph processing could now host such data in a performant and low-cost way that goes far beyond what today's memory-based systems could offer.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/23/2022<br>\n\t\t\t\t\tModified by: Yiying&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPersistent memory is a type of hardware that has DRAM-like performance and storage-like capacity and non-volatility. The goal of this project is to explore different design options to support a distributed set of persistent memory that can be shared by multiple applications and multiple servers.\n\nWe first built Hotpot [SoCC'17, NVMW'18], an open-source kernel-level distributed shared persistent memory system that provides low latency, transparent memory accesses, data persistence, data reliability, and high availability. Hotpot includes a basic data store system that splits a global persistent memory space into fix-sized chunks, each of which has a home node and possibly other nodes that access it. It also includes two transaction systems, one suppoting single writer and multiple readers and one supporting multiple writers and multiple readers. Finally, Hotpot supports replication and a set of recovery mechanisms.\n\nHotpot is the first system that supports distributed and shared accesses to persistent memory. As such, it allows applications to access large amounts of persistent memory and demonstrates one way to build large-scale persistent memory systems. However, it leaves several other key problems unsolved. First, Hotpot requires each server in a cluster to host both application computation and persistent memory, which means that to deploy persistent memory to an existing data center, we need to find free DDR slots in existing servers or purchase new servers just to host persistent memory. Second, Hotpot employs a coherence protocol that has non-trivial performance cost and demands additional network bandwdith consumption for the coherent traffic.\n\nTo solve the above problems, we proposed a new architecture to easily deploy persistent memory to existing data cetners and to efficiently use it [USENIX ATC'20]. Specifically, our proposal is to attach persistent memory directly to the netowrk as standalone devices with no local processing power. Applications run on separate servers (which we call compute nodes) and store data in the remote persistent memory devices. We call this model \"passive resource disaggregation\". This model has several key benefits to deploying persistent memory. Since no processing power is needed at the persistent-memory devices, the cost of building them is low. Since they are directly attached to the network, deploying persistent memory does not require any changes to existing data-center infrastructures. Furthermore, we lift the coherence-management tasks to the application level. Doing so avoid the high performance overhead caused by automatically provided coherence.\n\nWith the passive resource disaggregation model, we built three types of persistent-memory systems. The first system manages and processes data both at the compute nodes, which then access data in persistent-memory devices over the network. The second system manages and processes data at a centralized global server, which sits between compute nodes and persistent-memory devices. The third system separates the data plane and the control plane, with the former running at compute nodes and the latter running at a global metadata server. Our finding is that the third type has the best overall performance and scalability, and it largely outperforms Hotpot, with low energy cost.\n\nAfter building the above distributed persistent-memory systems, we shifted our focus to study potential ways to connect persisten-memory devices to other computing resources. Specifically, we closely inspected four types of high-speed coherent interconnect standards that were proposed by the industry: CXL (Compute Express Link), CCIX (Cache Coherent Interconnect for Accelerators), OpenCAPI (Open Coherent Accelerator Processor Interface), and GenZ. We studied the protocols of these standards, their use cases, performance implications, scalability, reliability, and their pros and cons. Our finding is that GenZ is the most scalable one and has the richest feature support, while CXL is more suitable for a single-server scale that has a CPU-centric view.\n\nOverall, our above efforts of building distributed persistent-memory systems have laid the foundation of deploying persistent memory in large scale. We have discovered key challenges in building performant, low-cost persistent memory systems, potential pitfalls, and solutions to these challenges. Our works have attracted attention from both academia and industry. With our solutions, applications that require large amounts of data such as machine learning, data analytics, and graph processing could now host such data in a performant and low-cost way that goes far beyond what today's memory-based systems could offer.\n\n\t\t\t\t\tLast Modified: 02/23/2022\n\n\t\t\t\t\tSubmitted by: Yiying Zhang"
 }
}