{
 "awd_id": "2008076",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Randomness Extraction and Pseudorandomness",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2020-07-24",
 "awd_max_amd_letter_date": "2020-07-24",
 "awd_abstract_narration": "Random numbers are surprisingly useful in computer science. For example, they are used for simulations of complex systems, such as the weather and the economy.  In addition, randomness is essential for \"streaming algorithms,\" where there is so much data arriving that it is impossible to store it all.  Moreover, randomness is vital for computer security.  While randomness has many applications, truly random numbers are difficult to obtain.  It is therefore important to develop techniques to get by with less randomness or lower quality randomness.  The main tool to reduce the amount of randomness required is a pseudorandom number generator. In contrast, the main tool to reduce the quality of randomness required is a randomness extractor.\r\n\r\nThis proposal addresses important questions about pseudorandom generators and randomness extractors that relate to the investigator's recent work.  For example, the investigator and his student gave an efficient algorithm that extracts randomness from two independent sources of low-quality randomness that was dramatically better than previously known. However, the error is too large for applications in cryptography. The investigator proposes to improve this, as well as work on other aspects of randomness extraction.  The investigator also proposes to construct pseudorandom generators that work for large classes of randomized algorithms, such as those using a small amount of memory.  Finally, the investigator proposes work connecting these objects to seemingly unrelated areas, such as big data in the form of streaming algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Zuckerman",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "David I Zuckerman",
   "pi_email_addr": "diz@utexas.edu",
   "nsf_id": "000181714",
   "pi_start_date": "2020-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Austin",
  "perf_str_addr": "3925 W Braker Lane, Suite 3340",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787595316",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Randomness is surprisingly useful in computer science.&nbsp; It is used for Monte Carlo simulations of complex systems, such as the weather and the economy.&nbsp; It is essential to computer security.&nbsp; The fastest and simplest algorithms for many problems involve randomization.&nbsp; One example is testing whether a number is prime, a fundamental problem that's crucial in cryptography.<br />On the other hand, high-quality randomness is difficult to obtain.&nbsp; It's therefore natural to ask exactly how useful is randomness.&nbsp; For example, after much effort researchers did discover an efficient deterministic algorithm for primality, although it is still slower than the randomized one.&nbsp; Does every efficient randomized algorithm have an equivalent efficient deterministic algorithm?&nbsp; Short of that, can we convert our randomized algorithms into ones requiring few random bits?&nbsp; Or can we convert them into a robust form that will work with low-quality randomness?&nbsp; The tool to address the first two questions is a pseudorandom generator.&nbsp; The tool to address the last question is a randomness extractor.<br />In this project, the PI achieved significant progress in constructing randomness extractors and pseudorandom generators. First, the PI and coauthors studied a model of low-quality randomness introduced in the 1980s where every new bit, or block of bits, has some randomness conditioned on previous bits.&nbsp; In the 1980s researchers showed that it is impossible to extract high-quality randomness from this low-quality randomness. Nevertheless, we showed how to extract randomness that is close enough to high-quality for many applications.<br />Second, the PI and coauthors studied the problem of converting efficient randomized algorithms into efficient deterministic ones.&nbsp; There are well known obstacles to proving such a result unconditionally, so researchers have studied this problem while making certain lower bound assumptions.&nbsp; A large body of previous work showed how to do this conversion with a large polynomial slowdown.&nbsp; We showed how to do it with just a roughly quadratic slowdown, although we needed a stronger assumption.&nbsp; This led to a flurry of follow-up work.<br />The PI and coauthors had several other related results.&nbsp; Eventually, theoretical results along these lines will hopefully lead to advances in practice.</p><br>\n<p>\n Last Modified: 01/12/2024<br>\nModified by: David&nbsp;I&nbsp;Zuckerman</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRandomness is surprisingly useful in computer science. It is used for Monte Carlo simulations of complex systems, such as the weather and the economy. It is essential to computer security. The fastest and simplest algorithms for many problems involve randomization. One example is testing whether a number is prime, a fundamental problem that's crucial in cryptography.\nOn the other hand, high-quality randomness is difficult to obtain. It's therefore natural to ask exactly how useful is randomness. For example, after much effort researchers did discover an efficient deterministic algorithm for primality, although it is still slower than the randomized one. Does every efficient randomized algorithm have an equivalent efficient deterministic algorithm? Short of that, can we convert our randomized algorithms into ones requiring few random bits? Or can we convert them into a robust form that will work with low-quality randomness? The tool to address the first two questions is a pseudorandom generator. The tool to address the last question is a randomness extractor.\nIn this project, the PI achieved significant progress in constructing randomness extractors and pseudorandom generators. First, the PI and coauthors studied a model of low-quality randomness introduced in the 1980s where every new bit, or block of bits, has some randomness conditioned on previous bits. In the 1980s researchers showed that it is impossible to extract high-quality randomness from this low-quality randomness. Nevertheless, we showed how to extract randomness that is close enough to high-quality for many applications.\nSecond, the PI and coauthors studied the problem of converting efficient randomized algorithms into efficient deterministic ones. There are well known obstacles to proving such a result unconditionally, so researchers have studied this problem while making certain lower bound assumptions. A large body of previous work showed how to do this conversion with a large polynomial slowdown. We showed how to do it with just a roughly quadratic slowdown, although we needed a stronger assumption. This led to a flurry of follow-up work.\nThe PI and coauthors had several other related results. Eventually, theoretical results along these lines will hopefully lead to advances in practice.\t\t\t\t\tLast Modified: 01/12/2024\n\n\t\t\t\t\tSubmitted by: DavidIZuckerman\n"
 }
}