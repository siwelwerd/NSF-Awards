{
 "awd_id": "2015485",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Advancing High-Dimensional Bayesian Asymptotics and Computation",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 160000.0,
 "awd_amount": 160000.0,
 "awd_min_amd_letter_date": "2020-06-26",
 "awd_max_amd_letter_date": "2020-06-26",
 "awd_abstract_narration": "The big data revolution has turned statistics and machine learning into highly active and fast-pace research areas that have seen great progress over the last few decades.  However  uncertainly quantification with big data and complex models remains a challenge in the field.   In theory, Bayesian statistics solves -- elegantly and straightforwardly -- the uncertainty quantification problem. There is therefore a need for ideas and methods for constructing useful and computationally scalable Bayesian procedures. This research project contributes towards that goal. The developed methodology can improve decision making in areas such as autonomous driving, medical diagnostics, bail decision, credit worthiness, criminal sentencing, to list a few. This research will also include training for graduate students. \r\n\r\n\r\nThis project contributes to the development of theoretically sound, and computationally scalable Bayesian methodologies for the recovery of high-dimensional parameters. Toward that goal, the PI will develop a novel and widely applicable quasi-Bayesian (semi-parametric) framework for learning high-dimensional parameters.  The project  will also contribute to the development of Bayesian asymptotic theory with the analysis of high-dimensional, non-identifiable models.  Several high-profile models (e.g. neural network models)  widely  used in the applications are non-identifiable.  By applying the new framework to canonical correlation analysis, this project will also contribute to the development of flexible Bayesian solutions for high-dimensional sparse canonical correlation analysis,  with wide applicability in bio-medical research. This research project will also contribute to the computational aspects of high-dimensional Bayesian statistics with the development of several novel MCMC and VA algorithms. Finally, this research project will also contribute more broadly to statistics and machine learning with the development of Bayesian generative adversarial networks (GAN).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yves",
   "pi_last_name": "Atchade",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yves Atchade",
   "pi_email_addr": "atchade@bu.edu",
   "nsf_id": "000090281",
   "pi_start_date": "2020-06-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022154715",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 160000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistics and machine learning have made great progress over the last few decades in automating complex decision making, and in aiding scientists advance their fields. However &nbsp;uncertainly quantification in these decisions remains a challenge. In theory, Bayesian statistics solves the uncertainty quantification problem. However deploying Bayesian procedures at scale is a major computational challenge, due the difficulty of sampling from, or rigorously approximating Bayesian posterior distributions. This project has contributed ideas and methods for constructing useful and computationally scalable Bayesian procedures.<br /><br />Specifically, we contributed to a line of work that aims to relax the Bayesian framework by allowing the use of flexible quasi-likelihood functions. The approach can be viewed as an approach to do semi-parametric modeling in a Bayesian setting. We showed through a rigorous mathematical analysis that the approach recovers correctly the parameter of interest. As an example, we applied the proposed framework to the estimation of sparse canonical correlation analysis (CCA), a statistical method widely used in bio-medical research. Our approach has produced the first rate-optimal, and computationally tractable algorithm for sparse CCA.<br /><br />As mentioned above, Bayesian inference can be used to rigorously quantify uncertainty in estimation and data-driven decisions. However the Bayesian approach leads to intractable sampling problems. We have developed a very fast Markov Chain Monte Carlo (MCMC) method that uses asynchronous updates, to sample approximately from Bayesian posterior. We have shown that the &nbsp;computational cost per iteration of our algorithm in generalized linear models grows only linearly with the number of data point and the number of relevant features (not the total number of features). We have showed that the algorithm is fast enough to tackle large over-parametrized models such deep neural networks. Mathematically, we have also established that in the case a linear regression model, the algorithm generates a Markov chain with fast mixing, and we are able to quantify the approximation error due to the asynchronous updates.&nbsp;<br /><br /><br />This research has also investigated the mathematical tools employed in the analysis of the convergence of MCMC algorithms. These algorithms are the goldstandard for Bayesian computation, and understanding the type of problems for which fast MCMC sampling is possible is a question of practical interest. One of the most common approach to investigate this question is to analyze the size of the spectral gap of the Markov chain. However this technique may be inappropriate when dealing with distributions with small isolated local modes. We introduced the concept of approximate spectral gap that generalizes the spectral gap, and can be used to analyze the mixing time of MCMC algorithms for which the usual spectral gap is degenerate or almost degenerate. The method can also be used to go beyond the worst-case analysis of mixing times.<br /><br /><br />This research &nbsp;has contributed to the development of the next generation of computational tools for rigorous uncertainty quantification. We have worked to advance these ideas, and turning them into usable product for the larger scientific community, through collaborations, software development, and presentation at seminars and conferences in statistics and machine learning.&nbsp;Several publications listed below came out of this research. We have also developed one Python library that implements some of the newly developed methods, available at</p>\r\n<p>https://github.com/xliu-522/SA-cSGLD</p>\r\n<p>On the educational side, this research and related ideas has helped graduate 3 PhD students, and mentored 2 postdocs. &nbsp;Two other current students have also worked on related topics.<br /><br />Y. Atchad\\'e, A. Bhattacharyya (2019). An approach to large-scale quasi-Bayesian inference with spike-and-slab priors, (arXiv:1803.10282).<br /><br />Y. Atchad\\'e and &nbsp;P. Dovonon, (2024). Consistent Bayesian variable selection in linear regression models with instrumental variables. &nbsp;(arXiv:1901.03182).<br /><br />L. Wang, X. Liu, A. Smith, and &nbsp;Y. Atchade (2024). On cyclical MCMC sampling. AISTATS 2024.<br /><br />P. Dovonon, Y. Atchade, and F. Tchatoka (2024). Efficiency bounds for moment condition models with mixed identification strength. Journal of Econometrics (to appear).<br /><br />Alexander Lin, Bahareh Tolooshams, Y. Atchad\\'e and Demba Ba (2023). Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models. &nbsp;ICML 2023.<br /><br />Qiuyun Zhu and Y. Atchad\\'e (2023). Minimax quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function. JASA 119(548), 2647&ndash;2657.<br /><br />Y. Atchad\\'e and L. Wang (2023). A fast asynchronous MCMC sampler for sparse Bayesian inference. JRSS-B 85, pages 1492-1516.<br /><br />Y. Atchad\\'e (2021). Approximate spectral Gaps for Markov chains mixing times in high dimensions. SIAM Journal on Mathematics of Data Science 3, 854-872.<br /><br />Joonha Park and Y. Atchad\\'e (2020). Markov chain Monte Carlo algorithms with sequential proposals. Statistics and Computing &nbsp;30, 1325&ndash;1345.<br /><br />D. Nguyen, P. de Valpine, Y. Atchad\\'e, D. Turek, N. &nbsp;Michaud, C. Paciorek (2020). Nested adaptation of MCMC algorithms, Bayesian Analysis 15 (4) 1323 - 1343.<br /><br />Y. Atchad\\'e and P. Dovonon (2020). &nbsp;Efficiency bounds for semiparametric models with singular score functions. &nbsp;Econometric Review 39, 612-648.<br /><br />P. Jacob, J. O'Leary, &nbsp;and Y. Atchad{\\'e} (2020). Unbiased Markov chain Monte Carlo with couplings, JRSS-B 82: 543-600. with discussion.</p><br>\n<p>\n Last Modified: 12/10/2024<br>\nModified by: Yves&nbsp;Atchade</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nStatistics and machine learning have made great progress over the last few decades in automating complex decision making, and in aiding scientists advance their fields. However uncertainly quantification in these decisions remains a challenge. In theory, Bayesian statistics solves the uncertainty quantification problem. However deploying Bayesian procedures at scale is a major computational challenge, due the difficulty of sampling from, or rigorously approximating Bayesian posterior distributions. This project has contributed ideas and methods for constructing useful and computationally scalable Bayesian procedures.\n\nSpecifically, we contributed to a line of work that aims to relax the Bayesian framework by allowing the use of flexible quasi-likelihood functions. The approach can be viewed as an approach to do semi-parametric modeling in a Bayesian setting. We showed through a rigorous mathematical analysis that the approach recovers correctly the parameter of interest. As an example, we applied the proposed framework to the estimation of sparse canonical correlation analysis (CCA), a statistical method widely used in bio-medical research. Our approach has produced the first rate-optimal, and computationally tractable algorithm for sparse CCA.\n\nAs mentioned above, Bayesian inference can be used to rigorously quantify uncertainty in estimation and data-driven decisions. However the Bayesian approach leads to intractable sampling problems. We have developed a very fast Markov Chain Monte Carlo (MCMC) method that uses asynchronous updates, to sample approximately from Bayesian posterior. We have shown that the computational cost per iteration of our algorithm in generalized linear models grows only linearly with the number of data point and the number of relevant features (not the total number of features). We have showed that the algorithm is fast enough to tackle large over-parametrized models such deep neural networks. Mathematically, we have also established that in the case a linear regression model, the algorithm generates a Markov chain with fast mixing, and we are able to quantify the approximation error due to the asynchronous updates.\n\n\nThis research has also investigated the mathematical tools employed in the analysis of the convergence of MCMC algorithms. These algorithms are the goldstandard for Bayesian computation, and understanding the type of problems for which fast MCMC sampling is possible is a question of practical interest. One of the most common approach to investigate this question is to analyze the size of the spectral gap of the Markov chain. However this technique may be inappropriate when dealing with distributions with small isolated local modes. We introduced the concept of approximate spectral gap that generalizes the spectral gap, and can be used to analyze the mixing time of MCMC algorithms for which the usual spectral gap is degenerate or almost degenerate. The method can also be used to go beyond the worst-case analysis of mixing times.\n\n\nThis research has contributed to the development of the next generation of computational tools for rigorous uncertainty quantification. We have worked to advance these ideas, and turning them into usable product for the larger scientific community, through collaborations, software development, and presentation at seminars and conferences in statistics and machine learning.Several publications listed below came out of this research. We have also developed one Python library that implements some of the newly developed methods, available at\r\n\n\nhttps://github.com/xliu-522/SA-cSGLD\r\n\n\nOn the educational side, this research and related ideas has helped graduate 3 PhD students, and mentored 2 postdocs. Two other current students have also worked on related topics.\n\nY. Atchad\\'e, A. Bhattacharyya (2019). An approach to large-scale quasi-Bayesian inference with spike-and-slab priors, (arXiv:1803.10282).\n\nY. Atchad\\'e and P. Dovonon, (2024). Consistent Bayesian variable selection in linear regression models with instrumental variables. (arXiv:1901.03182).\n\nL. Wang, X. Liu, A. Smith, and Y. Atchade (2024). On cyclical MCMC sampling. AISTATS 2024.\n\nP. Dovonon, Y. Atchade, and F. Tchatoka (2024). Efficiency bounds for moment condition models with mixed identification strength. Journal of Econometrics (to appear).\n\nAlexander Lin, Bahareh Tolooshams, Y. Atchad\\'e and Demba Ba (2023). Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models. ICML 2023.\n\nQiuyun Zhu and Y. Atchad\\'e (2023). Minimax quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function. JASA 119(548), 26472657.\n\nY. Atchad\\'e and L. Wang (2023). A fast asynchronous MCMC sampler for sparse Bayesian inference. JRSS-B 85, pages 1492-1516.\n\nY. Atchad\\'e (2021). Approximate spectral Gaps for Markov chains mixing times in high dimensions. SIAM Journal on Mathematics of Data Science 3, 854-872.\n\nJoonha Park and Y. Atchad\\'e (2020). Markov chain Monte Carlo algorithms with sequential proposals. Statistics and Computing 30, 13251345.\n\nD. Nguyen, P. de Valpine, Y. Atchad\\'e, D. Turek, N. Michaud, C. Paciorek (2020). Nested adaptation of MCMC algorithms, Bayesian Analysis 15 (4) 1323 - 1343.\n\nY. Atchad\\'e and P. Dovonon (2020). Efficiency bounds for semiparametric models with singular score functions. Econometric Review 39, 612-648.\n\nP. Jacob, J. O'Leary, and Y. Atchad{\\'e} (2020). Unbiased Markov chain Monte Carlo with couplings, JRSS-B 82: 543-600. with discussion.\t\t\t\t\tLast Modified: 12/10/2024\n\n\t\t\t\t\tSubmitted by: YvesAtchade\n"
 }
}