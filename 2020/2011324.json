{
 "awd_id": "2011324",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Multilevel Graph-Based Methods for Efficient Data Exploration",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 244217.0,
 "awd_amount": 244217.0,
 "awd_min_amd_letter_date": "2020-07-28",
 "awd_max_amd_letter_date": "2020-07-28",
 "awd_abstract_narration": "Graph theory helps scientists and engineers  model various types of relations between entities in a set, whether members of a social network, or molecules in a chemical compound for example.  Not surprisingly, with the advent of data-based methodologies that work by unraveling and exploiting relations between data items, graph theory tools are finding their way in a very broad range of applications.  The primary goal of this project is to examine a class of methods that manipulate graphs, specifically by developing effective multilevel algorithms that take advantage of divide and conquer approaches. In multilevel techniques, smaller and smaller graphs are extracted from some original graph with the goal of keeping as much of its intrinsic information as possible.  These smaller graphs are then employed instead of the original ones, resulting in significant gains in performance. This project addresses issues that are of great relevance to many current data-based methodologies and will be applicable across various disciplines. As such it will help promote interest in problems related to the current shift toward such methodologies because its research theme blends mathematical methods, innovations in algorithms, and applications. On the educational side, special courses and tutorials will be offered to graduate students from other disciplinary fields who wish to explore research in data sciences. This project will support one graduate student per year for each of the three years.\r\n\r\nThe rapid expansion of machine learning methodologies into a great variety of disciplines is pushing the demand for numerical methods that can effectively deal with large datasets.  Among these methods, those based on graph representations of data figure prominently.  The goal of this project is to develop effective multilevel algorithms that are rooted in graph theoretical approaches, for performing various machine learning tasks.  A primary focus of the planned research is that of \"graph coarsening\", a technique whereby an original graph is substantially reduced in size by agglomerating nearby nodes together, to produce a faithful representative of the original graph.  The project will exploit a class of methods based on multilevel coarsening, in which coarsening is applied recursively for a few levels.  The ultimate goal of a multilevel coarsening approach is to make it possible to perform the heavy computations with the coarsened graph which is much smaller, resulting in much faster processing, with minimal loss in accuracy. Coarsening is an effective alternative to random sampling, a well-established method that consists of replacing the original data by a subset of its columns or rows that are selected at random or quasi- randomly. This project will study, both empirically and theoretically, various coarsening strategies. For example, coarsening will be studied from the angle of a projection method for approximating eigenvectors.  Coarsening methods that try to preserve the eigenvectors exactly will also be studied.  Among the many possible applications of graph coarsening the project will specifically consider their use in speeding up the training of a class of neural networks known as Graph Convolutional Networks (GCNs).  A number of other research issues, all under the general theme of graph-based methods, will also be investigated.  For example, the project will study how a form of hypergraph coarsening can be used to provide a solution to the \"graph sparsification\" problem, whereby a sparser version of a given graph is sought, or to the \"column subset selection problem\" which consists of selecting important rows (or columns) from a given data matrix.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yousef",
   "pi_last_name": "Saad",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yousef Saad",
   "pi_email_addr": "saad@umn.edu",
   "nsf_id": "000303745",
   "pi_start_date": "2020-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "4-192 Keller Hall, 200 Union St.",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550167",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 244217.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Relationships between members of a population, or generally between two items of a set, are often represented by a graph. Such graphs provide valuable information that can be exploited to develop effective methods in data science.&nbsp; For example, one can determine groups of people, or 'clusters', from the graph that models such relationships&nbsp; as 'individuals (a) and (b) have published articles together' or 'individuals (a) and (b) have active e-mail correspondence'.&nbsp; Thus, it is not surprising that graphs are playing an increasingly important role in fields related to data science.&nbsp; Network analysis, graph neural networks, knowledge graphs, are just a few research topics associated with graph-based methodologies, and the list of applications is constantly growing. <br /><br />A graph is defined by a set of items along with a set of pairs between these items which express the fact that the two items in the pair are related.&nbsp; A common characteristic of graphs that arise from many real-life applications is that they can be quite large which makes standard methods inapplicable or too costly. Researchers in the scientific computing community have for a long time been exposed to a similar problem.&nbsp; In their case the graphs they encounter arise from two-dimensional&nbsp; or three-dimensional meshes on large computational domains. The number of mesh points can reach tens of millions in current state of the art simulations.&nbsp; To deal with these large problems it has become common practice to employ methods that exploit `graph coarsening'. Methods such as Algebraic Multigrid (AMG) do this by repeating the coarsening process recursively a few times in what is known as a multilevel coarsening approach.&nbsp; These methods have been quite successful.<br /><br />The initial goal of this project was to study how to adapt graph coarsening methods from scientific computing into machine learning. The project began with a simple idea of this type applied to Graph Convolutional Networks (GCN) by first coarsening the input graphs before it is utilized in the training for graph classification tasks.&nbsp; Although the idea amounts to&nbsp; a preprocessing technique, it resulted in an accuracy that is similar to, occasionally better than, when the full graph is used, at a much lower computational cost.<br /><br />In a similar vein, the investigators explored the problem of measuring the similarity between two graphs. In this context, the Earth Mover's Distance (EMD) and Wasserstein Weisfeiler-Lehman (WWL) graph kernels work by embedding the vertices of a graph and then computing a correspondence between embedding distributions. These methods work very well on small graphs, but are too computationally complex to be run on even moderately large graphs. It is therefore quite natural to invoke approximations based on graph coarsening to increase the speed of the graph kernel computation.&nbsp; This allows the kernels be applied to large graphs such as social network and protein structure datasets.&nbsp; On real world datasets, both labeled and unlabeled, Support Vector Machines (SVM) using the new kernels show similar accuracy to the original methods and higher accuracy than other popular kernels.&nbsp; The framework can also be applied to other vertex embedding methods.<br /><br />Computing eigenvalues of large matrices is an important problem in graph-based methods and so the investigating team&nbsp; explored how to compute spectra of large matrices in mixed precision arithmetic. Here the motivation stems from the observation that QR factorizations encounter significant difficulty in low precision and mixed precision environments.&nbsp; As a result, the investigating team explored ways of extracting approximate eigenvalues and vectors, without computing orthonormal bases. They found that that it is best to extract information from non-orthogonal bases by redefining the Garlerkin conditions, enabling classical solvers to avoid performance loss caused by the orthogonalization step entirely. Notably, this orthogonalization-free projection framework serves as a general strategy that can be combined with existing basis-building methods including subspace iteration, Krylov-based, and randomized range finder. The ortho-normalization process is replaced by a Hessenberg factorization to improve linear independence. An added benefit of the Hessenberg factorization is that it eliminates the need for inner products leading to easier parallelization.</p><br>\n<p>\n Last Modified: 04/24/2024<br>\nModified by: Yousef&nbsp;Saad</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRelationships between members of a population, or generally between two items of a set, are often represented by a graph. Such graphs provide valuable information that can be exploited to develop effective methods in data science. For example, one can determine groups of people, or 'clusters', from the graph that models such relationships as 'individuals (a) and (b) have published articles together' or 'individuals (a) and (b) have active e-mail correspondence'. Thus, it is not surprising that graphs are playing an increasingly important role in fields related to data science. Network analysis, graph neural networks, knowledge graphs, are just a few research topics associated with graph-based methodologies, and the list of applications is constantly growing. \n\nA graph is defined by a set of items along with a set of pairs between these items which express the fact that the two items in the pair are related. A common characteristic of graphs that arise from many real-life applications is that they can be quite large which makes standard methods inapplicable or too costly. Researchers in the scientific computing community have for a long time been exposed to a similar problem. In their case the graphs they encounter arise from two-dimensional or three-dimensional meshes on large computational domains. The number of mesh points can reach tens of millions in current state of the art simulations. To deal with these large problems it has become common practice to employ methods that exploit `graph coarsening'. Methods such as Algebraic Multigrid (AMG) do this by repeating the coarsening process recursively a few times in what is known as a multilevel coarsening approach. These methods have been quite successful.\n\nThe initial goal of this project was to study how to adapt graph coarsening methods from scientific computing into machine learning. The project began with a simple idea of this type applied to Graph Convolutional Networks (GCN) by first coarsening the input graphs before it is utilized in the training for graph classification tasks. Although the idea amounts to a preprocessing technique, it resulted in an accuracy that is similar to, occasionally better than, when the full graph is used, at a much lower computational cost.\n\nIn a similar vein, the investigators explored the problem of measuring the similarity between two graphs. In this context, the Earth Mover's Distance (EMD) and Wasserstein Weisfeiler-Lehman (WWL) graph kernels work by embedding the vertices of a graph and then computing a correspondence between embedding distributions. These methods work very well on small graphs, but are too computationally complex to be run on even moderately large graphs. It is therefore quite natural to invoke approximations based on graph coarsening to increase the speed of the graph kernel computation. This allows the kernels be applied to large graphs such as social network and protein structure datasets. On real world datasets, both labeled and unlabeled, Support Vector Machines (SVM) using the new kernels show similar accuracy to the original methods and higher accuracy than other popular kernels. The framework can also be applied to other vertex embedding methods.\n\nComputing eigenvalues of large matrices is an important problem in graph-based methods and so the investigating team explored how to compute spectra of large matrices in mixed precision arithmetic. Here the motivation stems from the observation that QR factorizations encounter significant difficulty in low precision and mixed precision environments. As a result, the investigating team explored ways of extracting approximate eigenvalues and vectors, without computing orthonormal bases. They found that that it is best to extract information from non-orthogonal bases by redefining the Garlerkin conditions, enabling classical solvers to avoid performance loss caused by the orthogonalization step entirely. Notably, this orthogonalization-free projection framework serves as a general strategy that can be combined with existing basis-building methods including subspace iteration, Krylov-based, and randomized range finder. The ortho-normalization process is replaced by a Hessenberg factorization to improve linear independence. An added benefit of the Hessenberg factorization is that it eliminates the need for inner products leading to easier parallelization.\t\t\t\t\tLast Modified: 04/24/2024\n\n\t\t\t\t\tSubmitted by: YousefSaad\n"
 }
}