{
 "awd_id": "2007462",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Computational Complexity Lower Bounds: Time, Space and Communication",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2020-06-24",
 "awd_max_amd_letter_date": "2020-06-24",
 "awd_abstract_narration": "While computers have revolutionized our world, the resources\r\nrequired to perform computational tasks are poorly understood.\r\nDeveloping a mathematical theory of computation is crucial in our\r\ninformation age, where computers are involved in essentially every\r\npart of our life. Computational complexity, the study of the amount\r\nof resources needed to perform computational tasks, is essential\r\nfor understanding the power of computation and for the development\r\nof a theory of computation. It is also essential in designing\r\nefficient communication protocols and secure cryptographic protocols,\r\nand in understanding human and machine learning. Proving lower\r\nbounds for the resources required by different computational\r\nmodels, and for different computational tasks, is among the most\r\nexciting, most challenging and most important topics in theoretical\r\ncomputer science. The project will study computational complexity\r\nlower bounds, focusing on two research directions: Lower bounds\r\nrelated to learning; and, studying the relative power of quantum\r\nand classical computational models.\r\n\r\nIn more details, we will focus on the following directions. (1)\r\nMemory-Samples lower bounds for learning: memory/samples lower\r\nbounds for online learning algorithms is a very exciting research\r\ndirection that has been studied in a line of recent work. For\r\nexample, it was proved that any algorithm for learning parities\r\nrequires either a memory of quadratic size or an exponential number\r\nof samples. The project will further study memory/samples lower\r\nbounds for learning and their relations to other topics in\r\ncomputational complexity. (2) The relative power of quantum and\r\nclassical computational models: The project will study gaps between\r\nquantum and classical complexity in various models. In particular, it\r\nwill investigate separations between quantum and classical communication complexity,\r\nas well as the relative power of quantum and classical algorithms\r\nin the context of memory/samples trade-offs for learning. (3)\r\nCircuit complexity lower bounds related to learning: The amazing\r\nsuccess of machine learning, and in particular deep learning,\r\nmotivates a study of closely related computational models. The\r\nproject will study linear-threshold circuits and other models of\r\ncircuits that are related to learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ran",
   "pi_last_name": "Raz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ran Raz",
   "pi_email_addr": "ranr@princeton.edu",
   "nsf_id": "000658795",
   "pi_start_date": "2020-06-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "87 Prospect Avenue",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  },
  {
   "pgm_ref_code": "7929",
   "pgm_ref_txt": "COMPUTATIONAL GEOMETRY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Below we describe three of the research outcomes of the project:</p>\n<p><strong>Certified Hardness vs. Randomness for Log-Space</strong></p>\n<p>We study the relative power of randomized versus deterministic algorithms with logarithmic space. Are randomized algorithms more powerful than deterministic ones? Using the well-known hardness-versus-randomness paradigm of Nisan and Wigderson, Klivans and van Melkebeek showed how to fully derandomize any randomized logspace algorithm, under a certain, widely believed, complexity-theoretic assumption. However, as in all previous works on the hardness-versus-randomness paradigm, their derandomized algorithm relies blindly on the complexity assumption. If the assumption is false, the derandomized algorithm may output incorrect values, and thus a user cannot be sure that an output given by the algorithm is correct.</p>\n<p>In a joint work with Pyne and Zhan, we show how to derandomize any randomized logspace algorithm, under the same complexity assumption, with a derandomized algorithm <em>D</em> that never outputs an incorrect value (even if the complexity assumption is false). If the assumption is true, <em>D</em> always outputs the correct value and if the assumption is false, <em>D</em> either outputs the correct value or alerts that the assumption was found to be false. Thus, if <em>D</em> outputs a value, that value is certified to be correct. Moreover, if <em>D</em> does not output a value, it alerts that the complexity assumption was found to be false and refutes the assumption.</p>\n<p><strong>Memory-Samples Lower Bounds for Learning:</strong></p>\n<p>In prior work, we demonstrated that certain online learning problems inherently require either super-linear (up to quadratic) memory size or a super-polynomial number of samples. In this project, in joint works with Garg, Kothari, Liu, Liu and Zhan, we further studied memory-samples lower bounds for learning and explored connections to other areas in complexity theory. Memory-constrained learning lower bounds highlight the critical role of memory in learning and cognitive processes. They may be relevant to understanding human learning and may have impacts on machine learning and optimization. Furthermore, they have applications in cryptography.</p>\n<p>Among the results that we obtained: We extended this line of work to noisy learning problems and proved improved memory-samples lower bounds in the noisy case. We also studied the quantum case, where the learning algorithm is a quantum algorithm with both, classical memory and quantum memory. We proved that for a large class of classical learning tasks, the known memory-samples lower bounds remain qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size <em>cn</em> (for some constant <em>c &gt; 0</em>).</p>\n<p><strong>A Parallel Repetition Theorem for All 3-Player Games with Binary Inputs:</strong></p>\n<p>Multi-player games are a central notion that changed the landscape of theoretical computer science and led to milestone achievements, such as, the PCP theorem, delegation of computation, and the theory of hardness of approximation. The same notion is also central in the study of quantum entanglement and is related to several topics in mathematics. In a <em>k</em>-player game, a referee chooses <em>k</em> random questions according to some (publicly known) joint distribution and sends the <em>i</em>-th question to the <em>i</em>-th player. Each player needs to send an answer, without any communication between the players. The players jointly win if a (publicly known) predicate holds. The value of the game is the maximal probability of success that the players can achieve. The parallel repetition of a multi-player game is a game where the players try to win <em>n</em> copies of the original game, simultaneously. The main question is what can be proved about the value of the parallel repetition of a game. For parallel repetition of 2-player games, with value &lt; 1, it is well known that the value of the <em>n</em>-fold parallel repetition of the game decays exponentially fast in <em>n</em>, but games involving 3 or more players have proven much more difficult to analyze, and the only known general bound on their parallel repeated value is an inverse Ackermann bound. Proving a better bound is a notoriously hard long-standing open problem.</p>\n<p>In a sequence works, joint with Girish, Holmgren, Mittal, and Zhan, we proved that for every 3-player game, with binary questions and value &lt; 1, the value of the <em>n</em>-fold parallel repetition of the game decays polynomially fast to 0. For many of these games, only inverse Ackerman bound was previously known and all previously known techniques failed. Along the way to proving this result, we prove several additional parallel repetition theorems for multi-player games.</p><br>\n<p>\n Last Modified: 11/06/2024<br>\nModified by: Ran&nbsp;Raz</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nBelow we describe three of the research outcomes of the project:\n\n\nCertified Hardness vs. Randomness for Log-Space\n\n\nWe study the relative power of randomized versus deterministic algorithms with logarithmic space. Are randomized algorithms more powerful than deterministic ones? Using the well-known hardness-versus-randomness paradigm of Nisan and Wigderson, Klivans and van Melkebeek showed how to fully derandomize any randomized logspace algorithm, under a certain, widely believed, complexity-theoretic assumption. However, as in all previous works on the hardness-versus-randomness paradigm, their derandomized algorithm relies blindly on the complexity assumption. If the assumption is false, the derandomized algorithm may output incorrect values, and thus a user cannot be sure that an output given by the algorithm is correct.\n\n\nIn a joint work with Pyne and Zhan, we show how to derandomize any randomized logspace algorithm, under the same complexity assumption, with a derandomized algorithm D that never outputs an incorrect value (even if the complexity assumption is false). If the assumption is true, D always outputs the correct value and if the assumption is false, D either outputs the correct value or alerts that the assumption was found to be false. Thus, if D outputs a value, that value is certified to be correct. Moreover, if D does not output a value, it alerts that the complexity assumption was found to be false and refutes the assumption.\n\n\nMemory-Samples Lower Bounds for Learning:\n\n\nIn prior work, we demonstrated that certain online learning problems inherently require either super-linear (up to quadratic) memory size or a super-polynomial number of samples. In this project, in joint works with Garg, Kothari, Liu, Liu and Zhan, we further studied memory-samples lower bounds for learning and explored connections to other areas in complexity theory. Memory-constrained learning lower bounds highlight the critical role of memory in learning and cognitive processes. They may be relevant to understanding human learning and may have impacts on machine learning and optimization. Furthermore, they have applications in cryptography.\n\n\nAmong the results that we obtained: We extended this line of work to noisy learning problems and proved improved memory-samples lower bounds in the noisy case. We also studied the quantum case, where the learning algorithm is a quantum algorithm with both, classical memory and quantum memory. We proved that for a large class of classical learning tasks, the known memory-samples lower bounds remain qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size cn (for some constant c  0).\n\n\nA Parallel Repetition Theorem for All 3-Player Games with Binary Inputs:\n\n\nMulti-player games are a central notion that changed the landscape of theoretical computer science and led to milestone achievements, such as, the PCP theorem, delegation of computation, and the theory of hardness of approximation. The same notion is also central in the study of quantum entanglement and is related to several topics in mathematics. In a k-player game, a referee chooses k random questions according to some (publicly known) joint distribution and sends the i-th question to the i-th player. Each player needs to send an answer, without any communication between the players. The players jointly win if a (publicly known) predicate holds. The value of the game is the maximal probability of success that the players can achieve. The parallel repetition of a multi-player game is a game where the players try to win n copies of the original game, simultaneously. The main question is what can be proved about the value of the parallel repetition of a game. For parallel repetition of 2-player games, with value n-fold parallel repetition of the game decays exponentially fast in n, but games involving 3 or more players have proven much more difficult to analyze, and the only known general bound on their parallel repeated value is an inverse Ackermann bound. Proving a better bound is a notoriously hard long-standing open problem.\n\n\nIn a sequence works, joint with Girish, Holmgren, Mittal, and Zhan, we proved that for every 3-player game, with binary questions and value n-fold parallel repetition of the game decays polynomially fast to 0. For many of these games, only inverse Ackerman bound was previously known and all previously known techniques failed. Along the way to proving this result, we prove several additional parallel repetition theorems for multi-player games.\t\t\t\t\tLast Modified: 11/06/2024\n\n\t\t\t\t\tSubmitted by: RanRaz\n"
 }
}