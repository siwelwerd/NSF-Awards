{
 "awd_id": "1953822",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "STTR Phase I:  Computerized System for Detection, Assessment, and Visualization of Intraoperative Bleeding During Robotic and Laparoscopic Surgery",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2020-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2020-05-26",
 "awd_max_amd_letter_date": "2020-05-26",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Technology Transfer (STTR) Phase I project is to reduce the morbidity and mortality of patients undergoing surgery.  Inadvertent bleeding during surgery represents a critical problem that occurs during all types of procedures on millions of patients around the world.  This project will advance a robotic surgical tool system using artificial intelligence (AI) to manage intraoperative bleeding.  Reduced blood loss will lead to a reduced demand for blood transfusions, reduced healthcare costs, and improved recuperation from surgery.   This technology will advance a universal standard for bolt-on safety utilities to the evolving surgical tool manufacturer market.\r\n\r\nThe Small Business Technology Transfer (STTR) Phase I project will advance the translation of an intelligent intraoperative system.  Currently, there is no tool to detect or characterize bleeding, so the surgeon must continually monitor the camera view for bleeding and estimate the source of the bleed, which is often submerged in a pool of blood.  The proposed effort will advance a prototype to assist a surgeon in detecting, visualizing, and characterizing arterial bleeding in real time during urological surgery. The source of the bleeding will be presented to the surgeon using 2D or 3D (augmented reality) overlays, enabling him/her to control the bleeding precisely and quickly.  The technology fuses robotics, computer vision, and machine learning in a novel manner to produce a surgical tool that will significantly advance the current state-of-the-art.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Madhu",
   "pi_last_name": "Reddiboina",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Madhu Reddiboina",
   "pi_email_addr": "madhu.reddiboina@rediminds.com",
   "nsf_id": "000803799",
   "pi_start_date": "2020-05-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "REDIMINDS, INC.",
  "inst_street_address": "29777 TELEGRAPH RD",
  "inst_street_address_2": "STE 1670",
  "inst_city_name": "SOUTHFIELD",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "2489968439",
  "inst_zip_code": "480341303",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "MI12",
  "org_lgl_bus_name": "REDIMINDS INC",
  "org_prnt_uei_num": "FKNMZ8DZTDX9",
  "org_uei_num": "FKNMZ8DZTDX9"
 },
 "perf_inst": {
  "perf_inst_name": "Tecredi, Inc",
  "perf_str_addr": "29777 Telegraph Rd, Suite # 1670",
  "perf_city_name": "Southfield",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "480347642",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "MI12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "150500",
   "pgm_ele_name": "STTR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8034",
   "pgm_ref_txt": "Hardware Components"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div>\n<p>Unexpected&nbsp;intraoperative&nbsp;bleeding&nbsp;has&nbsp;short and&nbsp;long-term&nbsp;consequences&nbsp;for patient outcomes.&nbsp;Instantaneous&nbsp;control of sudden bleeding is&nbsp;essential,&nbsp;and the time taken to control bleeding is&nbsp;dependent on surgeon&nbsp;experience.&nbsp;For minimally invasive surgeries, there is no tool&nbsp;to&nbsp;help&nbsp;surgeons&nbsp;localize&nbsp;the&nbsp;source of bleeding,&nbsp;characterize&nbsp;it,&nbsp;and control it in real&nbsp;time.&nbsp;The goal of this project&nbsp;was&nbsp;to create a&nbsp;versatile, cost-effective, real-time tool&nbsp;to&nbsp;localize&nbsp;and characterize sudden&nbsp;bleeding&nbsp;during surgery.&nbsp;Such assistance&nbsp;will improve surgical safety and allow surgeons&nbsp;to deal with emergent situations&nbsp;in a time sensitive manner,&nbsp;irrespective of their experience.&nbsp;In our discovery process,&nbsp;we&nbsp;interviewed forty&nbsp;customers including surgeons&nbsp;and remodeled&nbsp;our initial project&nbsp;incorporating their&nbsp;feedback&nbsp;to create a more proactive system.&nbsp;This system will&nbsp;help surgeons navigate through the&nbsp;undissected opaque renal hilum,&nbsp;pre-empt surgical bleeding from&nbsp;critical vessels&nbsp;with high&nbsp;bleed&nbsp;potential and&nbsp;avert or locate the exact source and&nbsp;control&nbsp;it in real time.&nbsp;To accommodate this change,&nbsp;we developed two systems: a computer vision algorithm to detect anatomical landmarks&nbsp;in&nbsp;the surgical scene&nbsp;and another&nbsp;system&nbsp;to detect and localize&nbsp;the source of bleeding&nbsp;and characterize its kind and the magnitude.&nbsp;&nbsp;</p>\n</div>\n<div>\n<p>Our&nbsp;current&nbsp;computer vision algorithm,&nbsp;developed on the surgical videos of laparoscopic live donor nephrectomy,&nbsp;can&nbsp;accurately detect&nbsp;renal vein, renal artery and spleen,&nbsp;critical&nbsp;landmarks&nbsp;for kidney&nbsp;surgery.&nbsp;The data processing&nbsp;pipeline was&nbsp;purpose-built&nbsp;to handle&nbsp;data collection,&nbsp;frame&nbsp;selection, remote&nbsp;annotation,&nbsp;and&nbsp;model training.&nbsp;Three qualified urological&nbsp;surgeons annotated&nbsp;images&nbsp;and worked closely with the technical team to develop consensus annotation processes. We&nbsp;developed methods to&nbsp;weed out&nbsp;incorrectly annotated images,&nbsp;significantly&nbsp;improving&nbsp;the performance of our models. In addition, techniques such as&nbsp;longer training&nbsp;cycles&nbsp;and&nbsp;data augmentation&nbsp;further improved the performance.&nbsp;The pipeline we developed is&nbsp;generic&nbsp;and can&nbsp;be&nbsp;applied&nbsp;for&nbsp;other surgical procedures.&nbsp;We measured&nbsp;model&nbsp;performance by using&nbsp;Intersection over Union (IoU), a&nbsp;statistical measure,&nbsp;to evaluate the overlap/similarity&nbsp;between annotation (or ground truth) and the prediction of the model.&nbsp;For renal artery, renal vein, and spleen, the mean&nbsp;IoUs&nbsp;of our two highest performing models&nbsp;are: .464, .708, .733 and .306, .712, .892 respectively.&nbsp;We found that even&nbsp;when the model&nbsp;had a&nbsp;relatively low&nbsp;IoU&nbsp;for a given image, it&nbsp;clearly outlined&nbsp;the location of veins and arteries and&nbsp;could&nbsp;provide&nbsp;correct context for a bleeding source.&nbsp;To strengthen the model further,&nbsp;we propose to&nbsp;add&nbsp;more&nbsp;anatomical objects of interest.&nbsp;Furthermore,&nbsp;the success of our model&nbsp;on these landmarks shows that it can be augmented with additional&nbsp;ground truth&nbsp;to detect other landmarks&nbsp;of interest.&nbsp;This&nbsp;effort&nbsp;and subsequent findings have been accepted for presentation at the&nbsp;Annual European Association of Urology (EAU) Congress&nbsp;2021&nbsp;and the American Urological Association (AUA) Annual Meeting&nbsp;2021&nbsp;and a publication&nbsp;is ready for submission.&nbsp;</p>\n</div>\n<div>\n<p>Part&nbsp;2&nbsp;of&nbsp;Phase&nbsp;I STTR research, the development of a&nbsp;responsive&nbsp;system to preempt surgical&nbsp;bleeding, was conducted in collaboration with Wayne State University.&nbsp;The&nbsp;bleed detection&nbsp;algorithm uses image processing techniques to recognize the homogenous texture of blood in surgical videos.&nbsp;It had two components 1. before bleeding&nbsp;occurs,&nbsp;the&nbsp;algorithm&nbsp;is able&nbsp;to&nbsp;sense the abrupt movement of the tools and alert the surgeon&nbsp;and 2. to identify the moment of bleeding and localize the source of bleeding&nbsp;through a filter design approach.&nbsp;By comparing&nbsp;frames, changes in the amount of blood can be detected. Each frame is&nbsp;processed&nbsp;by this algorithm&nbsp;during surgery to alert the surgeon if the change is above the expected threshold value, which can&nbsp;localize&nbsp;sources of bleeding before they become potential red-out conditions.&nbsp;This algorithm can, on average, detect bleeding within 0.635 seconds&nbsp;and locate&nbsp;the bleeding sources within 2.5% of discrepancy in pixels from their origins. By overlaying the pixel location of the bleed with the CV-detected location of veins and arteries, our tool will&nbsp;provide&nbsp;surgeons which vessel has been damaged&nbsp;with&nbsp;the context&nbsp;needed&nbsp;to&nbsp;operate safely and efficiently.&nbsp;</p>\n</div>\n<div>\n<p>Through our computer vision models,&nbsp;surgeons will be provided with the navigational&nbsp;support needed to&nbsp;accomplish surgical tasks&nbsp;without untoward events.&nbsp;Our proactive approach allows surgeons to identify veins and arteries&nbsp;that are&nbsp;potential source of bleeding.&nbsp;We are&nbsp;simultaneously&nbsp;working on developing a surgeon friendly user interface which&nbsp;could be through a live computer&nbsp;screen or as an&nbsp;augmented&nbsp;reality&nbsp;that&nbsp;can&nbsp;aid&nbsp;surgeons&nbsp;at critical stages of surgery.&nbsp;To&nbsp;further&nbsp;validate our surgical video&nbsp;models,&nbsp;we are also&nbsp;developing&nbsp;patient-specific 3D models from&nbsp;patient?s own CT scans.&nbsp;The models&nbsp;may then be used to automatically align 3D&nbsp;structures&nbsp;with the surgical video, allowing surgeons to visualize structures that are still covered by tissue.&nbsp;&nbsp;</p>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/02/2021<br>\n\t\t\t\t\tModified by: Madhu&nbsp;Reddiboina</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630251748061_Inference--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630251748061_Inference--rgov-800width.jpg\" title=\"Model Prediction\"><img src=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630251748061_Inference--rgov-66x44.jpg\" alt=\"Model Prediction\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Model predictions of spleen (green), renal artery (red), and renal vein (blue)</div>\n<div class=\"imageCredit\">RediMinds, Inc.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Madhu&nbsp;Reddiboina</div>\n<div class=\"imageTitle\">Model Prediction</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630251939777_ScreenShot2021-08-27at10.20.45AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630251939777_ScreenShot2021-08-27at10.20.45AM--rgov-800width.jpg\" title=\"Bleed Detection Algorithm\"><img src=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630251939777_ScreenShot2021-08-27at10.20.45AM--rgov-66x44.jpg\" alt=\"Bleed Detection Algorithm\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Output of algorithm for locating the bleeding after it is detected</div>\n<div class=\"imageCredit\">Wayne State University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Madhu&nbsp;Reddiboina</div>\n<div class=\"imageTitle\">Bleed Detection Algorithm</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630252980595_ArterialBleedingDetection--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630252980595_ArterialBleedingDetection--rgov-800width.jpg\" title=\"Arterial Bleed Detection\"><img src=\"/por/images/Reports/POR/2021/1953822/1953822_10672770_1630252980595_ArterialBleedingDetection--rgov-66x44.jpg\" alt=\"Arterial Bleed Detection\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Image processing approach to bleed detection</div>\n<div class=\"imageCredit\">Wayne State University</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Madhu&nbsp;Reddiboina</div>\n<div class=\"imageTitle\">Arterial Bleed Detection</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\nUnexpected intraoperative bleeding has short and long-term consequences for patient outcomes. Instantaneous control of sudden bleeding is essential, and the time taken to control bleeding is dependent on surgeon experience. For minimally invasive surgeries, there is no tool to help surgeons localize the source of bleeding, characterize it, and control it in real time. The goal of this project was to create a versatile, cost-effective, real-time tool to localize and characterize sudden bleeding during surgery. Such assistance will improve surgical safety and allow surgeons to deal with emergent situations in a time sensitive manner, irrespective of their experience. In our discovery process, we interviewed forty customers including surgeons and remodeled our initial project incorporating their feedback to create a more proactive system. This system will help surgeons navigate through the undissected opaque renal hilum, pre-empt surgical bleeding from critical vessels with high bleed potential and avert or locate the exact source and control it in real time. To accommodate this change, we developed two systems: a computer vision algorithm to detect anatomical landmarks in the surgical scene and another system to detect and localize the source of bleeding and characterize its kind and the magnitude.  \n\n\n\nOur current computer vision algorithm, developed on the surgical videos of laparoscopic live donor nephrectomy, can accurately detect renal vein, renal artery and spleen, critical landmarks for kidney surgery. The data processing pipeline was purpose-built to handle data collection, frame selection, remote annotation, and model training. Three qualified urological surgeons annotated images and worked closely with the technical team to develop consensus annotation processes. We developed methods to weed out incorrectly annotated images, significantly improving the performance of our models. In addition, techniques such as longer training cycles and data augmentation further improved the performance. The pipeline we developed is generic and can be applied for other surgical procedures. We measured model performance by using Intersection over Union (IoU), a statistical measure, to evaluate the overlap/similarity between annotation (or ground truth) and the prediction of the model. For renal artery, renal vein, and spleen, the mean IoUs of our two highest performing models are: .464, .708, .733 and .306, .712, .892 respectively. We found that even when the model had a relatively low IoU for a given image, it clearly outlined the location of veins and arteries and could provide correct context for a bleeding source. To strengthen the model further, we propose to add more anatomical objects of interest. Furthermore, the success of our model on these landmarks shows that it can be augmented with additional ground truth to detect other landmarks of interest. This effort and subsequent findings have been accepted for presentation at the Annual European Association of Urology (EAU) Congress 2021 and the American Urological Association (AUA) Annual Meeting 2021 and a publication is ready for submission. \n\n\n\nPart 2 of Phase I STTR research, the development of a responsive system to preempt surgical bleeding, was conducted in collaboration with Wayne State University. The bleed detection algorithm uses image processing techniques to recognize the homogenous texture of blood in surgical videos. It had two components 1. before bleeding occurs, the algorithm is able to sense the abrupt movement of the tools and alert the surgeon and 2. to identify the moment of bleeding and localize the source of bleeding through a filter design approach. By comparing frames, changes in the amount of blood can be detected. Each frame is processed by this algorithm during surgery to alert the surgeon if the change is above the expected threshold value, which can localize sources of bleeding before they become potential red-out conditions. This algorithm can, on average, detect bleeding within 0.635 seconds and locate the bleeding sources within 2.5% of discrepancy in pixels from their origins. By overlaying the pixel location of the bleed with the CV-detected location of veins and arteries, our tool will provide surgeons which vessel has been damaged with the context needed to operate safely and efficiently. \n\n\n\nThrough our computer vision models, surgeons will be provided with the navigational support needed to accomplish surgical tasks without untoward events. Our proactive approach allows surgeons to identify veins and arteries that are potential source of bleeding. We are simultaneously working on developing a surgeon friendly user interface which could be through a live computer screen or as an augmented reality that can aid surgeons at critical stages of surgery. To further validate our surgical video models, we are also developing patient-specific 3D models from patient?s own CT scans. The models may then be used to automatically align 3D structures with the surgical video, allowing surgeons to visualize structures that are still covered by tissue.  \n\n\n\t\t\t\t\tLast Modified: 09/02/2021\n\n\t\t\t\t\tSubmitted by: Madhu Reddiboina"
 }
}