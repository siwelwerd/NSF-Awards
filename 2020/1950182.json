{
 "awd_id": "1950182",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "High-Performance and CMOS-Compatible Electrochemical Random Access Memory For Neuromorphic Computing",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Usha Varshney",
 "awd_eff_date": "2020-03-15",
 "awd_exp_date": "2023-02-28",
 "tot_intn_awd_amt": 420000.0,
 "awd_amount": 420000.0,
 "awd_min_amd_letter_date": "2020-03-03",
 "awd_max_amd_letter_date": "2020-03-03",
 "awd_abstract_narration": "Artificial intelligence has made phenomenal progress in recent years.  It is having a remarkable social impact with emerging applications such as face recognition and self-driving cars.  However, such improvement comes with the cost of aggressively increased depth and size of the deep neural network models utilized, which leads to exponentially increasing computational load.  This poses significant challenges for hardware implementations in terms of computation, memory, and communication resources. The objective of this project is to develop the next-generation neuro-inspired deep-learning hardware, which has potential to perform the data-intensive computation required by the artificial-intelligence algorithms with thousands times higher energy efficiency, compared to what is possible using current silicon complementary metal-oxide-semiconductor technology. The educational goal is to sustain STEM workforce pipeline development by exploiting the outreach opportunities and knowledge generated in the proposed project. Efforts will be to establish hands-on module for K-6 students to learn the difference between computer-based expert system and the human/machine learning process, as well as the working principles of artificial synapses for neuromorphic computing, with the purpose of introducing engineering to them. At the undergraduate level, PI proposes to incorporate case-analysis in engineering class, by capitalizing on PI\u2019s industrial experiences. The target will be to help students develop the capability of using engineering judgement in decision-making regarding realistic technology development problems, which will have direct connection to what they learn in classroom.\r\n\r\nTo achieve this objective, new types of high-performance and silicon complementary metal-oxide-semiconductor compatible electrochemical random access memories will be designed, fabricated, characterized, and optimized.  These devices can serve as multi-level artificial synapses with near-symmetric weight update in response to pulsed input to dramatically accelerate the online training and the inference of deep neural networks.  More specifically, two novel device prototypes will be explored in parallel during the grant term: one operates based on the resistance switch in a functional oxide channel modulated by the gate-controlled reversible insertion of protons from oxides with high ionic conductivity; the other is based on the resistance switch in multilayered two-dimensional semiconductors modulated by the gate-controlled intercalation of copper ions from fast ion-transporting metal-chalcogenide glass.  A symmetric gate-channel stack will be adopted to minimize the drift of the device open-circuit potential during operation.  The scientific goal of this project is to elucidate the correlation between the intercalant types, properties of the corresponding solid-state electrolytes and the intercalatable channels, device dimensions, and the electrochemical random access memory performance, using a combination of experiment and physics-driven device modeling.  The technological goal is to move electrochemical random access memory from initial proof-of-concept demonstrations to a practical technology.  Material innovations will firstly be applied on all the components across the device gate-channel stack to drastically enhance their performance, especially the device speed, retention, and endurance.  Individual memory cells with sub-100 nm dimensions and 3 by 3 pseudo-crossbar arrays will then be demonstrated.  These efforts will help us assess the technological promise of electrochemical random access memory, especially their ultimately achievable speed and their scalability into both nanoscale devices and large-scale integrated arrays.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Qing",
   "pi_last_name": "Cao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Qing Cao",
   "pi_email_addr": "qingcao2@illinois.edu",
   "nsf_id": "000784565",
   "pi_start_date": "2020-03-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "Board of Trustees of the University of Illinois",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "151700",
   "pgm_ele_name": "EPMD-ElectrnPhoton&MagnDevices"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "107E",
   "pgm_ref_txt": "Magnetics and spin electronics"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 420000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The transformative changes brought by deep learning and artificial intelligence are accompanied by immense financial and environmental costs. Take OpenAI?s ChatGPT as example, on the current standard computing platforms, its training could cost over $4.6M. Its daily operation costs at least $100K and generates about 3.82 tons of CO<sub>2</sub> emission. This has inspired more specialized computer hardware to efficiently perform AI-specific calculations. The key component of AI hardware targeting at accelerating the training and execution of deep learning is a memory cell that combines several difficult-to-obtain properties: it can be reliably switched between a large number of analog resistance values for millions or billions of times at fast speed, can hold the data for sufficient long time, and more importantly, can be manufactured at small enough size and coupled with standard silicon transistors on the same chip.&nbsp;</p>\n<p>One highly promising candidate is electrochemical random-access memory, or ECRAM. It is basically a miniaturized battery where mobile ions are shuffled between two terminals through an electrolyte sandwiched in between for storing not only energy but also information.&nbsp; The number of ions in one terminal, called the channel of the ECRAM, determines its conductivity and thus the resistance between a pair of source and drain electrodes attached on opposite sides.&nbsp; And by applying electrical pulses on the other terminal above the channel, serving as the gate of the ECRAM, ions will be injected into the channel from the gate, or the reverse, to switch the channel resistance to a desired value. &nbsp;</p>\n<p>In this project sponsored by NSF, the research group of Qing Cao, a professor of materials science &amp; engineering and electrical engineering at the University of Illinois at Urbana-Champaign, has developed a completely inorganic ECRAM device. This device achieved the whole slew of properties needed for an ideal memory cell for deep learning accelerators.&nbsp; It adopted the same material for gate and channel. Such symmetric structure enables the reliable and symmetric analog modulation of the channel resistance with voltage pulses, and the resistivity can be held for hours with little degradation, which is sufficient for training most complicated neural networks.&nbsp; &nbsp;By employing the smallest ion, protons, as the information carrier, these devices are fast. They are capable of being programmed at close to megahertz rates, and they are durable, lasting for over 100 million read-write cycles.&nbsp; The energy consumption is below femto-joule per transaction, orders of magnitude more energy efficient compared to current commercial memory technologies.</p>\n<p>Most importantly, while other ECRAM devices draw inspirations from either neurological processes or lithium batteries and incorporate organic chemicals and/or lithium ions, the ECRAM prototypes developed in this project used inorganic materials that are fully compatible with silicon: tungsten oxide and zirconium oxide, with protons as the active ionic species. This allowed the fabrication of ECRAM devices directly on silicon transistors, demonstrating that this specialized hardware can integrate with and be controlled by standard microelectronics. Since it is compatible with standard microfabrication techniques developed for silicon chips, the footprint of these devices can be reduced down to ~100 nanometer scale, allowing high integration density and faster operations without degrading programming characteristics.</p>\n<p>In this project, the researchers fabricated arrays of ECRAMs on silicon microchips. They used such hybrid pseudo-crossbar arrays to perform matrix-vector multiplication, a mathematical operation crucial to deep learning, on a hardware level in experiment. This allowed matrix entries, or neural network weights, to be stored in-device. The array then efficiently performed the multiplication on the inputs, represented as applied voltages, by using the stored weights to change the resulting currents with high level of parallelism.&nbsp; The parallel update of the stored weights was also achieved. Compared to current silicon device design, the ECRAM-silicon hybrid accelerator can achieve comparable training accuracy for deep neural networks with significantly lower energy consumption and much smaller chip area cost. A prime application of this technology is for AI edge-computing applications sensitive to chip size and energy consumption.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/22/2023<br>\n\t\t\t\t\tModified by: Qing&nbsp;Cao</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1950182/1950182_10655965_1684772860806_Die_imagecopy--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1950182/1950182_10655965_1684772860806_Die_imagecopy--rgov-800width.jpg\" title=\"ECRAM array\"><img src=\"/por/images/Reports/POR/2023/1950182/1950182_10655965_1684772860806_Die_imagecopy--rgov-66x44.jpg\" alt=\"ECRAM array\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Optical Image of ECRAM-silicon transistor crossbar arrays as deep learning accelerators.</div>\n<div class=\"imageCredit\">Jinsong Cui</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Qing&nbsp;Cao</div>\n<div class=\"imageTitle\">ECRAM array</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe transformative changes brought by deep learning and artificial intelligence are accompanied by immense financial and environmental costs. Take OpenAI?s ChatGPT as example, on the current standard computing platforms, its training could cost over $4.6M. Its daily operation costs at least $100K and generates about 3.82 tons of CO2 emission. This has inspired more specialized computer hardware to efficiently perform AI-specific calculations. The key component of AI hardware targeting at accelerating the training and execution of deep learning is a memory cell that combines several difficult-to-obtain properties: it can be reliably switched between a large number of analog resistance values for millions or billions of times at fast speed, can hold the data for sufficient long time, and more importantly, can be manufactured at small enough size and coupled with standard silicon transistors on the same chip. \n\nOne highly promising candidate is electrochemical random-access memory, or ECRAM. It is basically a miniaturized battery where mobile ions are shuffled between two terminals through an electrolyte sandwiched in between for storing not only energy but also information.  The number of ions in one terminal, called the channel of the ECRAM, determines its conductivity and thus the resistance between a pair of source and drain electrodes attached on opposite sides.  And by applying electrical pulses on the other terminal above the channel, serving as the gate of the ECRAM, ions will be injected into the channel from the gate, or the reverse, to switch the channel resistance to a desired value.  \n\nIn this project sponsored by NSF, the research group of Qing Cao, a professor of materials science &amp; engineering and electrical engineering at the University of Illinois at Urbana-Champaign, has developed a completely inorganic ECRAM device. This device achieved the whole slew of properties needed for an ideal memory cell for deep learning accelerators.  It adopted the same material for gate and channel. Such symmetric structure enables the reliable and symmetric analog modulation of the channel resistance with voltage pulses, and the resistivity can be held for hours with little degradation, which is sufficient for training most complicated neural networks.   By employing the smallest ion, protons, as the information carrier, these devices are fast. They are capable of being programmed at close to megahertz rates, and they are durable, lasting for over 100 million read-write cycles.  The energy consumption is below femto-joule per transaction, orders of magnitude more energy efficient compared to current commercial memory technologies.\n\nMost importantly, while other ECRAM devices draw inspirations from either neurological processes or lithium batteries and incorporate organic chemicals and/or lithium ions, the ECRAM prototypes developed in this project used inorganic materials that are fully compatible with silicon: tungsten oxide and zirconium oxide, with protons as the active ionic species. This allowed the fabrication of ECRAM devices directly on silicon transistors, demonstrating that this specialized hardware can integrate with and be controlled by standard microelectronics. Since it is compatible with standard microfabrication techniques developed for silicon chips, the footprint of these devices can be reduced down to ~100 nanometer scale, allowing high integration density and faster operations without degrading programming characteristics.\n\nIn this project, the researchers fabricated arrays of ECRAMs on silicon microchips. They used such hybrid pseudo-crossbar arrays to perform matrix-vector multiplication, a mathematical operation crucial to deep learning, on a hardware level in experiment. This allowed matrix entries, or neural network weights, to be stored in-device. The array then efficiently performed the multiplication on the inputs, represented as applied voltages, by using the stored weights to change the resulting currents with high level of parallelism.  The parallel update of the stored weights was also achieved. Compared to current silicon device design, the ECRAM-silicon hybrid accelerator can achieve comparable training accuracy for deep neural networks with significantly lower energy consumption and much smaller chip area cost. A prime application of this technology is for AI edge-computing applications sensitive to chip size and energy consumption.\n\n \n\n\t\t\t\t\tLast Modified: 05/22/2023\n\n\t\t\t\t\tSubmitted by: Qing Cao"
 }
}