{
 "awd_id": "2016608",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CCRI: ENS: Collaborative Research: Open Computer System Usage Repository and Analytics Engine",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 499920.0,
 "awd_amount": 499920.0,
 "awd_min_amd_letter_date": "2020-08-15",
 "awd_max_amd_letter_date": "2022-07-19",
 "awd_abstract_narration": "In science and engineering research, large-scale, centrally managed computing clusters or \u201csupercomputers\u201d have been instrumental in enabling the kinds of resource-intensive simulations, analyses, and visualizations that have been used in computer-aided drug discovery, high strength materials design for cars and jet engines, and disease vector analysis to name a few. Such clusters are complex systems comprised of several hundred to thousand computer servers with fast network connections between them, various data storage resources, and highly optimized scientific software being shared with several hundred other researchers from diverse domains. Consequently, the overall dependability of such systems relies on the dependability of these individual highly interconnected elements as well as the characteristics of cascading failures. While computer systems researchers and practitioners have been at the forefront of designing and deploying dependable computing cluster systems, this task has been hampered by the lack of publicly available, real-world failure data from supercomputers currently in operation. Prior practice has largely involved tedious, manual collection and curation of small sets of data for use in specific analyses. This project will establish seamless, automated pipelines for acquiring, processing, and curating continuous, detailed system usage, monitoring, and failure data from large computing clusters at two organizations, Purdue University and the University of Texas at Austin. This data will be disseminated through a publicly accessible portal and complemented by a suite of in-situ analytics capabilities that will support and spur research in dependable computing systems. The data acquisition pipeline and analytics software will be made open-source and designed for ease of federation, extension, and adoption to cluster systems operated by other organizations.\r\n\r\nCluster computing systems are a key resource in time-sensitive, computationally intensive research such as virus structure modeling and drug discovery and have been at the forefront of efforts to tackle global pandemics. Both unanticipated system down-times and lack of actionable feedback to researchers on computational failures can have adverse effects on research timeliness and efficiency. This project will allow the practitioners and administrators of these systems to develop data-backed best practices for ensuring high availability and utilization for their clusters. The resulting large, public data repository consisting of data from clusters with diverse workloads spanning traditional high-performance computing, modern accelerator-based computing (for example on graphics processing units (GPUs)), and cloud-style applications will allow the systems research community to consider forward-looking research questions based on real system data. The project will train a cadre of students in data analysis on live production systems and this will provide them with a unique learning experience, interfacing with a variety of stakeholders.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Evans",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Richard T Evans",
   "pi_email_addr": "evans.richardtodd@gmail.com",
   "nsf_id": "000721178",
   "pi_start_date": "2020-08-15",
   "pi_end_date": "2022-07-19"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Harrell",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen L Harrell",
   "pi_email_addr": "sharrell@tacc.utexas.edu",
   "nsf_id": "000798073",
   "pi_start_date": "2022-07-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Harrell",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen L Harrell",
   "pi_email_addr": "sharrell@tacc.utexas.edu",
   "nsf_id": "000798073",
   "pi_start_date": "2021-02-25",
   "pi_end_date": "2022-07-19"
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "3925 W Braker Lane, Suite 3340",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787595316",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 499920.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><ol id=\"docs-internal-guid-33ade26f-7fff-9f62-986c-9c27a04a6a0b\">\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Job performance data collection</span></p>\r\n</li>\r\n</ol>\r\n<p dir=\"ltr\"><span>A streamlined process was employed to periodically collect and process job accounting and performance data from the NSF-funded Anvil HPC system. This process leverages the popular Open XDMoD open source tool that is used by various HPC resource providers to collect and process job scheduler and host-level resource utilization data at their sites. Our streamlined process extends the Python wrapper developed by the XDMoD team to fetch this data from the XDMoD server and transform it into flat csv files that can be easily ingested into a database. As part of our process, we first anonymize the job accounting and performance data to remove identifiable fields such as username, allocation, node name, and job name before ingestion into a TimescaleDB database that enables not just scalability, but also efficient query of time series job performance data. This process was utilized to produce a dataset of anonymized job accounting and performance data for nearly 1.4 million jobs from the Anvil HPC system. This data includes both CPU and GPU jobs (batch and interactive) executed on Anvil from July 2022 - May 2023.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>In this project year, we deployed a scalable database for hosting this job accounting and performance data on the NSF Jetstream2 cloud. Jetstream2 was chosen for long-term sustainability and extensibility due to the ability to scale up the compute resources in Jetstream2&rsquo;s OpenStack cloud. A Kubernetes cluster was set up on Jetstream2 and TimescaleDB with two replicas was deployed in this Kubernetes cluster. The database now contains both job accounting data and time series of host level resource utilization data for each of these 1.4 million jobs.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>The team also pulled performance data from multiple TACC clusters, including Stampede2, Stampede3 and Frontera.</span></p>\r\n<p dir=\"ltr\"><span>We also held preliminary discussions with other NSF HPC resource providers (e.g., the San Diego Supercomputer Center) to identify a process for integrating their data into our database. Our streamlined process can be employed in the future to fetch and process data from any HPC site that utilizes XDMoD for managing their job accounting and performance data.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<ol>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Analytics infrastructure</span></p>\r\n</li>\r\n</ol>\r\n<p dir=\"ltr\"><span>A public JupyterHub gateway has been deployed alongside the TimescaleDB database in the same Kubernetes cluster. This gateway (</span><a href=\"https://jupyter.fresco-hpc.org/\"><span>https://jupyter.fresco-hpc.org</span></a><span>) enables users to login with a Google account and launch a Jupyter notebook environment where they can query for data from the TimescaleDB database and analyze it. In order to simplify data query and analysis for lay users, a curated Jupyter notebook has also been developed which provides user-friendly widgets (Figure 1) for querying the database tables and computing simple statistics.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>The database has also been integrated with a public website for this project (</span><a href=\"https://www.frescodata.xyz/simple-repository-search/\"><span>https://www.frescodata.xyz/simple-repository-search/</span></a><span>) that enables a simple database search across the two data tables (Figure 2). This website uses an API container that has been deployed in the same Kubernetes cluster to relay queries from the website to the database and retrieve results, while implementing rate limiting and token based authorization.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span><span><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQLTE0N5EuY67XxoX5uS2wLuEPsM4xPHJrHiu6fMJ8jdzmSwn5G6OC8ti_nSAo4NKpFH5ghQ64soFDrjWMYPGWqMxyEbdEIgtrpx_6tD4oeFPfpK9n7uSxpDE6Ecw_mPyTKEkFuzp46l-sTkEWjhr-9oxXhpmubAGHDLwwGMLk45nUyYJWSNI?key=6knATkqItYmsOWsouIRslg\" alt=\"\" width=\"592\" height=\"436\" /></span></span></p>\r\n<p dir=\"ltr\"><span>Figure 1: Screenshot of the Jupyter notebook interactive interface for querying data from the database</span></p>\r\n<p><br /><span><img src=\"https://lh7-rt.googleusercontent.com/docsz/AD_4nXemWXzmrIMh2iOux7XgnCkM0Zjiuf4h5QBSSmGSglx4LpmK7UtOxulX8t2Jpn28rgxCWciz1l1fxAMGYWSmVj8gIZrWsR2JUBAhxA892vSFHXxeLquBBwLcfTk7XcGjK4zRX2tRFrar2lW_DLwj9jhHSRPwIN35-90_QcHdwXclQHHj95BGKGg?key=6knATkqItYmsOWsouIRslg\" alt=\"\" width=\"624\" height=\"267\" /></span></p>\r\n<p dir=\"ltr\"><span>Figure 2: Screenshot of the public FRESCO data repository website and the simple database search forms</span></p>\r\n<p>&nbsp;</p>\r\n<ol>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Database Interface</span></p>\r\n</li>\r\n</ol> \r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>A scalable database of anonymized job accounting and performance data for nearly 1.4 million jobs from the NSF Anvil system has been deployed. This database contains data for both CPU and GPU jobs and includes a wide range of job types supported by Anvil including batch and interactive Open OnDemand-based applications such as Jupyter notebooks, Matlab, R Studio, and bioinformatics workflows.&nbsp;</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Our streamlined process leverages the Open XDMoD tool which is widely used by HPC resource providers and can be used to easily integrate data from other systems.&nbsp;&nbsp;</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Stampede 2 has reached End of Life and has been decommissioned, so we have pivoted to pulling data from Frontera, which provides a larger data set. Log ingestion has been completed. This year we will correlate the data with job logs and performance data to create a coherent data set.</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Researchers can now access the deployed database via several interfaces including a JupyterHub gateway and a public website.</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>Our analytics work created a usable Analytics Toolbench and helped answer several intuitive queries like the Mean Time to Failure of one or a set of machines, correlation between load and performance degradation.&nbsp;</span></p>\r\n</li>\r\n</ul>\r\n<p><br /><br /></p><br>\n<p>\n Last Modified: 03/04/2025<br>\nModified by: Stephen&nbsp;Lien&nbsp;Harrell</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\r\n\r\n\n\nJob performance data collection\r\n\r\n\r\n\n\nA streamlined process was employed to periodically collect and process job accounting and performance data from the NSF-funded Anvil HPC system. This process leverages the popular Open XDMoD open source tool that is used by various HPC resource providers to collect and process job scheduler and host-level resource utilization data at their sites. Our streamlined process extends the Python wrapper developed by the XDMoD team to fetch this data from the XDMoD server and transform it into flat csv files that can be easily ingested into a database. As part of our process, we first anonymize the job accounting and performance data to remove identifiable fields such as username, allocation, node name, and job name before ingestion into a TimescaleDB database that enables not just scalability, but also efficient query of time series job performance data. This process was utilized to produce a dataset of anonymized job accounting and performance data for nearly 1.4 million jobs from the Anvil HPC system. This data includes both CPU and GPU jobs (batch and interactive) executed on Anvil from July 2022 - May 2023.\r\n\n\nIn this project year, we deployed a scalable database for hosting this job accounting and performance data on the NSF Jetstream2 cloud. Jetstream2 was chosen for long-term sustainability and extensibility due to the ability to scale up the compute resources in Jetstream2s OpenStack cloud. A Kubernetes cluster was set up on Jetstream2 and TimescaleDB with two replicas was deployed in this Kubernetes cluster. The database now contains both job accounting data and time series of host level resource utilization data for each of these 1.4 million jobs.\r\n\n\nThe team also pulled performance data from multiple TACC clusters, including Stampede2, Stampede3 and Frontera.\r\n\n\nWe also held preliminary discussions with other NSF HPC resource providers (e.g., the San Diego Supercomputer Center) to identify a process for integrating their data into our database. Our streamlined process can be employed in the future to fetch and process data from any HPC site that utilizes XDMoD for managing their job accounting and performance data.\r\n\n\n\r\n\r\n\r\n\n\nAnalytics infrastructure\r\n\r\n\r\n\n\nA public JupyterHub gateway has been deployed alongside the TimescaleDB database in the same Kubernetes cluster. This gateway (https://jupyter.fresco-hpc.org) enables users to login with a Google account and launch a Jupyter notebook environment where they can query for data from the TimescaleDB database and analyze it. In order to simplify data query and analysis for lay users, a curated Jupyter notebook has also been developed which provides user-friendly widgets (Figure 1) for querying the database tables and computing simple statistics.\r\n\n\nThe database has also been integrated with a public website for this project (https://www.frescodata.xyz/simple-repository-search/) that enables a simple database search across the two data tables (Figure 2). This website uses an API container that has been deployed in the same Kubernetes cluster to relay queries from the website to the database and retrieve results, while implementing rate limiting and token based authorization.\r\n\n\n\r\n\n\n\r\n\n\nFigure 1: Screenshot of the Jupyter notebook interactive interface for querying data from the database\r\n\n\n\n\r\n\n\nFigure 2: Screenshot of the public FRESCO data repository website and the simple database search forms\r\n\n\n\r\n\r\n\r\n\n\nDatabase Interface\r\n\r\n \r\n\r\n\r\n\n\nA scalable database of anonymized job accounting and performance data for nearly 1.4 million jobs from the NSF Anvil system has been deployed. This database contains data for both CPU and GPU jobs and includes a wide range of job types supported by Anvil including batch and interactive Open OnDemand-based applications such as Jupyter notebooks, Matlab, R Studio, and bioinformatics workflows.\r\n\r\n\r\n\n\nOur streamlined process leverages the Open XDMoD tool which is widely used by HPC resource providers and can be used to easily integrate data from other systems.\r\n\r\n\r\n\n\nStampede 2 has reached End of Life and has been decommissioned, so we have pivoted to pulling data from Frontera, which provides a larger data set. Log ingestion has been completed. This year we will correlate the data with job logs and performance data to create a coherent data set.\r\n\r\n\r\n\n\nResearchers can now access the deployed database via several interfaces including a JupyterHub gateway and a public website.\r\n\r\n\r\n\n\nOur analytics work created a usable Analytics Toolbench and helped answer several intuitive queries like the Mean Time to Failure of one or a set of machines, correlation between load and performance degradation.\r\n\r\n\r\n\n\n\n\n\t\t\t\t\tLast Modified: 03/04/2025\n\n\t\t\t\t\tSubmitted by: StephenLienHarrell\n"
 }
}