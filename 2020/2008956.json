{
 "awd_id": "2008956",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: RI: Small: Post hoc Explanations in the Wild: Exposing Vulnerabilities and Ensuring Robustness",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2020-09-01",
 "awd_max_amd_letter_date": "2020-10-19",
 "awd_abstract_narration": "The successful adoption of machine learning (ML) models in critical domains such as healthcare and criminal justice relies heavily on how well decision makers are able to understand and trust the functionality of these models. However, the proprietary nature and increasing complexity of ML models makes it challenging for domain experts to understand these complex \"black boxes\". Consequently, there has been a recent surge in techniques that explain black box models in a human interpretable manner by approximating them using simpler models. However, it is unclear to what extent these post hoc explanation techniques may mislead end users by giving them a false sense of security, and luring them into trusting and deploying untrustworthy black boxes. This project will build rigorous frameworks to expose the vulnerabilities of existing explanation techniques, assess how these vulnerabilities can manifest in real world applications, and develop new techniques to defend against these vulnerabilities. This project has the potential to significantly speed up the adoption of ML in a variety of domains including criminal justice (e.g., bail decisions), health care (e.g., patient diagnosis and treatment), and financial lending (e.g., loan approval).\r\n\r\nThe goal of this project is to characterize the vulnerabilities of existing explanation techniques, understand how adversaries can exploit these vulnerabilities, and develop techniques to defend against them. The project will focus on the following subtasks: 1) understanding the real-world consequences of misleading explanations by conducting user studies and detailed interviews with domain experts in healthcare and criminal justice 2) identifying critical vulnerabilities in state-of-the-art explanation techniques that  can be exploited by adversarial entities to generate misleading explanations, and 3) developing novel techniques for building robust and reliable explanations that are not prone to these vulnerabilities and thereby provide domain experts and other stakeholders with faithful explanations of complex black box models. With these contributions, the project will initiate a new body of research in ML interpretability that focuses on understanding how adversaries can manipulate explanation techniques, and how to defend against such attacks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sameer",
   "pi_last_name": "Singh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sameer Singh",
   "pi_email_addr": "sameer@uci.edu",
   "nsf_id": "000727594",
   "pi_start_date": "2020-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "4204 Bren Hall",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926973425",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-3cf0890b-7fff-c6d2-5abe-78cd00d9047c\"> </span></p>\n<p dir=\"ltr\"><span>Understanding and mitigating the vulnerabilities in machine learning explanation techniques is critical for ensuring the reliability and fairness of AI systems used in sensitive domains like healthcare and criminal justice. This project aimed to address these challenges through three goals: (1) understanding the real-world consequences of misleading explanations, (2) identifying critical vulnerabilities in state-of-the-art explanation techniques, and (3) developing novel techniques to build robust and reliable explanations. Achieving these goals is essential to prevent adversaries from exploiting these vulnerabilities and to provide stakeholders with trustworthy explanations of complex models.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The first goal was to understand the real-world consequences of misleading explanations. To achieve this, we conducted extensive user studies and interviews with domain experts in healthcare and criminal justice. These efforts revealed how misleading explanations could adversely impact decision-making processes in these fields. We developed and evaluated interactive dialogue systems like TalkToModel, facilitating dynamic interactions between users and machine learning models through natural language conversations. This approach proved effective, with most healthcare professionals and machine learning experts preferring this method over static explanations. The system's usability and effectiveness in providing reliable explanations were validated in real-world settings, confirming its potential to improve decision-making in critical domains.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The second goal focused on identifying critical vulnerabilities in state-of-the-art explanation techniques. Our research exposed significant weaknesses in popular methods such as LIME, SHAP, and counterfactual explanations. For instance, we demonstrated that adversarial entities could manipulate counterfactual explanations to produce biased or discriminatory outcomes. Additionally, more straightforward explanation techniques could sometimes perform as well as more complex ones, challenging the assumption that complexity correlates with reliability. These findings highlight the need for more robust and defensible explanation methods to safeguard against adversarial manipulation.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The third goal was to develop novel techniques for building robust and reliable explanations. We introduced frameworks like BayesLIME and BayesSHAP, which address the limitations of existing methods by providing uncertainty guarantees on the resulting explanations. Moreover, we created the AMPLIFY framework, which enhances model performance by integrating post hoc explanations to generate corrective signals for language models. This framework significantly improved prediction accuracy across various tasks, demonstrating the potential of post hoc explanations to not only provide insights but also enhance model reliability. Our research also identified key pretraining data subsets contributing to model robustness, offering valuable insights into developing more reliable and fair AI systems.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Altogether, the outcomes of this project provide several benchmarks, models, and insights into making AI systems more transparent, trustworthy, and fair in real-world applications.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 07/24/2024<br>\nModified by: Sameer&nbsp;Singh</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2008956/2008956_10703761_1721806759710_talktomodel--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2008956/2008956_10703761_1721806759710_talktomodel--rgov-800width.png\" title=\"TalkToModel\"><img src=\"/por/images/Reports/POR/2024/2008956/2008956_10703761_1721806759710_talktomodel--rgov-66x44.png\" alt=\"TalkToModel\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Instead of writing code, users have conversations with TalkToModel as follows. (1) Users supply natural language inputs. (2) The dialogue engine parses the input into an executable representation. (3) The execution engine runs the operations and the dialogue engine uses the results in its response.</div>\n<div class=\"imageCredit\">From: Explaining machine learning models with interactive natural language conversations using TalkToModel</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sameer&nbsp;Singh\n<div class=\"imageTitle\">TalkToModel</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nUnderstanding and mitigating the vulnerabilities in machine learning explanation techniques is critical for ensuring the reliability and fairness of AI systems used in sensitive domains like healthcare and criminal justice. This project aimed to address these challenges through three goals: (1) understanding the real-world consequences of misleading explanations, (2) identifying critical vulnerabilities in state-of-the-art explanation techniques, and (3) developing novel techniques to build robust and reliable explanations. Achieving these goals is essential to prevent adversaries from exploiting these vulnerabilities and to provide stakeholders with trustworthy explanations of complex models.\n\n\n\n\n\nThe first goal was to understand the real-world consequences of misleading explanations. To achieve this, we conducted extensive user studies and interviews with domain experts in healthcare and criminal justice. These efforts revealed how misleading explanations could adversely impact decision-making processes in these fields. We developed and evaluated interactive dialogue systems like TalkToModel, facilitating dynamic interactions between users and machine learning models through natural language conversations. This approach proved effective, with most healthcare professionals and machine learning experts preferring this method over static explanations. The system's usability and effectiveness in providing reliable explanations were validated in real-world settings, confirming its potential to improve decision-making in critical domains.\n\n\n\n\n\nThe second goal focused on identifying critical vulnerabilities in state-of-the-art explanation techniques. Our research exposed significant weaknesses in popular methods such as LIME, SHAP, and counterfactual explanations. For instance, we demonstrated that adversarial entities could manipulate counterfactual explanations to produce biased or discriminatory outcomes. Additionally, more straightforward explanation techniques could sometimes perform as well as more complex ones, challenging the assumption that complexity correlates with reliability. These findings highlight the need for more robust and defensible explanation methods to safeguard against adversarial manipulation.\n\n\n\n\n\nThe third goal was to develop novel techniques for building robust and reliable explanations. We introduced frameworks like BayesLIME and BayesSHAP, which address the limitations of existing methods by providing uncertainty guarantees on the resulting explanations. Moreover, we created the AMPLIFY framework, which enhances model performance by integrating post hoc explanations to generate corrective signals for language models. This framework significantly improved prediction accuracy across various tasks, demonstrating the potential of post hoc explanations to not only provide insights but also enhance model reliability. Our research also identified key pretraining data subsets contributing to model robustness, offering valuable insights into developing more reliable and fair AI systems.\n\n\n\n\n\nAltogether, the outcomes of this project provide several benchmarks, models, and insights into making AI systems more transparent, trustworthy, and fair in real-world applications.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 07/24/2024\n\n\t\t\t\t\tSubmitted by: SameerSingh\n"
 }
}