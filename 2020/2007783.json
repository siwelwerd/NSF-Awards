{
 "awd_id": "2007783",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small:  Reinforcement Learning with  Function Approximation: Convergent Algorithms and Finite-sample Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032928910",
 "po_email": "jafowler@nsf.gov",
 "po_sign_block_name": "James Fowler",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 330000.0,
 "awd_amount": 330000.0,
 "awd_min_amd_letter_date": "2020-06-24",
 "awd_max_amd_letter_date": "2020-06-24",
 "awd_abstract_narration": "The recent success of a machine-learning technique called reinforcement learning in benchmark tasks suggests a potential revolutionary advance in practical applications, and has dramatically boosted the interest in this technique. However, common algorithms that use this approach are highly data-inefficient, leading to impressive results only on simulated systems, where an infinite amount of data can be simulated. For example, for online tasks that most humans pick up within a few minutes, reinforcement learning algorithms take much longer to reach human-level performance. A good reinforcement learning algorithm called \"Rainbow deep Q-network\" needs about 18 million frames of simulation data to beat human in performance for the simplest of online tasks. This amount of data corresponds to about 80 person-hours of online experience. This level of data requirements limits the application of reinforcement learning algorithms in many practical applications that only have a limited amount of data. Theoretical understanding of how much data is needed for effective reinforcement learning is still very limited. This project aims to reduce the data requirements to train reinforcement learning algorithms by developing a comprehensive methodology for reinforcement learning algorithm design and analyzing convergence rates, which will in turn motivate design of fast and stable reinforcement learning algorithms. This project will have a direct impact on various engineering and science applications, e.g., the financial market, business strategy planning, industrial automation and online advertising.\r\n\r\nThis project will take a fresh perspective of using tools and concepts from both optimization and reinforcement learning. The following thrusts will be investigated in an increasing order of difficulty. 1) Linear function approximation: tools and insights will be developed to tackle challenges of non-smoothness and non-convexity in control problems. 2) General function approximation: new challenge of non-linearity will be addressed. 3) Neural function approximation: convergence to globally and/or universally optimal solutions will be investigated. In each of the three thrusts, new algorithms will be designed, and their convergence rates will be characterized. These results will be further used as guideline for parameter tuning, and to motivate design of fast and convergent algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shaofeng",
   "pi_last_name": "Zou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shaofeng Zou",
   "pi_email_addr": "shaofeng.zou@gmail.com",
   "nsf_id": "000780743",
   "pi_start_date": "2020-06-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Buffalo",
  "inst_street_address": "520 LEE ENTRANCE STE 211",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7166452634",
  "inst_zip_code": "142282577",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "NY26",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "GMZUKXFDJMA9",
  "org_uei_num": "LMCJKRFW5R81"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Buffalo",
  "perf_str_addr": "230 Davis Hall, University at Bu",
  "perf_city_name": "Buffalo",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "142602500",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "NY26",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 330000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focuses on control algorithms with function approximation in reinforcement learning. We take a fresh perspective of using tools and concepts from both optimization and reinforcement learning and investigate the following three thrusts in an increasing order of difficulty. 1) We focus on linear function approximation and develop tools and insights to tackle challenges of non-smoothness and non-convexity due to the max operator in control problems. 2) We focus on general function approximation and solve the new challenge of non-linearity. 3) We focus on the most interesting scenario with neural function approximation and characterize convergence to globally and/or universally optimal solutions by exploiting properties of over-parameterized neural network, e.g., great representation power and nice landscape property.&nbsp;The following are the major research findings from the project.</p>\r\n<p>1. We investigated the sample complexity of several&nbsp;fundamental reinforcement learning algorithms with linear function approximation including two time-scale value-based methods, e.g., TDC, greedy-gq, actor-critic methods with compatible function approximation, variance-reduction methods applied to reinforcement learning algorithms. We developed tight sample complexity bounds for these algorithms, and established fundamental understanding of how many samples are needed to train a good reinforcement learning policy.</p>\r\n<p>2. We investigated the sample complexity of reinforcement learning algorithms with general function approximation, e.g., a neural network. We also investigated whether we could use a linear function approximation and can avoid the function approximation error. Specifically, we analyzed the sample complexity of two timescale TDC algorithm with general function approximation, and actor-critic algorithms with compatible function approximation.</p>\r\n<p>3. We investigated the sample complexity of reinforcement learning problems with&nbsp;various challenges, e.g., multi-agent setting, problems with model uncertainty, decentralized setting, problems with safety/resource constraints, and Markov games. Our results in these settings highlight to address additional challenges, how many additional samples are needed.&nbsp;</p>\r\n<p>The results and findings of this project have been disseminated through numerous journal and conference publications as well as conference presentations in research communities of machine learning. The PI also delivered several tutorials on research outcomes from this project at ISIT, ICASSP, IEEE BigData. The PI also organized several special sessions on fundamental theory of reinforcement learning to disseminate the research outcomes from this project, and to foster potential collaborations.&nbsp;</p>\r\n<p>The students working on the project are trained with new mathematical techniques on the topics of reinforcement learning, Markov decision process, distributionally robust optimization and machine learning. They also had opportunities to collaborate with other researchers from other discipline, e.g., power systems and wireless communications, and to present the research results at various conferences.&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/23/2024<br>\nModified by: Shaofeng&nbsp;Zou</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project focuses on control algorithms with function approximation in reinforcement learning. We take a fresh perspective of using tools and concepts from both optimization and reinforcement learning and investigate the following three thrusts in an increasing order of difficulty. 1) We focus on linear function approximation and develop tools and insights to tackle challenges of non-smoothness and non-convexity due to the max operator in control problems. 2) We focus on general function approximation and solve the new challenge of non-linearity. 3) We focus on the most interesting scenario with neural function approximation and characterize convergence to globally and/or universally optimal solutions by exploiting properties of over-parameterized neural network, e.g., great representation power and nice landscape property.The following are the major research findings from the project.\r\n\n\n1. We investigated the sample complexity of severalfundamental reinforcement learning algorithms with linear function approximation including two time-scale value-based methods, e.g., TDC, greedy-gq, actor-critic methods with compatible function approximation, variance-reduction methods applied to reinforcement learning algorithms. We developed tight sample complexity bounds for these algorithms, and established fundamental understanding of how many samples are needed to train a good reinforcement learning policy.\r\n\n\n2. We investigated the sample complexity of reinforcement learning algorithms with general function approximation, e.g., a neural network. We also investigated whether we could use a linear function approximation and can avoid the function approximation error. Specifically, we analyzed the sample complexity of two timescale TDC algorithm with general function approximation, and actor-critic algorithms with compatible function approximation.\r\n\n\n3. We investigated the sample complexity of reinforcement learning problems withvarious challenges, e.g., multi-agent setting, problems with model uncertainty, decentralized setting, problems with safety/resource constraints, and Markov games. Our results in these settings highlight to address additional challenges, how many additional samples are needed.\r\n\n\nThe results and findings of this project have been disseminated through numerous journal and conference publications as well as conference presentations in research communities of machine learning. The PI also delivered several tutorials on research outcomes from this project at ISIT, ICASSP, IEEE BigData. The PI also organized several special sessions on fundamental theory of reinforcement learning to disseminate the research outcomes from this project, and to foster potential collaborations.\r\n\n\nThe students working on the project are trained with new mathematical techniques on the topics of reinforcement learning, Markov decision process, distributionally robust optimization and machine learning. They also had opportunities to collaborate with other researchers from other discipline, e.g., power systems and wireless communications, and to present the research results at various conferences.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 12/23/2024\n\n\t\t\t\t\tSubmitted by: ShaofengZou\n"
 }
}