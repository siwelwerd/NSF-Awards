{
 "awd_id": "2008173",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Learning to Optimize: Designing and Improving Optimizers by Machine Learning Algorithms",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2020-08-25",
 "awd_max_amd_letter_date": "2020-10-19",
 "awd_abstract_narration": "The goal of optimization is to find the best parameters to minimize an objective function. Existing optimizers are typically designed by humans and are often not good enough when facing more complex problems. For example, when training deep neural networks at scale, existing optimizers require a lot of tuning and may not find a good solution. It is hard for humans to design a perfect optimizer, but can a machine automatically design an optimizer based on the experiences on solving many different problems? To answer this question, the project investigates how to use machine learning to automatically design optimizers, and how to improve existing optimizers by machine learning. This new family of optimizers will be broadly applicable across the whole of data science. The developed algorithms and evaluation platforms will be made available to stimulate future work in this new research area. The project supports education and diversity through the recruitment of a diverse team, and incorporation of research results into courses at UCLA. \r\n\r\nThe goal of this project is to use Machine Learning (ML) to improve and automate existing optimization algorithms. In particular, the project focuses on two families of approaches: ML-learned optimizers and ML-assisted optimizers. For ML-learned optimizers, the update rule is modeled as a neural network with parameters learned from experience, and a series of studies are conducted to ensure the effectiveness and soundness of the designs. For ML-assisted optimizers, machine learning algorithms are developed to improve existing optimizers in terms of batch selection, learning rate scheduling, and automatic hyper-parameter tuning. A unified and comprehensive evaluation framework is developed to evaluate existing and newly developed optimizers by benchmarking their scalability, efficiency, robustness, and the performance under various computation budgets.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cho-Jui",
   "pi_last_name": "Hsieh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Cho-Jui Hsieh",
   "pi_email_addr": "chohsieh@cs.ucla.edu",
   "nsf_id": "000711637",
   "pi_start_date": "2020-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA Computer Science Dept.",
  "perf_str_addr": "404 Westwood Blvd., Rm. 284 E6",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-70d9c173-7fff-980d-d59b-1ed273a33ef9\"><span>Optimization algorithms are designed to find the most optimal parameters that minimize an objective function. However, current optimizers, typically crafted by humans, often fall short when tackling complex problems. For instance, when scaling up deep neural network training, these optimizers require extensive tuning and may still fail to yield satisfactory results. Moreover, a general-purpose optimizer may not work well for every specific optimization task, and it is impossible for human to design a dedicated optimizer for every specific problem. To tackle these hurdles, this project pioneers an optimizer search framework capable of autonomously discovering optimizers that surpass human-designed ones. To achieve this goal, we have formulated diverse search spaces for optimizer exploration and devised a suite of efficient algorithms for optimizer discovery. Our findings showcase that the optimizer search framework excels in identifying task-specific optimizers across a spectrum of tasks, outperforming established human-designed counterparts. In particular, our search has discovered a stochastic gradient descent variant that outshines human-designed optimizers in large-scale neural network training, leading to </span><span>&nbsp;faster training and reduced memory usage. </span><span>Beyond optimizer search, we demonstrate the potential of using AI to improve existing optimizers, including real-time hyper-parameter tuning and data subset selection to enhance training efficiency. Several optimizers designed by this endeavor have found widespread application. The outcomes of this funded project have been disseminated through publications in various venues such as ICML, NeurIPS, ICLR, CVPR, and ECCV, </span><span>which are leading conferences in machine learning. </span><span>&nbsp;Furthermore, the principal investigator has developed several graduate-level machine learning courses at UCLA, fostering the academic growth of several Ph.D. students supported by this project's funding.</span></span></p><br>\n<p>\n Last Modified: 04/14/2024<br>\nModified by: Cho-Jui&nbsp;Hsieh</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nOptimization algorithms are designed to find the most optimal parameters that minimize an objective function. However, current optimizers, typically crafted by humans, often fall short when tackling complex problems. For instance, when scaling up deep neural network training, these optimizers require extensive tuning and may still fail to yield satisfactory results. Moreover, a general-purpose optimizer may not work well for every specific optimization task, and it is impossible for human to design a dedicated optimizer for every specific problem. To tackle these hurdles, this project pioneers an optimizer search framework capable of autonomously discovering optimizers that surpass human-designed ones. To achieve this goal, we have formulated diverse search spaces for optimizer exploration and devised a suite of efficient algorithms for optimizer discovery. Our findings showcase that the optimizer search framework excels in identifying task-specific optimizers across a spectrum of tasks, outperforming established human-designed counterparts. In particular, our search has discovered a stochastic gradient descent variant that outshines human-designed optimizers in large-scale neural network training, leading to faster training and reduced memory usage. Beyond optimizer search, we demonstrate the potential of using AI to improve existing optimizers, including real-time hyper-parameter tuning and data subset selection to enhance training efficiency. Several optimizers designed by this endeavor have found widespread application. The outcomes of this funded project have been disseminated through publications in various venues such as ICML, NeurIPS, ICLR, CVPR, and ECCV, which are leading conferences in machine learning. Furthermore, the principal investigator has developed several graduate-level machine learning courses at UCLA, fostering the academic growth of several Ph.D. students supported by this project's funding.\t\t\t\t\tLast Modified: 04/14/2024\n\n\t\t\t\t\tSubmitted by: Cho-JuiHsieh\n"
 }
}