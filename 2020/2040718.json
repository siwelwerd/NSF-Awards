{
 "awd_id": "2040718",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NSF Convergence Accelerator Track D: The Data Hypervisor: Orchestrating Data and Models",
 "cfda_num": "47.084",
 "org_code": "15020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Mike Pozmantier",
 "awd_eff_date": "2020-09-15",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 954640.0,
 "awd_amount": 954640.0,
 "awd_min_amd_letter_date": "2020-09-10",
 "awd_max_amd_letter_date": "2020-10-14",
 "awd_abstract_narration": "The NSF Convergence Accelerator supports use-inspired, team-based, multidisciplinary efforts that address challenges of national importance and will produce deliverables of value to society in the near future. \r\n\r\nThis project, NSF Convergence Accelerator\u2013Track D: The Data Hypervisor: Orchestrating Data and Models, will design and implement the Data Station\u2014a new architecture where both data and derived data products are sealed and cannot be directly seen or downloaded by anyone. In the Data Station architecture, computation is brought to the data, rather than data being brought to users, as is common in traditional data lakes and warehouses. Sharing data and models has had a transformative impact on scientific problems from medical imaging to natural language understanding. Despite the potential upside, many researchers in both academia and industry are reluctant to centralize and share data to both internal and external researchers. Organizations today have to navigate complex regulatory considerations and protect intellectual property while incurring a significant technical investment in documenting and maintaining data. The Data Station will ease access to sensitive data, assist with data discovery and integration, and facilitate enforcement of arbitrary data access and governance policies. The project will work with partners in biomedicine, materials science, and enterprise data management to establish the capabilities and prove the concepts of the Data Station architecture.\r\n\r\nWhile building upon prior research in data systems, Data Station will introduce novel data-unaware task capsules that enable users to specify data-driven tasks such as traditional data queries and machine learning model training without the user requiring direct access to the data itself. The programming interfaces convey sufficient information for the Data Station to trigger the discovery of potentially relevant datasets; integrate and prune those datasets for computation; and compute the results by executing the task. In effect, Data Station inverts the traditional data querying modeling by bringing computations to the data. Task capsules also include a user-defined metric for determining what results are useful from the user\u2019s perspective as well as which trust constraints need to be met to validate the provenance of input datasets. The Data Station captures metadata every time a derived data product is created and provides a set of primitives to implement various data governance and data access policies necessary to address data contributor use cases. Only authorized users are able to access the data based on a novel access-token model implemented by Data Station that permits fine-grained yet scalable access control. Users must explicitly be authorized to access results via tokens obtained from data contributors. The Data Station project will engage a diverse set of partners in materials science, biomedicine, and enterprise scenarios to help design and apply the Data Station to various use cases. An education program will engage high school, undergraduate, and graduate students in researching, developing, and evaluating the Data Station.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "ITE",
 "org_div_long_name": "Innovation and Technology Ecosystems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ian",
   "pi_last_name": "Foster",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ian Foster",
   "pi_email_addr": "foster@uchicago.edu",
   "nsf_id": "000234022",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Franklin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Franklin",
   "pi_email_addr": "mjfranklin@uchicago.edu",
   "nsf_id": "000092590",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Raul",
   "pi_last_name": "Castro Fernandez",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raul Castro Fernandez",
   "pi_email_addr": "raulcf@uchicago.edu",
   "nsf_id": "000804594",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "SENDHIL",
   "pi_last_name": "MULLAINATHAN",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "SENDHIL MULLAINATHAN",
   "pi_email_addr": "smullain@uchicago.edu",
   "nsf_id": "000827763",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5801 S. Ellis Ave",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131Y00",
   "pgm_ele_name": "Convergence Accelerator Resrch"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 954640.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"md-end-block md-p md-focus\"><span class=\"md-plain md-expand\">Pooling and sharing data increases and distributes its value. For example, organizations that pool their data can build and mutually benefit from more powerful machine learning models. Health organizations that share data with each other can improve patient care. And researchers who share experimental data can accelerate scientific discovery. Despite the upsides of sharing, it is rare to find data-sharing consortia in practice. Many organizations that could benefit from data sharing face regulatory, legal, privacy, incentive, and technical barriers and, thus, can only release selected data in a controlled manner. Technically speaking, controlling how data is used is difficult, so many beneficial data-sharing consortia never materialize. Those that do are often built around data-sharing agreements resulting from long and tedious one-off negotiations that are inflexible to later changes in how data should be used.</span></p>\n<p class=\"md-end-block md-p\"><span class=\"md-plain\">In this project, we explored a novel approach to sharing data securely, i.e., so that those contributing data control how the data is shared with others at all times. We introduced Data Station, an intermediary \"data escrow\". A data escrow is a computational and data management infrastructure designed to enable the formation of data-sharing consortia. Agents share data with the escrow as if it were an extension of their infrastructure, i.e., it can be guaranteed that their data will remain confidential and that no one will access it (or any derived insights) without their explicit permission. Data users who want to extract insights from data delegate their computation to the escrow, and that computation will be executed only if permitted by the data owners. The escrow ensures that all data is protected, makes few assumptions on the threat model, and thus, allows owners and users to trust it. Finally, because many sharing scenarios involve regulatory and compliance requirements, all computation on the platform is (selectively) transparent so third-party auditors and compliance officers can audit the consortia.</span></p>\n<p class=\"md-end-block md-p\"><span class=\"md-plain\">In essence, data escrows invert the flow of data sharing and computation. Usually, if we want to let someone inspect a dataset, we share the dataset. With the escrow, we can only share the result of applying a function to the dataset but none of the raw data. The escrow is an intermediary. The main technical contributions of this project concentrated on ensuring the intermediary is trustworthy. Of all the technologies we could choose to achieve that goal, we explored the use of Trusted Execution Environments (TEE), which are common in today's hardware platforms and offer a high-performance alternative to other options to run computation on data without accessing the data such as multi-party computation, federated learning, and others.</span></p>\n<p class=\"md-end-block md-p\"><span class=\"md-plain\">The project had several important outcomes. We built a prototype of the Data Station. We used that prototype in four projects requiring explicit control of dataflows. In the first project, we use the data escrow to implement incentive mechanisms to get multiple parties to agree to share data securely, e.g., to build better machine learning models. In the second project, we deploy differential privacy---a privacy technique that adds noise to query results to prevent the analyst from de-identifying individuals in the dataset---on top of the escrow, demonstrating the advantages of the additional dataflow control. In the third project, we are using the escrow to implement the concept of a Data Union, a coalition of individuals who pool their data together before sharing it with third parties, such as big tech platforms, to control what the platforms learn about them. Finally, in the fourth project, we use the escrow as a mechanism to trade datasets: buyers will not pay before knowing the data is useful to their purpose, and sellers will not release the data before receiving a payment. With the escrow, we can signal buyers how valuable the data is without leaking any data, thus respecting the sellers' preferences. </span></p>\n<p class=\"md-end-block md-p\"><span class=\"md-plain md-expand\">Next, we are re-engineering parts of the escrow to make it more efficient and resilient, and we plan to continue our exploration of data-sharing applications and data markets.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/08/2023<br>\nModified by: Raul&nbsp;Castro Fernandez</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nPooling and sharing data increases and distributes its value. For example, organizations that pool their data can build and mutually benefit from more powerful machine learning models. Health organizations that share data with each other can improve patient care. And researchers who share experimental data can accelerate scientific discovery. Despite the upsides of sharing, it is rare to find data-sharing consortia in practice. Many organizations that could benefit from data sharing face regulatory, legal, privacy, incentive, and technical barriers and, thus, can only release selected data in a controlled manner. Technically speaking, controlling how data is used is difficult, so many beneficial data-sharing consortia never materialize. Those that do are often built around data-sharing agreements resulting from long and tedious one-off negotiations that are inflexible to later changes in how data should be used.\n\n\nIn this project, we explored a novel approach to sharing data securely, i.e., so that those contributing data control how the data is shared with others at all times. We introduced Data Station, an intermediary \"data escrow\". A data escrow is a computational and data management infrastructure designed to enable the formation of data-sharing consortia. Agents share data with the escrow as if it were an extension of their infrastructure, i.e., it can be guaranteed that their data will remain confidential and that no one will access it (or any derived insights) without their explicit permission. Data users who want to extract insights from data delegate their computation to the escrow, and that computation will be executed only if permitted by the data owners. The escrow ensures that all data is protected, makes few assumptions on the threat model, and thus, allows owners and users to trust it. Finally, because many sharing scenarios involve regulatory and compliance requirements, all computation on the platform is (selectively) transparent so third-party auditors and compliance officers can audit the consortia.\n\n\nIn essence, data escrows invert the flow of data sharing and computation. Usually, if we want to let someone inspect a dataset, we share the dataset. With the escrow, we can only share the result of applying a function to the dataset but none of the raw data. The escrow is an intermediary. The main technical contributions of this project concentrated on ensuring the intermediary is trustworthy. Of all the technologies we could choose to achieve that goal, we explored the use of Trusted Execution Environments (TEE), which are common in today's hardware platforms and offer a high-performance alternative to other options to run computation on data without accessing the data such as multi-party computation, federated learning, and others.\n\n\nThe project had several important outcomes. We built a prototype of the Data Station. We used that prototype in four projects requiring explicit control of dataflows. In the first project, we use the data escrow to implement incentive mechanisms to get multiple parties to agree to share data securely, e.g., to build better machine learning models. In the second project, we deploy differential privacy---a privacy technique that adds noise to query results to prevent the analyst from de-identifying individuals in the dataset---on top of the escrow, demonstrating the advantages of the additional dataflow control. In the third project, we are using the escrow to implement the concept of a Data Union, a coalition of individuals who pool their data together before sharing it with third parties, such as big tech platforms, to control what the platforms learn about them. Finally, in the fourth project, we use the escrow as a mechanism to trade datasets: buyers will not pay before knowing the data is useful to their purpose, and sellers will not release the data before receiving a payment. With the escrow, we can signal buyers how valuable the data is without leaking any data, thus respecting the sellers' preferences. \n\n\nNext, we are re-engineering parts of the escrow to make it more efficient and resilient, and we plan to continue our exploration of data-sharing applications and data markets.\n\n\n\t\t\t\t\tLast Modified: 11/08/2023\n\n\t\t\t\t\tSubmitted by: RaulCastro Fernandez\n"
 }
}