{
 "awd_id": "2004645",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research : Elements : Extending the physics reach of LHCb by developing and deploying  algorithms for a fully GPU-based first trigger stage",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032925147",
 "po_email": "dmassey@nsf.gov",
 "po_sign_block_name": "Daniel F. Massey",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 310000.0,
 "awd_amount": 310000.0,
 "awd_min_amd_letter_date": "2020-05-01",
 "awd_max_amd_letter_date": "2020-05-01",
 "awd_abstract_narration": "The development of the Standard Model (SM) of particle physics is a major intellectual achievement. The validity of this model was further confirmed by the discovery of the Higgs boson at the Large Hadron Collider (LHC) at CERN. However, the Standard Model leaves open many questions, including why matter dominates over anti-matter in the Universe and the properties of dark matter. Most explanations require new phenomena, which we call Beyond the Standard Model Physics (BSM), and which the LHCb experiment at CERN has been designed to explore. The LHC is the premier High Energy Physics particle accelerator in the world and is currently operating at the CERN laboratory near Geneva Switzerland, one of the foremost facilities for addressing these BSM questions. The LHCb experiment is one of four large experiments at the LHC and is designed to study in detail the decays of hadrons containing b or c quarks. The goal is to identify the existence of new physics beyond the Standard Model by examining the properties of hadrons containing these quarks. The new physics, or new forces, can be manifest by particles, as yet to be discovered, whose presence would modify decay rates and CP violating asymmetries of hadrons containing the b and c quarks, allowing new phenomena to be observed indirectly - or via direct observation of new force-carrying particles. The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment, in which both PIs participate, produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. The primary goal of this project is developing and deploying software that will maximize the performance of the LHCb trigger system - running its first processing stage on GPUs - so that the full physics discovery potential of LHCb is realized.\r\n\r\nThe LHCb detector is being upgraded for Run 3 (which will start to record data in 2022), when the trigger system will need to process 25 exabytes per year. Currently, only 0.3 of the 10 exabytes per year processed by the trigger is analyzed using high-level computing algorithms; the rest is discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in Run 3, LHCb plans to process the entire 25 exabytes each year using high-level computing algorithms. The PIs propose running the entire first trigger-processing stage on GPUs, which has zero (likely negative) net cost, and frees up all of the CPU resources for the second processing stage. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which will need to be reoptimized both for Run 3 conditions but also for usage on GPUs. The specific objectives of this proposal are developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, likely using ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances will make it possible to explore many potential explanations for dark matter, e.g., dark photon decays, and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mike",
   "pi_last_name": "Williams",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mike Williams",
   "pi_email_addr": "mwill@mit.edu",
   "nsf_id": "000630872",
   "pi_start_date": "2020-05-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "CERN",
  "perf_str_addr": "Route de Meyrin, 385",
  "perf_city_name": "Meyrin",
  "perf_st_code": "",
  "perf_st_name": "RI REQUIRED",
  "perf_zip_code": "",
  "perf_ctry_code": "SZ",
  "perf_cong_dist": "",
  "perf_st_cong_dist": "",
  "perf_ctry_name": "Switzerland",
  "perf_ctry_flag": "0"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "077Z",
   "pgm_ref_txt": "CSSI-1: Cyberinfr for Sustained Scientif"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 310000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger system design is dictated by the rate at which the sensors can be read out, the computational power of the system, and the available storage space. The LHCb detector has been upgraded for latest LHC data-taking run, with its trigger system now needing to process 25 exabytes per year. In the previous LHC run, only 0.3 of the 10 exabytes per year processed by the LHCb trigger were analyzed using high-level computing algorithms; the rest was discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in the latest run, LHCb now processes the entire 25 exabytes each year using high-level computing algorithms.<br /><br />This grant supported work that made it possible to run the entire first trigger-processing stage on GPUs. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which all needed to be reoptimized both for the latest running conditions but also for usage on GPUs. The primary achievements of this project were developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, the latter of which uses ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances are making it possible to explore many potential explanations for dark matter (e.g., dark photon decays to electron-positron pairs) and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.</p><br>\n<p>\n Last Modified: 07/02/2024<br>\nModified by: Mike&nbsp;Williams</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe data sets collected by the LHC experiments are some of the largest in the world. For example, the sensor arrays of the LHCb experiment produce about 100 TB/s and close to a zettabyte per year. Even after drastic data-reduction performed by custom-built read-out electronics, the data volume is still about 10 exabytes per year. Such large data sets cannot be stored indefinitely; therefore, all high energy physics (HEP) experiments employ a second data-reduction scheme executed in real time by a data-ingestion system - referred to as a trigger system in HEP - to decide whether each event is to be persisted for future analysis or permanently discarded. Trigger system design is dictated by the rate at which the sensors can be read out, the computational power of the system, and the available storage space. The LHCb detector has been upgraded for latest LHC data-taking run, with its trigger system now needing to process 25 exabytes per year. In the previous LHC run, only 0.3 of the 10 exabytes per year processed by the LHCb trigger were analyzed using high-level computing algorithms; the rest was discarded prior to this stage using simple algorithms executed on FPGAs. To significantly extend its physics reach in the latest run, LHCb now processes the entire 25 exabytes each year using high-level computing algorithms.\n\nThis grant supported work that made it possible to run the entire first trigger-processing stage on GPUs. The LHCb trigger makes heavy use of machine learning (ML) algorithms, which all needed to be reoptimized both for the latest running conditions but also for usage on GPUs. The primary achievements of this project were developing: GPU-based versions of the primary trigger-selection algorithms, which make heavy usage of ML; GPU-based calorimeter-clustering and electron-identification algorithms, the latter of which uses ML; and the infrastructure required to deploy ML algorithms within the GPU-based trigger framework. These advances are making it possible to explore many potential explanations for dark matter (e.g., dark photon decays to electron-positron pairs) and the matter/anti-matter asymmetry of our universe using data that would be otherwise inaccessible due to trigger-system limitations.\t\t\t\t\tLast Modified: 07/02/2024\n\n\t\t\t\t\tSubmitted by: MikeWilliams\n"
 }
}