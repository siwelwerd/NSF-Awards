{
 "awd_id": "1948133",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: III: Efficient and Robust Statistical Estimation from Nonlinear Compressed Measurements",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 174994.0,
 "awd_amount": 174994.0,
 "awd_min_amd_letter_date": "2020-03-17",
 "awd_max_amd_letter_date": "2020-03-17",
 "awd_abstract_narration": "This project advances the nation's development in science and engineering by providing new theory and algorithms for knowledge discovery from high-dimensional data. High-dimensional estimation, a computational procedure that extracts the most useful information from a large pool of redundant or irrelevant features, has played fundamental roles in various areas such as medical imaging, biology, and climatology. However, the well-established estimation schemes degrade dramatically when the data have complex structures, or when they are contaminated due to hardware failures, programming errors, or cyber-attacks. The goal of this project is to significantly broaden the understanding of the fundamental limits of learning algorithms against different types of structures and data errors, to offer a complete guideline for robust algorithmic design, and to highlight the extent to which an intelligent system behaves reliably and consistently. Outputs, such as theoretical results, algorithm implementation, and reusable empirical data, are designed to support a wide range of researchers in machine learning, high-dimensional statistics, signal processing, biology, and other related fields.\r\n\r\nThe project will be carried out by investigating the interplay of high-dimensional statistics, optimization, and learning theory. The investigator will develop a unified framework for nonlinear estimation in the high-dimensional regime, which uncovers parameter estimation from quantized measurements and learning with nonlinear activation functions in deep neural networks. In particular, to account for the nonlinear and possibly nonconvex nature, the investigator will develop efficient constrained optimization algorithms by leveraging inherent geometric structures into algorithmic design and theoretical analysis. Based on the unified framework and the established generic results, the investigator will revisit an ensemble of heuristic algorithms and will provide a theoretical justification on when and why they succeed in practice. Lastly, the investigator will design algorithms that are robust to various types of data corruption, such as adversarial noise, outlier, and malicious noise. To obtain a near-optimal dependence on the noise rate and data dimension in the sample complexity, a series of new statistical results will be established by leveraging tools from, and enriching theory in learning theory and robust statistics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jie",
   "pi_last_name": "Shen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jie Shen",
   "pi_email_addr": "Jie.shen@stevens.edu",
   "nsf_id": "000800725",
   "pi_start_date": "2020-03-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stevens Institute of Technology",
  "inst_street_address": "ONE CASTLE POINT ON HUDSON",
  "inst_street_address_2": "",
  "inst_city_name": "HOBOKEN",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "2012168762",
  "inst_zip_code": "070305906",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "NJ08",
  "org_lgl_bus_name": "THE TRUSTEES OF THE STEVENS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJ6CN5Y5A2R5"
 },
 "perf_inst": {
  "perf_inst_name": "Stevens Institute of Technology",
  "perf_str_addr": "1 Castle Point Terrace",
  "perf_city_name": "Hoboken",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "070305991",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "NJ08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 174994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project led to 10 peer-reviewed papers published in top-tier machine learning conferences and journals:</p>\n<p>[1] Chicheng Zhang, Jie Shen, Pranjal Awasthi. Efficient Active Learning of Sparse Halfspaces with Arbitrary Bounded Noise, Proceedings of the 34th Annual Conference on Neural Information Processing Systems, 2020</p>\n<p>[2] Jie Shen, Chicheng Zhang. Attribute-Efficient Learning of Halfspaces with Malicious Noise: Near-Optimal Label Complexity and Noise Tolerance, Proceedings of the 32nd International Conference on Algorithmic Learning Theory, 2021</p>\n<p>[3] Jie Shen. On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise, Proceedings of the 38th International Conference on Machine Learning, 2021</p>\n<p>[4] Jie Shen. Sample-Optimal PAC Learning of Halfspaces with Malicious<br />Noise, Proceedings of the 38th International Conference on Machine Learning, 2021</p>\n<p>[5] Jie Shen, Nan Cui, Jing Wang. Metric-Fair Active Learning, Proceedings of the 39th International Conference on Machine Learning, 2022</p>\n<p>[6] Shiwei Zeng, Jie Shen. Efficient PAC Learning from the Crowd with Pairwise Comparisons, Proceedings of the 39th International Conference on Machine Learning, 2022</p>\n<p>[7] Jing Wang, Jie Shen. Fast Spectral Analysis for Approximate Nearest Neighbor Search, Machine Learning, 2022</p>\n<p>[8] Tianhao Zhu, Jie Shen. Residual-Based Sampling for Online Outlier-Robust PCA, Proceedings of the 39th International Conference on Machine Learning, 2022</p>\n<p>[9] Shiwei Zeng, Jie Shen. List-Decodable Sparse Mean Estimation, Proceedings of the 36th Annual Conference on Neural Information Processing Systems, 2022</p>\n<p>[10]&nbsp;Shiwei Zeng, Jie Shen. Semi-Verified PAC Learning from the Crowd with Comparisons, Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023</p>\n<p>&nbsp;</p>\n<p>In most of these works, we consider learning halfspaces, namely, learning from 1-bit measurements. This is a popular sub-category of the nonlinear measurements considered in the project.</p>\n<p>[1]: We resolve an open question of learning halfspaces in the presence of bounded noise proposed 20 years ago. We show that, in stark contrast to the state of the art where learning needs double-exponentially many samples, polynomial sample size suffices. Second, we show that when the halfspace exhibits sparsity pattern, the sample size can be further reduced to sublinear in the dimension, hence well aligning the goal of the project: learning when the data are insufficient.</p>\n<p>[2,4]: We study the problem of learning halfspaces with malicious noise, which is possibly the strongest corruption model in the sense that both feature and label can be adversarially corrupted. We show that it is possible to achieve the information-theoretic sample complexity by efficient algorithms.</p>\n<p>[3]: We propose an online active learner for adversarial noise. Our work gives a complete answer to why it works well in practice by showing that it enjoys near- optimal noise tolerance, sample complexity, and label complexity. Additionally, we show that the algorithm can be made attribute-efficient, in the sense that the sample size becomes sublinear in the dimension when the model is sparse.</p>\n<p>[5]: We study a common concern in active learning: the fariness of the model produced by active learning algorithms may be unfair to certain individuals. We yet show that such intuition is false: even though active learners select examples for training, it still preserves fairness when equipped with proper constraints on the model.</p>\n<p>[6,9]: We consider how we can learn a reliable model when a fraction of the crowd workers intentionally corrupt the labels. We present efficient algorithms which enjoys provable guarantee on prediction accuracy, without suffering large amount of label queries from the crowd.</p>\n<p>[7]: We consider the problem of nearest neighbor search in the presence of noise, where each example may be perturbed by sub-gaussian noise. We develop a robust algorithm that returns the correct nearest neighbors in polynomial time when the variance of the noise is upper bounded by a constant.</p>\n<p>[8]: We study the classical Principal Component Analysis problem in the presence of adversarial outliers in the online setting, where samples arrive in a sequential manner. We present a new notion of robust residual-norm to measure how likely a sample acts as an outlier, and use it to filter potential outliers while recovering the true principal components.</p>\n<p>[10]: We consider mean estimation when the majority of the data are corrupted, and we show that by allowing the algorithm to output a list of finite models, at least one of them must be close to the true mean. Our main contribution falls into the first polynomial-time algorithm that enjoys sample complexity logarithmic in the dimension.</p>\n<p>Two female PhD students were supported to conduct research with the PI and to present the work in conferences.</p>\n<p>On the education front, the CRII project has resulted in the development of two new courses \"Statistical Machine Learning\" and \"Mathematical Foundations of Machine Learning\", and has resulted in a transformation of the course \"Artificial Intelligence\". All of these courses are open to junior and senior undergraduates and graduate students.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/11/2023<br>\n\t\t\t\t\tModified by: Jie&nbsp;Shen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project led to 10 peer-reviewed papers published in top-tier machine learning conferences and journals:\n\n[1] Chicheng Zhang, Jie Shen, Pranjal Awasthi. Efficient Active Learning of Sparse Halfspaces with Arbitrary Bounded Noise, Proceedings of the 34th Annual Conference on Neural Information Processing Systems, 2020\n\n[2] Jie Shen, Chicheng Zhang. Attribute-Efficient Learning of Halfspaces with Malicious Noise: Near-Optimal Label Complexity and Noise Tolerance, Proceedings of the 32nd International Conference on Algorithmic Learning Theory, 2021\n\n[3] Jie Shen. On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise, Proceedings of the 38th International Conference on Machine Learning, 2021\n\n[4] Jie Shen. Sample-Optimal PAC Learning of Halfspaces with Malicious\nNoise, Proceedings of the 38th International Conference on Machine Learning, 2021\n\n[5] Jie Shen, Nan Cui, Jing Wang. Metric-Fair Active Learning, Proceedings of the 39th International Conference on Machine Learning, 2022\n\n[6] Shiwei Zeng, Jie Shen. Efficient PAC Learning from the Crowd with Pairwise Comparisons, Proceedings of the 39th International Conference on Machine Learning, 2022\n\n[7] Jing Wang, Jie Shen. Fast Spectral Analysis for Approximate Nearest Neighbor Search, Machine Learning, 2022\n\n[8] Tianhao Zhu, Jie Shen. Residual-Based Sampling for Online Outlier-Robust PCA, Proceedings of the 39th International Conference on Machine Learning, 2022\n\n[9] Shiwei Zeng, Jie Shen. List-Decodable Sparse Mean Estimation, Proceedings of the 36th Annual Conference on Neural Information Processing Systems, 2022\n\n[10] Shiwei Zeng, Jie Shen. Semi-Verified PAC Learning from the Crowd with Comparisons, Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023\n\n \n\nIn most of these works, we consider learning halfspaces, namely, learning from 1-bit measurements. This is a popular sub-category of the nonlinear measurements considered in the project.\n\n[1]: We resolve an open question of learning halfspaces in the presence of bounded noise proposed 20 years ago. We show that, in stark contrast to the state of the art where learning needs double-exponentially many samples, polynomial sample size suffices. Second, we show that when the halfspace exhibits sparsity pattern, the sample size can be further reduced to sublinear in the dimension, hence well aligning the goal of the project: learning when the data are insufficient.\n\n[2,4]: We study the problem of learning halfspaces with malicious noise, which is possibly the strongest corruption model in the sense that both feature and label can be adversarially corrupted. We show that it is possible to achieve the information-theoretic sample complexity by efficient algorithms.\n\n[3]: We propose an online active learner for adversarial noise. Our work gives a complete answer to why it works well in practice by showing that it enjoys near- optimal noise tolerance, sample complexity, and label complexity. Additionally, we show that the algorithm can be made attribute-efficient, in the sense that the sample size becomes sublinear in the dimension when the model is sparse.\n\n[5]: We study a common concern in active learning: the fariness of the model produced by active learning algorithms may be unfair to certain individuals. We yet show that such intuition is false: even though active learners select examples for training, it still preserves fairness when equipped with proper constraints on the model.\n\n[6,9]: We consider how we can learn a reliable model when a fraction of the crowd workers intentionally corrupt the labels. We present efficient algorithms which enjoys provable guarantee on prediction accuracy, without suffering large amount of label queries from the crowd.\n\n[7]: We consider the problem of nearest neighbor search in the presence of noise, where each example may be perturbed by sub-gaussian noise. We develop a robust algorithm that returns the correct nearest neighbors in polynomial time when the variance of the noise is upper bounded by a constant.\n\n[8]: We study the classical Principal Component Analysis problem in the presence of adversarial outliers in the online setting, where samples arrive in a sequential manner. We present a new notion of robust residual-norm to measure how likely a sample acts as an outlier, and use it to filter potential outliers while recovering the true principal components.\n\n[10]: We consider mean estimation when the majority of the data are corrupted, and we show that by allowing the algorithm to output a list of finite models, at least one of them must be close to the true mean. Our main contribution falls into the first polynomial-time algorithm that enjoys sample complexity logarithmic in the dimension.\n\nTwo female PhD students were supported to conduct research with the PI and to present the work in conferences.\n\nOn the education front, the CRII project has resulted in the development of two new courses \"Statistical Machine Learning\" and \"Mathematical Foundations of Machine Learning\", and has resulted in a transformation of the course \"Artificial Intelligence\". All of these courses are open to junior and senior undergraduates and graduate students. \n\n\t\t\t\t\tLast Modified: 09/11/2023\n\n\t\t\t\t\tSubmitted by: Jie Shen"
 }
}