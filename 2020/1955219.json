{
 "awd_id": "1955219",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: CIF: Medium: Occlusion and Directional Resolution in Computational Imaging",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032928910",
 "po_email": "jafowler@nsf.gov",
 "po_sign_block_name": "James Fowler",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 599994.0,
 "awd_amount": 599994.0,
 "awd_min_amd_letter_date": "2020-06-19",
 "awd_max_amd_letter_date": "2021-09-08",
 "awd_abstract_narration": "Seeing around a corner without using a mirror seems like an impossible feat, and an ordinary camera does not seem to help.  In this project, the research team will develop systems to process images and videos from ordinary cameras to construct around-the-corner views and to infer other scene features that are hidden from direct view such as the numbers of people present.  The team will also augment the use of ordinary cameras with depth sensors such as those used in self-driving cars and immersive gaming.  Being able to see around corners has the potential to aid first responders and improve navigation safety.\r\n \r\nThe fundamental difficulty in using diffusely reflected light in imaging is that light from many directions is combined at the visible surface.  Methods for non-line-of-sight imaging have predominantly depended on separating light paths by their lengths using expensive, high-resolution time-of-flight (TOF) measurement.  Instead of depending entirely on TOF, this project emphasizes the exploitation of opaque objects that create restrictions and variations in how light is combined at a diffuse surface.  The research team will also develop a framework to express inverse problem difficulty that goes beyond condition numbers to provide localized and directional concepts for resolution.  Analyses will inspire and be informed by proof-of-concept experiments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vivek",
   "pi_last_name": "Goyal",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Vivek K Goyal",
   "pi_email_addr": "goyal@bu.edu",
   "nsf_id": "000661823",
   "pi_start_date": "2020-06-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0123",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 134117.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 465877.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Traditional imaging creates spatial correspondences by physically manipulating the propagation of light, such as with mirrors and lenses.&nbsp; Major limiting factors for resolution include diffraction (determined by light wavelength and aperture size), optical imperfections (such as aberrations, misalignment, and lack of focus), pixel size, and motion blur.&nbsp; Imaging processing computations can alleviate some of these limitations.&nbsp; More ambitiously, the field of computational imaging addresses the creation of surprising new image formation capabilities.&nbsp; Common examples of imaging in which computational is central include magnetic resonance imaging and x-ray computed tomography; in these, the measured data cannot be directly interpreted as an image, even a degraded one.&nbsp; Emerging examples of computational imaging include 3D imaging with lidar and imaging without line-of-sight view, without mirrors.&nbsp; Lidar is used for autonomous navigation, robotics, biometrics, and augmented reality.&nbsp; Non-line-of-sight (NLOS) imaging does not yet have widespread application, but it could be used for search-and-rescue operations and early detection of driving hazards.</p>\n<p>The main focus of this project was to develop methods for NLOS imaging, with an emphasis on how occlusions created by opaque objects can be exploited.&nbsp; Secondarily, the project explored other novel methods for computational imaging, including both analyses of fundamental limits and development of practical methods.</p>\n<p>Metaphorically, the problem of NLOS imaging is to use reflections from matte or diffuse surfaces as if these surfaces were mirrors.&nbsp; When light reaches a matte wall, it scatters in all directions. &nbsp;To infer the directions from which the light originated requires some mechanism to separate light contributions from different directions to regain the one-to-one spatial correspondences lost from the scattering. &nbsp;Over about a decade, methods have been advanced to accomplish this separation by time of flight (TOF) using pulsed illumination and ultrafast sensing, yielding several demonstrations of NLOS optical imaging. &nbsp;Less developed is a complementary approach of exploiting occlusions to separate light paths.&nbsp; Occlusion-aided NLOS imaging systems exhibit non-isotropic (or directional) capabilities that are reminiscent of certain classical imaging settings. &nbsp;For example, if a camera obscura had a thin, long vertical slit rather than a tiny, circular aperture, the image of the outside world produced in the dark room would resolve the scene horizontally but not vertically.</p>\n<p>The project introduced methods for NLOS imaging that use occlusion alone, without TOF, and methods that combine occlusion with TOF.&nbsp; Methods using occlusion alone can be passive, meaning they do not require the control of any light sources, and implemented with just an ordinary digital camera.&nbsp; The project demonstrated 2D reconstruction of the region hidden behind a wall using a single photograph of the floor on the visible side. &nbsp;Unlike previous work, which has assumed all light sources to be in the far field, the project introduced a more complete forward model to describe radial falloff, enabling 2D reconstructions of the hidden scene. &nbsp;The project also introduced an alternating nonlinear inversion algorithm for 2D reconstruction and demonstrated its robustness.</p>\n<p>One of the largest challenges in passive NLOS imaging is ambient background light, which limits the dynamic range of the measurement while carrying no useful information about the hidden part of the scene. &nbsp;The project introduced a new reconstruction approach that uses an optimized linear transformation to balance the rejection of uninformative light with the retention of informative light, resulting in fast (video-rate) reconstructions of hidden scenes from photographs of a blank wall under high ambient light conditions.</p>\n<p>In a built environment, wanting to see without direct line of sight is often due to being outside a doorway. &nbsp;The two vertical edges of the doorway provide occlusions that can be exploited for NLOS imaging. &nbsp;While each edge can separately yield a robust 1D reconstruction, joint processing suggests novelties in both forward modeling and inversion. &nbsp;The resulting <em>doorway camera</em> introduced in this project provides accurate and robust 2D reconstructions of the hidden scene.</p>\n<p>Including TOF information leads to more robust and accurate systems, but it requires more sophisticated equipment as well.&nbsp; The project team conceived, developed, implemented, and demonstrated a system for NLOS tracking and mapping using both TOF and occlusion. &nbsp;Like the passive systems described above, the SPAD field of view is adjacent to a wall edge to derive azimuthal resolution from the occlusion. &nbsp;Longitudinal resolution is derived from TOF. &nbsp;Uniquely compared to previous methods, the new system acquires data for each frame in a single snapshot without scanning, enabling the tracking of hidden objects in motion.&nbsp; Through additional modeling of occlusion within the hidden scene itself, reconstructions of occluded background regions are accumulated to form a map of the hidden scene.</p>\n<p>The project was central to the professional development of students and trainees at levels ranging from undergraduate to graduate and postdoctoral.&nbsp; This includes individuals who subsequently started academic or industrial research careers or moved on to advanced degree programs.</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 09/29/2024<br>\nModified by: Vivek&nbsp;K&nbsp;Goyal</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTraditional imaging creates spatial correspondences by physically manipulating the propagation of light, such as with mirrors and lenses. Major limiting factors for resolution include diffraction (determined by light wavelength and aperture size), optical imperfections (such as aberrations, misalignment, and lack of focus), pixel size, and motion blur. Imaging processing computations can alleviate some of these limitations. More ambitiously, the field of computational imaging addresses the creation of surprising new image formation capabilities. Common examples of imaging in which computational is central include magnetic resonance imaging and x-ray computed tomography; in these, the measured data cannot be directly interpreted as an image, even a degraded one. Emerging examples of computational imaging include 3D imaging with lidar and imaging without line-of-sight view, without mirrors. Lidar is used for autonomous navigation, robotics, biometrics, and augmented reality. Non-line-of-sight (NLOS) imaging does not yet have widespread application, but it could be used for search-and-rescue operations and early detection of driving hazards.\n\n\nThe main focus of this project was to develop methods for NLOS imaging, with an emphasis on how occlusions created by opaque objects can be exploited. Secondarily, the project explored other novel methods for computational imaging, including both analyses of fundamental limits and development of practical methods.\n\n\nMetaphorically, the problem of NLOS imaging is to use reflections from matte or diffuse surfaces as if these surfaces were mirrors. When light reaches a matte wall, it scatters in all directions. To infer the directions from which the light originated requires some mechanism to separate light contributions from different directions to regain the one-to-one spatial correspondences lost from the scattering. Over about a decade, methods have been advanced to accomplish this separation by time of flight (TOF) using pulsed illumination and ultrafast sensing, yielding several demonstrations of NLOS optical imaging. Less developed is a complementary approach of exploiting occlusions to separate light paths. Occlusion-aided NLOS imaging systems exhibit non-isotropic (or directional) capabilities that are reminiscent of certain classical imaging settings. For example, if a camera obscura had a thin, long vertical slit rather than a tiny, circular aperture, the image of the outside world produced in the dark room would resolve the scene horizontally but not vertically.\n\n\nThe project introduced methods for NLOS imaging that use occlusion alone, without TOF, and methods that combine occlusion with TOF. Methods using occlusion alone can be passive, meaning they do not require the control of any light sources, and implemented with just an ordinary digital camera. The project demonstrated 2D reconstruction of the region hidden behind a wall using a single photograph of the floor on the visible side. Unlike previous work, which has assumed all light sources to be in the far field, the project introduced a more complete forward model to describe radial falloff, enabling 2D reconstructions of the hidden scene. The project also introduced an alternating nonlinear inversion algorithm for 2D reconstruction and demonstrated its robustness.\n\n\nOne of the largest challenges in passive NLOS imaging is ambient background light, which limits the dynamic range of the measurement while carrying no useful information about the hidden part of the scene. The project introduced a new reconstruction approach that uses an optimized linear transformation to balance the rejection of uninformative light with the retention of informative light, resulting in fast (video-rate) reconstructions of hidden scenes from photographs of a blank wall under high ambient light conditions.\n\n\nIn a built environment, wanting to see without direct line of sight is often due to being outside a doorway. The two vertical edges of the doorway provide occlusions that can be exploited for NLOS imaging. While each edge can separately yield a robust 1D reconstruction, joint processing suggests novelties in both forward modeling and inversion. The resulting doorway camera introduced in this project provides accurate and robust 2D reconstructions of the hidden scene.\n\n\nIncluding TOF information leads to more robust and accurate systems, but it requires more sophisticated equipment as well. The project team conceived, developed, implemented, and demonstrated a system for NLOS tracking and mapping using both TOF and occlusion. Like the passive systems described above, the SPAD field of view is adjacent to a wall edge to derive azimuthal resolution from the occlusion. Longitudinal resolution is derived from TOF. Uniquely compared to previous methods, the new system acquires data for each frame in a single snapshot without scanning, enabling the tracking of hidden objects in motion. Through additional modeling of occlusion within the hidden scene itself, reconstructions of occluded background regions are accumulated to form a map of the hidden scene.\n\n\nThe project was central to the professional development of students and trainees at levels ranging from undergraduate to graduate and postdoctoral. This includes individuals who subsequently started academic or industrial research careers or moved on to advanced degree programs.\n\n\n\t\t\t\t\tLast Modified: 09/29/2024\n\n\t\t\t\t\tSubmitted by: VivekKGoyal\n"
 }
}