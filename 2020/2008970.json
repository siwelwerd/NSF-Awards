{
 "awd_id": "2008970",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Robust Uncertainty Quantification and Statistical Learning for Heavy Tails and Rare Events",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922948",
 "po_email": "slevine@nsf.gov",
 "po_sign_block_name": "Stacey Levine",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 370000.0,
 "awd_amount": 370000.0,
 "awd_min_amd_letter_date": "2020-07-27",
 "awd_max_amd_letter_date": "2020-07-27",
 "awd_abstract_narration": "Mathematical models based on probability and statistics are used in a wide variety of contexts, for example in artificial intelligence, finance and operations research, physical sciences, and many others.  A central question of interest is how much trust can be put in the predictions of a given model.  This is crucial when, as it often happens, there are significant uncertainties associated with the nature of the model itself. In this project, the investigators aim to develop a systematic mathematical framework based on the theory of information to address these issues. One focus of the research will be on prediction of the probability of rare (but potentially catastrophic) events. For example, if a given model predicts a catastrophic event to be a 100-year event, how do uncertainties in the model potentially change this prediction?  The investigators will build corresponding stress tests to assess the effects of uncertainties.  The research will also provide systematic tools to train new statistical learning models with data and provide performance guarantees. \r\n\r\nThis research will focus on the development of the probabilistic foundations of uncertainty quantification for complex systems and on related questions about statistical learning. The overarching goals are to provide computable performance guarantees when there is uncertainty in the model itself as well as to develop trustworthy and reliable inference algorithms. Many different metrics and information theoretic measurements are available to compare probability distributions (for example, the Kullback-Leibler divergence); a unifying theme of the project is to determine, in a principled manner, which method is most appropriate to a specific task. In this context, a task consists of extracting information from the model by evaluating certain quantities of interest, such as average values, variance, probability of some rare event, and so on. Using variational principles, new optimal information inequalities will be derived to address these issues. From a robustness perspective, this allows the design of finely tuned stress tests, that is, to build neighborhoods of models around a given baseline model and to compute worst-case scenarios, in the spirit of the stress tests used by financial institutions to protect against sudden changes under alternative scenarios. In the context of statistical learning, and especially approximate inference, central challenges are 1) to select the right divergence to minimize as means to learn probabilistic models, and 2) to provide performance guarantees for the learning process. The investigators will study these questions with emphasis on the case where the quantities of interest are rare (but potentially catastrophic) events. They will assess the impact of model uncertainty on these catastrophic events and on models with heavy tails. The project aims to provide mathematical foundations for performance guarantees in probabilistic algorithms used in a wide array of problems from materials science, to operations research, machine learning, and artificial intelligence. The focus on reliable predictions of extreme and rare events makes the project timely and widely applicable. For example, the robust uncertainty quantification perspective provides worst-case solutions, stress tests, and bias control for safety-critical problems (such as rogue waves in the ocean or power grid failure). Furthermore, probabilistic performance guarantees for approximate inference can make existing black box inference algorithms more trustworthy and transparent in a mathematically systematic manner.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Luc",
   "pi_last_name": "Rey-Bellet",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Luc Rey-Bellet",
   "pi_email_addr": "lr7q@math.umass.edu",
   "nsf_id": "000196997",
   "pi_start_date": "2020-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Markos",
   "pi_last_name": "Katsoulakis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Markos Katsoulakis",
   "pi_email_addr": "markos@math.umass.edu",
   "nsf_id": "000109971",
   "pi_start_date": "2020-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "100 Venture Way, Suite 201",
  "perf_city_name": "Hadley",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010359450",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126600",
   "pgm_ele_name": "APPLIED MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 370000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">Generative modeling is a type of artificial intelligence that uses data to construct a mechanism capable of generating new data similar to the original training data. Central to the training process of generative models are mathematical objects known as probability divergences, which quantify the dissimilarity between datasets. One of the primary outcomes of this project is to develop a family of such divergences that are both flexible and specifically adapted to the datasets used in machine learning applications. These divergences combine the strengths of optimal transport metrics, which optimize physical transport distributions, and information-theoretic divergences, which optimize the flow of information during the learning process.</p>\r\n<p class=\"p2\">One application of the proposed research is to enhance the learning process when training data is structured&mdash;for example, when it exhibits certain symmetries. Incorporating the symmetries of data into the learning process leads to more efficient and robust learning, which is crucial in many applications, particularly in the physical sciences, where data can be scarce and expensive to obtain.</p>\r\n<p class=\"p1\">Another significant outcome of this research is to establish connections between the emerging field of generative modeling and more established mathematical disciplines, such as optimization and control theory, with a particular focus on mean field games.</p>\r\n<p class=\"p2\">An immediate broader impact of the proposal is the training of a cohort of young scientists, who are currently working on the foundations of machine learning in various scientific institutions. These scientists will go on to train the next generation of students in the mathematical foundations of machine learning.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/26/2025<br>\nModified by: Luc&nbsp;Rey-Bellet</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nGenerative modeling is a type of artificial intelligence that uses data to construct a mechanism capable of generating new data similar to the original training data. Central to the training process of generative models are mathematical objects known as probability divergences, which quantify the dissimilarity between datasets. One of the primary outcomes of this project is to develop a family of such divergences that are both flexible and specifically adapted to the datasets used in machine learning applications. These divergences combine the strengths of optimal transport metrics, which optimize physical transport distributions, and information-theoretic divergences, which optimize the flow of information during the learning process.\r\n\n\nOne application of the proposed research is to enhance the learning process when training data is structuredfor example, when it exhibits certain symmetries. Incorporating the symmetries of data into the learning process leads to more efficient and robust learning, which is crucial in many applications, particularly in the physical sciences, where data can be scarce and expensive to obtain.\r\n\n\nAnother significant outcome of this research is to establish connections between the emerging field of generative modeling and more established mathematical disciplines, such as optimization and control theory, with a particular focus on mean field games.\r\n\n\nAn immediate broader impact of the proposal is the training of a cohort of young scientists, who are currently working on the foundations of machine learning in various scientific institutions. These scientists will go on to train the next generation of students in the mathematical foundations of machine learning.\r\n\n\n\t\t\t\t\tLast Modified: 02/26/2025\n\n\t\t\t\t\tSubmitted by: LucRey-Bellet\n"
 }
}