{
 "awd_id": "2008484",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: AF: Small: Adaptive Optimization of Stochastic and Noisy Function",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-05-31",
 "tot_intn_awd_amt": 85000.0,
 "awd_amount": 85000.0,
 "awd_min_amd_letter_date": "2020-08-21",
 "awd_max_amd_letter_date": "2020-10-19",
 "awd_abstract_narration": "The science of artificial intelligence, and the technology of machine learning (ML) in particular, has had a huge impact on modern society.  This impact is only expected to grow in the future.  At the heart of ML is the process of training the parameters of an intelligent (computer) system, which requires applied-mathematics techniques in the area known as mathematical optimization.  The many recent successes of ML, such as in computer vision and natural-language processing, have been made possible with the use of a certain mathematical-optimization algorithm.  This algorithm allows the intelligent system to learn through the iterative random selection of data points from within a large-scale dataset.  This random sampling is absolutely essential, since otherwise the learning process of any intelligent system would be slowed as the amount of available data increases.  However, despite these recent successes, optimization techniques such as this one have fundamental shortcomings that impede them from being effective for next-generation ML tasks.  For example, each application of the algorithm requires a careful data-dependent tuning process, which may cause the training of an intelligent system for a single task to require weeks or months of computation on a supercomputer.  One avenue for avoiding such computational expense is through the design of optimization techniques that \"adaptively\" tune themselves.  The goals of this project are to design and provide theoretical guarantees for such adaptive algorithms.\r\n\r\nThere have been various previously proposed enhancements and extensions to the aforementioned algorithm, known as the stochastic gradient (SG) algorithm.  However, many of these algorithms also possess the shortcoming of being nonadaptive, meaning that their successful application in practice requires expensive \"hyperparameter\" tuning efforts.  The adaptive algorithms considered in this project for the \"stochastic optimization\" setting of ML are based on the various successful methodologies in the \"deterministic optimization\" literature.  These include so-called \"line search\" and \"trust region\" methodologies.  However, since neither of these methodologies result in optimal worst-case complexity guarantees, the focus of the project is on the design of adaptive optimal-complexity methods, such as so-called \"cubic-regularization\" algorithms.  The design of adaptive cubic-regularization algorithms for the stochastic setting will be achieved by building on a theoretical framework that views adaptive minimization as a \"renewal-reward\" stochastic process.  This work will combine analytical techniques from the mathematical-optimization and stochastic-process literatures, and will provide a solid theoretical and practical foundation for researchers working in applied mathematics, computer science, statistics, and various engineering fields.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Curtis",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Frank E Curtis",
   "pi_email_addr": "fec309@lehigh.edu",
   "nsf_id": "000549599",
   "pi_start_date": "2020-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Lehigh University",
  "inst_street_address": "526 BRODHEAD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BETHLEHEM",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "6107583021",
  "inst_zip_code": "180153008",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "PA07",
  "org_lgl_bus_name": "LEHIGH UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E13MDBKHLDB5"
 },
 "perf_inst": {
  "perf_inst_name": "Lehigh University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "180153027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "PA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 85000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Mathematical optimization algorithms are essential tools for solving numerous important problems throughout science, engineering, and business.&nbsp; Notable modern examples include optimization problems arising in machine learning applications, such as for training artificial neural networks.&nbsp; Such problems represent an important subclass of continuous optimization problems that can be viewed as having a stochastic objective function in the sense that the objective is defined by an expectation.&nbsp; For example, the objective may be to train a machine learning model to minimize the expected risk of having the model misclassify a given data point, such as a piece of text or an image.&nbsp; Consequently, the algorithms that are employed to solve such optimization/learning problems are stochastic as well, since they do not assume that exact values of the objective function (or its so-called derivatives) can be computed.&nbsp; More generally, there are also other settings in which optimization problems arise for which it is only possible to evaluate the objective function (or its derivatives) approximately, which is to say that the evaluations are noisy.</p>\n<p>This project has aimed to produce new optimization algorithms and mathematical tools for analyzing their properties for solving problems with objective functions that are stochastic or noisy.&nbsp; More precisely, this project has aimed to produce and study algorithms that are adaptive, meaning that they automatically adjust themselves to better solve any given problem at hand.&nbsp; The principal investigator and collaborators have produced a suite of adaptive stochastic algorithms and corresponding convergence analysis of their properties. They have published their findings in multiple journal articles that are read regularly by scholars working in the fields of optimization and machine learning.&nbsp; They have also produced open-source software that demonstrates their theoretical findings and serves as tools that can be used by other researchers for further investigations.</p>\n<p>The principal investigator has mentored a postdoctoral researcher and multiple graduate students over the course of the project duration.&nbsp; These young scientists will soon be entering the workforce with expertise on fundamental tools for solving various important problems in the intersection between applied mathematics, statistics, and machine learning.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/03/2023<br>\n\t\t\t\t\tModified by: Frank&nbsp;E&nbsp;Curtis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMathematical optimization algorithms are essential tools for solving numerous important problems throughout science, engineering, and business.  Notable modern examples include optimization problems arising in machine learning applications, such as for training artificial neural networks.  Such problems represent an important subclass of continuous optimization problems that can be viewed as having a stochastic objective function in the sense that the objective is defined by an expectation.  For example, the objective may be to train a machine learning model to minimize the expected risk of having the model misclassify a given data point, such as a piece of text or an image.  Consequently, the algorithms that are employed to solve such optimization/learning problems are stochastic as well, since they do not assume that exact values of the objective function (or its so-called derivatives) can be computed.  More generally, there are also other settings in which optimization problems arise for which it is only possible to evaluate the objective function (or its derivatives) approximately, which is to say that the evaluations are noisy.\n\nThis project has aimed to produce new optimization algorithms and mathematical tools for analyzing their properties for solving problems with objective functions that are stochastic or noisy.  More precisely, this project has aimed to produce and study algorithms that are adaptive, meaning that they automatically adjust themselves to better solve any given problem at hand.  The principal investigator and collaborators have produced a suite of adaptive stochastic algorithms and corresponding convergence analysis of their properties. They have published their findings in multiple journal articles that are read regularly by scholars working in the fields of optimization and machine learning.  They have also produced open-source software that demonstrates their theoretical findings and serves as tools that can be used by other researchers for further investigations.\n\nThe principal investigator has mentored a postdoctoral researcher and multiple graduate students over the course of the project duration.  These young scientists will soon be entering the workforce with expertise on fundamental tools for solving various important problems in the intersection between applied mathematics, statistics, and machine learning.\n\n\t\t\t\t\tLast Modified: 07/03/2023\n\n\t\t\t\t\tSubmitted by: Frank E Curtis"
 }
}