{
 "awd_id": "2053279",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RTML: Large: Collaborative: Harmonizing Predictive Algorithms and Mixed-Signal/Precision Circuits via Computation-Data Access Exchange and Adaptive Dataflows",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 248511.0,
 "awd_amount": 248511.0,
 "awd_min_amd_letter_date": "2020-09-18",
 "awd_max_amd_letter_date": "2020-09-18",
 "awd_abstract_narration": "Recent advances in machine learning are fueling a growing demand for intelligent Internet of Things (IoT), i.e., edge network applications. Many of them, such as autonomous vehicles, robots, and healthcare wearables, require real-time and in-situ learning to be perceived as truly intelligent. However, the limited computing and energy resources available at the edge device (e.g., mobile devices, sensors) stand at odds with the massive and growing cost of state-of-the-art machine learning training, posing a grand challenge for real-time machine learning (RTML) at the edge. This goal of this project is to foster a systematic breakthrough in achieving efficient online training of state-of-the-art machine learning algorithms in pervasive resource-constrained platforms and applications. An order of magnitude advance in RTML would enable numerous edge devices to proactively interpret and learn from new data, improve their own performance using what they have learned, and adapt to dynamic environments, all in real time. Success in this project will enable truly intelligent edge devices to penetrate all walks of life and thus generate significant impacts on societies and economies. This project will lead to new courses and open-education resources that can attract diverse groups of students and eventually deliver a platform for inclusion and innovation.  \r\n\r\nThe project addresses the RTML grand challenge using a three-pronged 'co-design' approach that seamlessly integrates algorithm, architecture, and circuit-level innovations. Specifically, at the algorithm level, an efficient training framework for RTML, for which trained models are also natively efficient for inference, will be established. Aggressive time and energy reductions can be achieved, at first by improving general training techniques, and then by focusing particularly on online learning and adaptation. At the architecture level, the project will first target reducing the high cost of data movement by trading it for lower-cost computation, and then generate optimal dataflows and hardware architectures to maximize the joint benefits of algorithms and hardware. At the circuit level, the project will leverage adaptive low-precision algorithms and architectures to design ultra-energy-efficient mixed-signal compute fabrics. Statistical computing techniques will be incorporated to demonstrate efficient, scalable, and robust machine learning chips. Finally, at the system level, an integration effort will be included to aid the realization of realistic system goals and to evaluate the innovations of the three core thrusts.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhangyang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhangyang Wang",
   "pi_email_addr": "atlaswang@utexas.edu",
   "nsf_id": "000746175",
   "pi_start_date": "2020-09-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "3925 W Braker Lane Ste 3.340",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787595316",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "082Z",
   "pgm_ref_txt": "RTML-Real Time Machine Learning"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 248511.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p>\n<p>&nbsp;</p>\n<p>The project&rsquo;s overarching goal is to foster a systematic breakthrough in real-time machine learning (RTML), that can achieve up to three orders of magnitude reductions in latency and energy compared to state-of-the-art designs, via a holistic effort of algorithm, circuit, and architecture innovations. Recent breakthroughs in deep neural networks (DNNs) have revolutionized the performance of machine learning, and have fueled a growing demand for intelligent edge devices. However, many real-world applications require real-time and in-situ learning. The limited computing and energy resources available at the edge stands at odds with the massive and growing learning costs for state-of-the-art DNNs, constituting the grand challenge for RTML. The proposed research harmonizes predictive and online learning algorithms with mixed-signal and mixed-precision circuits, via computation-data access exchange and adaptive dataflows. The PIs will demonstrate their proposed innovations, through system-level integration and experimentation, for the target application of drone-based, on-board visual perception, specifically focusing on real-time and energy-efficient object detection from video.</p>\n<p>&nbsp;</p>\n<p>PI Wang led the algorithm-level innovations on efficient training. Compared to the extensively studied efficient inference, an efficient training algorithm is complicated by a myriad of factors: the per-iteration complexity (both feed-forward and backward computations), the empirical convergence rate (how many iterations it takes to converge), and more beyond the algorithm itself (hardware/architecture integration). Furthermore, different training scenarios have also presented different levels of efficiency challenges: from (the most basic) training offline from scratch, to continuously updating pre-trained models online, to transfer and lifelong learning. To address those challenges, we have accomplished to the following research outcomes during this project:</p>\n<ul>\n<li>&nbsp;Two general-purposed energy-efficient algorithms: E2-Train leveraged the ideas of predictive low-precision gradient and selective layer-wise updating; and EB-Train introduced the recent idea of lottery ticket into DNN training for the first time. Both demonstrate one magnitude higher energy efficiency compared to standard training algorithms without incurring accuracy loss.</li>\n<li>Energy-efficient AutoML algorithms: not only developing efficient models at run time, but also drastically improving the efficiency of the AutoML search and model training process itself.</li>\n<li>Sparse training and transfer learning algorithms and theory: sparse neural network is found to improve not only the transfer learning parameter efficiency, but also the robustness and the few-shot ability.</li>\n</ul>\n<p>The project also has more broad impact outcomes, besides publications/open-source codes/public datasets, including: (1) many invited talks and tutorials in leading universities, companies, and international conferences; (2) two new courses developed in UT Austin; (3) two PhD students and a few more undergraduates are involved in this research.</p>\n<p class=\"p1\">&nbsp;</p><br>\n<p>\n Last Modified: 12/20/2023<br>\nModified by: Zhangyang&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.\n\n\n\n\n\nThe projects overarching goal is to foster a systematic breakthrough in real-time machine learning (RTML), that can achieve up to three orders of magnitude reductions in latency and energy compared to state-of-the-art designs, via a holistic effort of algorithm, circuit, and architecture innovations. Recent breakthroughs in deep neural networks (DNNs) have revolutionized the performance of machine learning, and have fueled a growing demand for intelligent edge devices. However, many real-world applications require real-time and in-situ learning. The limited computing and energy resources available at the edge stands at odds with the massive and growing learning costs for state-of-the-art DNNs, constituting the grand challenge for RTML. The proposed research harmonizes predictive and online learning algorithms with mixed-signal and mixed-precision circuits, via computation-data access exchange and adaptive dataflows. The PIs will demonstrate their proposed innovations, through system-level integration and experimentation, for the target application of drone-based, on-board visual perception, specifically focusing on real-time and energy-efficient object detection from video.\n\n\n\n\n\nPI Wang led the algorithm-level innovations on efficient training. Compared to the extensively studied efficient inference, an efficient training algorithm is complicated by a myriad of factors: the per-iteration complexity (both feed-forward and backward computations), the empirical convergence rate (how many iterations it takes to converge), and more beyond the algorithm itself (hardware/architecture integration). Furthermore, different training scenarios have also presented different levels of efficiency challenges: from (the most basic) training offline from scratch, to continuously updating pre-trained models online, to transfer and lifelong learning. To address those challenges, we have accomplished to the following research outcomes during this project:\n\nTwo general-purposed energy-efficient algorithms: E2-Train leveraged the ideas of predictive low-precision gradient and selective layer-wise updating; and EB-Train introduced the recent idea of lottery ticket into DNN training for the first time. Both demonstrate one magnitude higher energy efficiency compared to standard training algorithms without incurring accuracy loss.\nEnergy-efficient AutoML algorithms: not only developing efficient models at run time, but also drastically improving the efficiency of the AutoML search and model training process itself.\nSparse training and transfer learning algorithms and theory: sparse neural network is found to improve not only the transfer learning parameter efficiency, but also the robustness and the few-shot ability.\n\n\n\nThe project also has more broad impact outcomes, besides publications/open-source codes/public datasets, including: (1) many invited talks and tutorials in leading universities, companies, and international conferences; (2) two new courses developed in UT Austin; (3) two PhD students and a few more undergraduates are involved in this research.\n\n\n\t\t\t\t\tLast Modified: 12/20/2023\n\n\t\t\t\t\tSubmitted by: ZhangyangWang\n"
 }
}