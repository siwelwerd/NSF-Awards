{
 "awd_id": "1954548",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: INT: COLLAB: Manufacturing USA: Intelligent Human-Robot Collaboration for Smart Factory",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Bruce Kramer",
 "awd_eff_date": "2019-08-31",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 660640.0,
 "awd_amount": 802638.0,
 "awd_min_amd_letter_date": "2020-01-10",
 "awd_max_amd_letter_date": "2022-06-30",
 "awd_abstract_narration": "This National Robotics Initiative (NRI) collaborative research project addresses the NSF Big Idea of Work at the Human-Technology Frontier by targeting human-robot collaboration in manufacturing.  Recent advances in sensing, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. However, effective, efficient and safe coordination between humans and robots on the factory floor has remained a significant challenge. To meet the need for safe and effective human-robot collaboration in manufacturing, the investigators will research an integrated set of algorithms and robotic test beds to sense, understand, predict and control the interaction of human workers and robots in collaborative manufacturing cells.  It is expected that these methods will  significantly improve the safety and productivity of hybrid human-robot production systems, thereby promoting their deployment in future \"smart factories\".  To broaden the impact of this project, a partnership with Manufacturing USA Institute(s) and professional societies will be established to provide human-robot collaboration learning modules for inclusion in robotics and smart manufacturing-related curricula.  These learning modules, together with annual events aimed at community college and pre-college students, and workshops for the dissemination of research results will raise public awareness and attract new entrants into the manufacturing and robotics industries, creating truly synergetic education opportunities in science, technology, engineering and mathematics, as well as accelerating the adoption of smart factory-enabling technologies.  \r\n\r\nThe project will address fundamental challenges in human-robot collaboration in the manufacturing environment, such as the limitation of one-to-one sensing between humans and robots, the lack of adaptive and stochastic modeling methods for reliable recognition and prediction of human actions and motions in different manufacturing scenarios, and multi-scale human-robot coordination. To address these challenges, multi-disciplinary research involving sensing, machine learning, stochastic modeling, robot path planning, and advanced manufacturing will be performed.  Specific tasks include algorithm development and deployment on lab-scale and real-world test beds to: (1) sense and recognize where objects (e.g., robots, humans, parts or tools) are located and what each worker is doing; (2) predict what the next human action will be; and (3) plan and control safe and optimal robot trajectories for individualized on-the-job assistance for humans, proactively avoiding worker injury. The outcomes from the project will be evaluated on the shop-floor at the collaborating company COsorizio MAcchine Uensili (COMAU) in Michigan, and the Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhaozheng",
   "pi_last_name": "Yin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhaozheng Yin",
   "pi_email_addr": "zhaozheng.yin@stonybrook.edu",
   "nsf_id": "000605796",
   "pi_start_date": "2020-01-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "The Research Foundation for The State University of New York",
  "perf_str_addr": "Old Computer Science Bld. Room 2314",
  "perf_city_name": "Stony Brook",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "088Y00",
   "pgm_ele_name": "AM-Advanced Manufacturing"
  },
  {
   "pgm_ele_code": "150400",
   "pgm_ele_name": "GOALI-Grnt Opp Acad Lia wIndus"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "019Z",
   "pgm_ref_txt": "Grad Prep APG:Enhan. Experience"
  },
  {
   "pgm_ref_code": "062Z",
   "pgm_ref_txt": "Harnessing the Data Revolution"
  },
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "1504",
   "pgm_ref_txt": "GRANT OPP FOR ACAD LIA W/INDUS"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "MANU",
   "pgm_ref_txt": "MANUFACTURING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 660638.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 109999.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Advancements in sensors, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. To facilitate such a transformative process, coordination between human workforce and robots has remained a major challenge, with human operators and robots being strictly separated due to safety concerns. The proposed research addresses this challenge by introducing a seamless integration of robots with humans within a shared workspace where both parties adapt to each other through perceived behaviors and intent, to interact physically and operate collectively. Particularly, various algorithms, methods and systems are developed, in order to (1) sense and cognize where objects (i.e., robots, humans, machines, parts and tools) are located and what each worker is doing; (2) predict what the next human action will be and the trajectory of the human worker's motion as part of the operational sequence for assembling a certain object/system; and (3) plan and control the safe and optimal robot trajectories to provide individualized on-the-job coordination and assistance for humans and avoid worker injury proactively.</p>\r\n<p class=\"Default\">To sense and understand the manufacturing environment and workers in a more comprehensive way, we propose multi-modal approaches for manufacturing object detection and worker activity recognition using IMU (Inertial Measurement Unit) signals, Electromyography (EMG) signals, images/videos from webcams, and skeleton models from Kinect/visual sensors. Novel feature transform was developed and applied to the IMU/EMG signals in both spatial and temporal domains. Visual features are extracted from both frame level and video clip level, leading to (1) multiple object detection algorithms based on feature aggregation, memory bank, and local continuity; (2) multiple video-based action analysis algorithms including action classification, action localization, action anticipation, and online action detection; and (3) 3D human pose reconstruction and prediction for action analysis. In addition, we have designed a dual-zone Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model to recognize assembly steps by (1) partitioning the visual input into an assembly zone and a tool-and-part zone and (2) leveraging scene-level features through the CNN and temporal-level features through the LSTM. We have developed a repetitive action counting model with skeleton landmarks and joint angles using a Video Transformer network. We have developed an approach to unmanned aerial vehicle (UAV) control through electrooculography (EOG) based eye movement tracking, which translates eye movements into UAV navigation commands using (1) thresholding algorithm and (2) LSTM based eye gaze controlled GUI. The experimental results of UAV control through EOG-based eye tracking showcase a unique integration of biometric technology in UAV control and demonstrates a new direction in human-robot interaction.</p>\r\n<p class=\"Default\">Algorithms to transform sensing, cognition, and prediction data into robot behaviors, collectively known as the Proactive Adaptive Collaborative Intelligence (PACI) controller, were developed. Robot behaviors include adaptations to robot motion and robot sequencing. To intelligently adapt robot behaviors, research included creating a framework for segmenting robot motion and determining allowable robot behavior modifications; then integrating this with algorithms to detect and predict human motion while maintaining productivity and safety. &nbsp;Key outcomes, achievements and activities also include: (1) internships providing immersive R&amp;D experience and access to industry testbeds for verification and validation (V&amp;V) testing of the project algorithms and methodology, (2) engagement of Florida Advanced Technology Education Programs for broader impact of the research, (3) Decision Switch Module (DSM) enhanced switching logic that balances robot motion planning, (4) HRC algorithms that incorporate industry required ISO protocols and standards, and (5) PACI robot behaviors/reactions maintaining safety and production efficiency.&nbsp;</p>\r\n<p>This interdisciplinary research brought together experts from sensing, machine learning, modeling, robotics, and advanced manufacturing, creating a truly synergetic effort for a systematic study of the challenging human-robot collaboration problem. The project provided unique and exciting opportunities for participating graduate students and research associates, with research outcomes disseminated via numerous publications. All the graduated PhD students are either working as Postdocs or related companies in the U.S currently. At the end of the project, one workshop on Human-Robot Collaboration &amp; AI Integration was organized with IMECE in November 2023, with participants from academia, government and industrial companies working in the fields of robotics and manufacturing, providing a forum for review of technological advancement, industries&rsquo; need, and project evaluation.&nbsp; This project was also a USA-Italy collaboration involving four U.S. universities, STIIMA-National Research Council of Italy, and industry.</p><br>\n<p>\n Last Modified: 12/21/2024<br>\nModified by: Zhaozheng&nbsp;Yin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAdvancements in sensors, computational intelligence, and big data analytics have been rapidly transforming and revolutionizing the manufacturing industry towards robot-rich and digitally connected factories. To facilitate such a transformative process, coordination between human workforce and robots has remained a major challenge, with human operators and robots being strictly separated due to safety concerns. The proposed research addresses this challenge by introducing a seamless integration of robots with humans within a shared workspace where both parties adapt to each other through perceived behaviors and intent, to interact physically and operate collectively. Particularly, various algorithms, methods and systems are developed, in order to (1) sense and cognize where objects (i.e., robots, humans, machines, parts and tools) are located and what each worker is doing; (2) predict what the next human action will be and the trajectory of the human worker's motion as part of the operational sequence for assembling a certain object/system; and (3) plan and control the safe and optimal robot trajectories to provide individualized on-the-job coordination and assistance for humans and avoid worker injury proactively.\r\n\n\nTo sense and understand the manufacturing environment and workers in a more comprehensive way, we propose multi-modal approaches for manufacturing object detection and worker activity recognition using IMU (Inertial Measurement Unit) signals, Electromyography (EMG) signals, images/videos from webcams, and skeleton models from Kinect/visual sensors. Novel feature transform was developed and applied to the IMU/EMG signals in both spatial and temporal domains. Visual features are extracted from both frame level and video clip level, leading to (1) multiple object detection algorithms based on feature aggregation, memory bank, and local continuity; (2) multiple video-based action analysis algorithms including action classification, action localization, action anticipation, and online action detection; and (3) 3D human pose reconstruction and prediction for action analysis. In addition, we have designed a dual-zone Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model to recognize assembly steps by (1) partitioning the visual input into an assembly zone and a tool-and-part zone and (2) leveraging scene-level features through the CNN and temporal-level features through the LSTM. We have developed a repetitive action counting model with skeleton landmarks and joint angles using a Video Transformer network. We have developed an approach to unmanned aerial vehicle (UAV) control through electrooculography (EOG) based eye movement tracking, which translates eye movements into UAV navigation commands using (1) thresholding algorithm and (2) LSTM based eye gaze controlled GUI. The experimental results of UAV control through EOG-based eye tracking showcase a unique integration of biometric technology in UAV control and demonstrates a new direction in human-robot interaction.\r\n\n\nAlgorithms to transform sensing, cognition, and prediction data into robot behaviors, collectively known as the Proactive Adaptive Collaborative Intelligence (PACI) controller, were developed. Robot behaviors include adaptations to robot motion and robot sequencing. To intelligently adapt robot behaviors, research included creating a framework for segmenting robot motion and determining allowable robot behavior modifications; then integrating this with algorithms to detect and predict human motion while maintaining productivity and safety. Key outcomes, achievements and activities also include: (1) internships providing immersive R&D experience and access to industry testbeds for verification and validation (V&V) testing of the project algorithms and methodology, (2) engagement of Florida Advanced Technology Education Programs for broader impact of the research, (3) Decision Switch Module (DSM) enhanced switching logic that balances robot motion planning, (4) HRC algorithms that incorporate industry required ISO protocols and standards, and (5) PACI robot behaviors/reactions maintaining safety and production efficiency.\r\n\n\nThis interdisciplinary research brought together experts from sensing, machine learning, modeling, robotics, and advanced manufacturing, creating a truly synergetic effort for a systematic study of the challenging human-robot collaboration problem. The project provided unique and exciting opportunities for participating graduate students and research associates, with research outcomes disseminated via numerous publications. All the graduated PhD students are either working as Postdocs or related companies in the U.S currently. At the end of the project, one workshop on Human-Robot Collaboration & AI Integration was organized with IMECE in November 2023, with participants from academia, government and industrial companies working in the fields of robotics and manufacturing, providing a forum for review of technological advancement, industries need, and project evaluation. This project was also a USA-Italy collaboration involving four U.S. universities, STIIMA-National Research Council of Italy, and industry.\t\t\t\t\tLast Modified: 12/21/2024\n\n\t\t\t\t\tSubmitted by: ZhaozhengYin\n"
 }
}