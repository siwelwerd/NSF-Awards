{
 "awd_id": "2006618",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Speech-Centered Robust and Generalizable Measurements of \"In the Wild\" Behavior for Mental Health Symptom Severity Tracking",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2020-08-18",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Bipolar disorder is a common and chronic illness characterized by pathological swings from euthymia (healthy) to mania (heightened energy) and depression (lowered energy).  Mood transitions are associated with profound consequences to one's personal, social, vocational, and financial well-being.  Current management is clinic-based and dependent on provider-patient interactions. Yet, increased demand for services has surpassed capacity, calling for radical changes in the delivery of care.  This project will create new algorithms that can process speech data naturally collected from smartphone use to measure behavior and changes in behaviors and to associate these measurements with the severity of the symptoms of bipolar disorder.  This will lead to the creation of new early warning signs, indications that clinical intervention is needed.  \r\n\r\nNatural behavior provides a wealth of information about the health an individual.  However, when assessing health, clinicians typically access cross-sectional medical data at point-of-care that is based on traditional medical methods (exams, labs, and surveys).  Next generation 'precision health' depends on an inclusive and holistic approach that captures changes in health as people live their lives. This is highly relevant as 130 million Americans live with chronic disease and need efficient monitoring strategies. Speech is a promising medium for monitoring mood. Clinicians subjectively assess both form and content of speech when evaluating human disease, as speech is altered by changes in mood and health states. Yet, while speech is easy to record, speech-centered mobile monitoring solutions are not currently publicly available.  The technology is neither sufficiently accurate nor robust.  The central challenge is the signal itself: speech is inherently variable and complex.  Existing techniques are insufficient to handle this complexity, limiting the accuracy and robustness of speech-centered mood monitoring technologies. This project will create novel and robust approaches to extracting mood symptom severity measures from speech. Mood is clinically quantified via the Hamilton Depression Rating Scale (HamD) and the Young Mania Rating Scale (YMRS).  The technology focuses on the creation of methods that accurately extract symptom-focused measures, whose variation lies between that of speech and mood severity, and that are robust to conditions, both environmental and social, in which the data were recorded.  The methods will be validated on an existing natural speech dataset at the University of Michigan.  The unification will provide critical steps towards speech-centered mHealth solutions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Emily",
   "pi_last_name": "Provost",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Emily M Provost",
   "pi_email_addr": "emilykmp@umich.edu",
   "nsf_id": "000607930",
   "pi_start_date": "2020-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Melvin",
   "pi_last_name": "McInnis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Melvin McInnis",
   "pi_email_addr": "mmcinnis@med.umich.edu",
   "nsf_id": "000707634",
   "pi_start_date": "2020-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "3003 South State St. Room 1062",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Title:</strong>&nbsp;RI: Small: Speech-Centered Robust and Generalizable Measurements of \"In the Wild\" Behavior for Mental Health Symptom Severity Tracking</p>\r\n<p>This project explored how natural, unstructured speech could be used to monitor mood states in individuals with bipolar disorder (BD). BD is a serious mental health condition marked by shifts in mood and energy between healthy (euthymia) and pathological states (mania and depression). Maintaining a healthy state is critical because episodes of mania or depression can disrupt personal, social, and work life, with severe cases leading to self-harm. Currently, mood monitoring relies on infrequent clinical evaluations, which are costly and resource-intensive. This project aimed to address this gap by developing innovative, speech-based tools for continuous and passive mood monitoring.</p>\r\n<p>Speech offers a unique opportunity for mood detection because it is easy to collect as part of daily life and is influenced by mood changes. However, the complexity and variability of speech make it challenging to build accurate, reliable tools. This project focused on creating features, or measurements, that could determine whether an individual is experiencing heightened mood severity. These features were designed to capture emotional variability, disfluencies, and linguistic patterns such as generic-you (e.g., &ldquo;you reap what you sow&rdquo;), all of which are linked to mood changes.</p>\r\n<p>Emotion is expressed through multiple modalities, including language and vocal tones, and mismatches between these modalities can indicate emotional disorders. Traditionally, emotion recognition relies on calculating statistics of low-level acoustic features (e.g., Mel Filterbanks) over sentences. In this project, we designed new features to describe differences between emotional and non-emotional data. We created new features that could transform inexpressive speech into expressive speech, and used these features to recognize emotion. We showed that these new features outperformed other self-supervised techniques and advancing emotion recognition frameworks. Additionally, we developed Emotional MisMatch (EMM), a feature quantifying discrepancies between language and acoustic emotion. Using a longitudinal dataset of individuals with BD, we demonstrated that symptomatic episodes showed significantly more EMM than euthymic moods. This automated mood identification system outperformed traditional methods, showcasing EMM's value in mood classification. Yet, one of the remaining challenges in emotion recognition is how to increase the accessibility of these tools.&nbsp;&nbsp;We investigated how to make the adaptation (i.e., fine-tuning) of large models more efficient. Fine-tuning is often resource-intensive, which limits access to advanced technologies for smaller research teams. By developing methods to streamline this process, we have contributed to democratizing access to cutting-edge tools, enabling broader participation in emotion research and speech analysis.</p>\r\n<p>Disfluencies, or interruptions in speech flow, are common in real-world communication but often overlooked in laboratory studies. We developed techniques to automatically detect and label disfluencies in noisy, unstructured recordings. These tools have provided an avenue for future work into how changes in language patterns can be used to uncover changes in mood symptom severity and have contributed to more robust speech-based monitoring systems.</p>\r\n<p>Another focus of this project was linguistic analysis of generic-you, a linguistic device linked to meaning-making. Analyzing Reddit data, we found that \"generic you\" enhanced persuasiveness, even when accounting for other factors. This study highlights how subtle linguistic choices can influence communication outcomes and provides avenues to explore how and whether these subtle linguistic devices can provide new insights into mood symptom severity in natural speech samples.</p>\r\n<p>Finally, we provided information to the community through a review published in IEEE Transactions on Affective Computing. We outlined challenges in emotion recognition, including the complexity of emotions, labeling difficulties, and ethical concerns around privacy and fairness. This work provides guidelines for responsible, human-centered computing applications in healthcare and education, advancing the field of emotion recognition.</p>\r\n<p>The intellectual merit of this project lies in its advancement of speech and language analysis. By addressing the complexities of real-world data, we developed tools and methods that tackle critical challenges in speech-based technologies, improving our ability to recognize mood severity in individuals with BD. This work enhances our understanding of human communication and lays a foundation for innovations in emotion recognition and mood monitoring.</p>\r\n<p>The broader impacts of this research are significant. Passive, speech-based mood monitoring could transform mental health care by enabling early intervention and better management of BD. The tools and methods developed also have applications in human-computer interaction, communication science, and artificial intelligence. By prioritizing accessibility, we ensured that these advances benefit a wide range of researchers and practitioners.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/12/2025<br>\nModified by: Emily&nbsp;M&nbsp;Provost</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nTitle:RI: Small: Speech-Centered Robust and Generalizable Measurements of \"In the Wild\" Behavior for Mental Health Symptom Severity Tracking\r\n\n\nThis project explored how natural, unstructured speech could be used to monitor mood states in individuals with bipolar disorder (BD). BD is a serious mental health condition marked by shifts in mood and energy between healthy (euthymia) and pathological states (mania and depression). Maintaining a healthy state is critical because episodes of mania or depression can disrupt personal, social, and work life, with severe cases leading to self-harm. Currently, mood monitoring relies on infrequent clinical evaluations, which are costly and resource-intensive. This project aimed to address this gap by developing innovative, speech-based tools for continuous and passive mood monitoring.\r\n\n\nSpeech offers a unique opportunity for mood detection because it is easy to collect as part of daily life and is influenced by mood changes. However, the complexity and variability of speech make it challenging to build accurate, reliable tools. This project focused on creating features, or measurements, that could determine whether an individual is experiencing heightened mood severity. These features were designed to capture emotional variability, disfluencies, and linguistic patterns such as generic-you (e.g., you reap what you sow), all of which are linked to mood changes.\r\n\n\nEmotion is expressed through multiple modalities, including language and vocal tones, and mismatches between these modalities can indicate emotional disorders. Traditionally, emotion recognition relies on calculating statistics of low-level acoustic features (e.g., Mel Filterbanks) over sentences. In this project, we designed new features to describe differences between emotional and non-emotional data. We created new features that could transform inexpressive speech into expressive speech, and used these features to recognize emotion. We showed that these new features outperformed other self-supervised techniques and advancing emotion recognition frameworks. Additionally, we developed Emotional MisMatch (EMM), a feature quantifying discrepancies between language and acoustic emotion. Using a longitudinal dataset of individuals with BD, we demonstrated that symptomatic episodes showed significantly more EMM than euthymic moods. This automated mood identification system outperformed traditional methods, showcasing EMM's value in mood classification. Yet, one of the remaining challenges in emotion recognition is how to increase the accessibility of these tools.We investigated how to make the adaptation (i.e., fine-tuning) of large models more efficient. Fine-tuning is often resource-intensive, which limits access to advanced technologies for smaller research teams. By developing methods to streamline this process, we have contributed to democratizing access to cutting-edge tools, enabling broader participation in emotion research and speech analysis.\r\n\n\nDisfluencies, or interruptions in speech flow, are common in real-world communication but often overlooked in laboratory studies. We developed techniques to automatically detect and label disfluencies in noisy, unstructured recordings. These tools have provided an avenue for future work into how changes in language patterns can be used to uncover changes in mood symptom severity and have contributed to more robust speech-based monitoring systems.\r\n\n\nAnother focus of this project was linguistic analysis of generic-you, a linguistic device linked to meaning-making. Analyzing Reddit data, we found that \"generic you\" enhanced persuasiveness, even when accounting for other factors. This study highlights how subtle linguistic choices can influence communication outcomes and provides avenues to explore how and whether these subtle linguistic devices can provide new insights into mood symptom severity in natural speech samples.\r\n\n\nFinally, we provided information to the community through a review published in IEEE Transactions on Affective Computing. We outlined challenges in emotion recognition, including the complexity of emotions, labeling difficulties, and ethical concerns around privacy and fairness. This work provides guidelines for responsible, human-centered computing applications in healthcare and education, advancing the field of emotion recognition.\r\n\n\nThe intellectual merit of this project lies in its advancement of speech and language analysis. By addressing the complexities of real-world data, we developed tools and methods that tackle critical challenges in speech-based technologies, improving our ability to recognize mood severity in individuals with BD. This work enhances our understanding of human communication and lays a foundation for innovations in emotion recognition and mood monitoring.\r\n\n\nThe broader impacts of this research are significant. Passive, speech-based mood monitoring could transform mental health care by enabling early intervention and better management of BD. The tools and methods developed also have applications in human-computer interaction, communication science, and artificial intelligence. By prioritizing accessibility, we ensured that these advances benefit a wide range of researchers and practitioners.\r\n\n\n\t\t\t\t\tLast Modified: 01/12/2025\n\n\t\t\t\t\tSubmitted by: EmilyMProvost\n"
 }
}