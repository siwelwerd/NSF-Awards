{
 "awd_id": "2006747",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Embracing Deep Neural Networks into Probabilistic Answer Set Programming",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 458499.0,
 "awd_amount": 458499.0,
 "awd_min_amd_letter_date": "2020-08-19",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "The integration of low-level perception with high-level reasoning is one of the fundamental problems in artificial intelligence. Today, the topic is revisited with the recent rise of deep neural networks. While deep learning excels in many perception tasks, it is not obvious how multiple aspects of commonsense reasoning, such as causality, defaults, abductive reasoning, and counterfactual reasoning, can be computed by neural networks. These subjects have been well-studied in the area of knowledge representation (KR) including answer set programming (ASP) but most KR formalisms are logic-oriented and do not incorporate high-dimensional feature space and pre-trained models for vision and text as in deep learning, which limits the applicability of KR in many practical applications involving uncertainty. The goal of the proposed research is to investigate a principled combination of knowledge representation, reasoning, and learning by integrating answer set programming with neural networks, which will enable representation, inference, and learning in both symbolic and sub-symbolic levels. \r\n\r\nThe project will investigate two different approaches to integration. One is a loose coupling that is based on the concept of neural atoms which serves as an interface between the neural network output and the parameters for probabilistic answer set programming. The other is a tighter coupling method that obtains fuzzy-valued atomic facts from the neural network and applies the fuzzy answer set semantics on the vectorized representation. Not only these methods allow for applying symbolic reasoning on the neural network perception result but also allow for making use of logical rules in training a neural network so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by ASP rules. The success of the project will contribute to identifying fundamental issues in bridging the gap between knowledge representation and machine learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joohyung",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Joohyung Lee",
   "pi_email_addr": "joolee@asu.edu",
   "nsf_id": "000492036",
   "pi_start_date": "2020-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852816011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 458499.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The integration of low-level perception with high-level reasoning remains a fundamental challenge in Artificial Intelligence. While deep learning has demonstrated remarkable proficiency in various perception tasks, its ability to incorporate commonsense knowledge and reasoning remains less apparent. Commonsense knowledge, often not explicitly present in data, is challenging to acquire automatically. Furthermore, it remains unclear how neural networks can effectively handle multiple facets of commonsense reasoning, including causality, defaults, inductive definitions, and counterfactual reasoning. This subject has been extensively explored in the field of knowledge representation (KR), particularly in answer set programming. However, most KR formalisms are logic-oriented and do not integrate the high-dimensional feature spaces and pre-trained models for vision and text prevalent in deep learning. This limitation restricts the applicability of KR in practical applications that involve uncertainty.</p>\r\n<p>The project explored a principled approach to combining knowledge representation, reasoning, and learning by integrating symbolic AI with neural networks. This integration facilitated representation, inference, and learning across both symbolic and subsymbolic levels. The following key outcomes were achieved:</p>\r\n<ol>\r\n<li><strong>NeurASP</strong>:      NeurASP extends answer set programs by incorporating neural networks. By      treating neural network outputs as probability distributions over atomic      facts in answer set programs, NeurASP offers a simple yet effective method      to integrate subsymbolic and symbolic computation. This framework enables      the training of neural networks using ASP rules, allowing them to learn      not only from implicit correlations in the data but also from explicit,      complex semantic constraints expressed by the rules.</li>\r\n<li><strong>Injecting      Logical Constraints into Neural Networks via Straight-Through Estimators</strong>:      The project demonstrated that a straight-through estimator (STE)      effectively incorporates logical constraints into neural network learning.      By minimizing the loss function with STE, neural networks enforce logical      constraints during learning, resulting in improved performance through      explicit constraint-based learning.</li>\r\n<li><strong>Symbolic      Reasoning to Orchestrate Neural Computation for Counterfactual Query      Answering</strong>: Causal and temporal reasoning about video dynamics poses a      significant challenge in AI. Prior neuro-symbolic models applying symbolic      reasoning over neural-based recognition and prediction have shown promise      but remained limited in addressing counterfactual questions. This project      highlighted the benefits of explicit causal modeling to enhance event      prediction, securing first place in the CLEVRER challenge.</li>\r\n<li><strong>Recurrent      Transformer</strong>: Constraint satisfaction problems (CSPs) involve finding      variable values that satisfy given constraints. The project demonstrated      that a Transformer model extended with recurrence is a viable approach to      solving CSPs in an end-to-end manner. This approach outperformed      state-of-the-art methods, such as Graph Neural Networks, SATNet, and other      neuro-symbolic models.</li>\r\n</ol>\r\n<p>Although the initial project scope did not include large language models (LLMs), advancements in LLMs and their reasoning capabilities closely aligned with the project&rsquo;s goals. Consequently, the research explored the reasoning abilities of LLMs. Despite the significant progress of LLMs in few-shot learning and chain-of-thought prompting, challenges persist in achieving consistent, coherent generations and solving complex reasoning problems. The project identified ways to leverage LLM strengths in semantic parsing and commonsense knowledge generation while utilizing symbolic AI's automated reasoning capabilities. The following methods were developed:</p>\r\n<ol>\r\n<li><strong>LLM+ASP</strong>:      The project demonstrated that LLMs could serve as effective few-shot      semantic parsers, converting natural language sentences into logical forms      suitable for input into answer set programs (ASPs). This combination      created a robust system capable of handling various question-answering      tasks without retraining for each new task. The approach required only a      few examples to guide LLM adaptation, with reusable ASP knowledge modules      applicable across multiple tasks.</li>\r\n<li><strong>LLM+AL</strong>:      A method termed \"LLM+AL\" bridged the natural language      understanding capabilities of LLMs with the symbolic reasoning strengths      of action languages. This approach combined the LLM's semantic parsing and      commonsense knowledge generation abilities with the action language's      automated reasoning capabilities. Despite occasional errors, LLM+AL      consistently delivered correct answers with minimal human corrections,      outperforming standalone LLMs.</li>\r\n<li><strong>Generating      ASP Programs via LLMs</strong>: The project demonstrated that LLMs could      generate answer set programs, overcoming the challenge of manual knowledge      encoding. With well-crafted prompts, pretrained LLMs produced reasonably      accurate ASPs, benefiting from the declarative semantics of ASP.</li>\r\n</ol>\r\n<p>In conclusion, the project successfully explored methods to integrate logical reasoning with neural network learning. The outcomes highlighted the significant advantages of combining symbolic AI and neural networks over their standalone use, advancing the field of neuro-symbolic AI.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/05/2025<br>\nModified by: Joohyung&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974593645_crcg--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974593645_crcg--rgov-800width.png\" title=\"Counterfactual Reasoning using a Causal Graph\"><img src=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974593645_crcg--rgov-66x44.png\" alt=\"Counterfactual Reasoning using a Causal Graph\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering</div>\n<div class=\"imageCredit\">J. Lee</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Joohyung&nbsp;Lee\n<div class=\"imageTitle\">Counterfactual Reasoning using a Causal Graph</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974772368_llm_al--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974772368_llm_al--rgov-800width.png\" title=\"LLM+AL Pipeline\"><img src=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974772368_llm_al--rgov-66x44.png\" alt=\"LLM+AL Pipeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions</div>\n<div class=\"imageCredit\">J. Lee</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Joohyung&nbsp;Lee\n<div class=\"imageTitle\">LLM+AL Pipeline</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974509804_ste--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974509804_ste--rgov-800width.png\" title=\"Logical constraint learning via straight-through estimators\"><img src=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974509804_ste--rgov-66x44.png\" alt=\"Logical constraint learning via straight-through estimators\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Injecting Logical Constraints into Neural Networks via Straight-Through Estimators</div>\n<div class=\"imageCredit\">J. Lee</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Joohyung&nbsp;Lee\n<div class=\"imageTitle\">Logical constraint learning via straight-through estimators</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974391132_recurrent_transformer--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974391132_recurrent_transformer--rgov-800width.png\" title=\"Recurrent transformer experiment\"><img src=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974391132_recurrent_transformer--rgov-66x44.png\" alt=\"Recurrent transformer experiment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Performance of recurrent transformer on unfounded visual sudoku problem</div>\n<div class=\"imageCredit\">J. Lee</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Joohyung&nbsp;Lee\n<div class=\"imageTitle\">Recurrent transformer experiment</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974715864_llm_asp--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974715864_llm_asp--rgov-800width.png\" title=\"LLM+ASP: dual process, neuro-symbolic approach\"><img src=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974715864_llm_asp--rgov-66x44.png\" alt=\"LLM+ASP: dual process, neuro-symbolic approach\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text</div>\n<div class=\"imageCredit\">J. Lee</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Joohyung&nbsp;Lee\n<div class=\"imageTitle\">LLM+ASP: dual process, neuro-symbolic approach</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974314415_neurasp_learning--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974314415_neurasp_learning--rgov-800width.png\" title=\"NeurASP learning\"><img src=\"/por/images/Reports/POR/2025/2006747/2006747_10699318_1735974314415_neurasp_learning--rgov-66x44.png\" alt=\"NeurASP learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The gradient computation of neurasp learning</div>\n<div class=\"imageCredit\">J. Lee</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Joohyung&nbsp;Lee\n<div class=\"imageTitle\">NeurASP learning</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe integration of low-level perception with high-level reasoning remains a fundamental challenge in Artificial Intelligence. While deep learning has demonstrated remarkable proficiency in various perception tasks, its ability to incorporate commonsense knowledge and reasoning remains less apparent. Commonsense knowledge, often not explicitly present in data, is challenging to acquire automatically. Furthermore, it remains unclear how neural networks can effectively handle multiple facets of commonsense reasoning, including causality, defaults, inductive definitions, and counterfactual reasoning. This subject has been extensively explored in the field of knowledge representation (KR), particularly in answer set programming. However, most KR formalisms are logic-oriented and do not integrate the high-dimensional feature spaces and pre-trained models for vision and text prevalent in deep learning. This limitation restricts the applicability of KR in practical applications that involve uncertainty.\r\n\n\nThe project explored a principled approach to combining knowledge representation, reasoning, and learning by integrating symbolic AI with neural networks. This integration facilitated representation, inference, and learning across both symbolic and subsymbolic levels. The following key outcomes were achieved:\r\n\r\nNeurASP:      NeurASP extends answer set programs by incorporating neural networks. By      treating neural network outputs as probability distributions over atomic      facts in answer set programs, NeurASP offers a simple yet effective method      to integrate subsymbolic and symbolic computation. This framework enables      the training of neural networks using ASP rules, allowing them to learn      not only from implicit correlations in the data but also from explicit,      complex semantic constraints expressed by the rules.\r\nInjecting      Logical Constraints into Neural Networks via Straight-Through Estimators:      The project demonstrated that a straight-through estimator (STE)      effectively incorporates logical constraints into neural network learning.      By minimizing the loss function with STE, neural networks enforce logical      constraints during learning, resulting in improved performance through      explicit constraint-based learning.\r\nSymbolic      Reasoning to Orchestrate Neural Computation for Counterfactual Query      Answering: Causal and temporal reasoning about video dynamics poses a      significant challenge in AI. Prior neuro-symbolic models applying symbolic      reasoning over neural-based recognition and prediction have shown promise      but remained limited in addressing counterfactual questions. This project      highlighted the benefits of explicit causal modeling to enhance event      prediction, securing first place in the CLEVRER challenge.\r\nRecurrent      Transformer: Constraint satisfaction problems (CSPs) involve finding      variable values that satisfy given constraints. The project demonstrated      that a Transformer model extended with recurrence is a viable approach to      solving CSPs in an end-to-end manner. This approach outperformed      state-of-the-art methods, such as Graph Neural Networks, SATNet, and other      neuro-symbolic models.\r\n\r\n\n\nAlthough the initial project scope did not include large language models (LLMs), advancements in LLMs and their reasoning capabilities closely aligned with the projects goals. Consequently, the research explored the reasoning abilities of LLMs. Despite the significant progress of LLMs in few-shot learning and chain-of-thought prompting, challenges persist in achieving consistent, coherent generations and solving complex reasoning problems. The project identified ways to leverage LLM strengths in semantic parsing and commonsense knowledge generation while utilizing symbolic AI's automated reasoning capabilities. The following methods were developed:\r\n\r\nLLM+ASP:      The project demonstrated that LLMs could serve as effective few-shot      semantic parsers, converting natural language sentences into logical forms      suitable for input into answer set programs (ASPs). This combination      created a robust system capable of handling various question-answering      tasks without retraining for each new task. The approach required only a      few examples to guide LLM adaptation, with reusable ASP knowledge modules      applicable across multiple tasks.\r\nLLM+AL:      A method termed \"LLM+AL\" bridged the natural language      understanding capabilities of LLMs with the symbolic reasoning strengths      of action languages. This approach combined the LLM's semantic parsing and      commonsense knowledge generation abilities with the action language's      automated reasoning capabilities. Despite occasional errors, LLM+AL      consistently delivered correct answers with minimal human corrections,      outperforming standalone LLMs.\r\nGenerating      ASP Programs via LLMs: The project demonstrated that LLMs could      generate answer set programs, overcoming the challenge of manual knowledge      encoding. With well-crafted prompts, pretrained LLMs produced reasonably      accurate ASPs, benefiting from the declarative semantics of ASP.\r\n\r\n\n\nIn conclusion, the project successfully explored methods to integrate logical reasoning with neural network learning. The outcomes highlighted the significant advantages of combining symbolic AI and neural networks over their standalone use, advancing the field of neuro-symbolic AI.\r\n\n\n\t\t\t\t\tLast Modified: 01/05/2025\n\n\t\t\t\t\tSubmitted by: JoohyungLee\n"
 }
}