{
 "awd_id": "1939725",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Towards a Computational Foundation for Fair Network Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 585592.0,
 "awd_amount": 601592.0,
 "awd_min_amd_letter_date": "2019-12-23",
 "awd_max_amd_letter_date": "2020-06-10",
 "awd_abstract_narration": "Network learning and mining plays a pivotal role across a number of disciplines, such as computer science, physics, social science, management, neural science, civil engineering, and e-commerce. Decades of research in this area has provided a wealth of theories, algorithms and open-source systems to answer who/what types of questions. For example, who is the most influential in a social network? What items shall we recommend to a given user on an e-commerce platform? What Twitter poster is likely to go viral? Who can be grouped into the same online community? What financial transactions between users look suspicious? The state-of-the-art techniques on answering these questions have been widely adopted in various real-world applications, often with a strong empirical performance as well as a solid theoretic foundation. Despite the remarkable progress in network learning, a fundamental question largely remains nascent: how can we make network learning results and process explainable, transparent, and fair? The answer to this question benefits a variety of high-impact network learning based applications in terms of their interpretability, transparency and fairness, including social network analysis, neural science, team science and management, intelligent transportation systems, critical infrastructures, and blockchain networks.\r\n\r\nThis project takes a shift for network learning, from answering who and what to answering how and why. It develops computational theories, algorithms and prototype systems in the context of network learning, forming three key pillars of fair network learning. The first pillar (interpretation) focuses on explaining the network learning results and process to end users, who are often not machine learning experts. In particular, this project develops theory and metrics to quantify the quality of explanations for network learning. Based on that, it brings explainability to network learning algorithms by carefully balancing the model fidelity and model interpretability. The second pillar (auditing) makes the network learning process transparent to end-users, focusing on demonstrating how the learning results of a given network learning algorithm relate to the underlying network structure. In particular, it develops a new fairness measure to accommodate the non-independent-and-identically-distributed nature of network learning. Based on this new fairness measure, it develops an algorithmic framework to audit a variety of network learning algorithms. The third pillar (de-biasing) explores how to mitigate potential biases to ensure fair network learning. Underpinning these pillars is a human-in-the-loop visual analytics framework to support users in identifying and mitigating bias in network learning. By assimilating the research outcome into the courses and summer programs that the research team has developed, this project trains students to value the spirit of fairness.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hanghang",
   "pi_last_name": "Tong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hanghang Tong",
   "pi_email_addr": "htong@illinois.edu",
   "nsf_id": "000648427",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "My",
   "pi_last_name": "Thai",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "My T Thai",
   "pi_email_addr": "mythai@cise.ufl.edu",
   "nsf_id": "000391957",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ross",
   "pi_last_name": "Maciejewski",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ross Maciejewski",
   "pi_email_addr": "rmacieje@asu.edu",
   "nsf_id": "000616070",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "Board of Trustees of the University of Illinois",
  "perf_str_addr": "506 S. Wright Street",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618012302",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0757",
   "pgm_ref_txt": "COOP PLAN OPs & SERVICES"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 601592.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-21f45756-7fff-3dbd-0714-029d05992974\"> </span></p>\r\n<p dir=\"ltr\"><span>Network learning and mining plays an important role in a number of important applications, including social network analysis, neural science, intelligent transportation systems, critical infrastructures, and blockchain networks. The great progress in network learning has been made in this area and a rich set of theories, algorithms and open-source systems have been developed, often with a strong empirical performance as well as a solid theoretic foundation. However, a fundamental question is relatively less studied, that is, how can we make network learning results and processes explainable, transparent, and fair? The answer to this question benefits a variety of high-impact network learning based applications in terms of their interpretability, transparency and fairness.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>This project developed computational theories, algorithms and prototype systems in the context of fair network learning. During the course of this project, we have made progress in the following three aspects.&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>First, in terms of fairness algorithms, we </span><span>developed a quantitative bias measure, three debiasing algorithms and an upper-bound to quantify the cost of enforcing individual fairness on graph mining. We further investigated individual fairness on the multi-view graph mining setting, and developed an effective solver based on multi-objective optimization that minimizes the utility loss while promoting individual fairness for multi-view graph clustering. In addition, we </span><span>developed a new method for graph neural networks based on Fused Gromov-Wasserstein (FGW) distance to ensure group fairness under a different distribution and </span><span>developed a new end-to-end algorithmic framework for multi-group fairness to ensure multiple sensitive attributes that are generalizable to various statistical notations of fairness. We also investigated degree-related fairness for graph convolutional networks (GCN), and in particular, we revealed its mathematical root cause via the analysis of gradients of weight matrices in GCN and developed two easy-to-implement algorithms to mitigate degree-related bias. For the machine learning model debiasing, </span><span>we developed an algorithm to reduce unfairness in the training data which enjoys a guaranteed improvement in demographic parity at the expense of a bounded reduction in balanced accuracy. Moreover, we investigated how to sustain a fair classifier across a sequence of learning tasks with covariate shifts and unlabeled data, and </span><span>&nbsp;developed an algorithm with optimized task sequence to mitigate the stability and plasticity trade-off.</span><span> We have also investigated the Group Fairness-aware Continual Learning (GFCL) which aims to eradicate discriminatory predictions against certain demo-graphic groups in a sequence of diverse learning tasks. We further explored an even more challenging GFCL problem &ndash; how to sustain a fair classifier across a sequence of tasks with covariate shifts and unlabeled data. We proposed the MacFRL solution, with its key idea to optimize the sequence of learning tasks. </span><span>Finally, we </span><span>developed a meta learning-based framework for</span><span> deceptive fairness attacks on graph neural networks, </span><span>that is broadly applicable with respect to various fairness definitions and graph learning models, as well as different choices of manipulation operations.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Second, in terms of explainability, we developed a model-agnostic method to explain the prediction results for graph neural networks, and </span><span>an explainer for temporal Graph Neural Network.</span><span> In addition, we developed a new model with a novel explainable features fusion technique for explaining knowledge distillation, which not only maintains the teacher's performance but also approximates the teacher's explanation and it significantly reduces the total number of student parameters while ensuring the consistency between teacher's and student's explanations. </span><span>We further designed a novel metric to evaluate the representation power of the explanation method and an algorithm to discover the internal mechanisms of neural networks in generating its predictions. We established </span><span>the fundamental limitations of perturbation-based explanation methods for temporal graph neural networks, by theoretically revealing that node-perturbation or&nbsp; edge-perturbation cannot reliably determine the paths that carry the prediction, or determine what nodes contribute to the prediction, thus cannot providing a faithful explanation.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Third, in terms of visual analytics systems, we developed a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm. We further designed </span><span>a novel framework to enable the exploration of multi-class bias in graph mining algorithms.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>The outcome of this project was broadly disseminated in the form of publications and presentations at various journals and conferences, tutorials and invited talks, as well as open-source softwares. During the course of the project, it trained multiple graduate and undergraduate students. The project also enriched the courses that PIs teach on data mining, theoretical computer science and visualization at their respective institutes and generated multiple tutorials on the related topics at conferences.&nbsp;&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/26/2025<br>\nModified by: Hanghang&nbsp;Tong</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nNetwork learning and mining plays an important role in a number of important applications, including social network analysis, neural science, intelligent transportation systems, critical infrastructures, and blockchain networks. The great progress in network learning has been made in this area and a rich set of theories, algorithms and open-source systems have been developed, often with a strong empirical performance as well as a solid theoretic foundation. However, a fundamental question is relatively less studied, that is, how can we make network learning results and processes explainable, transparent, and fair? The answer to this question benefits a variety of high-impact network learning based applications in terms of their interpretability, transparency and fairness.\r\n\n\n\r\n\n\nThis project developed computational theories, algorithms and prototype systems in the context of fair network learning. During the course of this project, we have made progress in the following three aspects.\r\n\n\n\r\n\n\nFirst, in terms of fairness algorithms, we developed a quantitative bias measure, three debiasing algorithms and an upper-bound to quantify the cost of enforcing individual fairness on graph mining. We further investigated individual fairness on the multi-view graph mining setting, and developed an effective solver based on multi-objective optimization that minimizes the utility loss while promoting individual fairness for multi-view graph clustering. In addition, we developed a new method for graph neural networks based on Fused Gromov-Wasserstein (FGW) distance to ensure group fairness under a different distribution and developed a new end-to-end algorithmic framework for multi-group fairness to ensure multiple sensitive attributes that are generalizable to various statistical notations of fairness. We also investigated degree-related fairness for graph convolutional networks (GCN), and in particular, we revealed its mathematical root cause via the analysis of gradients of weight matrices in GCN and developed two easy-to-implement algorithms to mitigate degree-related bias. For the machine learning model debiasing, we developed an algorithm to reduce unfairness in the training data which enjoys a guaranteed improvement in demographic parity at the expense of a bounded reduction in balanced accuracy. Moreover, we investigated how to sustain a fair classifier across a sequence of learning tasks with covariate shifts and unlabeled data, and developed an algorithm with optimized task sequence to mitigate the stability and plasticity trade-off. We have also investigated the Group Fairness-aware Continual Learning (GFCL) which aims to eradicate discriminatory predictions against certain demo-graphic groups in a sequence of diverse learning tasks. We further explored an even more challenging GFCL problem  how to sustain a fair classifier across a sequence of tasks with covariate shifts and unlabeled data. We proposed the MacFRL solution, with its key idea to optimize the sequence of learning tasks. Finally, we developed a meta learning-based framework for deceptive fairness attacks on graph neural networks, that is broadly applicable with respect to various fairness definitions and graph learning models, as well as different choices of manipulation operations.\r\n\n\n\r\n\n\nSecond, in terms of explainability, we developed a model-agnostic method to explain the prediction results for graph neural networks, and an explainer for temporal Graph Neural Network. In addition, we developed a new model with a novel explainable features fusion technique for explaining knowledge distillation, which not only maintains the teacher's performance but also approximates the teacher's explanation and it significantly reduces the total number of student parameters while ensuring the consistency between teacher's and student's explanations. We further designed a novel metric to evaluate the representation power of the explanation method and an algorithm to discover the internal mechanisms of neural networks in generating its predictions. We established the fundamental limitations of perturbation-based explanation methods for temporal graph neural networks, by theoretically revealing that node-perturbation or edge-perturbation cannot reliably determine the paths that carry the prediction, or determine what nodes contribute to the prediction, thus cannot providing a faithful explanation.\r\n\n\n\r\n\n\nThird, in terms of visual analytics systems, we developed a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm. We further designed a novel framework to enable the exploration of multi-class bias in graph mining algorithms.\r\n\n\n\r\n\n\nThe outcome of this project was broadly disseminated in the form of publications and presentations at various journals and conferences, tutorials and invited talks, as well as open-source softwares. During the course of the project, it trained multiple graduate and undergraduate students. The project also enriched the courses that PIs teach on data mining, theoretical computer science and visualization at their respective institutes and generated multiple tutorials on the related topics at conferences.\r\n\n\n\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 02/26/2025\n\n\t\t\t\t\tSubmitted by: HanghangTong\n"
 }
}