{
 "awd_id": "2008855",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Communication-Aware Decentralized Game-Theoretic Learning Algorithms for Networked Systems with Uncertainty",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 361086.0,
 "awd_amount": 361086.0,
 "awd_min_amd_letter_date": "2020-06-29",
 "awd_max_amd_letter_date": "2020-06-29",
 "awd_abstract_narration": "At the core of the opportunities for many emerging technological networked systems, such as Internet-of-things, smart grid, transportation systems, lies decentralization which means the deployment of interlinked agents, e.g., sensors, appliances, and devices, that operate in harmony. An implicit assumption in the decentralized operations in these systems is that these agents agree or are able to resolve their differences in what is their objective. In large scale networked systems, barring unreasonable accuracy of environmental information and unjustifiable levels of coordination, agents cannot be sure of what other agents are optimizing. This project will enable practical and scalable decentralized solutions to signal processing and communication problems in technological networked systems where wireless network technologies provide the backbone for information exchange. Furthermore, this project will involve mentoring of PhD students, providing research experience to undergraduate students, and the development of a graduate-level course on learning and networks at Texas A&M University.   \r\n\r\nThe premise of this project is that when agents disagree on their objectives in a networked multi-agent system due to environmental uncertainty, they are playing a game against uncertainty, and the optimality criterion is then defined by game-theoretic notions. Upon adopting, game-theoretic equilibria as the optimality criterion, the project is divided into three major thrusts. The first thrust will develop decentralized algorithms based on game-theoretic notions, and will characterize finite-time outcomes when  information aggregation or consensus in large-scale networked systems may not be possible. The second thrust aims to understand fundamental trade-offs in communication vs. optimality, and to develop decentralized algorithms that account for costly and faulty communication. The third thrust will seek to characterize the likelihood of converging to undesirable equilibria, and will use influence maximization methods to avoid bad outcomes. Overall, the project draws upon methods common in decentralized optimization, e.g., consensus, and in signal processing, e.g., communication censoring, and combines them with game-theoretic, best-response type, learning algorithms to design scalable and communication-aware decentralized algorithms.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ceyhun",
   "pi_last_name": "Eksin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ceyhun Eksin",
   "pi_email_addr": "eksinc@tamu.edu",
   "nsf_id": "000765622",
   "pi_start_date": "2020-06-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "3131 TAMU",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433131",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 361086.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Advances in decentralized operations for emerging networked systems&mdash;such as the Internet of Things, smart grids, and transportation networks&mdash;promise efficiency gains. Many distributed optimization algorithms assume agents share a common objective or can resolve their differences. However, in networked multi-agent systems, where objectives depend on others' actions and environmental factors, perfect information and coordination are often unrealistic. Consequently, agents cannot be certain what others are optimizing, making traditional notions of optimality unclear.</p>\r\n<p>This project defines optimality in such settings using game-theoretic equilibrium concepts and develops a scalable, communication-efficient framework for decentralized learning with convergence guarantees.</p>\r\n<p>The key contributions of this project are as follows: 1) One focus of the project was adapting well-known game-theoretic learning methods, such as best-response and fictitious play, to competitive environments where agents have limited and evolving information about their objectives and other agents. The research identified conditions under which these methods lead to stable outcomes, even when agents do not fully agree on the objectives given. 2) We tested these methods in a real-world-inspired scenario where autonomous robots had to collaborate to complete a target assignment task. The results highlighted important trade-offs between how much agents communicate and how well they perform, showing that limited, but smart, communication efforts can still lead to effective coordination. 3) The work was further extended to dynamic multi-agent settings, where agents make decisions over time in response to changes in the environment and the actions of others. We demonstrated that when agents use&nbsp;networked information exchanges&nbsp;alongside&nbsp;reinforcement learning, they can&nbsp;make better decisions while still ensuring stable learning and long-term performance.&nbsp;</p>\r\n<p>Equilibrium outcomes in multi-agent systems may not always align with the goals of a system designer. To address this, we explored strategies to guide system behavior through targeted interventions and information control. 4) A system designer can influence behavior by selecting a limited number of key agents to control. In network games where agents benefit from choosing different actions (anti-coordination), we showed that selecting agents strategically can steer the system toward better outcomes. We formulated this as an optimization problem and proved that a greedy selection approach is nearly optimal, making it a practical strategy for large networks. 5)&nbsp;Instead of directly controlling agents, a designer can adjust how much information agents receive to encourage desirable outcomes. In decision-making scenarios where agents rely on shared information, we analyzed how different levels of disclosure impact overall system performance. Our findings show that full transparency maximizes social welfare in some cases, while in others, selective information sharing leads to more stable and coordinated decisions.</p>\r\n<p>These results offer practical tools for designing better multi-agent systems, allowing external regulators or system designers to subtly guide individual decisions while improving overall system performance.</p>\r\n<p>Beyond its research contributions, the project played an important role in education and training. Over the course of the project, four PhD students were mentored, and several undergraduate students, including those from underrepresented backgrounds, were involved in research activities. A new undergraduate elective course on Algorithmic Game Theory was developed to introduce engineering students to key concepts in multi-agent learning and decentralized decision-making. A graduate course on network models and distributed learning algorithms was also co-developed to provide students with the tools needed to address challenges in autonomous and networked systems. Additionally, we organized a workshop on &ldquo;Multi-Agent Learning in Dynamic Environments&rdquo; at Texas A&amp;M, bringing together field experts, researchers and students to discuss new developments in the field.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/29/2025<br>\nModified by: Ceyhun&nbsp;Eksin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAdvances in decentralized operations for emerging networked systemssuch as the Internet of Things, smart grids, and transportation networkspromise efficiency gains. Many distributed optimization algorithms assume agents share a common objective or can resolve their differences. However, in networked multi-agent systems, where objectives depend on others' actions and environmental factors, perfect information and coordination are often unrealistic. Consequently, agents cannot be certain what others are optimizing, making traditional notions of optimality unclear.\r\n\n\nThis project defines optimality in such settings using game-theoretic equilibrium concepts and develops a scalable, communication-efficient framework for decentralized learning with convergence guarantees.\r\n\n\nThe key contributions of this project are as follows: 1) One focus of the project was adapting well-known game-theoretic learning methods, such as best-response and fictitious play, to competitive environments where agents have limited and evolving information about their objectives and other agents. The research identified conditions under which these methods lead to stable outcomes, even when agents do not fully agree on the objectives given. 2) We tested these methods in a real-world-inspired scenario where autonomous robots had to collaborate to complete a target assignment task. The results highlighted important trade-offs between how much agents communicate and how well they perform, showing that limited, but smart, communication efforts can still lead to effective coordination. 3) The work was further extended to dynamic multi-agent settings, where agents make decisions over time in response to changes in the environment and the actions of others. We demonstrated that when agents usenetworked information exchangesalongsidereinforcement learning, they canmake better decisions while still ensuring stable learning and long-term performance.\r\n\n\nEquilibrium outcomes in multi-agent systems may not always align with the goals of a system designer. To address this, we explored strategies to guide system behavior through targeted interventions and information control. 4) A system designer can influence behavior by selecting a limited number of key agents to control. In network games where agents benefit from choosing different actions (anti-coordination), we showed that selecting agents strategically can steer the system toward better outcomes. We formulated this as an optimization problem and proved that a greedy selection approach is nearly optimal, making it a practical strategy for large networks. 5)Instead of directly controlling agents, a designer can adjust how much information agents receive to encourage desirable outcomes. In decision-making scenarios where agents rely on shared information, we analyzed how different levels of disclosure impact overall system performance. Our findings show that full transparency maximizes social welfare in some cases, while in others, selective information sharing leads to more stable and coordinated decisions.\r\n\n\nThese results offer practical tools for designing better multi-agent systems, allowing external regulators or system designers to subtly guide individual decisions while improving overall system performance.\r\n\n\nBeyond its research contributions, the project played an important role in education and training. Over the course of the project, four PhD students were mentored, and several undergraduate students, including those from underrepresented backgrounds, were involved in research activities. A new undergraduate elective course on Algorithmic Game Theory was developed to introduce engineering students to key concepts in multi-agent learning and decentralized decision-making. A graduate course on network models and distributed learning algorithms was also co-developed to provide students with the tools needed to address challenges in autonomous and networked systems. Additionally, we organized a workshop on Multi-Agent Learning in Dynamic Environments at Texas A&M, bringing together field experts, researchers and students to discuss new developments in the field.\r\n\n\n\t\t\t\t\tLast Modified: 01/29/2025\n\n\t\t\t\t\tSubmitted by: CeyhunEksin\n"
 }
}