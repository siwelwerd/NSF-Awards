{
 "awd_id": "2026584",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FW-HTF-RM: Enhancing Future Work of Nursing Professionals through Collaborative Human-Robot Interfaces",
 "cfda_num": "47.041, 47.076",
 "org_code": "11090000",
 "po_phone": "7032925126",
 "po_email": "abaylor@nsf.gov",
 "po_sign_block_name": "Amy Baylor",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 1499705.0,
 "awd_amount": 1515705.0,
 "awd_min_amd_letter_date": "2020-08-11",
 "awd_max_amd_letter_date": "2022-07-01",
 "awd_abstract_narration": "This Future of Work at the Human-Technology Frontier (FW-HTF) research project advances a vision for the profession of nursing where collaborative human-robot interfaces (CHRIs) can enhance nurse productivity and reduce on-the-job stress. Interfaces in this project are broadly defined as any link between humans and intelligent machines such as robots, using advanced sensors, mobile computing, and display devices. In this project, interfaces will intelligently \u201cadapt\u201d to provide both physical and cognitive assistance to nurses and patients in future healthcare environments. The project will pursue four objectives. The team will develop a taxonomy of nursing tasks to determine those that can be justifiably delegated to intelligent robots. The team will then compare the ability of two novel CHRIs to facilitate stable and effective shared human-robot control of a robot-assisted walking task (i.e., fall prevention). They will also investigate a CHRI recommender system that coordinates patient sitting tasks such as vital signs monitor and item fetching among several nurses and mobile manipulators. The goal here is to determine the optimal number of robotic assistants per group of patients and nurses. Finally, the team will evaluate the social and economic impact of the technology on nurses, patients, and healthcare facilities. The project will promote the progress of science and advance the national health by providing a blueprint for engineering future nursing assistant robots, for informing healthcare facility design to accommodate the robots, and for advancing instruction on the use of intelligent robotic assistants into formal nursing education, nurse training, and credentialing. Other potential benefits of the project include the development of instructional programs in robotics and machine learning at the University of Louisville, involvement of undergraduate nursing students in this research, and outreach to rural primary care clinics and hospital settings through the Center for Health Systems Innovation at the Oklahoma State University.\r\n\r\nThis project includes four objectives: Behavioral observation, documentation reviews, task inventories and critical incidents will be analyzed to develop a task, skill, and context taxonomy to identify nursing tasks that can be assigned to intelligent robotic nurse assistants; The team will develop two CHRIs and then compare their performance in an assisted walking task with the abilities of human nursing staff. These interfaces utilize neural networks and generic algorithms and adjust to psycho-physiological and tactile signals from users and are designed to allow novice nurses and patients to operate robots with wearable sensors for prevention of falls. The team will then enhance the informational capabilities of the physical CHRIs using collaborative filtering, hybrid recommendation and machine learning techniques. The goal here is to develop an intelligent recommender system that will promote efficiency in the deployment of robotic assistants capable of performing patient sitting tasks such as vital signs monitoring and item fetching. The interfaces in this project will be evaluated by approximately 150 expert and novice users, nursing students and simulated patients to advance understanding of which types of tasks are better assigned to people and which may be delegated to robots in nursing scenarios. Finally, the team will perform economic analyses of the impact of the technology on nursing costs and the skilling needs for future healthcare industry through O*NET databases.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DRL",
 "org_div_long_name": "Division of Research on Learning in Formal and Informal Settings (DRL)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dan",
   "pi_last_name": "Popa",
   "pi_mid_init": "O",
   "pi_sufx_name": "",
   "pi_full_name": "Dan O Popa",
   "pi_email_addr": "dan.popa@louisville.edu",
   "nsf_id": "000085594",
   "pi_start_date": "2020-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Olfa",
   "pi_last_name": "Nasraoui",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Olfa Nasraoui",
   "pi_email_addr": "olfa.nasraoui@louisville.edu",
   "nsf_id": "000367596",
   "pi_start_date": "2020-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bryan",
   "pi_last_name": "Edwards",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Bryan D Edwards",
   "pi_email_addr": "bryan.edwards@okstate.edu",
   "nsf_id": "000350268",
   "pi_start_date": "2020-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Paiva",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "William D Paiva",
   "pi_email_addr": "wpaiva@okstate.edu",
   "nsf_id": "000740244",
   "pi_start_date": "2020-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mimia",
   "pi_last_name": "Logdson",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Mimia C Logdson",
   "pi_email_addr": "mclogs01@louisville.edu",
   "nsf_id": "000780841",
   "pi_start_date": "2020-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Louisville Research Foundation Inc",
  "inst_street_address": "2301 S 3RD ST",
  "inst_street_address_2": "",
  "inst_city_name": "LOUISVILLE",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "5028523788",
  "inst_zip_code": "402081838",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "KY03",
  "org_lgl_bus_name": "UNIVERSITY OF LOUISVILLE",
  "org_prnt_uei_num": "",
  "org_uei_num": "E1KJM4T54MK6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Louisville Research Foundation Inc",
  "perf_str_addr": "WS Speed 200",
  "perf_city_name": "Louisville",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "402920001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "KY03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "103Y00",
   "pgm_ele_name": "FW-HTF Futr Wrk Hum-Tech Frntr"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "063Z",
   "pgm_ref_txt": "FW-HTF Futr Wrk Hum-Tech Frntr"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 1499705.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, a diverse team of researchers from University of Louisville (Dr. Dan Popa, Dr. Cindy Logsdon, Dr. Olfa Nasraoui) collaborated with a team from Oklahoma State University (Dr. Bryan Edwards and Dr. William Paiva) to investigate the use of robotics and AI technologies for the future of work in nursing. The introduction of collaborative human robot interfaces (CHRIs) has the potential to revolutionize healthcare by allowing nurses access to physical aids for patient ambulation, patient observation, supply fetching, and electronic documentation.</p>\r\n<p>The project had 4 major objectives including:</p>\r\n<ul>\r\n</ul>\r\n<ul>\r\n<li>Objective 1: Develop a taxonomy of tasks and skills based on whether or not they can be easily and justifiably automated with robotics.</li>\r\n<li>Objective 2: Create convergent methodologies through which Collaborative Human Robot Interfaces (CHRIs) can be designed and evaluated for nursing applications.</li>\r\n<li>Objective 3: Evaluate the potential of CHRIs to enhance productivity and reduce stress of nursing staff.</li>\r\n<li>Objective 4: Estimate the potential economic impacts of introducing robots for routine nursing tasks.</li>\r\n</ul>\r\n<p>During the project, we used two archtypical nursing tasks to investigate the use of a nursing robot for ambulation with fall prevention and patient sitting. The Adaptive Robotic Nursing Assistant (ARNA), invented and built at the University of Louisville, was used as a robotic platform for evaluation of interface technologies in the context of nursing applications. Experiments were conducted with approximately 150 users, including nursing students, nurses, and patients over the duration of the project at the Louisville Automation and Robotics Institute (LARRI),&nbsp;at the School of Nursing, and at the University of Louisville Hospital. In addition, we surveyed close to 1000 professional nurses to develop a taxonomy of nursing tasks that can benefit from automation. We also interviewed hospital C-suite staff regarding industry pain-points, and potential of robots to lower healthcare costs.&nbsp;</p>\r\n<p>Our findings support the idea that both nurses and patients are enthusiastic about the use of robotic technologies in hospitals, to help with recovery and care of recovering patients, and to alleviate nursing shortages and their&nbsp;increasing&nbsp;workloads. Statistically significant results were collected from nursing students indicating that ARNA for patient ambulation was preferred over the gait-belt,&nbsp;a&nbsp;conventional walking standard of care.</p>\r\n<p>Remote teleoperation experiments were conducted with CHRIs where students in Oklahoma used the ARNA located in Louisville, KY, to fetch items in a patient sitting scenario. Results suggest that adaptive interfaces can achieve both motion smoothness, completion time reduction, and more intuitive use by untrained users. Finally, novel machine learning techniques were formulated and deployed to optimally coordinate the tasks of a number of robots and nurses working together on the floor of a simulated hospital.</p>\r\n<p>The project generated numerous interdisciplinary publications and trained 10 Ph.D. students, 5 Postdoctoral staff, and close to 20 undergraduates from Engineering, Computer Science, Nursing, Health Sciences and Business in a convergent research environment.&nbsp;</p>\r\n<p>Our team published several impactful papers in robotics and AI fields documenting the following advances:</p>\r\n<p>1) The team developed a new adaptive user interface (PNNUI - or Parallel Neural Network User Interface), that allows users to personalize their preferences online, e.g. while teleoperating the robot. We conducted a large remote telemanipulation experiment to demonstrate how CHRIs can help make robotics more intuitive and helpful for novice users such as nurses.</p>\r\n<p>2) The team developed Machine Learning algorithms that can enhance robot failure explainability and prediction, as well as coordinate teams of robots and nurses in a hospital environment.&nbsp;</p>\r\n<p>Our project has also had an impact to the field of Nursing and Health Sciences, through experiments and surveys including:</p>\r\n<p>1) The team conducted experiments that offer statistically significant results demonstrating technology acceptance of nursing robots over conventional methods for patient ambulation or patient sitting.</p>\r\n<p>2) The team proposed a new task taxonomy and collected data from hundreds of nurses rating nursing tasks in terms of:&nbsp;</p>\r\n<ol>\r\n<li>Standardization: the degree to which the task is performed with identical qualities and contents across nurses&nbsp;or situations.</li>\r\n<li>Mobility: The degree to which the task is performed at diverse physical locations.</li>\r\n<li>Emotional load: The degree to which the task requires&nbsp;emotional regulation and appropriate display during interaction with service recipients.</li>\r\n<li>Physical demand: The degree to which the task requires&nbsp;physical exertion.&nbsp;</li>\r\n<li>Predictability: The degree to which the task should occur&nbsp;in an expected manner.</li>\r\n</ol>\r\n<p>Based on this large dataset, in the near future we expect to&nbsp;generate a weighted algorithm for each task and rank order the tasks that nurses think can be automated. This knowledge could serve as a guide to future development of robotics technologies for nursing.</p><br>\n<p>\n Last Modified: 01/13/2025<br>\nModified by: Dan&nbsp;O&nbsp;Popa</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792717860_hieouterloop_resized--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792717860_hieouterloop_resized--rgov-800width.jpg\" title=\"Neuroadaptive Controller\"><img src=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792717860_hieouterloop_resized--rgov-66x44.jpg\" alt=\"Neuroadaptive Controller\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Block diagram of the neuroadaptive controller (NAC) with intent estimation (HIE) implemented on the ARNA robot.</div>\n<div class=\"imageCredit\">Payman Sharaffian</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Neuroadaptive Controller</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792168481_experiment_fow_resized--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792168481_experiment_fow_resized--rgov-800width.jpg\" title=\"Patient Walker Experiment\"><img src=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792168481_experiment_fow_resized--rgov-66x44.jpg\" alt=\"Patient Walker Experiment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The ARNA robot tested for patient walking with nursing students with different controllers and interface devices.</div>\n<div class=\"imageCredit\">Irina Kondaurova</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Patient Walker Experiment</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736791763481_fig1_resampled--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736791763481_fig1_resampled--rgov-800width.jpg\" title=\"Diagram of ARNA Robot\"><img src=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736791763481_fig1_resampled--rgov-66x44.jpg\" alt=\"Diagram of ARNA Robot\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Diagram of the Adaptive Robot Nursing Assistant (ARNA)</div>\n<div class=\"imageCredit\">Payman Sharaffian</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Diagram of ARNA Robot</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736789466041_pic_ULH1_fow_resized_2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736789466041_pic_ULH1_fow_resized_2--rgov-800width.jpg\" title=\"ARNA at ULH\"><img src=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736789466041_pic_ULH1_fow_resized_2--rgov-66x44.jpg\" alt=\"ARNA at ULH\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">ARNA robot at University of Louisville Hospital in 2024</div>\n<div class=\"imageCredit\">Payman Sharaffian</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">ARNA at ULH</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736464063609_pnnui--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736464063609_pnnui--rgov-800width.png\" title=\"Adaptive User Interface\"><img src=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736464063609_pnnui--rgov-66x44.png\" alt=\"Adaptive User Interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The implementation of an adaptive user interface using two parallel neural networks to minimize teleoperation time and motion jerk.</div>\n<div class=\"imageCredit\">Payman Sharaffian</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Adaptive User Interface</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792508424_Picture3_FOW_resized--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792508424_Picture3_FOW_resized--rgov-800width.jpg\" title=\"Patient Sitter Experiment\"><img src=\"/por/images/Reports/POR/2025/2026584/2026584_10695656_1736792508424_Picture3_FOW_resized--rgov-66x44.jpg\" alt=\"Patient Sitter Experiment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A patient tests the sitter teleoperation interface of the ARNA robot at University of Louisville Hospital in August 2024.</div>\n<div class=\"imageCredit\">Payman Sharaffian</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa\n<div class=\"imageTitle\">Patient Sitter Experiment</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn this project, a diverse team of researchers from University of Louisville (Dr. Dan Popa, Dr. Cindy Logsdon, Dr. Olfa Nasraoui) collaborated with a team from Oklahoma State University (Dr. Bryan Edwards and Dr. William Paiva) to investigate the use of robotics and AI technologies for the future of work in nursing. The introduction of collaborative human robot interfaces (CHRIs) has the potential to revolutionize healthcare by allowing nurses access to physical aids for patient ambulation, patient observation, supply fetching, and electronic documentation.\r\n\n\nThe project had 4 major objectives including:\r\n\r\n\r\n\r\nObjective 1: Develop a taxonomy of tasks and skills based on whether or not they can be easily and justifiably automated with robotics.\r\nObjective 2: Create convergent methodologies through which Collaborative Human Robot Interfaces (CHRIs) can be designed and evaluated for nursing applications.\r\nObjective 3: Evaluate the potential of CHRIs to enhance productivity and reduce stress of nursing staff.\r\nObjective 4: Estimate the potential economic impacts of introducing robots for routine nursing tasks.\r\n\r\n\n\nDuring the project, we used two archtypical nursing tasks to investigate the use of a nursing robot for ambulation with fall prevention and patient sitting. The Adaptive Robotic Nursing Assistant (ARNA), invented and built at the University of Louisville, was used as a robotic platform for evaluation of interface technologies in the context of nursing applications. Experiments were conducted with approximately 150 users, including nursing students, nurses, and patients over the duration of the project at the Louisville Automation and Robotics Institute (LARRI),at the School of Nursing, and at the University of Louisville Hospital. In addition, we surveyed close to 1000 professional nurses to develop a taxonomy of nursing tasks that can benefit from automation. We also interviewed hospital C-suite staff regarding industry pain-points, and potential of robots to lower healthcare costs.\r\n\n\nOur findings support the idea that both nurses and patients are enthusiastic about the use of robotic technologies in hospitals, to help with recovery and care of recovering patients, and to alleviate nursing shortages and theirincreasingworkloads. Statistically significant results were collected from nursing students indicating that ARNA for patient ambulation was preferred over the gait-belt,aconventional walking standard of care.\r\n\n\nRemote teleoperation experiments were conducted with CHRIs where students in Oklahoma used the ARNA located in Louisville, KY, to fetch items in a patient sitting scenario. Results suggest that adaptive interfaces can achieve both motion smoothness, completion time reduction, and more intuitive use by untrained users. Finally, novel machine learning techniques were formulated and deployed to optimally coordinate the tasks of a number of robots and nurses working together on the floor of a simulated hospital.\r\n\n\nThe project generated numerous interdisciplinary publications and trained 10 Ph.D. students, 5 Postdoctoral staff, and close to 20 undergraduates from Engineering, Computer Science, Nursing, Health Sciences and Business in a convergent research environment.\r\n\n\nOur team published several impactful papers in robotics and AI fields documenting the following advances:\r\n\n\n1) The team developed a new adaptive user interface (PNNUI - or Parallel Neural Network User Interface), that allows users to personalize their preferences online, e.g. while teleoperating the robot. We conducted a large remote telemanipulation experiment to demonstrate how CHRIs can help make robotics more intuitive and helpful for novice users such as nurses.\r\n\n\n2) The team developed Machine Learning algorithms that can enhance robot failure explainability and prediction, as well as coordinate teams of robots and nurses in a hospital environment.\r\n\n\nOur project has also had an impact to the field of Nursing and Health Sciences, through experiments and surveys including:\r\n\n\n1) The team conducted experiments that offer statistically significant results demonstrating technology acceptance of nursing robots over conventional methods for patient ambulation or patient sitting.\r\n\n\n2) The team proposed a new task taxonomy and collected data from hundreds of nurses rating nursing tasks in terms of:\r\n\r\nStandardization: the degree to which the task is performed with identical qualities and contents across nursesor situations.\r\nMobility: The degree to which the task is performed at diverse physical locations.\r\nEmotional load: The degree to which the task requiresemotional regulation and appropriate display during interaction with service recipients.\r\nPhysical demand: The degree to which the task requiresphysical exertion.\r\nPredictability: The degree to which the task should occurin an expected manner.\r\n\r\n\n\nBased on this large dataset, in the near future we expect togenerate a weighted algorithm for each task and rank order the tasks that nurses think can be automated. This knowledge could serve as a guide to future development of robotics technologies for nursing.\t\t\t\t\tLast Modified: 01/13/2025\n\n\t\t\t\t\tSubmitted by: DanOPopa\n"
 }
}