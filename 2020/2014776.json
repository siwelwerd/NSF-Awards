{
 "awd_id": "2014776",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Novel photographic steganography to reduce digital piracy on live streaming platforms",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2020-07-02",
 "awd_max_amd_letter_date": "2020-07-02",
 "awd_abstract_narration": "The broader impact of this Small Business Innovation Research (SBIR) Phase I project will be to detect screen-to-camera video piracy, through first-in-kind development of a robust video piracy detection method that uniquely applies camera-display messaging to copyright protection, while maintaining network quality standards.  Existing watermarking solutions cannot reliably handle video-in-video piracy, where users upload pirated media captured using a smartphone camera; either they fail to detect video piracy or exhibit injurious hypersensitivity. The proposed solution balances these two concerns by embedding unique and unextractable watermarks into video content, thereby improving the ability for platforms to accurately match uploaded content with messages in the copyright-protected database. The resulting technology will allow for the detection of copyright information in camera-captured images and video, so that distributors can automatically filter against the illegal sharing of copyright-protected material, dramatically reducing liability\u2014and costly copyright infringement fines\u2014for social media companies. \r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project seeks to develop an innovative camera-display message system that uses photographic steganography to embed covert messages in displayed video, addressing the need to detect screen-to-camera video piracy.  Although photographic steganography has been applied to still images, video message embedding is complicated by the presence of perceptible artifacts that degrade visual quality.  This project proposes to use novel spatio-temporal embedding that adapts to video content to reduce visual artifacts and support message recovery. This method circumvents the artifact-based constraints of traditional grid systems and will support the standard viewing quality. The following Phase I objectives will establish technical and commercial feasibility: 1) develop a video compression model that can handle multiple compression variations; 2) develop an optimal adaptive spatio-temporal embedding approach that reduces observed visual artifacts while maintaining a high accuracy rate; 3) design and implement a paired comparison protocol perceptual metric to evaluate video quality after message embedding; 4) develop a real-time robust screen detection and segmentation algorithm.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Wengrowski",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Wengrowski",
   "pi_email_addr": "eric@steg.ai",
   "nsf_id": "000817410",
   "pi_start_date": "2020-07-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "STEG AI CORPORATION",
  "inst_street_address": "5270 CALIFORNIA AVE STE 350",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "7326000291",
  "inst_zip_code": "926173233",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "STEG AI CORPORATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "HHTSWCNA6NM5"
 },
 "perf_inst": {
  "perf_inst_name": "STEG AI CORPORATION",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276811",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Steg AI builds software to protect and authenticate digital media using artificial intelligence technology. During Phase 1, Steg AI developed an innovative camera-display messaging system that uses photographic steganography to embed covert payloads in displayed videos and images. Provenance data is inserted into media to detect screen-to-camera piracy and authenticate digital content. Our unique approach allows the covert payload to be electronically displayed, transmitted as light in free space, and then camera-captured with minimal payload recovery error. Steg AI&rsquo;s technology enables precise content recognition which enables platforms to proactively detect and filter copyrighted content&mdash;a capability not currently supported by today&rsquo;s anti-piracy systems&mdash;thus providing a solution to the critical challenge of video and image provenance faced by social media companies and content creators/distributors.</p>\n<p>Unlike traditional watermarking approaches, our technique models the human vision system, compression algorithms, and camera-display transfer. Using a data-driven approach powered by deep-learning, coded media contains robust payloads that are visually unobtrusive to human observers.</p>\n<p>During Phase 1, we tested hypotheses for product-market fit by conducting over 200 customer discovery interviews through the I-Corps program. This served as an especially important effort because new commercial opportunities and technical considerations were discovered through customer discovery feedback. Our findings from the customer discovery effort directly informed the design of milestones to test technical feasibility. The most salient findings are: (1) The technology must be compatible with existing video compression technologies; (2) The technology must be minimally perceptible to creators and consumers; (3) The technology must robustly function in real-world environments.</p>\n<p>Our findings strongly support Steg AI&rsquo;s technological viability under real-world conditions. Critically, we have incorporated the technology advancements produced in the pursuit of the Phase 1 milestones into prototypes that have been enthusiastically validated by prominent social media and digital media licensing companies.</p>\n<p>Key findings from our technical milestones are:</p>\n<p>1. <strong>Reduction of spatial and temporal artifacts.</strong> Residual spatial and temporal artifacts can be made sufficiently small to enable embedding within high resolution video with dynamically changing payloads while maintaining robustness. Our data collection employs a Two-Alternative Forced Choice (2AFC) test, that asks which of two distorted videos is more similar to a reference video. 1000 users compared adaptive and non-adaptive encoding algorithms using 10 different reference videos for a total of 10,000 user responses. All three videos were played synchronously so users could directly compare spatial and temporal artifacts.</p>\n<p>For all algorithmic perceptual metrics tested, the Optimally Spatio-Temporal Adaptive Approach outperformed the non-adaptive approach. The user study also confirmed a decisive preference for the adaptive approach among real-world humans watching videos by a 4:1 margin.</p>\n<p>In summary, through experimental evaluation in real-world settings, we demonstrate that a spatially and temporally adaptive approach can dramatically improve the visual quality of coded images and videos. We met and even exceeded our technical milestones, achieving an 80% user preference for the new adaptive approach. These results demonstrate that the adaptive approach successfully leverages mismatches between computer and human vision systems to reduce artifacts without significantly sacrificing robustness. This is a keystone finding that demonstrates the viability of our technology for customers who are extremely sensitive to perceivable changes in the underlying content.</p>\n<p>2. <strong>Modeling</strong> <strong>human perception of watermark artifacts</strong><strong>.</strong> Mismatches between human and machine vision are modeled and exploited to produce spatio-temporally embedded videos that maintain high-quality visuals such that perceptual artifacts are virtually unnoticeable to viewers. Our test data set of 72,000 user responses was compared to the 2AFC scores predicted by each of the perceptual metrics. Our method best predicts actual human responses to watermark artifacts. Furthermore, the method is relatively invariant to the backbone architecture.</p>\n<p>3. <strong>Robustness to compression algorithms.</strong> Compression in the display and capture pipeline can be modeled as a differentiable transfer function. This transfer function is incorporated in watermark training to ensure that the bit error rate (BER) remains sufficiently small for the robust payload recovery requirements of a commercial product. Through experimental evaluation in real-world settings, we demonstrated that a variety of video compression algorithms commonly used for online and broadcast video distribution as well as video compression algorithms commonly used by smartphone cameras can be modeled. By achieving the technical milestone of minimized BER, we show that our photographic steganography algorithms are robust to a wide array of the most commonly adopted video compression algorithms. This finding demonstrates the compatibility of our technology with the video infrastructure technologies that our customers are already using</p>\n<p>4. <strong>Accurate screen detection.</strong> For real-world use with a handheld camera, screen detection and localization can be done robustly to handle crowded rooms, cluttered scenes, occlusions of screens, and variations in screen borders.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/14/2022<br>\n\t\t\t\t\tModified by: Eric&nbsp;Wengrowski</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSteg AI builds software to protect and authenticate digital media using artificial intelligence technology. During Phase 1, Steg AI developed an innovative camera-display messaging system that uses photographic steganography to embed covert payloads in displayed videos and images. Provenance data is inserted into media to detect screen-to-camera piracy and authenticate digital content. Our unique approach allows the covert payload to be electronically displayed, transmitted as light in free space, and then camera-captured with minimal payload recovery error. Steg AI\u2019s technology enables precise content recognition which enables platforms to proactively detect and filter copyrighted content&mdash;a capability not currently supported by today\u2019s anti-piracy systems&mdash;thus providing a solution to the critical challenge of video and image provenance faced by social media companies and content creators/distributors.\n\nUnlike traditional watermarking approaches, our technique models the human vision system, compression algorithms, and camera-display transfer. Using a data-driven approach powered by deep-learning, coded media contains robust payloads that are visually unobtrusive to human observers.\n\nDuring Phase 1, we tested hypotheses for product-market fit by conducting over 200 customer discovery interviews through the I-Corps program. This served as an especially important effort because new commercial opportunities and technical considerations were discovered through customer discovery feedback. Our findings from the customer discovery effort directly informed the design of milestones to test technical feasibility. The most salient findings are: (1) The technology must be compatible with existing video compression technologies; (2) The technology must be minimally perceptible to creators and consumers; (3) The technology must robustly function in real-world environments.\n\nOur findings strongly support Steg AI\u2019s technological viability under real-world conditions. Critically, we have incorporated the technology advancements produced in the pursuit of the Phase 1 milestones into prototypes that have been enthusiastically validated by prominent social media and digital media licensing companies.\n\nKey findings from our technical milestones are:\n\n1. Reduction of spatial and temporal artifacts. Residual spatial and temporal artifacts can be made sufficiently small to enable embedding within high resolution video with dynamically changing payloads while maintaining robustness. Our data collection employs a Two-Alternative Forced Choice (2AFC) test, that asks which of two distorted videos is more similar to a reference video. 1000 users compared adaptive and non-adaptive encoding algorithms using 10 different reference videos for a total of 10,000 user responses. All three videos were played synchronously so users could directly compare spatial and temporal artifacts.\n\nFor all algorithmic perceptual metrics tested, the Optimally Spatio-Temporal Adaptive Approach outperformed the non-adaptive approach. The user study also confirmed a decisive preference for the adaptive approach among real-world humans watching videos by a 4:1 margin.\n\nIn summary, through experimental evaluation in real-world settings, we demonstrate that a spatially and temporally adaptive approach can dramatically improve the visual quality of coded images and videos. We met and even exceeded our technical milestones, achieving an 80% user preference for the new adaptive approach. These results demonstrate that the adaptive approach successfully leverages mismatches between computer and human vision systems to reduce artifacts without significantly sacrificing robustness. This is a keystone finding that demonstrates the viability of our technology for customers who are extremely sensitive to perceivable changes in the underlying content.\n\n2. Modeling human perception of watermark artifacts. Mismatches between human and machine vision are modeled and exploited to produce spatio-temporally embedded videos that maintain high-quality visuals such that perceptual artifacts are virtually unnoticeable to viewers. Our test data set of 72,000 user responses was compared to the 2AFC scores predicted by each of the perceptual metrics. Our method best predicts actual human responses to watermark artifacts. Furthermore, the method is relatively invariant to the backbone architecture.\n\n3. Robustness to compression algorithms. Compression in the display and capture pipeline can be modeled as a differentiable transfer function. This transfer function is incorporated in watermark training to ensure that the bit error rate (BER) remains sufficiently small for the robust payload recovery requirements of a commercial product. Through experimental evaluation in real-world settings, we demonstrated that a variety of video compression algorithms commonly used for online and broadcast video distribution as well as video compression algorithms commonly used by smartphone cameras can be modeled. By achieving the technical milestone of minimized BER, we show that our photographic steganography algorithms are robust to a wide array of the most commonly adopted video compression algorithms. This finding demonstrates the compatibility of our technology with the video infrastructure technologies that our customers are already using\n\n4. Accurate screen detection. For real-world use with a handheld camera, screen detection and localization can be done robustly to handle crowded rooms, cluttered scenes, occlusions of screens, and variations in screen borders.\n\n\t\t\t\t\tLast Modified: 01/14/2022\n\n\t\t\t\t\tSubmitted by: Eric Wengrowski"
 }
}