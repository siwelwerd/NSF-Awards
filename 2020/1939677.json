{
 "awd_id": "1939677",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: FairGame: An Audit-Driven Game Theoretic Framework for Development and Certification of Fair AI",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2023-12-31",
 "tot_intn_awd_amt": 444145.0,
 "awd_amount": 444145.0,
 "awd_min_amd_letter_date": "2019-12-23",
 "awd_max_amd_letter_date": "2019-12-23",
 "awd_abstract_narration": "The increasing impact of AI technologies on real applications has subjected these to unprecedented scrutiny.  One of the major concerns is the extent to which these technologies reproduce or exacerbate inequity, with a number of high-profile examples, such as bias in recidivism prediction, illustrating the potential limitations of, and eroding trust in, AI.  While approaches have emerged that aim to guarantee some form of fairness of AI systems, most are restricted to relatively simple prediction problems, without accounting for specific use cases of predictions.  However, many practical uses of predictive models involve decisions that occur over time, and that are obtained by solving complex optimization problems.  Moreover, few general approaches exist even for ascertaining equitable outcomes of dynamic decisions, let alone providing guidance for ensuring equity in such settings. To address these limitations, this project is developing a framework called FairGame for the development and certification of fair autonomous decision-making algorithms. This project will also develop new courses and course modules at Washington University, take a lead role in a new interdisciplinary program in Computational and Data Sciences, seek to inform policymakers and regulators about computational approaches to ensuring fairness, and work to broaden participation in computing through, for example, the Missouri Louis Stokes Alliance for Minority Participation.\r\n\r\nThis project develops an audit-driven game theoretic framework for the development and certification of fair autonomous decision-making algorithms.  FairGame features a decision module that computes a decision policy, and a pseudo-adversarial auditor providing feedback to the decision module about possible fairness violations, as well as providing fairness certification. The FairGame framework conceptually resembles the well-known actor-critic methods in reinforcement learning; however, unlike actor-critic methods, it enforces that the auditor has only query access to the policy, and, conversely, the decision module can only query the auditor (which provides feedback on the decisions). Different notions of fairness and efficacy can be modeled as different types of two-player games between the decision module and the auditor.  This project will study foundational issues in this framework, including (a) the extent to which (probabilistically) certifying fairness in a black-box setting is possible, (b) practical algorithms for auditing, (c) iterative approaches for ensuring fair-decisions given a black-box access to an auditor, including policy gradient methods and Bayesian optimization, and (d) appropriate fairness and efficacy criteria, and (e) whether these criteria can satisfy different regulatory models, such as a requirement of \u201cmeaningful information about the logic\u201d or legally imposed requirements of nondiscrimination. The work will be informed by the real policy challenge of developing fair algorithms for provision of services to homeless households, and provide feedback in this domain to key stakeholders.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yevgeniy",
   "pi_last_name": "Vorobeychik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yevgeniy Vorobeychik",
   "pi_email_addr": "yvorobeychik@wustl.edu",
   "nsf_id": "000667978",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanmay",
   "pi_last_name": "Das",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sanmay Das",
   "pi_email_addr": "sanmay@vt.edu",
   "nsf_id": "000508218",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brendan",
   "pi_last_name": "Juba",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Brendan A Juba",
   "pi_email_addr": "bjuba@wustl.edu",
   "nsf_id": "000672054",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Roman",
   "pi_last_name": "Garnett",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Roman Garnett",
   "pi_email_addr": "garnett@wustl.edu",
   "nsf_id": "000696040",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chien-Ju",
   "pi_last_name": "Ho",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chien-Ju Ho",
   "pi_email_addr": "chienju.ho@wustl.edu",
   "nsf_id": "000753034",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington University",
  "inst_street_address": "1 BROOKINGS DR",
  "inst_street_address_2": "",
  "inst_city_name": "SAINT LOUIS",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "3147474134",
  "inst_zip_code": "631304862",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MO01",
  "org_lgl_bus_name": "WASHINGTON UNIVERSITY, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "L6NFUM28LQM5"
 },
 "perf_inst": {
  "perf_inst_name": "Washington University",
  "perf_str_addr": "",
  "perf_city_name": "Saint Louis",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "631304899",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  },
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 444145.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-12f889a6-7fff-042a-b7bf-a3a7a87ad127\"> </span></p>\n<p dir=\"ltr\"><span>This project had four goals: </span><span>1) develop computational methods for auditing systems to ensure fairness, 2) develop methods to understand how to ensure fairness of allocation decisions, and what impact this has on such decisions, 3) analyze legal implications of algorithmic approaches for fair AI, and 4) analyze practical implications of fair AI in high-consequence settings, such as resource allocation of limited housing and support resources among homeless households.</span></p>\n<p dir=\"ltr\"><span><br /></span><span>As a result of close collaborations between researchers in AI, social science, and law, we successfully accomplished these goals.&nbsp; In particular, we made a series of advances in analyzing the efficacy and fairness of group-fair algorithms in strategic settings.&nbsp; For example, we showed that in a number of settings, such algorithms can actually yield less fair outcomes than conventional machine learning approaches when individuals can strategically adapt to algorithmic decisions, and provided both sufficient and, under additional assumptions, necessary conditions for such fairness reversals to occur.&nbsp; We also analyzed human perceptions of justice and fairness in the context of decisions to allocate limited housing resources among homeless households, as well as simulated gigwork employment decisions, demonstrating the importance of procedural factors in fairness perceptions, as well as the impact of algorithmic recommendations on downstream resource allocation decisions made by people.&nbsp; Additionally, we provided several legal and policy analyses of algorithms that make use of group fairness constraints, observing that the significant non-determinism inherent in machine learning makes an adverse impact determination challenging in this context.&nbsp; Finally, we undertook the first systematic investigation of the algorithmic problem of dataset construction with the goal of achieving representativeness, as well as fairness in downstream tasks, modeling it as a multi-armed bandit problem with arms as distinct data sources.&nbsp; The resulting model and algorithmic approaches enabled us to obtain novel insights about data collection and fairness in settings where recruitment is an important part of the process, such as in clinical studies.</span></p>\n<p dir=\"ltr\"><span><br /></span><span>This project helped train 4 PhD students and 1 undergraduate student.&nbsp; The PhD students made foundational advances in the design of algorithms that achieve fairness, as well as approaches for collecting representative datasets.&nbsp; Finally, this project facilitated the development of educational materials for several undergraduate and graduate courses, including AI and Society, an undergraduate course at Washington University in Saint Louis (WUSTL), and Human-in-the-loop Computation, a graduate course at WUSTL.&nbsp; Specifically, the PI (Vorobeychik) developed a new course on AI and Society, taught at Washington University in St. Louis since Fall, 2019.&nbsp; This course includes an in-depth module on algorithmic fairness, tackling both the computational and philosophical issues involved, as well as a module on moral philosophy and ethics that aims to bridge theoretical concepts from moral philosophy with the practice of developing and analyzing algorithms.&nbsp; The materials for this portion of the course, particularly those involving in-class discussions, leveraged the research work associated with this project.&nbsp; The Co-PI (CJ Ho) developed the Human-in-the-loop Computation graduate course at WUSTL.&nbsp; Among its topics is consideration of the interaction between human cognition and reasoning and algorithmic recommendations, which reflects&mdash;among others&mdash;findings obtained in this project.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/06/2024<br>\nModified by: Yevgeniy&nbsp;Vorobeychik</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \n\n\nThis project had four goals: 1) develop computational methods for auditing systems to ensure fairness, 2) develop methods to understand how to ensure fairness of allocation decisions, and what impact this has on such decisions, 3) analyze legal implications of algorithmic approaches for fair AI, and 4) analyze practical implications of fair AI in high-consequence settings, such as resource allocation of limited housing and support resources among homeless households.\n\n\n\nAs a result of close collaborations between researchers in AI, social science, and law, we successfully accomplished these goals. In particular, we made a series of advances in analyzing the efficacy and fairness of group-fair algorithms in strategic settings. For example, we showed that in a number of settings, such algorithms can actually yield less fair outcomes than conventional machine learning approaches when individuals can strategically adapt to algorithmic decisions, and provided both sufficient and, under additional assumptions, necessary conditions for such fairness reversals to occur. We also analyzed human perceptions of justice and fairness in the context of decisions to allocate limited housing resources among homeless households, as well as simulated gigwork employment decisions, demonstrating the importance of procedural factors in fairness perceptions, as well as the impact of algorithmic recommendations on downstream resource allocation decisions made by people. Additionally, we provided several legal and policy analyses of algorithms that make use of group fairness constraints, observing that the significant non-determinism inherent in machine learning makes an adverse impact determination challenging in this context. Finally, we undertook the first systematic investigation of the algorithmic problem of dataset construction with the goal of achieving representativeness, as well as fairness in downstream tasks, modeling it as a multi-armed bandit problem with arms as distinct data sources. The resulting model and algorithmic approaches enabled us to obtain novel insights about data collection and fairness in settings where recruitment is an important part of the process, such as in clinical studies.\n\n\n\nThis project helped train 4 PhD students and 1 undergraduate student. The PhD students made foundational advances in the design of algorithms that achieve fairness, as well as approaches for collecting representative datasets. Finally, this project facilitated the development of educational materials for several undergraduate and graduate courses, including AI and Society, an undergraduate course at Washington University in Saint Louis (WUSTL), and Human-in-the-loop Computation, a graduate course at WUSTL. Specifically, the PI (Vorobeychik) developed a new course on AI and Society, taught at Washington University in St. Louis since Fall, 2019. This course includes an in-depth module on algorithmic fairness, tackling both the computational and philosophical issues involved, as well as a module on moral philosophy and ethics that aims to bridge theoretical concepts from moral philosophy with the practice of developing and analyzing algorithms. The materials for this portion of the course, particularly those involving in-class discussions, leveraged the research work associated with this project. The Co-PI (CJ Ho) developed the Human-in-the-loop Computation graduate course at WUSTL. Among its topics is consideration of the interaction between human cognition and reasoning and algorithmic recommendations, which reflectsamong othersfindings obtained in this project.\n\n\n\t\t\t\t\tLast Modified: 03/06/2024\n\n\t\t\t\t\tSubmitted by: YevgeniyVorobeychik\n"
 }
}