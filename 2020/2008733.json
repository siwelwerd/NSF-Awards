{
 "awd_id": "2008733",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF:Small: Data-Dependent Algorithms for High-Dimensional Data",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922095",
 "po_email": "kwimmer@nsf.gov",
 "po_sign_block_name": "Karl Wimmer",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 350000.0,
 "awd_amount": 350000.0,
 "awd_min_amd_letter_date": "2020-08-05",
 "awd_max_amd_letter_date": "2020-08-05",
 "awd_abstract_narration": "The goal of this project is to advance the state of the art of the search algorithms for modern high-dimensional datasets. Such datasets arise naturally from various representations of data objects: e.g., one 20x20 image can be represented as a 400-dimensional vector, a coordinate per pixel. The modern representations of objects have evolved to sophisticated methods (e.g., deep-learned representations for images), and which invariably lead to the basic setup of a dataset of high-dimensional vectors endowed with some similarity measure. Then a central problem is that of similarity search: given a dataset, find similar objects inside it, or, given a new object, find most similar dataset objects. These two primitives are central to many standard data-science questions, such as clustering or inference. This project will develop new algorithms for these primitives, in a variety of important application contexts, which use a variety of similarity measures. Most of the prior research focused on algorithms that are \"data oblivious\", which can be thought of as \"coloring\" the dataset objects as green, yellow, etc, and then only comparing green-vs-green, yellow-vs-yellow, etc. objects, thereby improving the algorithmic performance. The fundamental concept explored in this project is the data-dependent design of such coloring methods, which takes into account the entire dataset (i.e., the \"color\" of an object depends on all the other objects as well). This project has foundational connections to emerging research directions in mathematics and computational-complexity theory, on the theoretical side, as well as to data-science applications where similarity search is ubiquitous, on the practical side.\r\n\r\nThis project will explore the data-dependent methods for the two primitives, referred to as closest-pair (CP) and nearest-neighbor search (NNS) problems respectively, under various notions of similarity measures. Prior research by the investigator and co-authors\r\nsuggests that data-dependent methods may yield dramatic improvements in algorithm performance for problems on which remained elusive for decades. While such methods are common in practice, their limits and theoretical understanding is largely lacking, leaving room for substantial improvements in theory and practice.  The project focuses on three specific directions of exploration. 1) To design efficient algorithms for important classic distances or classes of distances, via the new data-dependent approach of cutting modulus, which is related to a new functional-analysis notion of \"average embedding\". 2) To develop data-dependent tools beyond this notion, e.g., for similarity measures that are not distances, as well as for obtaining algorithms with an instance-optimal performance. 3) To develop faster algorithms for CP, by exploiting the structure of the dataset, and ultimately combining the benefits of the two disjoint approaches used for CP so far: geometric and algebraic. The latter problem may shed new light on the Strong Exponential Time Hypothesis, which is central to the newly emerged field of Fine-Grained Complexity.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexandr",
   "pi_last_name": "Andoni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexandr Andoni",
   "pi_email_addr": "andoni@cs.columbia.edu",
   "nsf_id": "000711680",
   "pi_start_date": "2020-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 350000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project advanced the state of the art in high-dimensional problems, focusing on Nearest Neighbor Search (NNS) and related problems such as the Closest Pair (CP) problem. These fundamental primitives&mdash;often referred to as similarity search or vector search&mdash;play a crucial role in processing modern datasets, many derived from neural network embeddings of objects such as images or text. Their applications span a wide range of domains, from Machine Learning to core algorithmic steps in clustering algorithms. The widespread adoption of vector spaces and embeddings has made these concepts integral to modern algorithmic stack, as evidenced by their presence in Google Cloud documentation and their underlying &nbsp;role in Retrieval-Augmented Generation (RAG) for LLMs.</p>\r\n<p>Despite decades of research, significant gaps remain in our understanding of the best algorithms for these problems, which vary based on factors such as the (dis)similarity measure, target application, or other constraints. This project made substantial progress in advancing NNS, CP, and related problems across various settings, introducing novel approaches for designing efficient algorithms for such problems.</p>\r\n<p>A key tool in this project is the concept of <strong>data-dependent algorithms</strong> that was previously introduced by the PI in the context of high-dimensional problems. A classical approach in high-dimensional search involves assigning &ldquo;colors\" to objects such that (only) similar objects tend to have the exact same color. Then, in order to, say, find a pair of similar objects, it is enough to search within the same color group. The effectiveness of this method depends on the consistency of the color assignment for similar objects. It turns out that a more global color assignment&mdash;one that considers the entire dataset&mdash;can yield dramatically more efficient algorithms. This deep insight forms the foundation of the data-dependent approach.</p>\r\n<p>Previous work on data-dependent algorithms (by the PI and others) provided proof-of-concept results, showing dramatic improvements in a few specific cases. But it remained unclear how we can operationalize designing such algorithms. Are there general principles or tools for developing data-dependent algorithms? Can these techniques extend beyond NNS to other fundamental problems? This project successfully addressed these questions.&nbsp;</p>\r\n<p>A major <strong>highlight</strong> of this project is the development of <strong>two generic approaches</strong> to designing such data-dependent algorithms for NNS problems. The first approach is based on <em>average embeddings</em>, an extension of the concept of metric embedding from functional analysis. This concept, for the first time in over 20 years, unifies all known theoretical approaches to high-dimensional NNS. It has proven to be the most powerful tool to date for designing NNS algorithms. Leveraging this approach, the PI achieved significant improvements in classic dissimilarity measures, including ell_p spaces and edit distance. For the latter, it is a first-in-20-years improvement over the state-of-the-art.</p>\r\n<p>The second generic approach is based on communication games: a setup where two parties aim to determine similarity (such as &ldquo;are our binary vectors orthogonal or not&rdquo;) using minimal communication. Communication complexity has been extensively studied since the early days of Computer Science, offering a powerful toolkit for understanding the best possible communication. Using this method, the project delivered another first-in-20-years improvement&mdash;this time for Pattern Matching, a problem originally studied in Ron Rivest&rsquo;s 1974 thesis.</p>\r\n<p><strong>Other highlights</strong> of the project include:</p>\r\n<ul>\r\n<li>New <strong>practically-motivated data-dependent NNS algorithms</strong>: while practitioners often use data-dependent insights in real-world NNS algorithms, such methods typically lack formal guarantees. This project introduced a novel algorithm that learns from the dataset (using tools from game theory), improving runtime adaptively while maintaining rigorous theoretical guarantees.</li>\r\n<li>The project explored the closest pair (CP) problem in various settings, including a novel approach that <strong>combines geometric and algebraic techniques</strong>. Historically, researchers have only been able to leverage one of these techniques at a time, making it an open question whether combining them would yield even better algorithms. This project successfully demonstrated that such a hybrid approach can indeed be developed and lead to better algorithms.</li>\r\n<li>The project also gave algorithms for other more specialized settings of the nearest neighbor search, where none were known. One such setting is that of <strong>private NNS</strong>, where the goal is to bound the leakage of information from the dataset (think an ML system such as a RAG that cannot leak any information on the training data). Another such setting is a form of <strong>multi-hypothesis testing</strong>---a question in Statistics investigated as early as in 1947---where the project developed more computationally efficient algorithms than the standard solutions in Statistics.</li>\r\n</ul><br>\n<p>\n Last Modified: 02/02/2025<br>\nModified by: Alexandr&nbsp;Andoni</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project advanced the state of the art in high-dimensional problems, focusing on Nearest Neighbor Search (NNS) and related problems such as the Closest Pair (CP) problem. These fundamental primitivesoften referred to as similarity search or vector searchplay a crucial role in processing modern datasets, many derived from neural network embeddings of objects such as images or text. Their applications span a wide range of domains, from Machine Learning to core algorithmic steps in clustering algorithms. The widespread adoption of vector spaces and embeddings has made these concepts integral to modern algorithmic stack, as evidenced by their presence in Google Cloud documentation and their underlying role in Retrieval-Augmented Generation (RAG) for LLMs.\r\n\n\nDespite decades of research, significant gaps remain in our understanding of the best algorithms for these problems, which vary based on factors such as the (dis)similarity measure, target application, or other constraints. This project made substantial progress in advancing NNS, CP, and related problems across various settings, introducing novel approaches for designing efficient algorithms for such problems.\r\n\n\nA key tool in this project is the concept of data-dependent algorithms that was previously introduced by the PI in the context of high-dimensional problems. A classical approach in high-dimensional search involves assigning colors\" to objects such that (only) similar objects tend to have the exact same color. Then, in order to, say, find a pair of similar objects, it is enough to search within the same color group. The effectiveness of this method depends on the consistency of the color assignment for similar objects. It turns out that a more global color assignmentone that considers the entire datasetcan yield dramatically more efficient algorithms. This deep insight forms the foundation of the data-dependent approach.\r\n\n\nPrevious work on data-dependent algorithms (by the PI and others) provided proof-of-concept results, showing dramatic improvements in a few specific cases. But it remained unclear how we can operationalize designing such algorithms. Are there general principles or tools for developing data-dependent algorithms? Can these techniques extend beyond NNS to other fundamental problems? This project successfully addressed these questions.\r\n\n\nA major highlight of this project is the development of two generic approaches to designing such data-dependent algorithms for NNS problems. The first approach is based on average embeddings, an extension of the concept of metric embedding from functional analysis. This concept, for the first time in over 20 years, unifies all known theoretical approaches to high-dimensional NNS. It has proven to be the most powerful tool to date for designing NNS algorithms. Leveraging this approach, the PI achieved significant improvements in classic dissimilarity measures, including ell_p spaces and edit distance. For the latter, it is a first-in-20-years improvement over the state-of-the-art.\r\n\n\nThe second generic approach is based on communication games: a setup where two parties aim to determine similarity (such as are our binary vectors orthogonal or not) using minimal communication. Communication complexity has been extensively studied since the early days of Computer Science, offering a powerful toolkit for understanding the best possible communication. Using this method, the project delivered another first-in-20-years improvementthis time for Pattern Matching, a problem originally studied in Ron Rivests 1974 thesis.\r\n\n\nOther highlights of the project include:\r\n\r\nNew practically-motivated data-dependent NNS algorithms: while practitioners often use data-dependent insights in real-world NNS algorithms, such methods typically lack formal guarantees. This project introduced a novel algorithm that learns from the dataset (using tools from game theory), improving runtime adaptively while maintaining rigorous theoretical guarantees.\r\nThe project explored the closest pair (CP) problem in various settings, including a novel approach that combines geometric and algebraic techniques. Historically, researchers have only been able to leverage one of these techniques at a time, making it an open question whether combining them would yield even better algorithms. This project successfully demonstrated that such a hybrid approach can indeed be developed and lead to better algorithms.\r\nThe project also gave algorithms for other more specialized settings of the nearest neighbor search, where none were known. One such setting is that of private NNS, where the goal is to bound the leakage of information from the dataset (think an ML system such as a RAG that cannot leak any information on the training data). Another such setting is a form of multi-hypothesis testing---a question in Statistics investigated as early as in 1947---where the project developed more computationally efficient algorithms than the standard solutions in Statistics.\r\n\t\t\t\t\tLast Modified: 02/02/2025\n\n\t\t\t\t\tSubmitted by: AlexandrAndoni\n"
 }
}