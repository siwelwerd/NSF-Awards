{
 "awd_id": "2020784",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "A Machine Learning Approach to Improve Students\u2019 Scientific Reasoning and Writing",
 "cfda_num": "47.076",
 "org_code": "11040000",
 "po_phone": "7032922832",
 "po_email": "ptymann@nsf.gov",
 "po_sign_block_name": "Paul Tymann",
 "awd_eff_date": "2021-01-01",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 299196.0,
 "awd_amount": 299196.0,
 "awd_min_amd_letter_date": "2020-06-22",
 "awd_max_amd_letter_date": "2020-06-22",
 "awd_abstract_narration": "This project aims to serve the national interest by improving the scientific reasoning skills of undergraduates in general education STEM courses. Specifically, the project focuses on helping students learn to recognize scientific arguments and use evidence-based (scientific) reasoning. Introductory courses are typically the last formal exposure to science that non-science students will have.  Thus, general education STEM courses have a significant role in increasing civic science literacy. To support this goal, the project will create a writing dashboard that uses a machine learning algorithm to score how well a student\u2019s written response supports its claims with scientific evidence. The project will also develop a web browser extension that trains students to determine whether articles on the internet provide evidence to support scientific claims. Once the dashboard and web browser extension are developed in this exploratory project, the machine learning tools can be improved and deployed nationally for use by undergraduate students and instructors.  These tools have the potential for significant impact on undergraduate education, since they can assist instructors with assessing and providing feedback on writing, even in large classes.  Tools that can automate the process, even partially, could enhance the use of written assignments and assessments in STEM classes, thus helping students increase their reasoning and written communication skills.\r\n\r\nThis project will implement and study the efficacy of a writing dashboard and browser extension in three large introductory science courses. The dashboard will identify pairs of phrases that represent claims and evidence to support those claims. It will also score writing based on its use of jargon and its readability. The dashboard will be designed for instructors to use as a formative assessment tool that can provide constructive feedback on student writing. It will complement the instructor\u2019s grading process, providing a vehicle for discussing attributes associated with good scientific writing. The web browser extension will help students identify evidence-based scientific claims on the internet. Using the same machine learning technology as the writing dashboard, the browser extension will identify and highlight claims and evidence in articles available online and give an overall rating for the article\u2019s likely scientific quality, along with a rationale for the rating. The tools will be studied in three introductory science courses taken by non-science majors: astronomy, geosciences, and evolutionary biology. Students will be required to use the dashboard for three writing assignments, and they will use the browser extension for activities that require them to review and rate online scientific articles. To study potential improvements in students\u2019 scientific reasoning capacity, the project will adapt existing survey instruments and administer the revised surveys to students before and after the intervention. Instructors will be interviewed to understand the utility of the tools in the classroom. Beyond the university setting, these tools can also be used in high schools and the browser extension can be deployed in libraries and other informal settings to help improve scientific literacy and reasoning skills within the general population. This project is supported by the NSF Improving Undergraduate STEM Education Program: Education and Human Resources Program, which supports research and development projects to improve the effectiveness of STEM education for all students. Through the Engaged Student Learning track, the program supports the creation, exploration, and implementation of promising practices and tools.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Impey",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher D Impey",
   "pi_email_addr": "cimpey@as.arizona.edu",
   "nsf_id": "000161318",
   "pi_start_date": "2020-06-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanlyn",
   "pi_last_name": "Buxner",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Sanlyn R Buxner",
   "pi_email_addr": "buxner@psi.edu",
   "nsf_id": "000613319",
   "pi_start_date": "2020-06-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Wenger",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew C Wenger",
   "pi_email_addr": "mwenger@email.arizona.edu",
   "nsf_id": "000648349",
   "pi_start_date": "2020-06-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Danehy",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander B Danehy",
   "pi_email_addr": "adanehy@email.arizona.edu",
   "nsf_id": "000791689",
   "pi_start_date": "2020-06-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Arizona",
  "inst_street_address": "845 N PARK AVE RM 538",
  "inst_street_address_2": "",
  "inst_city_name": "TUCSON",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "5206266000",
  "inst_zip_code": "85721",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "AZ07",
  "org_lgl_bus_name": "UNIVERSITY OF ARIZONA",
  "org_prnt_uei_num": "",
  "org_uei_num": "ED44Y3W6P7B9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Arizona",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "857194824",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "AZ07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "8244",
   "pgm_ref_txt": "EHR CL Opportunities (NSF 14-302)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0420",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002021DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 299196.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>After training with the curated dataset, the neural net was deployed on thousands of articles about these two science areas drawn from billions of articles in the Common Crawl of the web. It was able to identity fake science with over 90% reliability in each science area. This success gives us confidence that we can train and deploy neural nets for natural language processing. This result applies across domains of science, which indicates that a single neural net can accomplish the goals of the project. We achieved an important milestone, using a set of 456 student writing assignments that had claim-evidence pairs selected by hand by advanced science undergraduates. The goal was to see how well machine learning could identify claim-evidence pairs. The writing assignments were divided randomly into three equal sets. One third was used to train BERT, a powerful neural network originally developed by Google. Another third was used to test how well the model learned its inputs. The final third was used to test the neural network on unseen data. The model performed with a high accuracy of 94%. More significantly, the model achieved a precision recall of 96%. Precision recall is a product of the fraction of positive identifications that was correct times the fraction of actual positives that was identified correctly. The results give us confidence we can implement a dashboard that can reliably judge student writing and give feedback for formative assessment of their writing about science.</p>\r\n<p>&nbsp;</p>\r\n<p>The last significant result for this project was based on learners in three Massive Open Online Courses (MOOCs) run by the PI, although it has immediate application to the undergraduate student audience that is the target of this project. Assessing writing in large classes for formal or informal learners presents a sizeable challenge. Most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience was adult learners in three MOOCs created by the PI and offered through Coursera. One course is on astronomy, the second is about astrobiology, and the third is on the history and philosophy of astronomy. The results will also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and it approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing. This is a novel result, and the subject of two publications in 2024.</p><br>\n<p>\n Last Modified: 01/15/2025<br>\nModified by: Christopher&nbsp;D&nbsp;Impey</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nAfter training with the curated dataset, the neural net was deployed on thousands of articles about these two science areas drawn from billions of articles in the Common Crawl of the web. It was able to identity fake science with over 90% reliability in each science area. This success gives us confidence that we can train and deploy neural nets for natural language processing. This result applies across domains of science, which indicates that a single neural net can accomplish the goals of the project. We achieved an important milestone, using a set of 456 student writing assignments that had claim-evidence pairs selected by hand by advanced science undergraduates. The goal was to see how well machine learning could identify claim-evidence pairs. The writing assignments were divided randomly into three equal sets. One third was used to train BERT, a powerful neural network originally developed by Google. Another third was used to test how well the model learned its inputs. The final third was used to test the neural network on unseen data. The model performed with a high accuracy of 94%. More significantly, the model achieved a precision recall of 96%. Precision recall is a product of the fraction of positive identifications that was correct times the fraction of actual positives that was identified correctly. The results give us confidence we can implement a dashboard that can reliably judge student writing and give feedback for formative assessment of their writing about science.\r\n\n\n\r\n\n\nThe last significant result for this project was based on learners in three Massive Open Online Courses (MOOCs) run by the PI, although it has immediate application to the undergraduate student audience that is the target of this project. Assessing writing in large classes for formal or informal learners presents a sizeable challenge. Most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience was adult learners in three MOOCs created by the PI and offered through Coursera. One course is on astronomy, the second is about astrobiology, and the third is on the history and philosophy of astronomy. The results will also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and it approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing. This is a novel result, and the subject of two publications in 2024.\t\t\t\t\tLast Modified: 01/15/2025\n\n\t\t\t\t\tSubmitted by: ChristopherDImpey\n"
 }
}