{
 "awd_id": "1953166",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Towards Adversarial Attack Resistant Machine Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2020-02-15",
 "awd_exp_date": "2024-01-31",
 "tot_intn_awd_amt": 299996.0,
 "awd_amount": 314996.0,
 "awd_min_amd_letter_date": "2020-02-18",
 "awd_max_amd_letter_date": "2020-07-08",
 "awd_abstract_narration": "Machine learning based pattern classification and related advances like deep learning have demonstrated impressive capabilities in multiple application domains, ranging from computer vision to medical diagnosis. However, it has also been shown that it is relatively straight-forward to create adversarial inputs that can fool machine learning models. The goal of this project is to develop defenses for machine learning models that are robust even in the face of sophisticated and determined adversaries. This project will have broad impact on the security of machine learning systems, advance cross-disciplinary research, and promote participation of undergraduates and under-represented groups in computer engineering research and education.\r\n\r\nThis project will pursue two lines of defenses designed to hinder gradient ascent techniques used in adversarial input generation. The first line of defense will add controlled noise to output confidence levels to deny an adversary access to the precise classification boundary, while seeking to preserve model accuracy. The second line of defense will pursue choosing a random model in a query step from a pool of multiple trained models which have similar classification accuracy but differ in internal parameters and confidence levels. To test effectiveness of defenses, this project will also develop a gray-box model for accelerating adversarial input generation from a black-box machine learning model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sandip",
   "pi_last_name": "Kundu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sandip Kundu",
   "pi_email_addr": "kundu@ecs.umass.edu",
   "nsf_id": "000388833",
   "pi_start_date": "2020-02-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "151 Holdsworth Way",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039284",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 314996.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Recent advances in machine learning (ML) systems have led to their widespread application across various domains. With the proliferation of new machine learning applications, ranging from image recognition, speech recognition, and natural language processing to self-driving cars and complex data analysis systems, the security of machine learning systems has become an increasingly significant concern.</p>\n<p>&nbsp;Various types of attacks can be perpetrated on machine learning models based on the attacker's motivation. One such attack is <strong>model stealing</strong>, wherein an attacker exploits the system's access to replicate its model. Another type is the <strong>model inversion attack</strong>, where the attacker aims to obtain private training data from a model. A third type is the <strong>model evasion attack</strong>, which focuses on creating input instances deliberately misclassified by the target model.</p>\n<p>&nbsp;Our research delves into the study of these attack types, identifying new approaches that attackers can exploit to significantly enhance their success rates. One notable approach involves collecting execution profile information, such as specific library calls and runtimes, to reason about the machine learning model. We discovered that tools like the <em>nvprof </em>tool from NVIDIA can be leveraged to gather detailed execution profile information. Using this information, we demonstrated the ability to accurately discern the model architecture with 100% accuracy for all models in the PyTorch model zoo. We replicated these results for ARM processors with Mali GPUs commonly found in mobile phones.</p>\n<p>We then demonstrated that with successful model stealing attack, we can accelerate model inversion and model evasion attacks by orders of magnitude. The mathematical foundation of these attacks lies in gradient calculation, expedited by access to a Whitebox machine learning model where all internal parameters are known. By performing a model stealing attack on a Blackbox model, one obtains a Whitebox model for these purposes. We further demonstrated that these attacks are <strong>transferable </strong>to original Blackbox models.</p>\n<p>&nbsp;In summary, our research has uncovered significant findings with potential implications for contemporary machine learning models. Additionally, we have explored several defense techniques against such attacks, with publications currently in the pipeline.</p><br>\n<p>\n Last Modified: 02/01/2024<br>\nModified by: Sandip&nbsp;Kundu</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nRecent advances in machine learning (ML) systems have led to their widespread application across various domains. With the proliferation of new machine learning applications, ranging from image recognition, speech recognition, and natural language processing to self-driving cars and complex data analysis systems, the security of machine learning systems has become an increasingly significant concern.\n\n\nVarious types of attacks can be perpetrated on machine learning models based on the attacker's motivation. One such attack is model stealing, wherein an attacker exploits the system's access to replicate its model. Another type is the model inversion attack, where the attacker aims to obtain private training data from a model. A third type is the model evasion attack, which focuses on creating input instances deliberately misclassified by the target model.\n\n\nOur research delves into the study of these attack types, identifying new approaches that attackers can exploit to significantly enhance their success rates. One notable approach involves collecting execution profile information, such as specific library calls and runtimes, to reason about the machine learning model. We discovered that tools like the nvprof tool from NVIDIA can be leveraged to gather detailed execution profile information. Using this information, we demonstrated the ability to accurately discern the model architecture with 100% accuracy for all models in the PyTorch model zoo. We replicated these results for ARM processors with Mali GPUs commonly found in mobile phones.\n\n\nWe then demonstrated that with successful model stealing attack, we can accelerate model inversion and model evasion attacks by orders of magnitude. The mathematical foundation of these attacks lies in gradient calculation, expedited by access to a Whitebox machine learning model where all internal parameters are known. By performing a model stealing attack on a Blackbox model, one obtains a Whitebox model for these purposes. We further demonstrated that these attacks are transferable to original Blackbox models.\n\n\nIn summary, our research has uncovered significant findings with potential implications for contemporary machine learning models. Additionally, we have explored several defense techniques against such attacks, with publications currently in the pipeline.\t\t\t\t\tLast Modified: 02/01/2024\n\n\t\t\t\t\tSubmitted by: SandipKundu\n"
 }
}