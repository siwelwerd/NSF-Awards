{
 "awd_id": "2032387",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CCSS: Collaborative Research: Ubiquitous Sensing for VR/AR Immersive Communication: A Machine Learning Perspective",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": "7032924568",
 "po_email": "hdai@nsf.gov",
 "po_sign_block_name": "Huaiyu Dai",
 "awd_eff_date": "2020-04-20",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 161720.0,
 "awd_amount": 161720.0,
 "awd_min_amd_letter_date": "2020-05-11",
 "awd_max_amd_letter_date": "2022-07-27",
 "awd_abstract_narration": "Virtual and augmented reality systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then used to construct an immersive representation of the scene on the user's head mounted display. Such systems are poised to enable and enhance numerous important applications, e.g., inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, due to its emerging nature, virtual/augmented reality immersive communication is presently limited to gaming or entertainment demonstrations featuring off-line captured/computer-generated content, studio-type settings, and high-end workstations to sustain its high data/computing workload. Moreover, there is little understanding of the fundamental trade-offs between the required signal acquisition density and sensor locations across space and time, the dynamics of the captured scene (motion, geometry, and textures), the available network and system resources, and the delivered immersion quality. This renders existing solutions impractical for deployment on bandwidth and energy constrained remote sensors. The project addresses these challenges via rigorous analysis and concerted algorithmic and application advances at the intersection of multi-view space-time sensing and signal representation, delay-sensitive communication, and machine learning. Education and outreach activities will immerse students in the exciting areas of visual sensing, wireless communications, and machine learning, and will engage underrepresented students spanning K-12 through undergraduate levels.\r\n\r\nThe objective of this project is to efficiently capture a remote environment using multiple camera sensors with the highest possible reconstruction quality under limited sampling and communication resources. This is achieved through four interrelated research tasks: (i) analysis of optimal space-time sampling policies that determine the sensors' locations and sampling rates to minimize the remote scene's reconstruction error; (ii) design of optimal signal representation methods that embed the sampled data jointly across space and time according to the allocated sampling rates; (iii) design of online learning sampling policies based on spectral graph theory that take sampling actions while exploring new sensor locations in the absence of a priori scene viewpoint signal knowledge; and (vi) design of computationally efficient self-organizing reinforcement learning methods that allow the wireless sensors to compute optimal transmission scheduling policies that meet the low-latency requirements of the overlaying virtual/augmented reality application while conserving their available energy. Integration, experimentation, and prototyping activities will be conducted to asses and validate the research advances in real-world settings. These technical advances will enable diverse applications of transformative impact.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jacob",
   "pi_last_name": "Chakareski",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jacob Chakareski",
   "pi_email_addr": "jacobcha@njit.edu",
   "nsf_id": "000663351",
   "pi_start_date": "2020-05-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Jersey Institute of Technology",
  "inst_street_address": "323 DR MARTIN LUTHER KING JR BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9735965275",
  "inst_zip_code": "071021824",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NJ10",
  "org_lgl_bus_name": "NEW JERSEY INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "SGBMHQ7VXNH5"
 },
 "perf_inst": {
  "perf_inst_name": "New Jersey Institute of Technology",
  "perf_str_addr": "University Heights",
  "perf_city_name": "Newark",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "071021982",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NJ10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "756400",
   "pgm_ele_name": "CCSS-Comms Circuits & Sens Sys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "153E",
   "pgm_ref_txt": "Wireless comm & sig processing"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 161720.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Virtual and augmented reality systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then used to construct an immersive representation of the scene on the user&rsquo;s head mounted display. Such systems are poised to enable and enhance numerous important applications, e.g., inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, due to its emerging nature, virtual/augmented reality immersive communication is presently limited to gaming or entertainment demonstrations featuring off-line captured/computer-generated content, studio-type settings, and high-end workstations to sustain its high data/computing workload. Moreover, there is little understanding of the fundamental trade-offs between the required signal acquisition density and sensor locations across space and time, the dynamics of the captured scene (motion, geometry, and textures), the available network and system resources, and the delivered immersion quality. This renders existing solutions impractical for deployment on bandwidth and energy constrained remote sensors. The project addresses these challenges via rigorous analysis and concerted algorithmic and application advances at the intersection of multi-view space-time sensing and signal representation, delay-sensitive communication, and machine learning. Education and outreach activities will immerse students in the exciting areas of visual sensing, wireless communications, and machine learning, and will engage underrepresented students spanning K-12 through undergraduate levels.</p>\n<p>The objective of this project is to efficiently capture a remote environment using multiple camera sensors with the highest possible reconstruction quality under limited sampling and communication resources. This is achieved through four interrelated research tasks: (i) analysis of optimal space-time sampling policies that determine the sensors' locations and sampling rates to minimize the remote scene&rsquo;s reconstruction error; (ii) design of optimal signal representation methods that embed the sampled data jointly across space and time according to the allocated sampling rates; (iii) design of online learning sampling policies based on spectral graph theory that take sampling actions while exploring new sensor locations in the absence of a priori scene viewpoint signal knowledge; and (vi) design of computationally efficient self-organizing reinforcement learning methods that allow the wireless sensors to compute optimal transmission scheduling policies that meet the low-latency requirements of the overlaying virtual/augmented reality application while conserving their available energy. Integration, experimentation, and prototyping activities will be conducted to asses and validate the research advances in real-world settings. These technical advances will enable diverse applications of transformative impact.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/11/2023<br>\n\t\t\t\t\tModified by: Jacob&nbsp;Chakareski</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2032387/2032387_10497047_1689106116858_ProjectIllustrationNSF_CCSS_Outcomes--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2032387/2032387_10497047_1689106116858_ProjectIllustrationNSF_CCSS_Outcomes--rgov-800width.jpg\" title=\"Virtual human teleportation via immersive communication.\"><img src=\"/por/images/Reports/POR/2023/2032387/2032387_10497047_1689106116858_ProjectIllustrationNSF_CCSS_Outcomes--rgov-66x44.jpg\" alt=\"Virtual human teleportation via immersive communication.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Self-organized IoT Wireless Communication for NextG XR Systems</div>\n<div class=\"imageCredit\">Jacob Chakareski</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jacob&nbsp;Chakareski</div>\n<div class=\"imageTitle\">Virtual human teleportation via immersive communication.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nVirtual and augmented reality systems comprise multi-view camera sensors that capture a scene from multiple perspectives. The captured data is then used to construct an immersive representation of the scene on the user\u2019s head mounted display. Such systems are poised to enable and enhance numerous important applications, e.g., inspection of large-scale infrastructure, archival of historical sites, search and rescue, disaster response, military reconnaissance, natural resource management, and immersive telepresence. However, due to its emerging nature, virtual/augmented reality immersive communication is presently limited to gaming or entertainment demonstrations featuring off-line captured/computer-generated content, studio-type settings, and high-end workstations to sustain its high data/computing workload. Moreover, there is little understanding of the fundamental trade-offs between the required signal acquisition density and sensor locations across space and time, the dynamics of the captured scene (motion, geometry, and textures), the available network and system resources, and the delivered immersion quality. This renders existing solutions impractical for deployment on bandwidth and energy constrained remote sensors. The project addresses these challenges via rigorous analysis and concerted algorithmic and application advances at the intersection of multi-view space-time sensing and signal representation, delay-sensitive communication, and machine learning. Education and outreach activities will immerse students in the exciting areas of visual sensing, wireless communications, and machine learning, and will engage underrepresented students spanning K-12 through undergraduate levels.\n\nThe objective of this project is to efficiently capture a remote environment using multiple camera sensors with the highest possible reconstruction quality under limited sampling and communication resources. This is achieved through four interrelated research tasks: (i) analysis of optimal space-time sampling policies that determine the sensors' locations and sampling rates to minimize the remote scene\u2019s reconstruction error; (ii) design of optimal signal representation methods that embed the sampled data jointly across space and time according to the allocated sampling rates; (iii) design of online learning sampling policies based on spectral graph theory that take sampling actions while exploring new sensor locations in the absence of a priori scene viewpoint signal knowledge; and (vi) design of computationally efficient self-organizing reinforcement learning methods that allow the wireless sensors to compute optimal transmission scheduling policies that meet the low-latency requirements of the overlaying virtual/augmented reality application while conserving their available energy. Integration, experimentation, and prototyping activities will be conducted to asses and validate the research advances in real-world settings. These technical advances will enable diverse applications of transformative impact.\n\n\t\t\t\t\tLast Modified: 07/11/2023\n\n\t\t\t\t\tSubmitted by: Jacob Chakareski"
 }
}