{
 "awd_id": "2039342",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC: SAVED: Secure Audio and Video Data from Deepfake Attacks Leveraging Environmental Fingerprints",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 257049.0,
 "awd_amount": 257049.0,
 "awd_min_amd_letter_date": "2020-09-02",
 "awd_max_amd_letter_date": "2020-10-14",
 "awd_abstract_narration": "The fast development of artificial intelligence (AI) and machine learning algorithms is escalating the technology that empowers the ability to distort reality. It has taken an exponential leap forward to deepfake attacks, which create audio and video of real people saying and doing things they never said or did. It is ever more realistic and increasingly resistant to detection. Deepfaked video, audio, or photos published on social media platforms are highly disturbing and able to mislead the public, raising further challenges in policy, technology, social, and legal aspects. Today's deepfake tools allow people to become anyone, from Elon Musk to Eminem, during a video chat. Recent deepfake video attacks on some public scenarios have raised more concerns. Disinformation may actually cause a disturbance in our society and ruin the foundation of trust. Government agencies like the U.S. Defense Advanced Research Projects Agency (DARPA) are concerned about losing the war against deepfake attacks that use the popular machine learning technique to automatically incorporate artificial components into existing video streams. The detailed technical routines and countermeasures against deepfake attacks have not been well investigated, leaving alone a potentially effective approach to tackle the emerging threats online in real-time.\r\n\r\nThis project introduces a novel solution to secure audio and video data streams against deepfake attacks. Instead of engaging in the endless AI arm races that fight fire with fire, where new machine learning algorithms keep making fake audio and video more real, this project tackles the challenging problem out of the box based on a key observation. Every audio or video stream has unique environmental fingerprints, e.g. the Electrical Network Frequency (ENF) signals, embedded when it was generated. The environmental fingerprints are random signals, which are unique, unpredictable, and unrepeatable. This project will investigate three typical application scenarios: (1) an accurate detection of deepfaked AVS data uploaded on the Internet, like social media posts; (2) an instant and accurate detection of false AVS injection attacks against online, real-time applications, like teleconferencing; and (3) a lightweight but robust version that fits on the Internet of Video Things applications, like smart public safety surveillance, which requires instant decision-making at the network edge. In addition, this project will gain deeper insights into the characteristics of the environmental fingerprints taking an information theory approach. The success of this research will deliver a disruptive technology that enables the ultimate win of the battle against the deepfake attacks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Chen",
   "pi_email_addr": "ychen@binghamton.edu",
   "nsf_id": "000488001",
   "pi_start_date": "2020-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Binghamton",
  "inst_street_address": "4400 VESTAL PKWY E",
  "inst_street_address_2": "",
  "inst_city_name": "BINGHAMTON",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6077776136",
  "inst_zip_code": "13902",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "L9ZDVULCHCV3",
  "org_uei_num": "NQMVAAQUFU53"
 },
 "perf_inst": {
  "perf_inst_name": "Binghamton University",
  "perf_str_addr": "4400 Vestal Pkwy E",
  "perf_city_name": "Binghamton",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "139024400",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 257049.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Primary Goals:</strong><br />This project aimed to secure audio and video streaming (AVS) data against deepfake attacks leveraging environmental &#64257;ngerprints. Deepfaked videos, audio, or photos in social media are alarming and can mislead the public, raising further challenges in policy, technology, social, and legal aspects. Instead of falling into an endless artificial intelligence (AI) arms race where new machine learning (ML) algorithms keep making fake audio/video more real, this project tackled the challenging problem out of the box based on an observation: every AVS data has unique environmental fingerprints, i.e., the Electrical Network Frequency (ENF) signals. This project obtained an insightful understanding of two di&#64256;erent environmental &#64257;ngerprints embedded in deepfaked AVS data: the embedded Electrical Network Frequency (ENF) signal as the random electromagnetic environmental &#64257;ngerprint and the spatiotemporal correlation index (STCI) as the network architectural &#64257;ngerprint and investigating the feasibility and e&#64256;ectiveness of proposed countermeasures at variant typical application scenarios.</p>\n<p><strong>Objectives:</strong><br />Our first objective was to collect sufficient audio and video data streams and reference ENF signals. These data are critical for this project's success, and no ENF data set was available in the public domain. We have to create our own. Meanwhile, being aware of the need for a decentralized experimental study, we proposed a project for the senior design team. This work enabled us to recruit more undergraduate students to help us build and deploy the ENF collectors at multiple geolocations in New York State. This effort brought us a larger-scale database of ENF signal traces, which will be a critical resource for many research groups in areas that depend on high-quality ENF signal databases.</p>\n<p>Secondly, leveraging the data set and the IoVT platform we have created, we have achieved all of the project's milestones: (1) detecting deepfake attacks in video conferencing using ENF signals embedded in audio streams, (2) authenticating the video and/or audio inputs in the IoVT environments, specifically, for smart public safety surveillance, (3) authenticating the multimedia contents posted on social media platforms, and validating the effectiveness and robustness of the proposed ENF-based approach as a complementary technique to fight the battle against deepfake attacks.&nbsp;<br />In addition, we have recruited two PhD students, both of whom earned their doctoral degrees in 2023, and multiple undergraduate students in our research team. Seven undergraduate students have graduated (three in 2021 and four in 2022). Furthermore, the team has been promoting the importance of the battle against deepfake attacks and disinformation at multiple events, including presenting research results, organizing panels, and giving keynote talks at conferences. The main goals include (1) to attract more students to this critical area, specifically the young generation in K12 schools, (2) to promote awareness of the compelling threats brought by AI/ML-enabled fake information, and (3) to inspire more discussions and spark new ideas.<br /><br /><strong>Significant Technical Contributions:</strong><br />1. We proposed EconLedger, an ENF-based consensus mechanism that enables secure and lightweight distributed ledgers for small-scale IoVT networks. EconLedger relies on a novel Proof-of-ENF (PoENF) algorithm. A validator can generate a new block only if a proper ENF-containing multimedia signal proof is produced within the current round. A proof-of-concept prototype validated that EconLedger provides a trust-free and partially decentralized security infrastructure for IoVT networks.</p>\n<p>2. We proposed DeFake - Decentralized ENF-consensus-based deepFake detection, which detects real-time audio manipulations. Since the fluctuations in an ENF signal are distributed and random, the PoENF algorithm can guarantee byzantine-resistant deepfakes detection on audio streams with minimal computational resources. By utilizing audio conferencing or audio editing applications as the frontend software service, the DeFake solution can effectively and efficiently verify the authenticity of the recorded video clip.</p>\n<p>3. Following DeFake, we developed DeFakePro, a decentralized consensus-based Deepfake detection for online video conferencing. The similarity in ENF signal fluctuations is utilized to authenticate the media broadcasted in conferencing tools. If the video conferencing setup with malicious participants to broadcast deep fake video recordings to other participants, the DeFakePro system verifies the authenticity of the incoming media in both audio and video channels.</p>\n<p>4. We studied the ENF signals under low Signal-to-Noise Ratio (SNR) conditions. Short-Time Fourier Transform (STFT) and Multiple Signal Classification (MUSIC) spectrum estimation techniques were considered to detect the Instantaneous Frequency (IF) of interest. We enhanced the ENF signal through an artificial power source in a noisy environment for reliable authentication using the spectral combination technique and a Robust Filtering Algorithm (RFA).&nbsp;</p>\n<p>5. The ENF-based deepfake detection approaches will be forfeited if attackers can create false ENF fingerprints to fool the detector. A thorough experimental study validates the robustness of ENF signals as a fingerprint for digital media authentication. Taking statistical, supervised learning, and deep learning approaches, we showed that it is infeasible to forecast ENF signals based on historical records. While strict theoretical proof is yet to be done, we experimentally verified ENF signals is a reliable fingerprint for digital media authentication.</p><br>\n<p>\n Last Modified: 01/12/2024<br>\nModified by: Yu&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098535451_02_Deepfake_attack--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098535451_02_Deepfake_attack--rgov-800width.png\" title=\"Deepfake Attack Example\"><img src=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098535451_02_Deepfake_attack--rgov-66x44.png\" alt=\"Deepfake Attack Example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An example of Deepfake attack.</div>\n<div class=\"imageCredit\">Deeraj Nagothu, Yu Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Chen\n<div class=\"imageTitle\">Deepfake Attack Example</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098883546_05_ENF_scores_distribution--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098883546_05_ENF_scores_distribution--rgov-800width.png\" title=\"Detection Example\"><img src=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098883546_05_ENF_scores_distribution--rgov-66x44.png\" alt=\"Detection Example\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The distribution of ENF scores calculated by distributed IoVT nodes</div>\n<div class=\"imageCredit\">Deeraj Nagothu, Ronghua Xu, Yu Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Chen\n<div class=\"imageTitle\">Detection Example</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098616450_03_Econledger_Network--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098616450_03_Econledger_Network--rgov-800width.png\" title=\"EconLedger\"><img src=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098616450_03_Econledger_Network--rgov-66x44.png\" alt=\"EconLedger\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Econledger Network Architecture</div>\n<div class=\"imageCredit\">Deeraj Nagothu, Ronghua Xu, Yu Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Chen\n<div class=\"imageTitle\">EconLedger</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098452112_01_Overview--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098452112_01_Overview--rgov-800width.png\" title=\"Overview\"><img src=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098452112_01_Overview--rgov-66x44.png\" alt=\"Overview\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A systematic view of this project's contributions.</div>\n<div class=\"imageCredit\">Deeraj Nagothu, Yu Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Chen\n<div class=\"imageTitle\">Overview</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098803594_04_DeFakePro--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098803594_04_DeFakePro--rgov-800width.png\" title=\"DeFakePro\"><img src=\"/por/images/Reports/POR/2024/2039342/2039342_10704493_1705098803594_04_DeFakePro--rgov-66x44.png\" alt=\"DeFakePro\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Main functional blocks of the DeFakePro system</div>\n<div class=\"imageCredit\">Deeraj Nagothu, Ronghua Xu, Yu Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu&nbsp;Chen\n<div class=\"imageTitle\">DeFakePro</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nPrimary Goals:\nThis project aimed to secure audio and video streaming (AVS) data against deepfake attacks leveraging environmental &#64257;ngerprints. Deepfaked videos, audio, or photos in social media are alarming and can mislead the public, raising further challenges in policy, technology, social, and legal aspects. Instead of falling into an endless artificial intelligence (AI) arms race where new machine learning (ML) algorithms keep making fake audio/video more real, this project tackled the challenging problem out of the box based on an observation: every AVS data has unique environmental fingerprints, i.e., the Electrical Network Frequency (ENF) signals. This project obtained an insightful understanding of two di&#64256;erent environmental &#64257;ngerprints embedded in deepfaked AVS data: the embedded Electrical Network Frequency (ENF) signal as the random electromagnetic environmental &#64257;ngerprint and the spatiotemporal correlation index (STCI) as the network architectural &#64257;ngerprint and investigating the feasibility and e&#64256;ectiveness of proposed countermeasures at variant typical application scenarios.\n\n\nObjectives:\nOur first objective was to collect sufficient audio and video data streams and reference ENF signals. These data are critical for this project's success, and no ENF data set was available in the public domain. We have to create our own. Meanwhile, being aware of the need for a decentralized experimental study, we proposed a project for the senior design team. This work enabled us to recruit more undergraduate students to help us build and deploy the ENF collectors at multiple geolocations in New York State. This effort brought us a larger-scale database of ENF signal traces, which will be a critical resource for many research groups in areas that depend on high-quality ENF signal databases.\n\n\nSecondly, leveraging the data set and the IoVT platform we have created, we have achieved all of the project's milestones: (1) detecting deepfake attacks in video conferencing using ENF signals embedded in audio streams, (2) authenticating the video and/or audio inputs in the IoVT environments, specifically, for smart public safety surveillance, (3) authenticating the multimedia contents posted on social media platforms, and validating the effectiveness and robustness of the proposed ENF-based approach as a complementary technique to fight the battle against deepfake attacks.\nIn addition, we have recruited two PhD students, both of whom earned their doctoral degrees in 2023, and multiple undergraduate students in our research team. Seven undergraduate students have graduated (three in 2021 and four in 2022). Furthermore, the team has been promoting the importance of the battle against deepfake attacks and disinformation at multiple events, including presenting research results, organizing panels, and giving keynote talks at conferences. The main goals include (1) to attract more students to this critical area, specifically the young generation in K12 schools, (2) to promote awareness of the compelling threats brought by AI/ML-enabled fake information, and (3) to inspire more discussions and spark new ideas.\n\nSignificant Technical Contributions:\n1. We proposed EconLedger, an ENF-based consensus mechanism that enables secure and lightweight distributed ledgers for small-scale IoVT networks. EconLedger relies on a novel Proof-of-ENF (PoENF) algorithm. A validator can generate a new block only if a proper ENF-containing multimedia signal proof is produced within the current round. A proof-of-concept prototype validated that EconLedger provides a trust-free and partially decentralized security infrastructure for IoVT networks.\n\n\n2. We proposed DeFake - Decentralized ENF-consensus-based deepFake detection, which detects real-time audio manipulations. Since the fluctuations in an ENF signal are distributed and random, the PoENF algorithm can guarantee byzantine-resistant deepfakes detection on audio streams with minimal computational resources. By utilizing audio conferencing or audio editing applications as the frontend software service, the DeFake solution can effectively and efficiently verify the authenticity of the recorded video clip.\n\n\n3. Following DeFake, we developed DeFakePro, a decentralized consensus-based Deepfake detection for online video conferencing. The similarity in ENF signal fluctuations is utilized to authenticate the media broadcasted in conferencing tools. If the video conferencing setup with malicious participants to broadcast deep fake video recordings to other participants, the DeFakePro system verifies the authenticity of the incoming media in both audio and video channels.\n\n\n4. We studied the ENF signals under low Signal-to-Noise Ratio (SNR) conditions. Short-Time Fourier Transform (STFT) and Multiple Signal Classification (MUSIC) spectrum estimation techniques were considered to detect the Instantaneous Frequency (IF) of interest. We enhanced the ENF signal through an artificial power source in a noisy environment for reliable authentication using the spectral combination technique and a Robust Filtering Algorithm (RFA).\n\n\n5. The ENF-based deepfake detection approaches will be forfeited if attackers can create false ENF fingerprints to fool the detector. A thorough experimental study validates the robustness of ENF signals as a fingerprint for digital media authentication. Taking statistical, supervised learning, and deep learning approaches, we showed that it is infeasible to forecast ENF signals based on historical records. While strict theoretical proof is yet to be done, we experimentally verified ENF signals is a reliable fingerprint for digital media authentication.\t\t\t\t\tLast Modified: 01/12/2024\n\n\t\t\t\t\tSubmitted by: YuChen\n"
 }
}