{
 "awd_id": "2031736",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID: Advanced Topic Modeling Methods to Analyze Text Responses in COVID-19 Survey Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2020-05-15",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 176785.0,
 "awd_amount": 191785.0,
 "awd_min_amd_letter_date": "2020-05-04",
 "awd_max_amd_letter_date": "2020-07-17",
 "awd_abstract_narration": "As the COVID-19 pandemic continues, public and private organizations are deploying surveys to inform responses and policy choices. Survey designs using multiple choice responses are by far the most common -- \"open ended\" questions, where survey participants provide a longer-form written response, are used far less. This is true despite the fact that when you allow people to provide unconstrained spoken or text responses, it is possible to obtain richer, fine-grained information clarifying the other responses, as well as useful \u201cbottom up\u201d information that the survey designers did not know to ask for.  A key problem is that analyzing the unstructured language in open-ended responses is a labor-intensive process, creating obstacles to using them especially when speedy analysis is needed and resources are limited. Computational methods can help, but they often fail to provide coherent, interpretable categories, or they can fail to do a good job connecting the text in the survey with the closed-end responses. This project will develop new computational methods for fast and effective analysis of survey data that includes text responses, and it will apply these methods to support organizations doing high-impact survey work related to COVID-19 response.  This will improve these organizations\u2019 ability to understand and mitigate the impact of the COVID-19 pandemic.\r\n\r\nThis project\u2019s technical approach builds on recent techniques bringing together deep learning and Bayesian topic models. Several key technical innovations will be introduced that are specifically geared toward improving the quality of information available in surveys that include both closed- and open-ended responses. A common element in these approaches is the extension of methods commonly used in supervised learning settings, such as task-based fine-tuning of embeddings and knowledge distillation, to unsupervised topic modeling, with a specific focus on producing diverse, human-interpretable topic categories that are well aligned with discrete attributes such as demographic characteristics, closed-end responses, and experimental condition. Project activities include assisting in the analysis of organizations' survey data, conducting independent surveys aligned with their needs to obtain additional relevant data, and the public release of a clean, easy to use computational toolkit facilitating more widespread adoption of these new methods.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Resnik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Philip Resnik",
   "pi_email_addr": "resnik@umd.edu",
   "nsf_id": "000187209",
   "pi_start_date": "2020-05-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425103",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "158Y00",
   "pgm_ele_name": "COVID-19 Research"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "096Z",
   "pgm_ref_txt": "COVID-19 Research"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "1N20",
   "app_name": "R&RA CARES Act DEFC N",
   "app_symb_id": "040100",
   "fund_code": "010N2021DB",
   "fund_name": "R&RA CARES Act DEFC N",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 191785.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Survey data has been, and continues to be, essential for public and private organizations in order to inform responses and policy choices related to the COVID-19 pandemic, and it is equally important for researchers in understanding the pandemic's impact. In general, surveys tend to use <em>closed-end</em> responses like multiple-choice questions or ratings on a 1-to-5 scale; <em>open-ended </em>questions, eliciting a written response, are used far less. This is true despite the fact that unconstrained text responses can yield rich and nuanced information as well as bottom up information that the survey designers did not know to ask about.</p>\n<p>One of the central obstacles to using open-ended questions, though, is that analyzing people's language in survey responses is a costly and labor-intensive enterprise. Generally analysts read all the responses in a slow, systematic process in order to manually identify themes or categories of response. In more rigorous analyses, additional effort is expended to establish that multiple analysts agree on what the categories are and that they make sense.&nbsp; As an alternative, there are automated ways to identify categories in bodies of text and such methods are highly scalable; however, in practice there is no single accepted way to automate the process, and many survey analysts are unsatisfied with the ability of fully automatic techniques to produce sufficiently high quality results. In addition, there tends to be a sociological gap between the survey research and technological research communities, resulting in automated techniques not being widely adopted.</p>\n<p><span>This project developed new computational methods for addressing limitations of open-ended responses in surveys, and it has applied these methods to support partners conducting and analyzing surveys related to the pandemic. On the technical side, the project focused on topic models, a category of computational methods that can extract human-interpretable categories from large collections of text. One of the project's surprising findings was that, although newer \"neural network\" topic models had been claimed to improve on the earlier topic models introduced in the early 2000s, flaws in the way that neural models are evaluated cast significant doubt on the validity of those claims. This led to a new approach to computational model evaluation that is much more directly tied to the processes that survey researchers actually follow in practice when they analyze text responses in the traditional, labor-intensive way. Results of careful and comprehensive experimentation found that the earlier \"classical\" models are actually superior to the newer neural methods when it comes to criteria that survey analysts care about. Building on that finding to address more generally the crucial issue of analyst trust in the results of such computational methods, the project developed TOPCAT (Topic-Oriented Protocol for Content Analysis of Text),</span><span> </span><span>a software-enabled, human-centered process designed around the traditional qualitative content analysis process, with the goal of widespread utility for \"qual\" researchers who analyze open-ended responses and other collections of text.</span></p>\n<p>In addition to its technical aims, the project aimed from the very beginning to use its methods and the knowledge being developed in an ongoing way to help organizations \"in the trenches\"&nbsp;with their surveys during the pandemic. This included successfully assisting in the analysis of open-ended responses for a number of surveys: one in collaboration with the CDC/National Center for Healthcare Statistics; one &nbsp;conducted by the Pandemic Crisis Services Response Coalition, a national organization focused on mental health issues; one conducted by the NYU School of Nursing looking at the experiences of front-line healthcare providers; a nationally representative survey conducted by Westat and the Stanford Medical School on the social and economic impact of COVID-19; a large survey by the Global Consortium of Nursing &amp; Midwifery Studies with both English and Spanish responses from the U.S. and Latin America (with further plans to continue working with surveys in multiple languages being conducted in the Caribbean, Europe, the Middle East, Asia, and Africa); and a second nationally representative survey on COVID impact designed and deployed in collaboration with Westat.&nbsp; Although not directly related to COVID-19, analysis was conducted using TOPCAT with 16,000 responses in a Reddit thread asking formerly suicidal Redditors what had gotten them through the dark times, with two suicide prevention experts providing the necessary subject matter expertise, providing insights that will be valuable to suicidologists and clinicians encountering people in suicidal crisis.</p>\n<p><span>This project supported the training of several graduate students, and the TOPCAT protocol, including detailed analyst instructions and supporting software for the end-to-end process, is being made publicly available.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/07/2023<br>\n\t\t\t\t\tModified by: Philip&nbsp;Resnik</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSurvey data has been, and continues to be, essential for public and private organizations in order to inform responses and policy choices related to the COVID-19 pandemic, and it is equally important for researchers in understanding the pandemic's impact. In general, surveys tend to use closed-end responses like multiple-choice questions or ratings on a 1-to-5 scale; open-ended questions, eliciting a written response, are used far less. This is true despite the fact that unconstrained text responses can yield rich and nuanced information as well as bottom up information that the survey designers did not know to ask about.\n\nOne of the central obstacles to using open-ended questions, though, is that analyzing people's language in survey responses is a costly and labor-intensive enterprise. Generally analysts read all the responses in a slow, systematic process in order to manually identify themes or categories of response. In more rigorous analyses, additional effort is expended to establish that multiple analysts agree on what the categories are and that they make sense.  As an alternative, there are automated ways to identify categories in bodies of text and such methods are highly scalable; however, in practice there is no single accepted way to automate the process, and many survey analysts are unsatisfied with the ability of fully automatic techniques to produce sufficiently high quality results. In addition, there tends to be a sociological gap between the survey research and technological research communities, resulting in automated techniques not being widely adopted.\n\nThis project developed new computational methods for addressing limitations of open-ended responses in surveys, and it has applied these methods to support partners conducting and analyzing surveys related to the pandemic. On the technical side, the project focused on topic models, a category of computational methods that can extract human-interpretable categories from large collections of text. One of the project's surprising findings was that, although newer \"neural network\" topic models had been claimed to improve on the earlier topic models introduced in the early 2000s, flaws in the way that neural models are evaluated cast significant doubt on the validity of those claims. This led to a new approach to computational model evaluation that is much more directly tied to the processes that survey researchers actually follow in practice when they analyze text responses in the traditional, labor-intensive way. Results of careful and comprehensive experimentation found that the earlier \"classical\" models are actually superior to the newer neural methods when it comes to criteria that survey analysts care about. Building on that finding to address more generally the crucial issue of analyst trust in the results of such computational methods, the project developed TOPCAT (Topic-Oriented Protocol for Content Analysis of Text), a software-enabled, human-centered process designed around the traditional qualitative content analysis process, with the goal of widespread utility for \"qual\" researchers who analyze open-ended responses and other collections of text.\n\nIn addition to its technical aims, the project aimed from the very beginning to use its methods and the knowledge being developed in an ongoing way to help organizations \"in the trenches\" with their surveys during the pandemic. This included successfully assisting in the analysis of open-ended responses for a number of surveys: one in collaboration with the CDC/National Center for Healthcare Statistics; one  conducted by the Pandemic Crisis Services Response Coalition, a national organization focused on mental health issues; one conducted by the NYU School of Nursing looking at the experiences of front-line healthcare providers; a nationally representative survey conducted by Westat and the Stanford Medical School on the social and economic impact of COVID-19; a large survey by the Global Consortium of Nursing &amp; Midwifery Studies with both English and Spanish responses from the U.S. and Latin America (with further plans to continue working with surveys in multiple languages being conducted in the Caribbean, Europe, the Middle East, Asia, and Africa); and a second nationally representative survey on COVID impact designed and deployed in collaboration with Westat.  Although not directly related to COVID-19, analysis was conducted using TOPCAT with 16,000 responses in a Reddit thread asking formerly suicidal Redditors what had gotten them through the dark times, with two suicide prevention experts providing the necessary subject matter expertise, providing insights that will be valuable to suicidologists and clinicians encountering people in suicidal crisis.\n\nThis project supported the training of several graduate students, and the TOPCAT protocol, including detailed analyst instructions and supporting software for the end-to-end process, is being made publicly available.\n\n \n\n\t\t\t\t\tLast Modified: 08/07/2023\n\n\t\t\t\t\tSubmitted by: Philip Resnik"
 }
}