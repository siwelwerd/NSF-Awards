{
 "awd_id": "2008868",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Rehabilitating Constants in Sublinear Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2020-06-17",
 "awd_max_amd_letter_date": "2020-06-17",
 "awd_abstract_narration": "The scale of data produced in the world is growing faster than the\r\nability to process it.  One approach for dealing with this data deluge\r\ninvolves sublinear algorithms, which are designed to estimate some\r\nfeature of data without needing to store, or sometimes even see, the\r\nentire data set.  Sublinear algorithms include distribution testing\r\n(e.g., estimating if a lottery is fair or not), streaming algorithms\r\n(e.g., finding the most common URLs on the web), and property testing\r\n(e.g., estimating the maximum degree of a network).  For these\r\nproblems, computer scientists have carefully studied how the\r\ncomplexity of the solution grows with the problem parameters---for\r\nexample, estimating if a lottery is fair requires a number of draws\r\nthat scales with the square root of the number of possible numbers\r\ndrawn.  But results so far have not been able to analyze the solution\r\ncomplexity for concrete instances (e.g., for a birthday lottery with\r\n366 possible numbers, how many samples are necessary to verify\r\nfairness?).  This project aims to change that, by finding solutions\r\nwith not only good asymptotic scaling, but good constant factors.\r\n\r\nDeveloping sublinear algorithms with good constant factors will\r\nrequire new algorithmic techniques.  The sublinear-algorithms\r\nliterature is based on several widespread techniques like probability\r\namplification that are simple, general, and optimal up to constant\r\nfactors---and significantly suboptimal in their constant factors.  By\r\nreplacing these techniques with more fine-grained ones, this project\r\naims to develop new algorithms with better performance in practice.\r\nThis project also aims to empirically measure the worst-case\r\nperformance of algorithms, by identifying which input distributions\r\nare provably hardest to solve.  By testing different algorithms in\r\npractice, the project will discover and compare the actual impact of\r\ndifferent algorithmic choices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Price",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Price",
   "pi_email_addr": "ecprice@cs.utexas.edu",
   "nsf_id": "000700457",
   "pi_start_date": "2020-06-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Texas at Austin",
  "perf_str_addr": "2317 Speedway",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121810",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7934",
   "pgm_ref_txt": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The field of sublinear algorithms has historically ignored constant<br />factors.&nbsp; This has led to a proliferation of techniques that are<br />simple, easy, and terrible in practice.&nbsp; In this project we examined<br />constant factors in sample and query complexity, developing more<br />efficient techniques that demonstrably work better in practice.<br /><br />Much of the project focused on estimating the average (i.e., the<br />arithmetic mean) of a distribution from samples.&nbsp; The sample average<br />is a fairly good estimator of the distribution average, but it turns<br />out that one can do better.&nbsp; First, the sample average is very<br />sensitive to outliers, which can make it unreliable.&nbsp; Second, the<br />sample average is not always efficient -- for example, for the Laplace<br />distribution the sample median takes 30% fewer samples than the sample<br />mean.&nbsp; There are classical statistics solutions to either one of these<br />issues in isolation, but not to both at the same time.&nbsp; We developed<br />techniques that can handle both issues in a variety of settings, such<br />as estimating known distributions in multiple dimensions or symmetric<br />distributions in one dimension.&nbsp; We also showed how to get better<br />constants for the generic setting of estimating bounded-covariance<br />distributions in high dimension.<br /><br />In addition to mean estimation, we also gave new algorithms with<br />better constants and better practical performance for noisy binary<br />search and uniformity testing.&nbsp; Uniformity testing addresses questions<br />like: how many rolls of a many-sided die does it take to tell if it is<br />unbiased?&nbsp; We showed that a classical statistics method -- the Pearson<br />chi-square test -- often outperforms more modern methods that were<br />developed in theoretical computer science without regard for constant<br />factors; we then gave a new method that combines the advantages of<br />each approach.</p><br>\n<p>\n Last Modified: 10/29/2024<br>\nModified by: Eric&nbsp;Price</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe field of sublinear algorithms has historically ignored constant\nfactors. This has led to a proliferation of techniques that are\nsimple, easy, and terrible in practice. In this project we examined\nconstant factors in sample and query complexity, developing more\nefficient techniques that demonstrably work better in practice.\n\nMuch of the project focused on estimating the average (i.e., the\narithmetic mean) of a distribution from samples. The sample average\nis a fairly good estimator of the distribution average, but it turns\nout that one can do better. First, the sample average is very\nsensitive to outliers, which can make it unreliable. Second, the\nsample average is not always efficient -- for example, for the Laplace\ndistribution the sample median takes 30% fewer samples than the sample\nmean. There are classical statistics solutions to either one of these\nissues in isolation, but not to both at the same time. We developed\ntechniques that can handle both issues in a variety of settings, such\nas estimating known distributions in multiple dimensions or symmetric\ndistributions in one dimension. We also showed how to get better\nconstants for the generic setting of estimating bounded-covariance\ndistributions in high dimension.\n\nIn addition to mean estimation, we also gave new algorithms with\nbetter constants and better practical performance for noisy binary\nsearch and uniformity testing. Uniformity testing addresses questions\nlike: how many rolls of a many-sided die does it take to tell if it is\nunbiased? We showed that a classical statistics method -- the Pearson\nchi-square test -- often outperforms more modern methods that were\ndeveloped in theoretical computer science without regard for constant\nfactors; we then gave a new method that combines the advantages of\neach approach.\t\t\t\t\tLast Modified: 10/29/2024\n\n\t\t\t\t\tSubmitted by: EricPrice\n"
 }
}