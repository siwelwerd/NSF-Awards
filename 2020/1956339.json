{
 "awd_id": "1956339",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:RI:AF:Medium:Exchanging Knowledge Beyond Data Between Human and Machine Learner",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2020-09-10",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Recent advances in deep learning have made dramatic progress in solving basic perceptual tasks such as speech recognition and object detection. To pave the way for the many human-centered applications that these advances might enable, in healthcare for instance, it is important to move beyond classification problems: to think of machine learning systems as producing not just category predictions, but also the reasons for them. Moreover, these patterns of reasoning need to be comprehensible to humans. To enable this, this project will focus on the exchange of knowledge between humans and machine learning systems and how such exchange of knowledge beyond data can lead to better predictions that are also human-interpretable. The project will result in technological advances that will have the potential to significantly impact the usability of machine learning in human-facing applications.\r\n\r\nThe technical aims of this project are developed along two broad themes. The first addresses the question, \"How can we involve human feedback in the machine learning process to create succinct models that are interpretable and generate predictions that are explainable?\" By enabling humans to provide rich feedback in the form of rules-of-thumb as relational knowledge, the project aims to derive succinct interpretable machine learning models that are amenable to simple explanations that are more compatible with the causal world-view of humans. To enhance the interpretability of machine learning, the project will further explore how human feedback based on relational knowledge can be leveraged to reduce the size of data sets required to train accurate models. The second addresses the question, \"How can we encode and exploit relational information in deriving interpretable and explainable models for reasoning?\" The project will explore the encoding of relational knowledge in both vector spaces and logical models and further investigate how relational knowledge can be used for analogical reasoning, semantic understanding, and relational queries.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bill",
   "pi_last_name": "Lin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bill Lin",
   "pi_email_addr": "billlin@ece.ucsd.edu",
   "nsf_id": "000441658",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanjoy",
   "pi_last_name": "Dasgupta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sanjoy Dasgupta",
   "pi_email_addr": "dasgupta@cs.ucsd.edu",
   "nsf_id": "000188407",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This collaborative project between UCSD and UCLA aimed to develop machine learning methods that promote knowledge exchange between humans and machines. The UCSD focus is on developing interpretable machine learning models and understanding how human feedback can enhance explainability.&nbsp;</p>\r\n<p>In particular, we developed several neural net models that can be interpreted as decision rules: i.e., explanations are in the form a human-understandable IF-THEN rule over predicates on user-defined features. The first of these architectures is a three-layer neural net called DR-net (\"decision rule neural network\"), in which the first layer corresponds to user-defined features, the neurons in the intermediate layer map to interpretable IF-THEN rules as logical conjunctions (AND) of predicates over user-defined features, and the neuron in the output layer maps to a to a disjunction of the middle-layer rules to form a decision rule set.</p>\r\n<p>We also developed two other neural net model based on this three-layer architecture. One model called DT-net (\"disjunctive threshold network\") is based on a disjunction of threshold functions. In this model, the intermediate layer neurons no longer constrained to compute conjunctions (AND) of input features; rather, they compute arbitrary linear functions. Although the final architecture no longer translates one-to-one to an IF-THEN (conjunctive) rule, we showed how different human-understandable IF-THEN rules can nonetheless be obtained by means of a simple greedy procedure. We developed another model called CT-net (\"conjunctive threshold network\"), which based a \"conjunction\" of threshold functions at the output. Human-understandable explanations in the form of an IF-THEN rule can also be readily obtained for predictions from a trained CT-net by means of a simple greedy procedure.</p>\r\n<p>Besides interpretable neural network models for tabular machine learning, we also developed a new paradigm for learning an interpretable decision rule set by means of two-level logic minimization. Tabular datasets can be viewed as logic functions that can be simplified using two-level logic minimization to produce minimal logic formulas in disjunctive normal form, which in turn can be readily viewed as an explainable decision rule set for binary classification. However, there are two problems with using logic minimization for tabular machine learning: tabular datasets often contain overlapping examples that have different class labels, which have to be resolved before logic minimization can be applied since logic minimization assumes consistent logic functions; even without inconsistencies, logic minimization alone generally produces complex models with poor generalization. To solve these problems, we developed a novel statistical framework for removing inconsistent training samples so that logic minimization can become an effective approach to tabular machine learning.</p>\r\n<p>In addition, we explored the k-means clustering problem, which is a popular unsupervised learning technique used to group data, but the resulting clusters become difficult to interpret in high-dimensional spaces. To improve explainability, we approximated k-means clusters using a decision tree with k leaves, where each leaf corresponds to a cluster. This tree provides an interpretable alternative to traditional k-means, and we quantified the approximation error, which we term the \"price of explainability.\"</p>\r\n<p>We also assessed the faithfulness of post-hoc explanation systems like LIME and Anchors, which provide local explanations for complex classifiers. Faithfulness was evaluated in terms of consistency (explanations should yield the same predictions for similar inputs) and sufficiency (the highlighted features should be sufficient to determine the classifier&rsquo;s output). To this end, we developed statistical measures and estimators to assess these properties, showing that their accuracy depends heavily on the degree of compression achieved by the explanation system.</p>\r\n<p>Moreover, we explored how to combine simple, human-understandable rules-of-thumb to build classifiers, often encountered in crowdsourced and semi-supervised learning. We compared two approaches: the Dawid-Skene Model (a probabilistic approach assuming conditional independence of rules, which may fail when this assumption is violated) and an Adversarial Approach by Balsubramani and Freund (a game-theoretic method that selects the labeling minimizing worst-case loss without making strong assumptions). Our analysis showed that the adversarial approach performs well in practice and converges to correct predictions at the same rate as the probabilistic model when its assumptions hold.aWe also developed confidence intervals for predictions in weak supervision and crowdsourced learning scenarios, where weak rules or heuristics guide the predictions. While frameworks such as Dawid-Skene provide predictions, they do not guarantee certainty. Our theory of consistency for confidence intervals ensures that these intervals accurately reflect the reliability of predictions, helping users identify trustworthy predictions.</p>\r\n<p>Finally, we developed a general-purpose active learning algorithm for data in metric spaces. The algorithm maintains neighborhoods of different sizes and uses label queries to identify regions with strong biases toward certain labels. When intersecting neighborhoods show different labels, the overlapping region is treated as a \"known unknown,\" prioritized in future queries. Our method provides label complexity bounds without relying on strong data assumptions, making it broadly applicable.</p><br>\n<p>\n Last Modified: 03/31/2025<br>\nModified by: Bill&nbsp;Lin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis collaborative project between UCSD and UCLA aimed to develop machine learning methods that promote knowledge exchange between humans and machines. The UCSD focus is on developing interpretable machine learning models and understanding how human feedback can enhance explainability.\r\n\n\nIn particular, we developed several neural net models that can be interpreted as decision rules: i.e., explanations are in the form a human-understandable IF-THEN rule over predicates on user-defined features. The first of these architectures is a three-layer neural net called DR-net (\"decision rule neural network\"), in which the first layer corresponds to user-defined features, the neurons in the intermediate layer map to interpretable IF-THEN rules as logical conjunctions (AND) of predicates over user-defined features, and the neuron in the output layer maps to a to a disjunction of the middle-layer rules to form a decision rule set.\r\n\n\nWe also developed two other neural net model based on this three-layer architecture. One model called DT-net (\"disjunctive threshold network\") is based on a disjunction of threshold functions. In this model, the intermediate layer neurons no longer constrained to compute conjunctions (AND) of input features; rather, they compute arbitrary linear functions. Although the final architecture no longer translates one-to-one to an IF-THEN (conjunctive) rule, we showed how different human-understandable IF-THEN rules can nonetheless be obtained by means of a simple greedy procedure. We developed another model called CT-net (\"conjunctive threshold network\"), which based a \"conjunction\" of threshold functions at the output. Human-understandable explanations in the form of an IF-THEN rule can also be readily obtained for predictions from a trained CT-net by means of a simple greedy procedure.\r\n\n\nBesides interpretable neural network models for tabular machine learning, we also developed a new paradigm for learning an interpretable decision rule set by means of two-level logic minimization. Tabular datasets can be viewed as logic functions that can be simplified using two-level logic minimization to produce minimal logic formulas in disjunctive normal form, which in turn can be readily viewed as an explainable decision rule set for binary classification. However, there are two problems with using logic minimization for tabular machine learning: tabular datasets often contain overlapping examples that have different class labels, which have to be resolved before logic minimization can be applied since logic minimization assumes consistent logic functions; even without inconsistencies, logic minimization alone generally produces complex models with poor generalization. To solve these problems, we developed a novel statistical framework for removing inconsistent training samples so that logic minimization can become an effective approach to tabular machine learning.\r\n\n\nIn addition, we explored the k-means clustering problem, which is a popular unsupervised learning technique used to group data, but the resulting clusters become difficult to interpret in high-dimensional spaces. To improve explainability, we approximated k-means clusters using a decision tree with k leaves, where each leaf corresponds to a cluster. This tree provides an interpretable alternative to traditional k-means, and we quantified the approximation error, which we term the \"price of explainability.\"\r\n\n\nWe also assessed the faithfulness of post-hoc explanation systems like LIME and Anchors, which provide local explanations for complex classifiers. Faithfulness was evaluated in terms of consistency (explanations should yield the same predictions for similar inputs) and sufficiency (the highlighted features should be sufficient to determine the classifiers output). To this end, we developed statistical measures and estimators to assess these properties, showing that their accuracy depends heavily on the degree of compression achieved by the explanation system.\r\n\n\nMoreover, we explored how to combine simple, human-understandable rules-of-thumb to build classifiers, often encountered in crowdsourced and semi-supervised learning. We compared two approaches: the Dawid-Skene Model (a probabilistic approach assuming conditional independence of rules, which may fail when this assumption is violated) and an Adversarial Approach by Balsubramani and Freund (a game-theoretic method that selects the labeling minimizing worst-case loss without making strong assumptions). Our analysis showed that the adversarial approach performs well in practice and converges to correct predictions at the same rate as the probabilistic model when its assumptions hold.aWe also developed confidence intervals for predictions in weak supervision and crowdsourced learning scenarios, where weak rules or heuristics guide the predictions. While frameworks such as Dawid-Skene provide predictions, they do not guarantee certainty. Our theory of consistency for confidence intervals ensures that these intervals accurately reflect the reliability of predictions, helping users identify trustworthy predictions.\r\n\n\nFinally, we developed a general-purpose active learning algorithm for data in metric spaces. The algorithm maintains neighborhoods of different sizes and uses label queries to identify regions with strong biases toward certain labels. When intersecting neighborhoods show different labels, the overlapping region is treated as a \"known unknown,\" prioritized in future queries. Our method provides label complexity bounds without relying on strong data assumptions, making it broadly applicable.\t\t\t\t\tLast Modified: 03/31/2025\n\n\t\t\t\t\tSubmitted by: BillLin\n"
 }
}