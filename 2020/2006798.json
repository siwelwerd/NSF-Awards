{
 "awd_id": "2006798",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: AF: Small: Fine-Grained Complexity of Approximate Problems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-07-31",
 "awd_max_amd_letter_date": "2020-07-31",
 "awd_abstract_narration": "Classically, an algorithm is called \"efficient\" if its running time is polynomial in the input size (i.e., as the input size doubles, the runtime is multiplied by some constant term). As the data becomes large, however, many such algorithms are no longer efficient in practice. For example, a quadratic-time algorithm (whose runtime grows fourfold after doubling the input size) can easily take hundreds of CPU-years on inputs of gigabyte size. For even larger inputs, the running time of a practically efficient algorithm must be effectively linear in the input  size. For many  problems such algorithms exist; for others, despite decades of effort, no such algorithms have been discovered yet. A recently developed theory of \"fine-grained complexity\" attempts to provide an explanation to  this phenomenon, by identifying natural assumptions that imply that  some of the existing algorithms cannot be improved. The goal of this project is to make progress on some of the key directions in this area, by developing new algorithms where possible, and showing limitations otherwise.\r\n\r\nThe project will focus on approximate algorithms, because such algorithms are often very useful in practice, and their existence is often not precluded by the existing hardness results. On a high-level, the project will investigate the following topics: (1) approximate algorithms, limitations, and limitations-inspired algorithms, and (2) new hardness assumptions for improving existing hardness results. The specific goals include key computational problems over graphs, sequences and kernels.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Piotr",
   "pi_last_name": "Indyk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Piotr Indyk",
   "pi_email_addr": "indyk@mit.edu",
   "nsf_id": "000488958",
   "pi_start_date": "2020-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  },
  {
   "pgm_ref_code": "7929",
   "pgm_ref_txt": "COMPUTATIONAL GEOMETRY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project focused on developing highly efficient algorithms for computational problems involving sequences, graphs, and kernel matrices. The specific outcomes of the project include algorithms for the following classes of problems:</p>\n<ul>\n<li><strong>Kernel matrices:</strong>&nbsp;A kernel matrix represents the similarities between a given set of elements (images, documents, etc.). For each pair of objects, the kernel matrix stores a value (typically between 0 and 1) that describes how similar the objects are to each other. Kernel matrices provide a natural representation of relationships within a dataset and are popular building blocks in machine learning algorithms. However, their major drawback is size, which is quadratic in the number of elements. For large datasets, such matrices cannot be efficiently generated or operated on. In this project, we developed algorithms for problems over such matrices that operate in less-than-quadratic time. In particular, we developed faster algorithms for computing similarity between kernel matrices and for computing their principal components. We also developed new methods for \"kernel density estimation,\" a foundational tool upon which the aforementioned efficient algorithms for kernel matrices rely, as well as improved methods for other density estimation problems.</li>\n<li><strong>Graphs:</strong>&nbsp;A graph is an alternative representation of relationships between a collection of data items, where items are represented as nodes, and edges between the nodes represent connections between them. For example, each node can represent a person, and an edge can represent a &ldquo;friendship&rdquo; relationship. In this project, we developed more time-and-space efficient algorithms for computation over graphs. In particular, we provided an improved algorithm for estimating the number of triangles in graphs, i.e., triples A, B, C such that A is connected to B, B is connected to C, and C is connected to A. The frequency of such triples provides important information about the nature of the graph. We also developed more efficient algorithms for finding matchings in graphs. All our algorithms for graphs are \"learning-based,\" i.e., they utilize past examples of graphs to optimize their performance.</li>\n</ul>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/31/2024<br>\nModified by: Piotr&nbsp;Indyk</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe project focused on developing highly efficient algorithms for computational problems involving sequences, graphs, and kernel matrices. The specific outcomes of the project include algorithms for the following classes of problems:\n\nKernel matrices:A kernel matrix represents the similarities between a given set of elements (images, documents, etc.). For each pair of objects, the kernel matrix stores a value (typically between 0 and 1) that describes how similar the objects are to each other. Kernel matrices provide a natural representation of relationships within a dataset and are popular building blocks in machine learning algorithms. However, their major drawback is size, which is quadratic in the number of elements. For large datasets, such matrices cannot be efficiently generated or operated on. In this project, we developed algorithms for problems over such matrices that operate in less-than-quadratic time. In particular, we developed faster algorithms for computing similarity between kernel matrices and for computing their principal components. We also developed new methods for \"kernel density estimation,\" a foundational tool upon which the aforementioned efficient algorithms for kernel matrices rely, as well as improved methods for other density estimation problems.\nGraphs:A graph is an alternative representation of relationships between a collection of data items, where items are represented as nodes, and edges between the nodes represent connections between them. For example, each node can represent a person, and an edge can represent a friendship relationship. In this project, we developed more time-and-space efficient algorithms for computation over graphs. In particular, we provided an improved algorithm for estimating the number of triangles in graphs, i.e., triples A, B, C such that A is connected to B, B is connected to C, and C is connected to A. The frequency of such triples provides important information about the nature of the graph. We also developed more efficient algorithms for finding matchings in graphs. All our algorithms for graphs are \"learning-based,\" i.e., they utilize past examples of graphs to optimize their performance.\n\n\n\n\t\t\t\t\tLast Modified: 05/31/2024\n\n\t\t\t\t\tSubmitted by: PiotrIndyk\n"
 }
}