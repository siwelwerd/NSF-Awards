{
 "awd_id": "2009071",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small:  Low-Latency and High-Quality Simultaneous Translation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2020-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 449980.0,
 "awd_amount": 465980.0,
 "awd_min_amd_letter_date": "2020-08-05",
 "awd_max_amd_letter_date": "2023-02-27",
 "awd_abstract_narration": "Simultaneous language translation (interpretation) is widely used in many situations including multilateral organizations such as the United Nations, international summits and conferences, and legal proceedings. However, the concurrent perception and production in two languages makes this task extremely challenging and exhausting for humans. The number of professional simultaneous interpreters is extremely limited worldwide, and they have to work in groups of two or three where each interpreter can only sustain for about 15-30 minutes. Therefore, there is a critical need to develop simultaneous translation techniques to reduce the burden of human interpreters and make this service more accessible and affordable. However, simultaneous translation is also notoriously difficult for machines and accomplishing it consistently and reliably is considered one of the holy grails of Artificial Intelligence. Various methods have been proposed to solve this problem, but with three major limitations: (a) their translation model is still a full-sentence translation model; (b) they cannot achieve short latencies such as \"3-seconds delay\" common in human interpretation; and (c) their systems are complicated and difficult to train.  Therefore, this project aims to develop new algorithms, techniques, and datasets for high-quality simultaneous machine translation with minimum delay (low latency). The technologies developed by this project will make simultaneous translation more affordable and accessible, which will improve the efficiency of human communication across linguistic barriers. This project also supports STEM education of underrepresented minorities (who do not speak English natively) by recruiting them in machine translation studies.\r\n\r\nBased on the principal investigator's successful prior work, the key idea in this project is to discard the conventional full-sentence translation paradigm and the classical sequence-to-sequence framework which processes the full input sentence before starting to translate and are thus ill-suited to simultaneous translation. Instead, this project adopts a \"prefix-to-prefix\" framework which starts translation after processing only a few input words, mimicking human interpreters. Though extremely simple, this framework achieves low latency and high translation quality. Using this framework, this project aims to (1) Develop an algorithm to detect and fix anticipation mistakes on the fly, and explore new evaluation metrics that can work for translations with revisions; (2) Develop dynamic and flexible translation strategies to balance quality and latency; (3) Construct better training data for simultaneous translation by revising the reference translations in a parallel text to remove unnecessary reorderings; (4) Apply the prefix-to-prefix framework to incremental text-to-speech synthesis (TTS), thus completing the end-to-end simultaneous speech-to-speech pipeline, improve its quality and latency, and compare with human simultaneous interpreters.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Liang",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Liang Huang",
   "pi_email_addr": "huanlian@oregonstate.edu",
   "nsf_id": "000629761",
   "pi_start_date": "2020-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973318507",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 449980.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This grant aims to improve simultaneous translation in terms of both training and decoding algorithms, and to move beyond classical text-to-text simultaneous translation to more realistic scenarios such as speech-to-text and text-to-sign language.</p>\r\n<p><br />First of all, this grant did result in better simultaneous translation algorithms, and we will highlight two examples below. The first one, known as \"direct simultaneous speech-to-text translation\", deals with the highly useful and practical scenario of speech-to-text simultaneous translation, which can be used to transcribe a live speech, presentation, or lecture in real-time. The classical method is to first perform streaming speech recognition (into text) and then do simultaneous text-to-text translation. However, this approach suffers from error propagation and extra latency. We devise a novel paradigram to use two separte but synchronized decoders on streaming speech recognition and direct speech-to-text translation, with the first one guiding (but not feeding as input to) the second. This paradigm results in improved translation quality and reduced latency. The second example aims at improving the quality and quantity of the training data for simultaneous translation. Due to the lack of large-scale, high-quality simultaneous translation datasets, most simultaneous translation systems are still trained on full-sentence translation data, which is far from ideal due to the abundance of unnecessary long-distance reorderings. We alleviate this problem by rewriting the target side of existing full-sentence corpora into simultaneous-style translation.</p>\r\n<p><br />Secondly, this grant also explored new directions beyond text and speech modalities. Specifically, we just started working on simultaneous translation between spoken (or written) languages and sign languages. This direction would greatly benefit the deaf and hard-of-hearing populations in the world. In the US alone, there are about 0.5-2 million deaf people and about 11 million deaf or hard-of-hearing people, and about 1-1.5 million people use American Sign Language (ASL). An automatic translation technology between ASL and English would greatly improve the quality of life for millions of Americans.</p>\r\n<p>&nbsp;</p>\r\n<p>Finally, this grant resulted in 10 top conference and journal publications, including one in the prestigious journal Proceedings of the National Academy of Sciences (PNAS). It supported the training of five PhD students (one of them female), one MS student, and five undergraduate students. Among them, two PhD students graduated and took research scientist jobs in industry, and one undergraduate student started PhD, with two more applying for PhD.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/22/2024<br>\nModified by: Liang&nbsp;Huang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis grant aims to improve simultaneous translation in terms of both training and decoding algorithms, and to move beyond classical text-to-text simultaneous translation to more realistic scenarios such as speech-to-text and text-to-sign language.\r\n\n\n\nFirst of all, this grant did result in better simultaneous translation algorithms, and we will highlight two examples below. The first one, known as \"direct simultaneous speech-to-text translation\", deals with the highly useful and practical scenario of speech-to-text simultaneous translation, which can be used to transcribe a live speech, presentation, or lecture in real-time. The classical method is to first perform streaming speech recognition (into text) and then do simultaneous text-to-text translation. However, this approach suffers from error propagation and extra latency. We devise a novel paradigram to use two separte but synchronized decoders on streaming speech recognition and direct speech-to-text translation, with the first one guiding (but not feeding as input to) the second. This paradigm results in improved translation quality and reduced latency. The second example aims at improving the quality and quantity of the training data for simultaneous translation. Due to the lack of large-scale, high-quality simultaneous translation datasets, most simultaneous translation systems are still trained on full-sentence translation data, which is far from ideal due to the abundance of unnecessary long-distance reorderings. We alleviate this problem by rewriting the target side of existing full-sentence corpora into simultaneous-style translation.\r\n\n\n\nSecondly, this grant also explored new directions beyond text and speech modalities. Specifically, we just started working on simultaneous translation between spoken (or written) languages and sign languages. This direction would greatly benefit the deaf and hard-of-hearing populations in the world. In the US alone, there are about 0.5-2 million deaf people and about 11 million deaf or hard-of-hearing people, and about 1-1.5 million people use American Sign Language (ASL). An automatic translation technology between ASL and English would greatly improve the quality of life for millions of Americans.\r\n\n\n\r\n\n\nFinally, this grant resulted in 10 top conference and journal publications, including one in the prestigious journal Proceedings of the National Academy of Sciences (PNAS). It supported the training of five PhD students (one of them female), one MS student, and five undergraduate students. Among them, two PhD students graduated and took research scientist jobs in industry, and one undergraduate student started PhD, with two more applying for PhD.\r\n\n\n\t\t\t\t\tLast Modified: 12/22/2024\n\n\t\t\t\t\tSubmitted by: LiangHuang\n"
 }
}