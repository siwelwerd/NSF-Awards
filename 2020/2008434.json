{
 "awd_id": "2008434",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: AF: Small: Adaptive Optimization of Stochastic and Noisy Function",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922095",
 "po_email": "kwimmer@nsf.gov",
 "po_sign_block_name": "Karl Wimmer",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 85000.0,
 "awd_amount": 101000.0,
 "awd_min_amd_letter_date": "2020-08-21",
 "awd_max_amd_letter_date": "2023-08-24",
 "awd_abstract_narration": "The science of artificial intelligence, and the technology of machine learning (ML) in particular, has had a huge impact on modern society.  This impact is only expected to grow in the future.  At the heart of ML is the process of training the parameters of an intelligent (computer) system, which requires applied-mathematics techniques in the area known as mathematical optimization.  The many recent successes of ML, such as in computer vision and natural-language processing, have been made possible with the use of a certain mathematical-optimization algorithm.  This algorithm allows the intelligent system to learn through the iterative random selection of data points from within a large-scale dataset.  This random sampling is absolutely essential, since otherwise the learning process of any intelligent system would be slowed as the amount of available data increases.  However, despite these recent successes, optimization techniques such as this one have fundamental shortcomings that impede them from being effective for next-generation ML tasks.  For example, each application of the algorithm requires a careful data-dependent tuning process, which may cause the training of an intelligent system for a single task to require weeks or months of computation on a supercomputer.  One avenue for avoiding such computational expense is through the design of optimization techniques that \"adaptively\" tune themselves.  The goals of this project are to design and provide theoretical guarantees for such adaptive algorithms.\r\n\r\nThere have been various previously proposed enhancements and extensions to the aforementioned algorithm, known as the stochastic gradient (SG) algorithm.  However, many of these algorithms also possess the shortcoming of being nonadaptive, meaning that their successful application in practice requires expensive \"hyperparameter\" tuning efforts.  The adaptive algorithms considered in this project for the \"stochastic optimization\" setting of ML are based on the various successful methodologies in the \"deterministic optimization\" literature.  These include so-called \"line search\" and \"trust region\" methodologies.  However, since neither of these methodologies result in optimal worst-case complexity guarantees, the focus of the project is on the design of adaptive optimal-complexity methods, such as so-called \"cubic-regularization\" algorithms.  The design of adaptive cubic-regularization algorithms for the stochastic setting will be achieved by building on a theoretical framework that views adaptive minimization as a \"renewal-reward\" stochastic process.  This work will combine analytical techniques from the mathematical-optimization and stochastic-process literatures, and will provide a solid theoretical and practical foundation for researchers working in applied mathematics, computer science, statistics, and various engineering fields.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Katya",
   "pi_last_name": "Scheinberg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Katya Scheinberg",
   "pi_email_addr": "katyascheinberg@gmail.com",
   "nsf_id": "000544723",
   "pi_start_date": "2020-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148530001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 85000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main goal of this project was design and analysis of&nbsp; adaptive stochastic optimization algorithms with complexity guarantees. Specifically we extended existing work to relax standard assumptions on the nature of stochastic (and deterministic) noise, encountered by the algorithm when computing or estimating&nbsp; function values and derivatives.&nbsp; &nbsp;The type of noise we considered in their project are biased estimators, estimators with bounded, irreducible noise, estimators with positive probability of arbitrarily corrupted values as well as biased estimators with otherwise, light tailed distributions. In all these cases we analyzed the behavior of stochastic adaptive methods, such as step search (generalization of line-search), trust region and cubicly regularized Newton methods.&nbsp; These algorithms were shown to&nbsp; make progress and converge to neighborhood of stationary points at fast rates until the noise in the function value or gradient estimates (which cannot be controlled) interferes with further progress. Specific bounds on the convergence neighborhoods have been derived. Three PhD students conducted research and published papers under this project.&nbsp;</p>\r\n<p>In addition, this project included an RUE Project&nbsp; that involved three undergraduate students. These students all developed and implemented optimization methods based on standard as well as robust gradient estimators. This development involved careful study of different type of corruptions that arise in optimization, different algorithms for robust gradient estimation, especially in high dimensions, as well as different type of gradient descent methods. The findings of this part of the project identify interesting gaps between theory and practice that PI plans to investigate in the future.&nbsp;</p><br>\n<p>\n Last Modified: 12/30/2024<br>\nModified by: Katya&nbsp;Scheinberg</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe main goal of this project was design and analysis of adaptive stochastic optimization algorithms with complexity guarantees. Specifically we extended existing work to relax standard assumptions on the nature of stochastic (and deterministic) noise, encountered by the algorithm when computing or estimating function values and derivatives. The type of noise we considered in their project are biased estimators, estimators with bounded, irreducible noise, estimators with positive probability of arbitrarily corrupted values as well as biased estimators with otherwise, light tailed distributions. In all these cases we analyzed the behavior of stochastic adaptive methods, such as step search (generalization of line-search), trust region and cubicly regularized Newton methods. These algorithms were shown to make progress and converge to neighborhood of stationary points at fast rates until the noise in the function value or gradient estimates (which cannot be controlled) interferes with further progress. Specific bounds on the convergence neighborhoods have been derived. Three PhD students conducted research and published papers under this project.\r\n\n\nIn addition, this project included an RUE Project that involved three undergraduate students. These students all developed and implemented optimization methods based on standard as well as robust gradient estimators. This development involved careful study of different type of corruptions that arise in optimization, different algorithms for robust gradient estimation, especially in high dimensions, as well as different type of gradient descent methods. The findings of this part of the project identify interesting gaps between theory and practice that PI plans to investigate in the future.\t\t\t\t\tLast Modified: 12/30/2024\n\n\t\t\t\t\tSubmitted by: KatyaScheinberg\n"
 }
}