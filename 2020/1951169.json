{
 "awd_id": "1951169",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  Natural Language Voice Controlled Science Equipment",
 "cfda_num": "47.041, 47.084",
 "org_code": "15030000",
 "po_phone": "7032928323",
 "po_email": "bschrag@nsf.gov",
 "po_sign_block_name": "Benaiah Schrag",
 "awd_eff_date": "2020-09-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 1270000.0,
 "awd_min_amd_letter_date": "2020-09-15",
 "awd_max_amd_letter_date": "2022-08-03",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research SBIR Phase II project is the development of a natural language voice-controlled lab assistant technology to enable hands-on science learning by students that are blind or have low vision, as well as the general student population. The Lab Assistant application will use state-of-the-art voice recognition technology for educational science experiments. The Lab Assistant will integrate with sensor hardware and mobile apps to enable hands-on experiments in physics, biology, chemistry, and engineering. The impacts of the Lab Assistant are anticipated to include the development of software that can respond to a wide spectrum of natural language questions and the systems integration of many cutting-edge technologies (wireless sensors, mobile apps, voice recognition software, and assistive technology) into a simple user interface for science education. The voice interactions will help students overcome disabilities with traditional touch and visual technologies, work more independently due to the presence of auditory help, and gain confidence in their STEM abilities. For teachers, the Lab Assistant will provide an \"expert in the room\" to help guide the hands-on activities.  The Lab Assistant will be especially useful to teachers without formal science training, that nevertheless need to lead hands-on STEM activities.\r\n\r\n \r\nThis Small Business Innovation Research SBIR Phase II project will develop a natural language voice-controlled lab assistant technology that will enable students to control and interact with hands-on science lab equipment using natural language dialog. The research objective is to enable any student to conduct hands-on experiments and receive audible responses based on their own experimental measurements. Researchers will measure the change in student mastery of the Science and Engineering Practices required in the Next Generation Science Standards and the impact on student attitudes and confidence in science, technology, engineering, and math disciplines. Researchers will also measure the teachers\u2019 abilities to facilitate authentic science activities for students with disabilities and the general student population while using the lab assistant. The technology may enable students that are blind or low vision to be able to work more independently and gain higher level mastery of the Science and Engineering Practices. The general student population will also be able to use the lab assistant for increased engagement in science activities.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Clifton",
   "pi_last_name": "Roozeboom",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Clifton L Roozeboom",
   "pi_email_addr": "clifton.roozeboom@gmail.com",
   "nsf_id": "000696713",
   "pi_start_date": "2020-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Myriad Sensors",
  "inst_street_address": "9304 CENTURY OAK CT",
  "inst_street_address_2": "",
  "inst_city_name": "BRENTWOOD",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "7196513764",
  "inst_zip_code": "370273321",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "TN07",
  "org_lgl_bus_name": "MYRIAD SENSORS, INC.",
  "org_prnt_uei_num": "F47EL8SWG943",
  "org_uei_num": "F47EL8SWG943"
 },
 "perf_inst": {
  "perf_inst_name": "Myriad Sensors, Inc",
  "perf_str_addr": "385 S. Monroe St",
  "perf_city_name": "San Jose",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "951285107",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "165E",
   "pgm_ref_txt": "SBIR Phase IIB"
  },
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 750000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 520000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the NSF SBIR Phase II Project- Natural Language Voice Controlled Science Equipment project was to develop a voice-enabled lab assistant to improve the ability of blind and visually-impaired (BVI) students to fully participate and engage in real world lab experiments. These STEM-focused activities have traditionally been off-limits to BVI students due to the challenges they represent for the students and their instructors. This project created an integrated solution providing students with the ability to: (1) control and interact with handheld PocketLab sensors for real-time data capture and analysis, (2) analyze data within the PocketLab Notebook web application, providing assessment and evaluation that is equivalent to what is expected of their sighted peers, (3) explore science and engineering concepts via a voice-enabled lab assistant capable of understanding natural language queries and presenting information in a non-visual manner. The researchers conducted user testing with BVI students, assistive technology educators, and normally-sighted students to test and iterate on features of the technology and improve on implementation in the classroom.</p>\n<p>&nbsp;</p>\n<p>The project was extended into a Phase IIB for researchers to build off the technology to develop the Science Outcomes Assessment Platform (SOAP), which developed a system to deploy and manage educational lessons and content in a science classroom. Specific project objectives that researchers achieved were: (1) organize data streams, user data, and platform metrics into a data product for and measuring student outcomes in regards to hands-on science activities, (2) create an authoring system for third parties to develop, test, and deploy content research question and target populations, (3) develop a dashboard of student metrics for teachers and administrators. During the project, the researchers completed the development and integrated the functionality into a commercially available web application that is used by multiple education companies with students in classrooms from kindergarten through college.</p><br>\n<p>\n Last Modified: 03/04/2024<br>\nModified by: Clifton&nbsp;L&nbsp;Roozeboom</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/1951169/1951169_10706834_1709567498488_Screen_Shot_2024_03_04_at_9.48.56_AM--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/1951169/1951169_10706834_1709567498488_Screen_Shot_2024_03_04_at_9.48.56_AM--rgov-800width.png\" title=\"PocketLab science experiment\"><img src=\"/por/images/Reports/POR/2024/1951169/1951169_10706834_1709567498488_Screen_Shot_2024_03_04_at_9.48.56_AM--rgov-66x44.png\" alt=\"PocketLab science experiment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">High school students conducting a physics experiment using PocketLab technology developed in part in the NSF SBIR Phase II project Natural Language Voice Controlled Science Equipment</div>\n<div class=\"imageCredit\">PocketLab</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Clifton&nbsp;L&nbsp;Roozeboom\n<div class=\"imageTitle\">PocketLab science experiment</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of the NSF SBIR Phase II Project- Natural Language Voice Controlled Science Equipment project was to develop a voice-enabled lab assistant to improve the ability of blind and visually-impaired (BVI) students to fully participate and engage in real world lab experiments. These STEM-focused activities have traditionally been off-limits to BVI students due to the challenges they represent for the students and their instructors. This project created an integrated solution providing students with the ability to: (1) control and interact with handheld PocketLab sensors for real-time data capture and analysis, (2) analyze data within the PocketLab Notebook web application, providing assessment and evaluation that is equivalent to what is expected of their sighted peers, (3) explore science and engineering concepts via a voice-enabled lab assistant capable of understanding natural language queries and presenting information in a non-visual manner. The researchers conducted user testing with BVI students, assistive technology educators, and normally-sighted students to test and iterate on features of the technology and improve on implementation in the classroom.\n\n\n\n\n\nThe project was extended into a Phase IIB for researchers to build off the technology to develop the Science Outcomes Assessment Platform (SOAP), which developed a system to deploy and manage educational lessons and content in a science classroom. Specific project objectives that researchers achieved were: (1) organize data streams, user data, and platform metrics into a data product for and measuring student outcomes in regards to hands-on science activities, (2) create an authoring system for third parties to develop, test, and deploy content research question and target populations, (3) develop a dashboard of student metrics for teachers and administrators. During the project, the researchers completed the development and integrated the functionality into a commercially available web application that is used by multiple education companies with students in classrooms from kindergarten through college.\t\t\t\t\tLast Modified: 03/04/2024\n\n\t\t\t\t\tSubmitted by: CliftonLRoozeboom\n"
 }
}