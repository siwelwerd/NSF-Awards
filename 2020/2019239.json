{
 "awd_id": "2019239",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FMitF: Track I: Focusing Incremental Abstraction-based Verification on Neural Networks Input Distributions",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922585",
 "po_email": "pprabhak@nsf.gov",
 "po_sign_block_name": "Pavithra Prabhakar",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 510000.0,
 "awd_amount": 510000.0,
 "awd_min_amd_letter_date": "2020-06-18",
 "awd_max_amd_letter_date": "2020-06-18",
 "awd_abstract_narration": "The promise of machine learning is that it will improve the operation of systems across a variety of domains, such as agriculture, transportation, and medicine.  With that promise comes the risk that such systems will not operate as intended, which may lead to harm to individuals or society.  The project's impacts are in mitigating these risks by developing practical methods for assuring the correct operation of machine-learning systems.  While risk is not unique to systems that incorporate machine learning, such systems present additional challenges to assuring their correct operation.   Consider a camera-based driving system that aims to recognize a stop sign.  Such a system must correctly identify a sign from among the enormous number of possible images while considering variables, such as, angle, lighting, distance, and any obstructions.  Assuring such a system is correct requires evaluating the system on all such images, but it would take many years to evaluate each in turn on even the fastest computer.  The project's novelties are in assuring correct behavior for groups of inputs collectively, which promises to make the assurance process practical.\r\n\r\nThis project develops techniques to accelerate verification algorithms for assuring the correct operation of machine-learning models.  First, these techniques exploit the fact that the system will only ever be required to consider a small fraction of the set of all possible inputs.  Those inputs can be described symbolically and considered in groups to accelerate verification.  Second, these techniques exploit the fact that a system may respond to different inputs by performing identical processing.   Focusing verification on the processing performed by the system, rather than the inputs that are processed, allows verification to group sets of inputs to further accelerate assurance. The project develops a series of prototype implementations and benchmarks that demonstrate the utility and cost-effectiveness of the research, and that can be leveraged by the broader community for comparative evaluation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Dwyer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Dwyer",
   "pi_email_addr": "md3cn@virginia.edu",
   "nsf_id": "000103915",
   "pi_start_date": "2020-06-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sebastian",
   "pi_last_name": "Elbaum",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Sebastian G Elbaum",
   "pi_email_addr": "selbaum@virginia.edu",
   "nsf_id": "000412723",
   "pi_start_date": "2020-06-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Preston",
   "pi_last_name": "Fletcher",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Preston T Fletcher",
   "pi_email_addr": "tomfletcher@virginia.edu",
   "nsf_id": "000569294",
   "pi_start_date": "2020-06-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "Rice Hall, Room 424",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044740",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "094Y00",
   "pgm_ele_name": "FMitF: Formal Methods in the F"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "071Z",
   "pgm_ref_txt": "FMitF-Formal Methods in the Field"
  },
  {
   "pgm_ref_code": "8206",
   "pgm_ref_txt": "Formal Methods and Verification"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 510000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Neural networks are improving rapidly in their ability to accurately approximate complex functions and can exceed the performance of human-built solutions in a number of domains.&nbsp; To gain the benefits of increased performance, developers across a variety of sectors including: transportation, medicine, defense, and financial, are considering deploying neural networks as components of their systems.&nbsp; To do so confidently requires evidence that the networks work as intended.&nbsp; Recent research has adapted formal verification techniques to target the rigorous analysis of neural networks for this purpose, but these techniques perform highly over-approximative reasoning about about network behavior which leads to either a failure to confirm desired behavior or false reports of erroneous behavior, thereby limiting the usefulness of the techniques.&nbsp; To address this challenge, we pursued two complementary prongs of research.<br /><br />First, we exploit emerging techniques for training latent-space generative models for a dataset and compose those models with networks trained to make predictions from that dataset.&nbsp; Verifying such a composition leads to a significant reduction in over-approximation, which in turn leads directly to a reduction in false error reports and to increases in the verification problems that can be solved.&nbsp; This class of distribution-aware verification and validation techniques has broad applicability, for instance, to black box test adequacy metrics and principled test generation techniques for neural networks.<br /><br />Second, reducing over-approximation means that the verifier need not reason about behavior that is infeasible relative to the generative model.&nbsp; This allows for the development of cost-effective optimization techniques that seek to constrain the space of neural activations over which the verification algorithm operates.&nbsp; We have developed a suite of techniques that perform such optimization, by identifying and increasing the number of stable neurons, at network training time and also on-the-fly during verification.&nbsp; These methods have been integrated into state-of-the-art neural network verifiers.<br /><br />This project has introduced a completely new family of methods for verification and validation of neural networks.&nbsp; The results and findings establish a solid theoretical foundation for future work, develop engineering solutions that significantly advance the state-of-the-art, and that freely available in open-source implementations that researchers and practitioners can use them and to promote open-science.<br /><br />More broadly the project has advanced science and engineering in support of more trustworthy machine learning which is an important area of need for the nation.</p><br>\n<p>\n Last Modified: 10/06/2024<br>\nModified by: Matthew&nbsp;Dwyer</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nNeural networks are improving rapidly in their ability to accurately approximate complex functions and can exceed the performance of human-built solutions in a number of domains. To gain the benefits of increased performance, developers across a variety of sectors including: transportation, medicine, defense, and financial, are considering deploying neural networks as components of their systems. To do so confidently requires evidence that the networks work as intended. Recent research has adapted formal verification techniques to target the rigorous analysis of neural networks for this purpose, but these techniques perform highly over-approximative reasoning about about network behavior which leads to either a failure to confirm desired behavior or false reports of erroneous behavior, thereby limiting the usefulness of the techniques. To address this challenge, we pursued two complementary prongs of research.\n\nFirst, we exploit emerging techniques for training latent-space generative models for a dataset and compose those models with networks trained to make predictions from that dataset. Verifying such a composition leads to a significant reduction in over-approximation, which in turn leads directly to a reduction in false error reports and to increases in the verification problems that can be solved. This class of distribution-aware verification and validation techniques has broad applicability, for instance, to black box test adequacy metrics and principled test generation techniques for neural networks.\n\nSecond, reducing over-approximation means that the verifier need not reason about behavior that is infeasible relative to the generative model. This allows for the development of cost-effective optimization techniques that seek to constrain the space of neural activations over which the verification algorithm operates. We have developed a suite of techniques that perform such optimization, by identifying and increasing the number of stable neurons, at network training time and also on-the-fly during verification. These methods have been integrated into state-of-the-art neural network verifiers.\n\nThis project has introduced a completely new family of methods for verification and validation of neural networks. The results and findings establish a solid theoretical foundation for future work, develop engineering solutions that significantly advance the state-of-the-art, and that freely available in open-source implementations that researchers and practitioners can use them and to promote open-science.\n\nMore broadly the project has advanced science and engineering in support of more trustworthy machine learning which is an important area of need for the nation.\t\t\t\t\tLast Modified: 10/06/2024\n\n\t\t\t\t\tSubmitted by: MatthewDwyer\n"
 }
}