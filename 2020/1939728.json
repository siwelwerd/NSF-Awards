{
 "awd_id": "1939728",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Identifying, Measuring, and Mitigating Fairness Issues in AI",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2020-01-01",
 "awd_exp_date": "2021-12-31",
 "tot_intn_awd_amt": 216908.0,
 "awd_amount": 232908.0,
 "awd_min_amd_letter_date": "2019-12-23",
 "awd_max_amd_letter_date": "2020-06-15",
 "awd_abstract_narration": "Bias and Discrimination in Artificial Intelligence (AI) has been receiving increasing attention. Unfortunately, the positive concept Fair AI is difficult to define. For example, it is hard to distinguish between (desired) personalization and (undesired) bias. These differences often depend on context, such as the use of gender or ethnicity in making a medical diagnosis vs. using the same attributes in determining if insurance should cover a medical procedure. This is particularly difficult as AI systems are used in new contexts, enabling products and services that have not been seen before and for which societal concepts of fairness are not yet established. This multidisciplinary project will construct a framework and taxonomy for understanding fairness in societal contexts. Human-computer interaction methods will be developed to learn perceptions of fairness based on human interaction with AI systems. Automated methods will be developed to relate these perceptions to the framework, enabling developers (and eventually automated AI systems) to respond to and correct issues perceived by users of the systems.\r\n\r\nThis exploratory project will develop a taxonomy incorporating concepts of Aristotelian fairness (distributive vs. corrective justice) and Rawlsian fairness (equality of rights and opportunities). A formal literature survey will be used to establish a framework for societal contexts of fairness and how they relate to the Taxonomy. Experiments with perceptions of models both in isolation and in comparison will be used to evaluate situations where people perceive AI systems as fair or unfair. Tools will be developed to identify and explain fairness issues in terms of the taxonomy, based on the elicited perceptions and societal context of the system. While beyond the scope of this project, the outcome of these tools could potentially be used to automatically adjust AI systems to reduce unfairness.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Clifton",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher W Clifton",
   "pi_email_addr": "clifton@cs.purdue.edu",
   "nsf_id": "000436614",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Murat",
   "pi_last_name": "Kantarcioglu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Murat Kantarcioglu",
   "pi_email_addr": "muratk@vt.edu",
   "nsf_id": "000288472",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Blase",
   "pi_last_name": "Ur",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Blase Ur",
   "pi_email_addr": "blase@uchicago.edu",
   "nsf_id": "000754479",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lindsay",
   "pi_last_name": "Weinberg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lindsay Weinberg",
   "pi_email_addr": "lweinber@purdue.edu",
   "nsf_id": "000797517",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Yeomans",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher L Yeomans",
   "pi_email_addr": "cyeomans@purdue.edu",
   "nsf_id": "000803817",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "305 N University St",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072107",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  },
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0757",
   "pgm_ref_txt": "COOP PLAN OPs & SERVICES"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 232908.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Fairness in Artificial Intelligence (AI) is a growing concern, and research has begun to address this issue.&nbsp; However, much of the work seeks to satisfy straightforward statistical measure of bias and discrimination.&nbsp; The reality is that \"fair\" is much more nuanced.<br /><br />Our multidisciplinary team; with PIs from philosophy, machine learning, privacy, human-computer interaction, and critical theory, science and technology studies, and feminist studies; has explored issues of fairness from several angles, including perceptions of AI fairness (through literature review, crowdsourced studies, and focus groups), sources of unfairness (including data and inherent issues with machine learning), and methods to mitigate bias (such as tools to alert data scientists to situations with potential bias, and using synthetic data to alleviate data-induced bias in machine learning.)<br /><br />Key outcomes include:</p>\n<ul>\n<li>Articles reframing AI fairness in the context of social justice, including a framing as equal concession, and a critical review categorizing issues with current approaches to formulating Fair AI.</li>\n<li>Articles describing crowdsourced perceptions of fairness, and the variation in fairness perceptions between individuals and the impact of intersectionality on these perceptions.&nbsp; This has further been investigated with a deeper dive into expert perceptions of fairness in a healthcare context.</li>\n<li>User study of how explanations of data label use for machine learning impact human annotators in the context of different interfaces and instructions for annotation, providing a means to avoid unfair and spurious correlations.</li>\n<li>Developed the Retrograde extension for Jupyter Notebooks; a tool that automatically identifies protected classes, and proxy variables for those protected classes, and reports results independently for demographics subgroups to highlight potential disparities to analyst/developers using scikit-learn.</li>\n<li>Machine learning tends to favor majority groups; we developed a method to generate synthetic data to enhance outcomes by reducing underrepresentation.</li>\n<li>We showed that even with equally represented groups and equalized outcomes, machine learning can in some situations be expected to produce biased outcomes; this represents an AI-induced systemic bias.</li>\n</ul>\n<p>The project has also had a strong influence in education.&nbsp; Numerous undergraduate and graduate students have been directly involved in the research.&nbsp; In addition, the project has had a long-term educational impact through inclusion of an ethics component in a new Bachelor's in AI at Purdue, and a new engineering and computing ethics course at the University of Chicago.<br /><br />While this has been a short project (one year of work, performed over two due to the pandemic), it has resulted in significant new insights into why AI unfairness is a problem, how to understand that problem, and means to develop (or in some cases, not deploy) AI so as to enhance social justice.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/28/2022<br>\n\t\t\t\t\tModified by: Christopher&nbsp;W&nbsp;Clifton</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFairness in Artificial Intelligence (AI) is a growing concern, and research has begun to address this issue.  However, much of the work seeks to satisfy straightforward statistical measure of bias and discrimination.  The reality is that \"fair\" is much more nuanced.\n\nOur multidisciplinary team; with PIs from philosophy, machine learning, privacy, human-computer interaction, and critical theory, science and technology studies, and feminist studies; has explored issues of fairness from several angles, including perceptions of AI fairness (through literature review, crowdsourced studies, and focus groups), sources of unfairness (including data and inherent issues with machine learning), and methods to mitigate bias (such as tools to alert data scientists to situations with potential bias, and using synthetic data to alleviate data-induced bias in machine learning.)\n\nKey outcomes include:\n\nArticles reframing AI fairness in the context of social justice, including a framing as equal concession, and a critical review categorizing issues with current approaches to formulating Fair AI.\nArticles describing crowdsourced perceptions of fairness, and the variation in fairness perceptions between individuals and the impact of intersectionality on these perceptions.  This has further been investigated with a deeper dive into expert perceptions of fairness in a healthcare context.\nUser study of how explanations of data label use for machine learning impact human annotators in the context of different interfaces and instructions for annotation, providing a means to avoid unfair and spurious correlations.\nDeveloped the Retrograde extension for Jupyter Notebooks; a tool that automatically identifies protected classes, and proxy variables for those protected classes, and reports results independently for demographics subgroups to highlight potential disparities to analyst/developers using scikit-learn.\nMachine learning tends to favor majority groups; we developed a method to generate synthetic data to enhance outcomes by reducing underrepresentation.\nWe showed that even with equally represented groups and equalized outcomes, machine learning can in some situations be expected to produce biased outcomes; this represents an AI-induced systemic bias.\n\n\nThe project has also had a strong influence in education.  Numerous undergraduate and graduate students have been directly involved in the research.  In addition, the project has had a long-term educational impact through inclusion of an ethics component in a new Bachelor's in AI at Purdue, and a new engineering and computing ethics course at the University of Chicago.\n\nWhile this has been a short project (one year of work, performed over two due to the pandemic), it has resulted in significant new insights into why AI unfairness is a problem, how to understand that problem, and means to develop (or in some cases, not deploy) AI so as to enhance social justice.\n\n\t\t\t\t\tLast Modified: 04/28/2022\n\n\t\t\t\t\tSubmitted by: Christopher W Clifton"
 }
}