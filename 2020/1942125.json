{
 "awd_id": "1942125",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Toward Equitable Social Media Content Moderation for Marginalized Individuals and Communities",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2020-02-01",
 "awd_exp_date": "2025-01-31",
 "tot_intn_awd_amt": 549257.0,
 "awd_amount": 549257.0,
 "awd_min_amd_letter_date": "2020-02-24",
 "awd_max_amd_letter_date": "2024-05-17",
 "awd_abstract_narration": "This research examines the causes and consequences of online platforms banning users and content that do not actually violate site policies, or fall into gray areas with respect to site policies and community norms. It will achieve: (1) A characterization of the types and prevalence of reasons people are banned or have content removed from social media sites, how these relate to systemic biases, and the implications of these practices. (2) An in-depth description of processes and support structures marginalized online communities enact to address inequities resulting from moderation. (3) A combined qualitative and computational technical approach that more accurately delineates between social media content that should and should not have been banned, according to community norms and shifting site policies. (4) Design recommendations for a context-aware content moderation system that balances community needs, norms, and values with platform requirements and can be applied across social media sites. (5) Theoretical insights around current inequities in online content moderation and how moderation goals can be achieved more equitably by recognizing and respecting marginalized online communities\u2019 needs. \r\n\r\nThe following studies will be conducted: 1) Interviews with marginalized people whose content or account was removed from a social media site even though in line with site policies or falling into a gray area; 2) A virtual ethnography in three marginalized online communities who are frequent targets of user bans and content takedowns. 3) Use of participatory design and focus group methods to determine how to build a context-aware content moderation system that can be applied across social media sites. 4) Implementation of this sociotechnical system online, and evaluation to understand the extent to which users consider it equitable and how well it meets their needs. In addition, a digital literacy online resource will help educate people about content and account removals and how to regain site access, and the results of the research will educate social media platforms about challenges marginalized populations face around content moderation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Oliver",
   "pi_last_name": "Haimson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Oliver Haimson",
   "pi_email_addr": "haimson@umich.edu",
   "nsf_id": "000792219",
   "pi_start_date": "2020-02-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "3003 South State Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091274",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 92376.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 117421.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 266745.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 72715.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This CAREER project investigated the impacts of social media content moderation on marginalized individuals and communities &ndash; particularly users whose content or accounts were removed inappropriately or in ambiguous cases where policies are inconsistently applied. Over five years, this research advanced both theoretical understanding and practical approaches to equitable content moderation for marginalized groups.</p>\r\n<p>We conducted a range of qualitative, quantitative, mixed-methods, and design research studies including surveys, interviews, diary studies, and digital ethnographies. Our findings shed light on how moderation disproportionately affects marginalized groups such as transgender and nonbinary people, Black people, queer Chinese content creators, and ethnic minorities in the Global South.</p>\r\n<p><strong>Key Findings:</strong></p>\r\n<ul>\r\n<li><strong>Disproportionate Impact and Moderation Gray Areas:</strong> Our research revealed that marginalized users &ndash; especially transgender and Black users &ndash; experience content and account takedowns even when their content complies with social media platforms&rsquo; stated guidelines. These cases highlight what we call moderation gray areas, where policy ambiguity enables bias and contextual misunderstandings.</li>\r\n<li><strong>Shadowbanning and Invisible Labor:</strong> Marginalized creators engage in extensive invisible labor to navigate shadowbanning, an opaque moderation practice that reduces their visibility and has substantial tangible consequences. These creators&rsquo; invisible labor includes mental and emotional labor, misdirected labor, and community labor.</li>\r\n<li><strong>Folk Theories and Platform Spirit:</strong> Users often come up with informal folk theories to interpret why moderation actions happen, based on perceived platform values (&ldquo;platform spirit&rdquo;). Users&rsquo; folk theories offer insight into how moderation systems often misalign with community expectations and values.</li>\r\n<li><strong>Cross-Cultural and Transnational Insights:</strong> We explored moderation experiences on platforms like China&rsquo;s Douyin and in contexts such as Nigeria, where users face compounded risks due to state censorship and lack of rights protections. These studies offer new understandings of platform governance in high-stakes environments.</li>\r\n<li><strong>Educational Contribution:</strong> We developed the Online Identity Help Center&nbsp;<span>(<a href=\"https://www.oihc.org/\">https://www.oihc.org/</a>)</span>, an educational resource to improve digital literacy around moderation policies and user rights. This site helps users understand platform rules, navigate appeals processes, and regain account access.</li>\r\n<li><strong>Platform Justice Advocate:</strong> We propose a new role, the <strong>Platform Justice Advocate</strong>, as a community-based mediator who brings deep contextual knowledge of marginalized communities to platform governance, challenging one-size-fits-all approaches and advocating for fairer systems.</li>\r\n</ul>\r\n<ul>\r\n</ul>\r\n<p><strong>Intellectual Merit:</strong></p>\r\n<p>This project significantly contributes to the fields of Human-Computer Interaction (HCI), social computing, and platform governance. It introduces four new theoretical concepts: <strong>moderation gray areas</strong>, <strong>collaborative algorithm investigation</strong>, <strong>trans-centered moderation</strong>, and <strong>platform justice advocacy</strong>. These concepts help explain how users experience, interpret, and resist content moderation systems, particularly when platforms fail to account for social complexities and identity-based marginalization.</p>\r\n<p>Our research provides empirical evidence that current content moderation systems replicate systemic inequities, often exacerbating harm for already-marginalized users. By documenting these dynamics through qualitative, quantitative, and mixed methods, we offer robust insights into where content moderation fails and how it might be improved. These findings have been widely cited across computing, media studies, and public policy, helping to shape future academic inquiry, platform design, and policy.</p>\r\n<p><strong>Broader Impacts:</strong></p>\r\n<p>This project had substantial broader impacts. We trained and mentored over 20 student researchers, many from underrepresented groups, who gained experience in qualitative methods, quantitative methods, digital ethnography, and academic publishing. Several have gone on to academic and industry roles, further amplifying the project&rsquo;s influence.</p>\r\n<p>Our findings have informed public conversations and platform policy debates, and our work has been cited in academic, legal, and advocacy contexts. By centering the needs and experiences of marginalized users, this project contributes toward building more equitable online spaces and platform governance frameworks that recognize social difference and contextual nuance. Further, this project&rsquo;s educational components bridge scholarship and practice, translating our research results into actionable resources for everyday users.</p><br>\n<p>\n Last Modified: 04/10/2025<br>\nModified by: Oliver&nbsp;Haimson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis CAREER project investigated the impacts of social media content moderation on marginalized individuals and communities  particularly users whose content or accounts were removed inappropriately or in ambiguous cases where policies are inconsistently applied. Over five years, this research advanced both theoretical understanding and practical approaches to equitable content moderation for marginalized groups.\r\n\n\nWe conducted a range of qualitative, quantitative, mixed-methods, and design research studies including surveys, interviews, diary studies, and digital ethnographies. Our findings shed light on how moderation disproportionately affects marginalized groups such as transgender and nonbinary people, Black people, queer Chinese content creators, and ethnic minorities in the Global South.\r\n\n\nKey Findings:\r\n\r\nDisproportionate Impact and Moderation Gray Areas: Our research revealed that marginalized users  especially transgender and Black users  experience content and account takedowns even when their content complies with social media platforms stated guidelines. These cases highlight what we call moderation gray areas, where policy ambiguity enables bias and contextual misunderstandings.\r\nShadowbanning and Invisible Labor: Marginalized creators engage in extensive invisible labor to navigate shadowbanning, an opaque moderation practice that reduces their visibility and has substantial tangible consequences. These creators invisible labor includes mental and emotional labor, misdirected labor, and community labor.\r\nFolk Theories and Platform Spirit: Users often come up with informal folk theories to interpret why moderation actions happen, based on perceived platform values (platform spirit). Users folk theories offer insight into how moderation systems often misalign with community expectations and values.\r\nCross-Cultural and Transnational Insights: We explored moderation experiences on platforms like Chinas Douyin and in contexts such as Nigeria, where users face compounded risks due to state censorship and lack of rights protections. These studies offer new understandings of platform governance in high-stakes environments.\r\nEducational Contribution: We developed the Online Identity Help Center(https://www.oihc.org/), an educational resource to improve digital literacy around moderation policies and user rights. This site helps users understand platform rules, navigate appeals processes, and regain account access.\r\nPlatform Justice Advocate: We propose a new role, the Platform Justice Advocate, as a community-based mediator who brings deep contextual knowledge of marginalized communities to platform governance, challenging one-size-fits-all approaches and advocating for fairer systems.\r\n\r\n\r\n\r\n\n\nIntellectual Merit:\r\n\n\nThis project significantly contributes to the fields of Human-Computer Interaction (HCI), social computing, and platform governance. It introduces four new theoretical concepts: moderation gray areas, collaborative algorithm investigation, trans-centered moderation, and platform justice advocacy. These concepts help explain how users experience, interpret, and resist content moderation systems, particularly when platforms fail to account for social complexities and identity-based marginalization.\r\n\n\nOur research provides empirical evidence that current content moderation systems replicate systemic inequities, often exacerbating harm for already-marginalized users. By documenting these dynamics through qualitative, quantitative, and mixed methods, we offer robust insights into where content moderation fails and how it might be improved. These findings have been widely cited across computing, media studies, and public policy, helping to shape future academic inquiry, platform design, and policy.\r\n\n\nBroader Impacts:\r\n\n\nThis project had substantial broader impacts. We trained and mentored over 20 student researchers, many from underrepresented groups, who gained experience in qualitative methods, quantitative methods, digital ethnography, and academic publishing. Several have gone on to academic and industry roles, further amplifying the projects influence.\r\n\n\nOur findings have informed public conversations and platform policy debates, and our work has been cited in academic, legal, and advocacy contexts. By centering the needs and experiences of marginalized users, this project contributes toward building more equitable online spaces and platform governance frameworks that recognize social difference and contextual nuance. Further, this projects educational components bridge scholarship and practice, translating our research results into actionable resources for everyday users.\t\t\t\t\tLast Modified: 04/10/2025\n\n\t\t\t\t\tSubmitted by: OliverHaimson\n"
 }
}