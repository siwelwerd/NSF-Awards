{
 "awd_id": "1955370",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CNS Core: Medium: Learning to Cache and Caching to Learn in High Performance Caching Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ann Von Lehmen",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2020-09-10",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Caching is fundamental to cloud computing and content distribution, and is important to the vast number of applications and services they support.  Crucial performance metrics of a caching algorithm are its ability to quickly and accurately learn a changing popularity distribution. However, there is a serious disconnect between empirical studies using real-world traces that account for popularity changes, and analytical performance analysis results that assume a fixed popularity.  A basic goal of this project is to develop a methodology based on online learning and reinforcement learning for caching algorithm design with provable performance guarantees.  This enables the systematic design of caching algorithms that can be tailored to a variety of application contexts. The use-case of these algorithms is in high performance caching networks that support large-scale cloud applications and services.  Emulation of high-performance caching systems to leverage and to empirically evaluate the online learning algorithms developed supports this goal, and provides a real-world context for the methodology developed.  The results will also enhance the performance of content distribution platforms.  At the same time the project develops fundamental theories that pertain to the area of machine learning, specifically to online learning. \r\n\r\nThis project aims at optimally utilizing locally available memory and computing resources of caches, while ensuring provably good performance via fast and accurate learning of content popularity. This requires the conjunction of several mathematical tools to analyze online learning algorithms, as well as strong systems development skills to make the algorithms a reality. The project addresses these key challenges in two main themes. The first theme focuses on systematic design of distributed online learning in networks of caches using collaborative filtering for distributed identification of popular content, and multi-agent reinforcement learning for joint learning and content placement. The second theme focuses on building high performing caching systems using the algorithms developed in the first theme, and quantifying the impacts of the algorithms on real-world applications such as Hipster Shop, an open-source e-commerce website, and Spark data-analytics job pipelines. The immediate impact of this project is in creating high performance caching schemes that apply to cloud computing and content distribution networks. This project also advances the fundamental theory of online learning. The project includes an education plan focusing on machine learning and caching, and outreach in the form of summer camps and seminars for high school students.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohammad",
   "pi_last_name": "Alizadeh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohammad Alizadeh",
   "pi_email_addr": "alizadeh@csail.mit.edu",
   "nsf_id": "000703497",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"flex-1 overflow-hidden\">\n<div class=\"react-scroll-to-bottom--css-xxbud-79elbk h-full\">\n<div class=\"react-scroll-to-bottom--css-xxbud-1n7m0yu\">\n<div class=\"flex flex-col text-sm pb-9\">\n<div class=\"w-full text-token-text-primary\">\n<div class=\"px-4 py-2 justify-center text-base md:gap-6 m-auto\">\n<div class=\"flex flex-1 text-base mx-auto gap-3 md:px-5 lg:px-1 xl:px-5 md:max-w-3xl lg:max-w-[40rem] xl:max-w-[48rem] group final-completion\">\n<div class=\"relative flex w-full flex-col agent-turn\">\n<div class=\"flex-col gap-1 md:gap-3\">\n<div class=\"flex flex-grow flex-col max-w-full\">\n<div class=\"min-h-[20px] text-message flex flex-col items-start gap-3 whitespace-pre-wrap break-words [.text-message+&amp;]:mt-5 overflow-x-auto\">\n<div class=\"markdown prose w-full break-words dark:prose-invert light\">\n<p>Reinforcement learning (RL) has emerged as a powerful approach to learning sophisticated control and decision-making strategies in networked systems, but real-world deployments have been rare. One major hurdle has been the gap between simulation and reality, where the environment simulators used to train RL agents do not match the real-world dynamics.</p>\n<p>This project investigated techniques to address these challenges in networked systems. Our methods apply to a broad class of networking problems, e.g., video bitrate adaptation, caching algorithms, scheduling, and load balancing in cloud environments. Our main findings are summarized as follows:</p>\n<p><strong>Online RL in non-stationary systems:</strong> We conducted an empirical study of state-of-the-art on-policy and off-policy RL algorithms in several systems to (i) understand the ways RL-based systems can fail when trained online in a non-stationary environment; (ii) design techniques to limit the impact of misbehaving policies on system performance. Our results highlight several failure modes for online RL&mdash;e.g., training instability, catastrophic forgetting, hyper-parameter sensitivity&mdash;but show that carefully designing the system architecture around the learned agent can enable safe online learning.</p>\n<p>A key challenge in online learning is that the agent tends to forget prior knowledge as it trains on new experiences, a problem known as catastrophic forgetting. We developed Locally Constrained Policy Optimization (LCPO), an online RL approach that combats catastrophic forgetting by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context.</p>\n<p><strong>Learning techniques for unbiased trace-driven simulation:</strong> An alternative to online training is to improve the fidelity of simulators using trace data. Current trace-driven simulation methods suffer from biases, since they assume that a trace collected from a system remains valid despite the intervention (e.g., a new algorithm or design choice) being simulated. We developed CausalSim, a causal framework for mitigating bias in trace-driven simulation. CausalSim learns a causal model of the system dynamics and the latent factors representing the underlying system conditions during trace collection. It then uses these causal models to estimate how the simulated intervention would have affected the traces, enabling the correction of bias during simulation. Our extensive evaluation of CausalSim on both real and synthetic datasets, including more than ten months of real data from the Puffer video streaming system, showed that it improves simulation accuracy compared to expert-designed and traditional learning baselines. Moreover, CausalSim provided new insights about ABR algorithms compared to the biased baseline simulator, which we validated with a real deployment.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/15/2024<br>\nModified by: Mohammad&nbsp;Alizadeh</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement learning (RL) has emerged as a powerful approach to learning sophisticated control and decision-making strategies in networked systems, but real-world deployments have been rare. One major hurdle has been the gap between simulation and reality, where the environment simulators used to train RL agents do not match the real-world dynamics.\n\n\nThis project investigated techniques to address these challenges in networked systems. Our methods apply to a broad class of networking problems, e.g., video bitrate adaptation, caching algorithms, scheduling, and load balancing in cloud environments. Our main findings are summarized as follows:\n\n\nOnline RL in non-stationary systems: We conducted an empirical study of state-of-the-art on-policy and off-policy RL algorithms in several systems to (i) understand the ways RL-based systems can fail when trained online in a non-stationary environment; (ii) design techniques to limit the impact of misbehaving policies on system performance. Our results highlight several failure modes for online RLe.g., training instability, catastrophic forgetting, hyper-parameter sensitivitybut show that carefully designing the system architecture around the learned agent can enable safe online learning.\n\n\nA key challenge in online learning is that the agent tends to forget prior knowledge as it trains on new experiences, a problem known as catastrophic forgetting. We developed Locally Constrained Policy Optimization (LCPO), an online RL approach that combats catastrophic forgetting by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context.\n\n\nLearning techniques for unbiased trace-driven simulation: An alternative to online training is to improve the fidelity of simulators using trace data. Current trace-driven simulation methods suffer from biases, since they assume that a trace collected from a system remains valid despite the intervention (e.g., a new algorithm or design choice) being simulated. We developed CausalSim, a causal framework for mitigating bias in trace-driven simulation. CausalSim learns a causal model of the system dynamics and the latent factors representing the underlying system conditions during trace collection. It then uses these causal models to estimate how the simulated intervention would have affected the traces, enabling the correction of bias during simulation. Our extensive evaluation of CausalSim on both real and synthetic datasets, including more than ten months of real data from the Puffer video streaming system, showed that it improves simulation accuracy compared to expert-designed and traditional learning baselines. Moreover, CausalSim provided new insights about ABR algorithms compared to the biased baseline simulator, which we validated with a real deployment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 03/15/2024\n\n\t\t\t\t\tSubmitted by: MohammadAlizadeh\n"
 }
}