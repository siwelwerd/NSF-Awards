{
 "awd_id": "1953189",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: New Methods, Theory and Applications for Nonsmooth Manifold-Based Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2020-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-03-12",
 "awd_max_amd_letter_date": "2022-06-27",
 "awd_abstract_narration": "Nowadays, the availability of massive data is continuously increasing, primarily due to the continued advancement of technology. As a consequence, massive high-dimensional data are ubiquitous in many scientific and engineering disciplines, such as bioinformatics, computer vision, neuroimaging, and signal processing. The nonsmooth manifold-based learning with high-dimensional and multidimensional data is in general complicated due to its intrinsic non-convexity and non-smoothness. This project will address both statistical and computational issues of nonsmooth manifold-based learning and explore its new applications. \r\n\r\nIt is known that statistical modeling of high-dimensional data may include the non-smooth regularization in the objective function, and some may even involve non-convex manifold constraints such as orthogonality constraints. The manifold-based learning offers a powerful framework for dimension reduction and signal processing. The combination of non-smooth regularization and non-convex manifold constraints brings new opportunities and challenges for designing optimization algorithms with convergence guarantees and also for developing new statistical methods and theory. The research outcomes of this project will provide new powerful analytic tools in nonsmooth manifold-based learning with theoretical guarantees. Software packages will be developed to make the research outcomes readily available to other researchers and practitioners.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lingzhou",
   "pi_last_name": "Xue",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lingzhou Xue",
   "pi_email_addr": "lzxue@psu.edu",
   "nsf_id": "000657122",
   "pi_start_date": "2020-03-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Xiang",
   "pi_last_name": "Zhan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiang Zhan",
   "pi_email_addr": "xyz5074@psu.edu",
   "nsf_id": "000755270",
   "pi_start_date": "2020-03-12",
   "pi_end_date": "2021-06-25"
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 56223.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 74520.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 69257.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>\n<div id=\":1ep\" class=\"Am aiL Al editable LW-avf tS-tW tS-tY\">Manifold optimization finds many applications in various fields of data science, including machine learning, statistics, bioinformatics, genomics, information science, and so on. For example, Sparse principal component analysis (PCA), sparse canonical correlation analysis (CCA), and sparse spectral clustering all end up with solving manifold optimization problems with nonsmooth objectives. These tools have been widely used to analyze data from genetic studies, microbiome studies, neuroscience, and social science.&nbsp;</div>\n<div class=\"Am aiL Al editable LW-avf tS-tW tS-tY\"></div>\n<div id=\":1ep\" class=\"Am aiL Al editable LW-avf tS-tW tS-tY\">\n<div>In this project, new optimization methods and theory&nbsp;<span>have been developed to study&nbsp;</span><span>several important questions in data science, including the&nbsp;</span><span>Alternating Manifold Proximal Gradient (AManPG) method, the Manifold Proximal Linear (ManPL) method, and the stochastic Manifold Gradient Descent (stManPG) method. These new optimization algorithms have direct applications on high-dimenisonal statistical methods such as sparse PCA, sparse CCA, sparse spectral clutering, online sparse PCA, and so on.&nbsp;</span></div>\n<div><span><br /></span></div>\n<div><span>In this&nbsp;</span><span>project, we have also leveraged&nbsp;the recent development of optimization tools and&nbsp;</span><span>developed new statistical learning methods and theory for</span><span>&nbsp;n</span>on-smooth manifold learning, high-dimensional statistics, statistical network analysis, microbiome compositional data analysis,&nbsp;<span>high-dimensional regression with imaging data, nonparametric forecasting for large panel data, and many others.</span></div>\n<div></div>\n<div><span>Moreover, software packages have been developed to make the research outcomes readily available to other researchers and practitioners.</span></div>\n</div>\n</p><br>\n<p>\n Last Modified: 09/29/2024<br>\nModified by: Lingzhou&nbsp;Xue</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\nManifold optimization finds many applications in various fields of data science, including machine learning, statistics, bioinformatics, genomics, information science, and so on. For example, Sparse principal component analysis (PCA), sparse canonical correlation analysis (CCA), and sparse spectral clustering all end up with solving manifold optimization problems with nonsmooth objectives. These tools have been widely used to analyze data from genetic studies, microbiome studies, neuroscience, and social science.\n\n\nIn this project, new optimization methods and theoryhave been developed to studyseveral important questions in data science, including theAlternating Manifold Proximal Gradient (AManPG) method, the Manifold Proximal Linear (ManPL) method, and the stochastic Manifold Gradient Descent (stManPG) method. These new optimization algorithms have direct applications on high-dimenisonal statistical methods such as sparse PCA, sparse CCA, sparse spectral clutering, online sparse PCA, and so on.\n\n\nIn thisproject, we have also leveragedthe recent development of optimization tools anddeveloped new statistical learning methods and theory fornon-smooth manifold learning, high-dimensional statistics, statistical network analysis, microbiome compositional data analysis,high-dimensional regression with imaging data, nonparametric forecasting for large panel data, and many others.\n\nMoreover, software packages have been developed to make the research outcomes readily available to other researchers and practitioners.\n\n\t\t\t\t\tLast Modified: 09/29/2024\n\n\t\t\t\t\tSubmitted by: LingzhouXue\n"
 }
}