{
 "awd_id": "1948201",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FET: Small: Heterogeneous Learning Architectures and Training Algorithms for Hardware Accelerated Deep Spiking Neural Computation",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927843",
 "po_email": "sabasu@nsf.gov",
 "po_sign_block_name": "Sankar Basu",
 "awd_eff_date": "2019-07-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 499337.0,
 "awd_amount": 499337.0,
 "awd_min_amd_letter_date": "2019-10-22",
 "awd_max_amd_letter_date": "2019-10-22",
 "awd_abstract_narration": "This project aims to address the present performance and energy efficiency crisis in computing across broad areas of data-driven applications by developing energy-efficient new spiking neural architectures, training algorithms, and hardware computing devices.  Inspirations from biological brains will be taken to support the development of algorithms and hardware systems to close the widening gap between the supply and demand of computing power. The outcomes from this project will be strongly interdisciplinary and are expected to stimulate technical advancements in machine learning and bridge between neural networks, neuroscience, and hardware engineering. The research will provide rich training and educational opportunities to students. Research participation from undergraduate students and underrepresented groups will be promoted through various outreach programs.  The results of this project will be disseminated in broad research and industrial communities and integrated into the graduate-level curriculum. Research collaboration with industry will be sought to guide this work toward addressing real-world challenges and provide mentoring and training of students in the industrial setting. \r\n\r\nBrain-inspired models of computation and hardware computing systems hold the promise of delivering the amount of computing power required in processing increasingly large volumes of data in the post Moore's Law era, without a correspondingly high energy cost. This project will focus on improving the performances of spiking neural models for real-life learning tasks by addressing two pressing inter-dependent research roadblocks: lack of computationally powerful learning architectures, and lack of practical algorithms that can effectively train complex spiking neural models. Synergies between neuroscience and deep learning will be explored to develop heterogeneous deep spiking neural architectures and learning algorithms to address the corresponding training bottlenecks. Efficient spiking neural processors will be demonstrated on reconfigurable computing devices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peng",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peng Li",
   "pi_email_addr": "lip@ece.ucsb.edu",
   "nsf_id": "000388188",
   "pi_start_date": "2019-10-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931062050",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 499337.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern deep neural networks (DNNs) and various accelerators have achieved great success. Nevertheless, it is believed that biological brains operate rather differently. Compared with DNNs that lack processing of spike timing and event-driven operations, biologically realistic spiking neural networks (SNNs) provide a promising paradigm for exploiting spatio-temporal patterns for added computing power, and enable ultra-low power event-driven neuromorphic hardware. There are theoretical evidences supporting that SNNs possess greater computational power over traditional artificial neural networks (ANNs). Over the years, many neuromorphic systems have been demonstrated using MOS, memristors, and spin-based devices.&nbsp;&nbsp;</p>\r\n<p>Despite of the great progresses made, the ideas on spiking neural computation tend to be scattered and conceptual; SNN models are very difficult to train in general, are often limited in size and not well connected with real-world needs. While having the promise for big long-term payoffs, spiking neural network research has been stagnant on delivering competitive performances for wider ranges of real-world applications.</p>\r\n<p>This project aims to raise the performances of SNNs for challenging real-life learning tasks by addressing two pressing inter-dependent research roadblocks: 1) lack of computationally powerful SSN learning architectures, 2) and lack of practical algorithms that can effectively train complex spiking models}. &nbsp;As such, the research team of this project explored the synergies between neuroscience and deep learning, developed&nbsp; heterogeneous deep architectures of spiking neurons and learning algorithms, and demonstrated efficient SNN hardware accelerator architectures.</p>\r\n<p>The primary outcomes of this project are as follows:</p>\r\n<p>(a)&nbsp; Developed fundamental new backpropagation-based methods to train recurrent spiking neural networks and train a given SNN to precisely learn targeted temporal behavior; &nbsp;&nbsp;</p>\r\n<p>(b)&nbsp; Investigated architectural design of scalable recurrent spiking neural networks and developed feasible gradient-based and biologically inspired architectural optimization and parameter tuning algorithms for such networks;</p>\r\n<p>(c)&nbsp;&nbsp; Explored several innovative SNN hardware accelerator architectures by leveraging a number of techniques including novel temporal coding, proper packing of temporal workloads to enable parallel processing, efficient power management, optimized dataflows, and 3D integration.</p>\r\n<p>(d)&nbsp; Demonstrated a field programmable gate array (FPGA) based spiking neural network hardware accelerator design incorporating efficient biologically plausible on-chip training;</p>\r\n<p>(e)&nbsp; Investigated design of spiking neural networks that are robust with respect to adversarial attacks by exploring biologically inspired hemostasis mechnisms.</p>\r\n<p>&nbsp;</p>\r\n<p>The research outcomes of this work have been disseminated though a sequence of publications at leading venues for machine learning, neuroscience, computer architecture, circuit design/electronic design automation, and neural networks, and by invited talks and graduate-level teaching.</p>\r\n<p>One of the publications of this project received the best paper award (computer architecture track) from IEEE International Conference on Computer Design (ICCD) in 2020, and another publication was featured as a spot light paper by Conference on Neural Information Processing Systems (NeurIPS) in 2020. &nbsp;</p>\r\n<p>Under PI Li&rsquo;s advising, a number of Ph. D., M.S., and undergraduate students trained. Among these students, three M. S. students and one undergraduate student graduated and are now either working toward a graduate degree or working in the U.S. high-tech industry. &nbsp;Two Ph. D. students graduated and now work as a hardware design engineer and a machine leaning research scientist, respectively, at Meta. One of the two graduated Ph. D. students was awarded a UCSB ECE Ph.D. Dissertation Fellowship.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 02/11/2025<br>\nModified by: Peng&nbsp;Li</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nModern deep neural networks (DNNs) and various accelerators have achieved great success. Nevertheless, it is believed that biological brains operate rather differently. Compared with DNNs that lack processing of spike timing and event-driven operations, biologically realistic spiking neural networks (SNNs) provide a promising paradigm for exploiting spatio-temporal patterns for added computing power, and enable ultra-low power event-driven neuromorphic hardware. There are theoretical evidences supporting that SNNs possess greater computational power over traditional artificial neural networks (ANNs). Over the years, many neuromorphic systems have been demonstrated using MOS, memristors, and spin-based devices.\r\n\n\nDespite of the great progresses made, the ideas on spiking neural computation tend to be scattered and conceptual; SNN models are very difficult to train in general, are often limited in size and not well connected with real-world needs. While having the promise for big long-term payoffs, spiking neural network research has been stagnant on delivering competitive performances for wider ranges of real-world applications.\r\n\n\nThis project aims to raise the performances of SNNs for challenging real-life learning tasks by addressing two pressing inter-dependent research roadblocks: 1) lack of computationally powerful SSN learning architectures, 2) and lack of practical algorithms that can effectively train complex spiking models}. As such, the research team of this project explored the synergies between neuroscience and deep learning, developed heterogeneous deep architectures of spiking neurons and learning algorithms, and demonstrated efficient SNN hardware accelerator architectures.\r\n\n\nThe primary outcomes of this project are as follows:\r\n\n\n(a) Developed fundamental new backpropagation-based methods to train recurrent spiking neural networks and train a given SNN to precisely learn targeted temporal behavior; \r\n\n\n(b) Investigated architectural design of scalable recurrent spiking neural networks and developed feasible gradient-based and biologically inspired architectural optimization and parameter tuning algorithms for such networks;\r\n\n\n(c) Explored several innovative SNN hardware accelerator architectures by leveraging a number of techniques including novel temporal coding, proper packing of temporal workloads to enable parallel processing, efficient power management, optimized dataflows, and 3D integration.\r\n\n\n(d) Demonstrated a field programmable gate array (FPGA) based spiking neural network hardware accelerator design incorporating efficient biologically plausible on-chip training;\r\n\n\n(e) Investigated design of spiking neural networks that are robust with respect to adversarial attacks by exploring biologically inspired hemostasis mechnisms.\r\n\n\n\r\n\n\nThe research outcomes of this work have been disseminated though a sequence of publications at leading venues for machine learning, neuroscience, computer architecture, circuit design/electronic design automation, and neural networks, and by invited talks and graduate-level teaching.\r\n\n\nOne of the publications of this project received the best paper award (computer architecture track) from IEEE International Conference on Computer Design (ICCD) in 2020, and another publication was featured as a spot light paper by Conference on Neural Information Processing Systems (NeurIPS) in 2020. \r\n\n\nUnder PI Lis advising, a number of Ph. D., M.S., and undergraduate students trained. Among these students, three M. S. students and one undergraduate student graduated and are now either working toward a graduate degree or working in the U.S. high-tech industry. Two Ph. D. students graduated and now work as a hardware design engineer and a machine leaning research scientist, respectively, at Meta. One of the two graduated Ph. D. students was awarded a UCSB ECE Ph.D. Dissertation Fellowship.\r\n\n\n\t\t\t\t\tLast Modified: 02/11/2025\n\n\t\t\t\t\tSubmitted by: PengLi\n"
 }
}