{
 "awd_id": "2003683",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CDS&E: Collaborative Research: Hierarchical Kernel Matrices for Scientific and Data Applications",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927116",
 "po_email": "sghafoor@nsf.gov",
 "po_sign_block_name": "Sheikh Ghafoor",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 315981.0,
 "awd_amount": 315981.0,
 "awd_min_amd_letter_date": "2020-08-18",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Kernel matrices in machine learning and scientific computing describe the relationships between collections of points which may represent various types of information.  The increasing size of data sets in various disciplines and the increasing computational capability of computer hardware make it essential that our algorithms and software for kernel matrices are scalable, and that the time it takes for their execution grows linearly or close to linearly, with the problem size. Otherwise, such large-scale data problems may not be tractable.  This project addresses the scaling bottlenecks associated with handling the kernel matrix by exploiting a hierarchical structure that is often found in these matrices.  By accelerating computations with kernel matrices, this research enables large-scale data analysis and scientific simulation in diverse areas such as uncertainty quantification, integral equation problems, particle simulations, and geostatistics.  High-performance software implementing the newly developed methods will be developed in an open-source environment.\r\n\r\nThis project specifically addresses high-dimensional problems, the use of specialized kernel functions in machine learning, and the high initial computational cost of constructing a hierarchical representation for a kernel matrix.  New methods developed will be applied to large-scale cases in a scientific application and a machine learning application: Brownian dynamics and Gaussian process regression.  In machine learning, the new methods will complement existing large-scale approaches for Gaussian processes.  High-performance software will address specific scaling challenges in constructing hierarchical matrices.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Edmond",
   "pi_last_name": "Chow",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Edmond Chow",
   "pi_email_addr": "echow@cc.gatech.edu",
   "nsf_id": "000596585",
   "pi_start_date": "2020-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Ave",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320420",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  },
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "026Z",
   "pgm_ref_txt": "NSCI: National Strategic Computing Initi"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 315981.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Kernel matrices arise in both machine learning and scientific computing, describing the similarity or interactions between feature vectors or points.&nbsp; As data sizes and simulation resolutions continue to increase, and as computational hardware continues to advance, it is necessary to develop scalable algorithms and efficient implementations to compute with kernel matrices.</span><span><br /></span></p>\r\n<p><span>This project has made substantial contributions to computational methods for kernel matrices.&nbsp; A major outcome of this project is the development of a new preconditioner for kernel matrices that can accelerate iterative linear solvers for the most challenging problems.&nbsp; The new preconditioner, called Adaptive Factorized Nystrom (AFN), is designed to be efficient and robust for a large range of kernel correlation length scales and noise level parameters.&nbsp; This is borne out by numerical tests in a number of settings, including for high-dimensional datasets.&nbsp; The AFN preconditioner factorizes into block triangular matrices, making it also possible to use in accelerating stochastic log-determinant calculations in statistics, machine learning, and computational physics.</span><span><br /></span></p>\r\n<p><span>Another major outcome of the project is related to the efficient construction of hierarchical matrix representations of kernel matrices.&nbsp; This allows one to operate with kernel matrices very rapidly.&nbsp; Although many such construction algorithms exist, an ongoing challenge is the case where data is high-dimensional.&nbsp; The project developed an approach for the high-dimensional case that is data-driven, only relying on the data and not the kernel function, and can thus be applied independently of the kernel function.&nbsp; In tests with general kernels, the data-driven method significantly reduces memory usage, particularly when the data lies on manifold structures.&nbsp; In the case of the Coulomb kernel, the algorithm displays performance on par with the Fast Multipole Method and its variants.</span><span><br /></span></p>\r\n<p><span>A third major outcome of the project is an algorithm and theoretical foundation for the low-rank approximation of kernel matrices using geometric criteria.&nbsp; The results are very general, and can handle kernel matrices defined using two sets of data points that are intermingled and not separated.&nbsp; Additionally, a linear complexity computational algorithm, the anchor net method, was developed for low-rank approximation.&nbsp; This method consistently delivers robust performance across symmetric positive semi-definite and indefinite kernel matrices, applicable from low-dimensional to high-dimensional datasets.</span><span><br /></span></p>\r\n<p><span>A fourth major outcome is the development of HiGP, a high-performance Python package for using Gaussian processes (GPs) with large datasets, incorporating many developments of the project.&nbsp; HiGP functionality includes estimating GP hyperparameters, GP regression, and GP classification.&nbsp; HiGP uses highly-optimized, multithreaded C++ code, implementing iterative solvers and the AFN preconditioner.&nbsp; It also provides for preconditioned stochastic estimation of log-determinants important in gradient-based hyperparameter estimation.&nbsp; HiGP is open source and ready to be used by the scientific community.</span><span><br /></span></p>\r\n<p><span>The outcomes of this project have been disseminated through publications in journals in applied mathematics and computational physics, and through invited and contributed presentations at workshops, symposia, and universities.&nbsp; It has trained two PhD students and two post-doctoral scholars.&nbsp; The project team also organized the 13th International Conference on Preconditioning Techniques for Scientific and Industrial Applications, where the project results were also presented.&nbsp; Additionally, the team organized a summer school for undergraduates at an HBCU.&nbsp; The summer school aimed to foster applications of machine learning in scientific contexts.&nbsp; This project has substantially enhanced the understanding of solvers for large-scale kernel matrices and broadened the capability to reliably tackle many scientific and data science applications, including integral equations, particle simulations, and Gaussian processes.</span></p><br>\n<p>\n Last Modified: 01/28/2025<br>\nModified by: Edmond&nbsp;Chow</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nKernel matrices arise in both machine learning and scientific computing, describing the similarity or interactions between feature vectors or points. As data sizes and simulation resolutions continue to increase, and as computational hardware continues to advance, it is necessary to develop scalable algorithms and efficient implementations to compute with kernel matrices.\n\r\n\n\nThis project has made substantial contributions to computational methods for kernel matrices. A major outcome of this project is the development of a new preconditioner for kernel matrices that can accelerate iterative linear solvers for the most challenging problems. The new preconditioner, called Adaptive Factorized Nystrom (AFN), is designed to be efficient and robust for a large range of kernel correlation length scales and noise level parameters. This is borne out by numerical tests in a number of settings, including for high-dimensional datasets. The AFN preconditioner factorizes into block triangular matrices, making it also possible to use in accelerating stochastic log-determinant calculations in statistics, machine learning, and computational physics.\n\r\n\n\nAnother major outcome of the project is related to the efficient construction of hierarchical matrix representations of kernel matrices. This allows one to operate with kernel matrices very rapidly. Although many such construction algorithms exist, an ongoing challenge is the case where data is high-dimensional. The project developed an approach for the high-dimensional case that is data-driven, only relying on the data and not the kernel function, and can thus be applied independently of the kernel function. In tests with general kernels, the data-driven method significantly reduces memory usage, particularly when the data lies on manifold structures. In the case of the Coulomb kernel, the algorithm displays performance on par with the Fast Multipole Method and its variants.\n\r\n\n\nA third major outcome of the project is an algorithm and theoretical foundation for the low-rank approximation of kernel matrices using geometric criteria. The results are very general, and can handle kernel matrices defined using two sets of data points that are intermingled and not separated. Additionally, a linear complexity computational algorithm, the anchor net method, was developed for low-rank approximation. This method consistently delivers robust performance across symmetric positive semi-definite and indefinite kernel matrices, applicable from low-dimensional to high-dimensional datasets.\n\r\n\n\nA fourth major outcome is the development of HiGP, a high-performance Python package for using Gaussian processes (GPs) with large datasets, incorporating many developments of the project. HiGP functionality includes estimating GP hyperparameters, GP regression, and GP classification. HiGP uses highly-optimized, multithreaded C++ code, implementing iterative solvers and the AFN preconditioner. It also provides for preconditioned stochastic estimation of log-determinants important in gradient-based hyperparameter estimation. HiGP is open source and ready to be used by the scientific community.\n\r\n\n\nThe outcomes of this project have been disseminated through publications in journals in applied mathematics and computational physics, and through invited and contributed presentations at workshops, symposia, and universities. It has trained two PhD students and two post-doctoral scholars. The project team also organized the 13th International Conference on Preconditioning Techniques for Scientific and Industrial Applications, where the project results were also presented. Additionally, the team organized a summer school for undergraduates at an HBCU. The summer school aimed to foster applications of machine learning in scientific contexts. This project has substantially enhanced the understanding of solvers for large-scale kernel matrices and broadened the capability to reliably tackle many scientific and data science applications, including integral equations, particle simulations, and Gaussian processes.\t\t\t\t\tLast Modified: 01/28/2025\n\n\t\t\t\t\tSubmitted by: EdmondChow\n"
 }
}