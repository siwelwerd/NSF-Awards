{
 "awd_id": "1935928",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Collaborative: When Adversarial Learning Meets Differential Privacy: Theoretical Foundation and Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jeremy Epstein",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 249999.0,
 "awd_amount": 249999.0,
 "awd_min_amd_letter_date": "2020-06-11",
 "awd_max_amd_letter_date": "2020-06-11",
 "awd_abstract_narration": "The pervasiveness of machine learning exposes new and severe vulnerabilities in software systems, where deployed deep neural networks can be exploited to reveal sensitive information in private training data, and to make the models misclassify. However, existing learning algorithms have not been designed to be simultaneously robust to such privacy and integrity attacks, in both theory and practice. In field trials, such lack of protection and efficacy significantly degrades the performance of machine learning-based systems, and puts sensitive data at high risk, thereby exposing service providers to legal action based on HIPAA/HITECH law and related regulations. This project aims to develop the first framework to advance and seamlessly integrate key techniques, including adversarial learning, privacy preserving, and certified defenses, offering tight and reliable protection against both privacy and integrity attacks, while retaining high model utility in deep neural networks. The system is being developed for scalable, complex, and commonly used machine learning frameworks, providing a fundamental impact to both industry and educational environments.\r\n\r\nAn ultimate goal of this project is to build a core foundation of privacy preservation in adversarial learning, to better address the trade-off between model utility, privacy loss, and certified defenses. Accordingly, the team theoretically connects adversarial learning and privacy preservation by introducing a new set of rigorous theories to address the trade-off between model utility and privacy loss. To further strengthen the safety of the system, the team will conduct a new class of attacks towards discovering previously unknown and unprotected vulnerabilities, including highly sensitive and hidden correlation structures among data instances, which will be used to amplify existing model attacks. Based upon that effort, vulnerable features and correlations will be automatically identified and protected, towards unified robust and privacy preserving learning, given both model training and inference. Finally, the team will optimize the trade-off among model utility, privacy loss, and certified defenses. The project is expected to lay a theoretical and practical foundation of key privacy-preserving techniques to protect users' personal and highly sensitive data in adversarial learning under model attacks.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hai",
   "pi_last_name": "Phan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hai Phan",
   "pi_email_addr": "phan@njit.edu",
   "nsf_id": "000661557",
   "pi_start_date": "2020-06-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Jersey Institute of Technology",
  "inst_street_address": "323 DR MARTIN LUTHER KING JR BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9735965275",
  "inst_zip_code": "071021824",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NJ10",
  "org_lgl_bus_name": "NEW JERSEY INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "SGBMHQ7VXNH5"
 },
 "perf_inst": {
  "perf_inst_name": "New Jersey Institute of Technology",
  "perf_str_addr": "University Heights",
  "perf_city_name": "Newark",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "071021982",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NJ10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 249999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-9a47dff4-7fff-ed34-f001-5ab5b41f7ecf\">\r\n<p dir=\"ltr\"><span>The pervasiveness of machine learning exposes new and severe vulnerabilities in software systems, where deployed deep neural networks can be exploited to reveal sensitive information in private training data, and to make the models misclassify. However, existing learning algorithms have not been designed to be simultaneously robust to such privacy and integrity attacks, in both theory and practice. In field trials, such lack of protection and efficacy significantly degrades the performance of machine learning-based systems, and puts sensitive data at high risk, thereby exposing service providers to legal action based on HIPAA/HITECH law and related regulations. This project aims to develop the first framework to advance and seamlessly integrate key techniques, including adversarial learning, privacy preserving, and certified defenses, offering tight and reliable protection against both privacy and integrity attacks, while retaining high model utility in deep neural networks. The system is being developed for scalable, complex, and commonly used machine learning frameworks, providing a fundamental impact to both industry and educational environments.&nbsp;</span></p>\r\n<br />\r\n<p dir=\"ltr\"><span>The project has four technical components: 1) Preserving DP in Adversarial Learning; 2) Robustness Bounds for DP in Adversarial Learning; 3) Revealing New Vulnerabilities based on Hidden and Correlated Features; and 4) Unified Robust and DP Learning against Model Attacks.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span><br /></span></p>\r\n<p dir=\"ltr\"><span>To fulfill the technical components&rsquo; scope, the project developed novel mechanisms to preserve differential privacy in adversarial learning and beyond, including federated learning, large language models, and explainable machine learning. In addition, the project developed a series of new privacy attacks to reveal previously unknown vulnerabilities in these learning paradigms. The project further developed unified certified bounds for differential privacy, robustness, and fairness in centralized and federated learning against privacy and integrity attacks. Finally, the project integrated these novel approaches into a unified system, which has been prototyped and evaluated in real-world settings. As a result, the project opened several new research directions in privacy and integrity preservation in large language models and emerging machine learning applications such as software code generation. The project incorporated research results into course development, which contributed significantly to broadening participation in computing.</span></p>\r\n</span></p><br>\n<p>\n Last Modified: 11/26/2024<br>\nModified by: Hai&nbsp;Phan</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nThe pervasiveness of machine learning exposes new and severe vulnerabilities in software systems, where deployed deep neural networks can be exploited to reveal sensitive information in private training data, and to make the models misclassify. However, existing learning algorithms have not been designed to be simultaneously robust to such privacy and integrity attacks, in both theory and practice. In field trials, such lack of protection and efficacy significantly degrades the performance of machine learning-based systems, and puts sensitive data at high risk, thereby exposing service providers to legal action based on HIPAA/HITECH law and related regulations. This project aims to develop the first framework to advance and seamlessly integrate key techniques, including adversarial learning, privacy preserving, and certified defenses, offering tight and reliable protection against both privacy and integrity attacks, while retaining high model utility in deep neural networks. The system is being developed for scalable, complex, and commonly used machine learning frameworks, providing a fundamental impact to both industry and educational environments.\r\n\n\r\n\n\nThe project has four technical components: 1) Preserving DP in Adversarial Learning; 2) Robustness Bounds for DP in Adversarial Learning; 3) Revealing New Vulnerabilities based on Hidden and Correlated Features; and 4) Unified Robust and DP Learning against Model Attacks.\r\n\n\n\n\r\n\n\nTo fulfill the technical components scope, the project developed novel mechanisms to preserve differential privacy in adversarial learning and beyond, including federated learning, large language models, and explainable machine learning. In addition, the project developed a series of new privacy attacks to reveal previously unknown vulnerabilities in these learning paradigms. The project further developed unified certified bounds for differential privacy, robustness, and fairness in centralized and federated learning against privacy and integrity attacks. Finally, the project integrated these novel approaches into a unified system, which has been prototyped and evaluated in real-world settings. As a result, the project opened several new research directions in privacy and integrity preservation in large language models and emerging machine learning applications such as software code generation. The project incorporated research results into course development, which contributed significantly to broadening participation in computing.\r\n\t\t\t\t\tLast Modified: 11/26/2024\n\n\t\t\t\t\tSubmitted by: HaiPhan\n"
 }
}