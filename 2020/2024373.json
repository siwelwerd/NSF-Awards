{
 "awd_id": "2024373",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  A Novel Human Machine Interface for Assistive Robots",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2021-01-31",
 "tot_intn_awd_amt": 256000.0,
 "awd_amount": 256000.0,
 "awd_min_amd_letter_date": "2020-08-10",
 "awd_max_amd_letter_date": "2020-08-10",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project will benefit the 30 million disabled survivors of stroke, spinal cord injury, and traumatic brain injury around the globe. Making assistive robots more affordable and easy to use is important for this market, projected to grow to $1.2 billion in 2024.  The proposed wearable technology will enable effective communication and control of robots designed to assist patients with activities of daily living, which will help patients improve quality of life, lower costs for long-term care, and preserve physical and mental health with increased activities. The data may be used to monitor rehabilitation and deterioration progress, which could encourage patients to exercise more frequently and enable more personalized treatment.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project will explore the feasibility of commercializing a lightweight and compact optical sensor band for detecting commands from disabled users and controlling assistive robots. Most commercially available human-robot interface options are not practical or reliable for chronic hemiparetic patients due to the diverse and evolving patient conditions. The research objective involves designing an optical sensor array to capture muscle activities and developing an adaptive muscle synergy-based classification algorithm to convert optical data to user command. The project will optimize the configuration for near-infrared emitters and phototransistors to efficiently monitor muscle activities on the forearm. Advanced signal processing and machine learning techniques will be used to filter the data collected from multimodal sensor inputs to ensure robust performance regardless of user conditions.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Faye",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Faye Wu",
   "pi_email_addr": "yfwu@manus-robotics.com",
   "nsf_id": "000793670",
   "pi_start_date": "2020-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "MANUS ROBOTICS INC.",
  "inst_street_address": "21 BROADWAY",
  "inst_street_address_2": "",
  "inst_city_name": "ARLINGTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "7815386539",
  "inst_zip_code": "024745539",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "MANUS ROBOTICS INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "LRG5XDZ9U8C6"
 },
 "perf_inst": {
  "perf_inst_name": "MANUS ROBOTICS INC.",
  "perf_str_addr": "24 Hartwell Ave, 2nd Floor",
  "perf_city_name": "Lexington",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024213132",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "010E",
   "pgm_ref_txt": "DISABILITY RES & HOMECARE TECH"
  },
  {
   "pgm_ref_code": "8034",
   "pgm_ref_txt": "Hardware Components"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 256000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This SBIR Phase I project aims to explore the feasibility of developing a compact optical sensor band that enables intuitive and reliable control of assistive robots. With several rounds of redesign and testing, Manus Robotics was able to build a multi-channel sensor prototype for measuring muscle activities around the forearm, create an algorithm to identify unique gesture patterns based on both optical and motion data, and demonstrate the efficacy of the prototype sensor band as a human machine interface (HMI) by controlling robot movements with hand gestures. &nbsp;&nbsp;&nbsp;</p>\n<p>The final sensor band prototype, consisting of 9 near-infrared emitters and 4 detectors, can measure muscle activity at multiple locations and depths around the forearm simultaneously. Emitter light intensity and detector sensitivity can be adjusted individually and independently using a fully integrated chip that contains optical driver and data acquisition system, resulting in substantial space and power savings. Emitters are pulse modulated in firmware with time division multiplexing so that all near-infrared sources can work coherently in close proximity without interfering with one another. Signals acquired by detectors are processed in firmware with a lock-in detection algorithm to reject noise and signal artifacts outside the modulating frequency band. The sensor arrangement in the prototype was determined by monitoring muscle activity along the forearm as different gestures are performed and, based on the data color maps, selecting the combination that offered the most distinguishable features.</p>\n<p>The filtered sensor signals, including both near-infrared and inertial measurements, are fed into the gesture detection algorithm for further processing. After comparing 20 different models, including both supervised and unsupervised learning methods, a Long Short Term Memory (LSTM) Network model, combined with a Decision Tree, was found to be the most effective in identifying hand gestures. Five gestures were trained and tested, including closing the fist, flexing the wrist with an open palm, twisting the hand to the left and right, and relaxing; the algorithm achieved a prediction accuracy of 97% and an average online prediction delay of 0.47s.</p>\n<p>Using the five trained hand gestures, the performance of the sensor band as an effective input device was then evaluated. First tested in TurtleSim, a Robot Operating System (ROS) based simulator, the sensor band was used to control the movement of a TurtleBot on the screen. After debugging any robot control related issues based on the simulation results, the sensor band was used to control an UR5e collaborative robot arm. The desired robot motion and trajectories were taught to the robot by manually moving the robot in free-drive mode and recording the corresponding joint angles. Then, the labeled gestures were linked to the recorded robot movements via ROS. Using the prototype sensor band, the robot arm was successfully commanded to perform a few example tasks such as grasping and moving an object from one location to another, and holding the object close to the user for easier access.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/30/2021<br>\n\t\t\t\t\tModified by: Faye&nbsp;Wu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis SBIR Phase I project aims to explore the feasibility of developing a compact optical sensor band that enables intuitive and reliable control of assistive robots. With several rounds of redesign and testing, Manus Robotics was able to build a multi-channel sensor prototype for measuring muscle activities around the forearm, create an algorithm to identify unique gesture patterns based on both optical and motion data, and demonstrate the efficacy of the prototype sensor band as a human machine interface (HMI) by controlling robot movements with hand gestures.    \n\nThe final sensor band prototype, consisting of 9 near-infrared emitters and 4 detectors, can measure muscle activity at multiple locations and depths around the forearm simultaneously. Emitter light intensity and detector sensitivity can be adjusted individually and independently using a fully integrated chip that contains optical driver and data acquisition system, resulting in substantial space and power savings. Emitters are pulse modulated in firmware with time division multiplexing so that all near-infrared sources can work coherently in close proximity without interfering with one another. Signals acquired by detectors are processed in firmware with a lock-in detection algorithm to reject noise and signal artifacts outside the modulating frequency band. The sensor arrangement in the prototype was determined by monitoring muscle activity along the forearm as different gestures are performed and, based on the data color maps, selecting the combination that offered the most distinguishable features.\n\nThe filtered sensor signals, including both near-infrared and inertial measurements, are fed into the gesture detection algorithm for further processing. After comparing 20 different models, including both supervised and unsupervised learning methods, a Long Short Term Memory (LSTM) Network model, combined with a Decision Tree, was found to be the most effective in identifying hand gestures. Five gestures were trained and tested, including closing the fist, flexing the wrist with an open palm, twisting the hand to the left and right, and relaxing; the algorithm achieved a prediction accuracy of 97% and an average online prediction delay of 0.47s.\n\nUsing the five trained hand gestures, the performance of the sensor band as an effective input device was then evaluated. First tested in TurtleSim, a Robot Operating System (ROS) based simulator, the sensor band was used to control the movement of a TurtleBot on the screen. After debugging any robot control related issues based on the simulation results, the sensor band was used to control an UR5e collaborative robot arm. The desired robot motion and trajectories were taught to the robot by manually moving the robot in free-drive mode and recording the corresponding joint angles. Then, the labeled gestures were linked to the recorded robot movements via ROS. Using the prototype sensor band, the robot arm was successfully commanded to perform a few example tasks such as grasping and moving an object from one location to another, and holding the object close to the user for easier access.\n\n \n\n\t\t\t\t\tLast Modified: 04/30/2021\n\n\t\t\t\t\tSubmitted by: Faye Wu"
 }
}