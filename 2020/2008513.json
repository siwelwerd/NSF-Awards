{
 "awd_id": "2008513",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CIF: Small: A Unified Framework of Distributional Optimization via Variational Transport",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2020-07-29",
 "awd_max_amd_letter_date": "2020-07-29",
 "awd_abstract_narration": "Distributional optimization refers to a class of mathematical problems where the optimizing variable in the objective function is a probability measure over some space. Because of its highly technical nature, distributional optimization has remained largely unexplored with advances made only on specific problem instances. This project proposes a unified framework to explore challenging distributional optimization problems in a wide range of important application domains. The main objective is to develop a comprehensive theory supporting the principled design of novel and efficient optimization algorithms. To do so, establishing connections between several mathematical disciplines will be required, including optimization theory, optimal transport, functional inequalities, and statistics. This will promote the cross-fertilization of ideas and lead to the creation of training material from an interdisciplinary perspective. The resulting open-source packages will be made available to support research efforts in related fields that our daily lives depend on. \r\n\r\nProblems in distributional optimization are infinite-dimensional optimization problems where the optimization variable is a probability measure. Many research problems fall into this class of problems; in particular, any non-convex optimization problem over Euclidean space can be cast as a convex distributional optimization problem. Traditionally, specific instances of these problems have been studied independently of each other. Formulating these seemingly different optimization problems into a single unified framework will allow more powerful mathematical techniques and tools to be used. This will lead to deeper insights into the structure of solutions, and to efficient algorithms tailored to large-scale applications in artificial intelligence and data science. The proposed framework is based on optimal transport theory that endows the space of distributions with a natural geometry. The proposed algorithm utilizes the gradient flow of the objective with respect to this geometry. To achieve scalability, the optimization variable is approximated by a collection of particles, with the algorithm now describing the collective dynamics of the particles. A novel variational approach will be used to approximate the gradient descent direction. The theoretical properties of this algorithm will be investigated thoroughly, including provable performance guarantees, convergence rates, and statistical properties. Case studies will be carried out by specializing this unified framework to applications such as Bayesian inference and distributionally robust learning. The acceleration of this algorithm will also be investigated by incorporating existing optimization techniques, such as momentum and variance reduction, as a way to improve convergence rates. Finally, the project will explore how to adapt this algorithm from minimization problems to min-max problems in order to deal with game-theoretic applications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yongxin",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yongxin Chen",
   "pi_email_addr": "yongchen@gatech.edu",
   "nsf_id": "000755659",
   "pi_start_date": "2020-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "225 North Avenue, NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">This project focuses on the distributional optimization problem, a unique type of infinite-dimensional optimization where the optimization variable is a probability distribution. Distributional optimization arises in numerous tasks, including, but not limited to, Bayesian inference, where the distribution represents the belief updated based on observations, and distributionally robust machine learning, where the goal is to optimize for the worst-case data distribution. Moreover, any non-convex optimization problem can be reformulated as a convex distributional optimization problem, positioning distributional optimization as a promising paradigm for addressing challenging non-convex optimization tasks.</p>\r\n<p class=\"p2\">&nbsp;The research established several principled approaches for solving distributional optimization problems by leveraging the geometry induced by optimal transport (OT) theory on the space of probability distributions. Both the theoretical foundations and algorithmic aspects of these approaches have been systematically investigated. These methods have been applied to significant areas, including Markov chain Monte Carlo (MCMC) sampling, Particle filtering, Control of collective dynamics, and OT gradient flows.</p>\r\n<p class=\"p2\">&nbsp;The theoretical and algorithmic tools developed in this project have the potential to drive technological advancements in multiple domains, including: i) Bayesian inference: Applications in data science and machine learning; ii) Estimation and filtering: Applications in autonomy and aerospace systems; and iii) Control engineering: Applications in robotics and collective dynamics. To disseminate the results, research papers have been published in leading journals and conference proceedings. Additionally, several open-source Python libraries have been developed and released to empower researchers in related fields (see Figure 1). The project has also provided research opportunities for students, fostering workforce development, and has led to the organization of workshops in the control, data science, and applied mathematics communities to engage broader audiences.</p>\r\n<p class=\"p2\">The project achieved several complementary goals.</p>\r\n<p class=\"p2\">1) OT Gradient Flow-Based Algorithms for Distributional Optimization.&nbsp;</p>\r\n<p class=\"p1\">A significant outcome of this project is the development of algorithms based on optimal transport gradient flows for solving general distributional optimization problems. These algorithms exhibit excellent convergence properties under mild assumptions characterized by functional inequalities. Key Features of the OT Gradient Flow-Based Approach are as follows. i) Time Discretization: Both explicit and implicit time discretization schemes were implemented, with the implicit scheme demonstrating superior stability. ii) Variational Approach: A variational formulation was employed to estimate the OT gradient direction effectively. iii) Scalability: To ensure scalability, the algorithms were implemented using particle-based methods instead of computationally expensive mesh-based approaches. These algorithms were investigated both theoretically and empirically. Notably, Figure 2 illustrates how our algorithm simulates the solution of an aggregation-diffusion equation without requiring spatial discretization, showcasing its efficacy.</p>\r\n<p class=\"p2\">2) MCMC Sampling for Bayesian Inference.</p>\r\n<p class=\"p2\">Another major contribution of this project is in the area of MCMC sampling, a popular framework in Bayesian inference. This problem can be viewed as a specialized form of optimization over the manifold of probability distributions, where the optimization objective is the Kullback-Leibler (KL) divergence. By exploiting the geometry induced by OT theory, we developed several effective MCMC algorithms that bridge sampling and optimization. A particularly exciting innovation is the development of the proximal sampler, inspired by the proximal method in classical optimization. This sampler applies to a wide range of sampling tasks with minimal assumptions and exhibits superior convergence guarantees compared to existing MCMC methods. It demonstrates improved non-asymptotic complexity bounds across standard MCMC settings (as detailed in the paper ``Improved Dimension Dependence of a Proximal Algorithm for Sampling''). Figure 3 highlights an implementation of the proximal sampler, showing its competitive performance compared to other state-of-the-art MCMC algorithms.</p>\r\n<p class=\"p2\">&nbsp;3) Applications in Control Engineering.</p>\r\n<p class=\"p2\">The framework of distributional optimization was also applied to control engineering, addressing tasks such as i) State Estimation: Estimating the distribution of the system state given noisy observations; and ii) Control of Collective Dynamics: Steering the group behavior of a large collection of interacting systems. A novel particle filtering algorithm was developed for state estimation. This algorithm was further extended to handle Lie group state spaces, making it particularly suitable for aerospace applications where state variables often reside on manifolds (e.g., attitudes and orientations). Moreover, an effective algorithm was designed to control the behavior of large collections of interacting agents, enabling applications in robotics, swarm control, and distributed systems.</p>\r\n<p class=\"p2\">&nbsp;To summarize, this project has advanced the field of distributional optimization by developing a suite of theoretical and algorithmic tools grounded in optimal transport theory. These tools have been applied to diverse tasks, including MCMC sampling, particle filtering, and control of collective dynamics. In addition to these advancements, the project has had a tangible impact on the research community through the dissemination of results in publications, open-source software, and workshops. The methods and tools developed in this project promise to drive innovation in fields such as data science, robotics, autonomy, and scientific computing, addressing real-world challenges.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/28/2025<br>\nModified by: Yongxin&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738122954723_Figure_1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738122954723_Figure_1--rgov-800width.png\" title=\"opensource package\"><img src=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738122954723_Figure_1--rgov-66x44.png\" alt=\"opensource package\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">snapshot of one opensource package</div>\n<div class=\"imageCredit\">Yongxin Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yongxin&nbsp;Chen\n<div class=\"imageTitle\">opensource package</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738123120632_Figure_3--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738123120632_Figure_3--rgov-800width.png\" title=\"proximal sampler\"><img src=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738123120632_Figure_3--rgov-66x44.png\" alt=\"proximal sampler\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">proximal sampler performance comparison</div>\n<div class=\"imageCredit\">Yongxin Chen</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Yongxin&nbsp;Chen\n<div class=\"imageTitle\">proximal sampler</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738123005495_Figure_2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738123005495_Figure_2--rgov-800width.png\" title=\"Variational Wasserstein gradient\"><img src=\"/por/images/Reports/POR/2025/2008513/2008513_10690821_1738123005495_Figure_2--rgov-66x44.png\" alt=\"Variational Wasserstein gradient\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">illustration of an algorithm</div>\n<div class=\"imageCredit\">Yongxin Chen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yongxin&nbsp;Chen\n<div class=\"imageTitle\">Variational Wasserstein gradient</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project focuses on the distributional optimization problem, a unique type of infinite-dimensional optimization where the optimization variable is a probability distribution. Distributional optimization arises in numerous tasks, including, but not limited to, Bayesian inference, where the distribution represents the belief updated based on observations, and distributionally robust machine learning, where the goal is to optimize for the worst-case data distribution. Moreover, any non-convex optimization problem can be reformulated as a convex distributional optimization problem, positioning distributional optimization as a promising paradigm for addressing challenging non-convex optimization tasks.\r\n\n\nThe research established several principled approaches for solving distributional optimization problems by leveraging the geometry induced by optimal transport (OT) theory on the space of probability distributions. Both the theoretical foundations and algorithmic aspects of these approaches have been systematically investigated. These methods have been applied to significant areas, including Markov chain Monte Carlo (MCMC) sampling, Particle filtering, Control of collective dynamics, and OT gradient flows.\r\n\n\nThe theoretical and algorithmic tools developed in this project have the potential to drive technological advancements in multiple domains, including: i) Bayesian inference: Applications in data science and machine learning; ii) Estimation and filtering: Applications in autonomy and aerospace systems; and iii) Control engineering: Applications in robotics and collective dynamics. To disseminate the results, research papers have been published in leading journals and conference proceedings. Additionally, several open-source Python libraries have been developed and released to empower researchers in related fields (see Figure 1). The project has also provided research opportunities for students, fostering workforce development, and has led to the organization of workshops in the control, data science, and applied mathematics communities to engage broader audiences.\r\n\n\nThe project achieved several complementary goals.\r\n\n\n1) OT Gradient Flow-Based Algorithms for Distributional Optimization.\r\n\n\nA significant outcome of this project is the development of algorithms based on optimal transport gradient flows for solving general distributional optimization problems. These algorithms exhibit excellent convergence properties under mild assumptions characterized by functional inequalities. Key Features of the OT Gradient Flow-Based Approach are as follows. i) Time Discretization: Both explicit and implicit time discretization schemes were implemented, with the implicit scheme demonstrating superior stability. ii) Variational Approach: A variational formulation was employed to estimate the OT gradient direction effectively. iii) Scalability: To ensure scalability, the algorithms were implemented using particle-based methods instead of computationally expensive mesh-based approaches. These algorithms were investigated both theoretically and empirically. Notably, Figure 2 illustrates how our algorithm simulates the solution of an aggregation-diffusion equation without requiring spatial discretization, showcasing its efficacy.\r\n\n\n2) MCMC Sampling for Bayesian Inference.\r\n\n\nAnother major contribution of this project is in the area of MCMC sampling, a popular framework in Bayesian inference. This problem can be viewed as a specialized form of optimization over the manifold of probability distributions, where the optimization objective is the Kullback-Leibler (KL) divergence. By exploiting the geometry induced by OT theory, we developed several effective MCMC algorithms that bridge sampling and optimization. A particularly exciting innovation is the development of the proximal sampler, inspired by the proximal method in classical optimization. This sampler applies to a wide range of sampling tasks with minimal assumptions and exhibits superior convergence guarantees compared to existing MCMC methods. It demonstrates improved non-asymptotic complexity bounds across standard MCMC settings (as detailed in the paper ``Improved Dimension Dependence of a Proximal Algorithm for Sampling''). Figure 3 highlights an implementation of the proximal sampler, showing its competitive performance compared to other state-of-the-art MCMC algorithms.\r\n\n\n3) Applications in Control Engineering.\r\n\n\nThe framework of distributional optimization was also applied to control engineering, addressing tasks such as i) State Estimation: Estimating the distribution of the system state given noisy observations; and ii) Control of Collective Dynamics: Steering the group behavior of a large collection of interacting systems. A novel particle filtering algorithm was developed for state estimation. This algorithm was further extended to handle Lie group state spaces, making it particularly suitable for aerospace applications where state variables often reside on manifolds (e.g., attitudes and orientations). Moreover, an effective algorithm was designed to control the behavior of large collections of interacting agents, enabling applications in robotics, swarm control, and distributed systems.\r\n\n\nTo summarize, this project has advanced the field of distributional optimization by developing a suite of theoretical and algorithmic tools grounded in optimal transport theory. These tools have been applied to diverse tasks, including MCMC sampling, particle filtering, and control of collective dynamics. In addition to these advancements, the project has had a tangible impact on the research community through the dissemination of results in publications, open-source software, and workshops. The methods and tools developed in this project promise to drive innovation in fields such as data science, robotics, autonomy, and scientific computing, addressing real-world challenges.\r\n\n\n\t\t\t\t\tLast Modified: 01/28/2025\n\n\t\t\t\t\tSubmitted by: YongxinChen\n"
 }
}