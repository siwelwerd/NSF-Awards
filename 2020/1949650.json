{
 "awd_id": "1949650",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Medium: Hidden Rules in Neural Networks as Attacks and Adversarial Defenses",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2020-03-01",
 "awd_exp_date": "2024-02-29",
 "tot_intn_awd_amt": 1200000.0,
 "awd_amount": 1296000.0,
 "awd_min_amd_letter_date": "2020-03-12",
 "awd_max_amd_letter_date": "2021-06-23",
 "awd_abstract_narration": "Recent advances in Deep Neural Networks (DNNs) have enabled significant progress in technological challenges such as voice/facial recognition, language translation and image recognition. Yet DNNs remain vulnerable to a class of hidden attacks called \"backdoor\" or \"Trojan\" attacks, where hidden rules are trained into a model which only become active on model input with some unusual properties,  comprising a \"trigger.\" They are strong enough that the presence of a small, inconspicuous trigger can make the model produce unexpected (and often erroneous) results, e.g., recognize anyone with a black ankh tattoo as a predetermined celebrity. Despite recent efforts, these attacks remain poorly understood, and robust defenses remain elusive.  This project studies this class of attacks in depth to understand their potential impact on real machine learning systems and potential defenses.\r\n\r\nMore specifically, the project will first catalog the breadth of backdoor attacks across multiple domains (and potential defenses), including images (facial and object recognition), text (natural language processing and sentiment analysis), and audio (speaker recognition and voice transcription). The project will then explore their practical implications outside the digital domain, including backdoor attacks in the physical world (such as on facial recognition), and advanced backdoors that coexist with transfer learning, the prevailing method for sharing DNN models today. Finally, the project will explore potential positive uses of backdoors as model-training tools, spawning a novel protection mechanism for DNN models, by trapping adversarial attacks with honey-pots built using backdoor techniques. The techniques will incorporate evaluation of both advanced attacks and defenses across a broad range of applications, datasets and models, and whenever possible, experiments in the physical domain. Successful results from this project should alert security professionals to the risk of backdoors in DNNs, while providing the software and algorithmic tools necessary for robust defenses.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ben",
   "pi_last_name": "Zhao",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Ben Y Zhao",
   "pi_email_addr": "ravenben@cs.uchicago.edu",
   "nsf_id": "000238651",
   "pi_start_date": "2020-03-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Haitao",
   "pi_last_name": "Zheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Haitao Zheng",
   "pi_email_addr": "htzheng@cs.uchicago.edu",
   "nsf_id": "000230003",
   "pi_start_date": "2020-03-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Pedro",
   "pi_last_name": "Lopes",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pedro Lopes",
   "pi_email_addr": "pedrolopes@cs.uchicago.edu",
   "nsf_id": "000779652",
   "pi_start_date": "2020-03-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5730, S. Ellis Ave.",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606371403",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 1248000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 48000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">Data poisoning attacks, particularly backdoors, represent a substantial challenge for Deep Neural Networks (DNNs). This project targets the unsolved and<span>&nbsp; </span>increasingly crucial challenge of developing robust detection and mitigation against poisoning and backdoor attacks on DNNs in the image domain.<span>&nbsp; </span>Over a period of four years, we have explored this topic in three key directions:<span>&nbsp; </span>(1)<span>&nbsp; </span>exploring practical variants beyond digitally-crafted backdoors and their defenses, (2) developing new defenses against poisoning attacks,<span>&nbsp; </span>and (3) transforming poisoning effects into defenses.<span>&nbsp;</span></span></p>\n<p class=\"p1\"><span class=\"s1\">In the first direction,<span>&nbsp; </span>we demonstrated the feasibility and effectiveness of<span>&nbsp; </span><em>physical backdoors</em><span>&nbsp; </span>where the backdoor trigger is an everyday accessary or object<span>&nbsp; </span>(e.g., headband, pencil).<span>&nbsp; </span>Our empirical studies on facial and object recognition demonstrated that<span>&nbsp; </span>physical backdoor attacks are highly effective and present a realistic threat.<span>&nbsp; </span>Yet state-of-the-art backdoor defenses consistently fail to mitigate them, because the use of physical objects breaks core assumptions used to construct these defenses. <span>&nbsp; </span>At the same time,<span>&nbsp; </span>we recognized that<span>&nbsp; </span>research on physical backdoors is limited by the lack of access to large datasets containing real images of physical objects.<span>&nbsp; </span>We created and distributed a tool that identifies ``naturally occurring triggers&rsquo;&rsquo; from public image datasets,<span>&nbsp; </span>allowing researchers to curate realistic datasets and making the study of physical backdoors more accessible.</span></p>\n<p class=\"p1\"><span class=\"s1\">In the second direction,<span>&nbsp; </span>we extended the scope of poison/backdoor defense to include a forensic traceback tool, which traces back a successful poison/backdoor attack to its root cause, offering a path forward for mitigation to prevent similar attacks in the future. <span>&nbsp; </span>Starting with evidence of the attack (an input sample that triggers the misclassification), our tool seeks to identify a particular subset of training data responsible for corrupting the model with the observed misclassification behavior. Combined with metadata or logs that track the provenance of training data, this enables practitioners to identify either the source of the poison data, or a vulnerability in the data pipeline where the poison data was inserted. Either result leads to direct mitigation steps that would patch the pipeline and improve robustness to similar attacks in the future.<span>&nbsp; </span>This forensic tool is a significant departure from existing works that focus entirely on attack prevention.<span>&nbsp;</span></span></p>\n<p class=\"p1\"><span class=\"s1\">In the third direction,<span>&nbsp; </span>we showed that one can intentionally inject trapdoors<span>&nbsp; </span>that act as a honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples.<span>&nbsp; </span>We proved that as attackers&rsquo; optimization algorithms gravitate towards trapdoors, they produce attack inputs similar to trapdoors in the feature space, which can be identified by our defense.<span>&nbsp; </span>We showed that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks, with negligible impact on normal classification.<span>&nbsp;</span></span></p>\n<p class=\"p1\"><span class=\"s1\">We further extended this idea to tackle the problem of protecting user privacy against unauthorized face recognition models.&nbsp; We proposed and developed&nbsp;Fawkes, a tool that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes (we call them <em>cloaks</em>) to their own photos before releasing them. When used to train facial recognition models, these <em>cloaked</em><span>&nbsp; </span>images produce functional models that consistently cause normal images of the user to be misidentified. We experimentally demonstrated that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. Even when clean, uncloaked images are &ldquo;leaked&rdquo; to the tracker and used for training, Fawkes can still maintain a high (80+%) protection success rate, and it is robust against a variety of countermeasures that try to detect or disrupt image cloaks.</span><span class=\"s2\"><br /></span></p>\n<p class=\"p2\"><span class=\"s1\">Regarding broader impacts, our project has led to a set of new discoveries and important results in the field of adversarial machine learning and HCI, which could drastically change how ML, security, and HCI developers view the role of adversarial machine learning in practical systems. Furthermore, this project has functioned as both an educational resource and a recruitment tool for underrepresented minorities at University of Chicago and local high-school students, and a collaborative platform that fosters interdisciplinary engagement, bringing together researchers from diverse fields such as machine learning, security, HCI, and systems.&nbsp; This project has facilitated the completion of three PhD theses and resulted in 19 conference publications that have undergone rigorous peer review.<span>&nbsp; </span>The Fawkes project has received significant media coverage around the globe (see http://sandlab.cs.uchicago.edu/fawkes/),&nbsp; and our software release has been downloaded by more than 1M users since August 2020.&nbsp;</span></p>\n<p class=\"p1\">&nbsp;</p><br>\n<p>\n Last Modified: 05/24/2024<br>\nModified by: Ben&nbsp;Y&nbsp;Zhao</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nData poisoning attacks, particularly backdoors, represent a substantial challenge for Deep Neural Networks (DNNs). This project targets the unsolved and increasingly crucial challenge of developing robust detection and mitigation against poisoning and backdoor attacks on DNNs in the image domain. Over a period of four years, we have explored this topic in three key directions: (1) exploring practical variants beyond digitally-crafted backdoors and their defenses, (2) developing new defenses against poisoning attacks, and (3) transforming poisoning effects into defenses.\n\n\nIn the first direction, we demonstrated the feasibility and effectiveness of physical backdoors where the backdoor trigger is an everyday accessary or object (e.g., headband, pencil). Our empirical studies on facial and object recognition demonstrated that physical backdoor attacks are highly effective and present a realistic threat. Yet state-of-the-art backdoor defenses consistently fail to mitigate them, because the use of physical objects breaks core assumptions used to construct these defenses.  At the same time, we recognized that research on physical backdoors is limited by the lack of access to large datasets containing real images of physical objects. We created and distributed a tool that identifies ``naturally occurring triggers from public image datasets, allowing researchers to curate realistic datasets and making the study of physical backdoors more accessible.\n\n\nIn the second direction, we extended the scope of poison/backdoor defense to include a forensic traceback tool, which traces back a successful poison/backdoor attack to its root cause, offering a path forward for mitigation to prevent similar attacks in the future.  Starting with evidence of the attack (an input sample that triggers the misclassification), our tool seeks to identify a particular subset of training data responsible for corrupting the model with the observed misclassification behavior. Combined with metadata or logs that track the provenance of training data, this enables practitioners to identify either the source of the poison data, or a vulnerability in the data pipeline where the poison data was inserted. Either result leads to direct mitigation steps that would patch the pipeline and improve robustness to similar attacks in the future. This forensic tool is a significant departure from existing works that focus entirely on attack prevention.\n\n\nIn the third direction, we showed that one can intentionally inject trapdoors that act as a honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples. We proved that as attackers optimization algorithms gravitate towards trapdoors, they produce attack inputs similar to trapdoors in the feature space, which can be identified by our defense. We showed that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks, with negligible impact on normal classification.\n\n\nWe further extended this idea to tackle the problem of protecting user privacy against unauthorized face recognition models. We proposed and developedFawkes, a tool that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes (we call them cloaks) to their own photos before releasing them. When used to train facial recognition models, these cloaked images produce functional models that consistently cause normal images of the user to be misidentified. We experimentally demonstrated that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. Even when clean, uncloaked images are leaked to the tracker and used for training, Fawkes can still maintain a high (80+%) protection success rate, and it is robust against a variety of countermeasures that try to detect or disrupt image cloaks.\n\n\n\nRegarding broader impacts, our project has led to a set of new discoveries and important results in the field of adversarial machine learning and HCI, which could drastically change how ML, security, and HCI developers view the role of adversarial machine learning in practical systems. Furthermore, this project has functioned as both an educational resource and a recruitment tool for underrepresented minorities at University of Chicago and local high-school students, and a collaborative platform that fosters interdisciplinary engagement, bringing together researchers from diverse fields such as machine learning, security, HCI, and systems. This project has facilitated the completion of three PhD theses and resulted in 19 conference publications that have undergone rigorous peer review. The Fawkes project has received significant media coverage around the globe (see http://sandlab.cs.uchicago.edu/fawkes/), and our software release has been downloaded by more than 1M users since August 2020.\n\n\n\t\t\t\t\tLast Modified: 05/24/2024\n\n\t\t\t\t\tSubmitted by: BenYZhao\n"
 }
}