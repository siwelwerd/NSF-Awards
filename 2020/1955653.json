{
 "awd_id": "1955653",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: The role of trust when learning from robots",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927878",
 "po_email": "slim@nsf.gov",
 "po_sign_block_name": "Soo-Siang Lim",
 "awd_eff_date": "2020-09-15",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 375000.0,
 "awd_amount": 375000.0,
 "awd_min_amd_letter_date": "2020-09-21",
 "awd_max_amd_letter_date": "2020-09-21",
 "awd_abstract_narration": "A decade ago, robots typically played the role of \u201ctool\u201d or \u201cteammate\u201d. Today, although there are some clear cases where the \u201ctool\u201d or \u201cteammate\u201d model is appropriate, most collaborative robots in long-term deployments in homes, workplaces, or schools readily switch back and forth between being an agentic \u201cteammate\u201d to an inanimate \u201ctool\u201d. While people tolerate this shift in perceived agency, it is unknown how this shift impacts interpersonal properties that are typically attributed only to agentic \u201cteammate\u201d robots. This project will evaluate factors that affect how people trust robots, recognizing that the way humans \u201ctrust\u201d non-agentic automation is fundamentally different from the way that we \u201ctrust\u201d agents and agentic robots. What happens to the trust formed while a robot is an agent when it becomes an inanimate tool? What happens when the inanimate tool returns to being an agentic teammate? The proposed research will fill a significant gap in our understanding of how young humans develop trust in robots. While trust in non-agentic robots is well understood, there has been little systematic study of trust in robots that function as collaborative tools. This work has broad applications to future deployment of robots as systems that vary over time between agentic (human-like) and non-agentic (object-like) behavior. \r\n\r\nThe investigators will concentrate on the role of agency in establishing trust in human-robot interactions in an important application domain: children\u2019s learning. Educational robots designed specifically for children are increasingly common, often replacing human channels of social information. However, these robots cannot be successful without trust; because children are inherently social and collaborative learners, trust is a prerequisite for successful learning. Integrating insights from interactive robot design into experiments with preschool and early school age children, the project will determine how shifts in perceived agency impact the formation, maintenance, and repair of trust: Study 1 investigates how variations of low-level perceptual cues over a single interaction influence trust and subsequent learning. Study 2 examines how variations of high-level social cues lead to differential trust and subsequent learning. For these experiments, a set of age-appropriate collaborative learning games were designed. The investigators also created a coding scheme for child behavior as well as a post-interaction child interview to assess children\u2019s perceptions of the robots and to measure effectiveness of learning. Findings and activities of this project could have broad impacts in multiple arenas including: (1) design guidelines that will influence a broad range of application areas including healthcare, manufacturing, and education; (2) enhancement/augmentation of learning, education and training, including research offerings for graduate and undergraduate investigators; (3) broadening of participation in one area of computing, and (4) dissemination of science to the general public and to the research community.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Scassellati",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Brian M Scassellati",
   "pi_email_addr": "brian.scassellati@yale.edu",
   "nsf_id": "000197372",
   "pi_start_date": "2020-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "AKWatson Hall",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127Y00",
   "pgm_ele_name": "Sci of Lrng & Augmented Intel"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "059Z",
   "pgm_ref_txt": "Science of Learning"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 375000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Every day, we put our trust in technologies to give us accurate information &ndash; we ask smart speakers, like Amazon Alexa, what the temperature is; we use apps on our phones to learn new languages; we seek help from chatbots, like ChatGPT, to write email, essays, and even software. When we are adults, these instances of trust may seem like small additions to what we already know about the world, but what if we were expected to trust technologies as we are forming the foundations of our knowledge? In this project, we looked at whether 4- to 7-year-old children, and by comparison adults, would trust an interactive technology when working together on a task.&nbsp; Will children trust these machines implicitly, believing their answers to always be true?&nbsp; How will children react if these machines make mistakes?&nbsp;</p>\r\n<p>In a series of studies in both children and adults, we examined how people trust in the answers given to them by intelligent-seeming technologies.&nbsp; We focused particularly on children between the ages of 4 years and 7 years, a time both when children are learning fundamental skills about where to go when they seek answers and a time when technologies at home and in school begin to offer answers.&nbsp; In our most important study, we asked both children and adults to work collectively with a robot or a human partner.&nbsp; The partner initially gave useful help, providing correct answers to unanswered questions, but eventually began to give incorrect answers.&nbsp; We varied the way in which the partner responded to the error, sometimes providing apologies and expressing remorse, other times claiming that the error was a deliberate deception.&nbsp; We found that older children were less trusting than both younger children and adults and were even more skeptical after errors. Trust decreased most rapidly when errors were intentional, but only children (and especially older children) outright rejected help from intentionally unhelpful partners. As an exception to this general trend, older children maintained their trust for longer when a robot (but not a human) apologized for its mistake. Our work suggests that educational technology design cannot be &ldquo;one size fits all&rdquo; but rather must account for developmental changes in children's learning goals.</p><br>\n<p>\n Last Modified: 03/11/2025<br>\nModified by: Brian&nbsp;M&nbsp;Scassellati</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nEvery day, we put our trust in technologies to give us accurate information  we ask smart speakers, like Amazon Alexa, what the temperature is; we use apps on our phones to learn new languages; we seek help from chatbots, like ChatGPT, to write email, essays, and even software. When we are adults, these instances of trust may seem like small additions to what we already know about the world, but what if we were expected to trust technologies as we are forming the foundations of our knowledge? In this project, we looked at whether 4- to 7-year-old children, and by comparison adults, would trust an interactive technology when working together on a task. Will children trust these machines implicitly, believing their answers to always be true? How will children react if these machines make mistakes?\r\n\n\nIn a series of studies in both children and adults, we examined how people trust in the answers given to them by intelligent-seeming technologies. We focused particularly on children between the ages of 4 years and 7 years, a time both when children are learning fundamental skills about where to go when they seek answers and a time when technologies at home and in school begin to offer answers. In our most important study, we asked both children and adults to work collectively with a robot or a human partner. The partner initially gave useful help, providing correct answers to unanswered questions, but eventually began to give incorrect answers. We varied the way in which the partner responded to the error, sometimes providing apologies and expressing remorse, other times claiming that the error was a deliberate deception. We found that older children were less trusting than both younger children and adults and were even more skeptical after errors. Trust decreased most rapidly when errors were intentional, but only children (and especially older children) outright rejected help from intentionally unhelpful partners. As an exception to this general trend, older children maintained their trust for longer when a robot (but not a human) apologized for its mistake. Our work suggests that educational technology design cannot be one size fits all but rather must account for developmental changes in children's learning goals.\t\t\t\t\tLast Modified: 03/11/2025\n\n\t\t\t\t\tSubmitted by: BrianMScassellati\n"
 }
}