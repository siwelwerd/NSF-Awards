{
 "awd_id": "2012465",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Fast Optimization Methods and Application to Data Science and Nonlinear Partial Differential Equations",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 249999.0,
 "awd_amount": 249999.0,
 "awd_min_amd_letter_date": "2020-07-28",
 "awd_max_amd_letter_date": "2020-07-28",
 "awd_abstract_narration": "This projects incorporates several recent developments in optimization methods and nonlinear multigrid methods to provide a new technique to improve the computational efficiency of practical applications. Successful integration of our fast optimization methods will open a wide new area of applications ranging from numerical solution of partial differential equations to optimization methods for large-scale machine learning. Social media such as Facebook and GitHub  will be used to disseminate basics on applied and computational mathematics and promote the research to a wider audience in both academia and industry, as well as increase the public awareness of how computational mathematics help the advancement of research in other physical and data sciences. This project will provide training opportunities for graduate students.\r\n\r\nThe project focuses on a particular nonlinear multigrid method, the fast subspace descent (FASD) method, for solving optimization problems arising from various applications such as numerical solution of partial differential equations and data science problems. For example, the nonlinear multigrid methods to be studied can address the challenging problems in engineering applications including gradient flow in phase field models, Poisson-Boltzmann equation in math biology, and convex composite optimization problems in data science. Acceleration has been one of the most productive ideas in modern optimization theory. This framework brings more insight and mathematical tools for the design and analysis of old and new optimization methods, especially the accelerated gradient descent methods. Another important aspect of this project will be the rigorous theoretical foundation for a large class of optimization methods.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Long",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Long Chen",
   "pi_email_addr": "chenlong@math.uci.edu",
   "nsf_id": "000227333",
   "pi_start_date": "2020-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "Rowland Hall room 510F",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926973875",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 249999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><h2 class=\"md-end-block md-heading md-focus\"><span class=\"md-plain md-expand\">Overview</span></h2>\n<p class=\"md-end-block md-p\"><span class=\"md-plain\">This project made significant advances in optimization methods, data science applications, and the resolution of complex mathematical problems. Key achievements include:</span></p>\n<ol class=\"ol-list\">\n<li class=\"md-list-item\">\n<p class=\"md-end-block md-p\"><span class=\"md-pair-s \"><strong>Novel Convergence Analysis</strong></span><span class=\"md-plain\">: Conducted a novel convergence analysis of the Fast Subspace Descent Methods (FASD) for convex optimization problems, enhancing the robustness and applicability of these methods.</span></p>\n</li>\n<li class=\"md-list-item\">\n<p class=\"md-end-block md-p\"><span class=\"md-pair-s \"><strong>Unified Framework for Acceleration</strong></span><span class=\"md-plain\">: Developed a unified framework for designing and analyzing accelerated optimization methods, strengthening their theoretical foundation and efficiency.</span></p>\n</li>\n<li class=\"md-list-item\">\n<p class=\"md-end-block md-p\"><span class=\"md-pair-s \"><strong>Accelerated Over-Relaxation Heavy-Ball (AOR-HB) Method</strong></span><span class=\"md-plain\">: Created the AOR-HB method, which enables global and accelerated convergence for solving optimization problems with superior generalization ability.</span></p>\n</li>\n<li class=\"md-list-item\">\n<p class=\"md-end-block md-p\"><span class=\"md-pair-s \"><strong>Transformed Primal-Dual with Variable Preconditioners (TPDv) Algorithm</strong></span><span class=\"md-plain\">: Introduced a transformed primal-dual gradient flow technique and developed the TPDv algorithm, which demonstrates superior performance in solving nonlinear problems compared to existing methods.</span></p>\n</li>\n<li class=\"md-list-item md-focus-container\">\n<p class=\"md-end-block md-p md-focus\"><span class=\"md-pair-s \"><strong>Deep Learning Method for Real-Time Simulations</strong></span><span class=\"md-plain md-expand\">: Created a deep learning method (structure-conforming operator learning) that provides real-time solutions for data science problems, enhancing the speed and accuracy of predictions based on data.</span></p>\n</li>\n</ol>\n<h2 class=\"md-end-block md-heading md-focus\"><span class=\"md-plain md-expand\">Intellectual Merit</span></h2>\n<p class=\"md-end-block md-p\"><span class=\"md-plain\">The integration of advanced optimization and computational algorithms has been a crucial intellectual development, particularly in data science and machine learning. This project contributed new insights and mathematical tools for designing and analyzing optimization methods, particularly in accelerated optimization techniques. Additionally, the project established a rigorous theoretical foundation for a broad class of optimization methods, significantly advancing the field of modern optimization theory.</span></p>\n<p class=\"md-end-block md-p\">&nbsp;</p>\n<h2 class=\"md-end-block md-heading\"><span class=\"md-plain\">Broader Impacts</span></h2>\n<p class=\"md-end-block md-p md-focus\"><span class=\"md-plain md-expand\">The project&rsquo;s contributions have broad applications, ranging from the numerical solution of PDEs to machine learning optimization methods. The developed nonlinear multigrid methods are applicable to engineering challenges, mathematical biology, and data science. Furthermore, the project played a vital role in educating the next generation of computational mathematicians through its integration into the iFEM package and a course on numerical methods. The research was also promoted through social media platforms, increasing public awareness of the role of computational mathematics in advancing physical and data sciences.</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/15/2024<br>\nModified by: Long&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748546968_NonlinearProblem2--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748546968_NonlinearProblem2--rgov-800width.png\" title=\"ferromagnetism\"><img src=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748546968_NonlinearProblem2--rgov-66x44.png\" alt=\"ferromagnetism\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Ferromagnetism</div>\n<div class=\"imageCredit\">Long Chen, Ruchi Guo, and Jingrong Wei</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Long&nbsp;Chen\n<div class=\"imageTitle\">ferromagnetism</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748611378_OperatorLearning--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748611378_OperatorLearning--rgov-800width.png\" title=\"Operator Learning\"><img src=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748611378_OperatorLearning--rgov-66x44.png\" alt=\"Operator Learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Operator Learning</div>\n<div class=\"imageCredit\">Shuhao Cao, Long Chen, and Ruchi Guo</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Long&nbsp;Chen\n<div class=\"imageTitle\">Operator Learning</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748463779_NonlinearProblem1--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748463779_NonlinearProblem1--rgov-800width.png\" title=\"Darcy-Forchheimer model\"><img src=\"/por/images/Reports/POR/2024/2012465/2012465_10690036_1723748463779_NonlinearProblem1--rgov-66x44.png\" alt=\"Darcy-Forchheimer model\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Darcy-Forchheimer model</div>\n<div class=\"imageCredit\">Long Chen, Ruchi Guo and Jingrong Wei</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Long&nbsp;Chen\n<div class=\"imageTitle\">Darcy-Forchheimer model</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "Overview\n\n\nThis project made significant advances in optimization methods, data science applications, and the resolution of complex mathematical problems. Key achievements include:\n\n\n\n\nNovel Convergence Analysis: Conducted a novel convergence analysis of the Fast Subspace Descent Methods (FASD) for convex optimization problems, enhancing the robustness and applicability of these methods.\n\n\n\n\nUnified Framework for Acceleration: Developed a unified framework for designing and analyzing accelerated optimization methods, strengthening their theoretical foundation and efficiency.\n\n\n\n\nAccelerated Over-Relaxation Heavy-Ball (AOR-HB) Method: Created the AOR-HB method, which enables global and accelerated convergence for solving optimization problems with superior generalization ability.\n\n\n\n\nTransformed Primal-Dual with Variable Preconditioners (TPDv) Algorithm: Introduced a transformed primal-dual gradient flow technique and developed the TPDv algorithm, which demonstrates superior performance in solving nonlinear problems compared to existing methods.\n\n\n\n\nDeep Learning Method for Real-Time Simulations: Created a deep learning method (structure-conforming operator learning) that provides real-time solutions for data science problems, enhancing the speed and accuracy of predictions based on data.\n\n\nIntellectual Merit\n\n\nThe integration of advanced optimization and computational algorithms has been a crucial intellectual development, particularly in data science and machine learning. This project contributed new insights and mathematical tools for designing and analyzing optimization methods, particularly in accelerated optimization techniques. Additionally, the project established a rigorous theoretical foundation for a broad class of optimization methods, significantly advancing the field of modern optimization theory.\n\n\n\nBroader Impacts\n\n\nThe projects contributions have broad applications, ranging from the numerical solution of PDEs to machine learning optimization methods. The developed nonlinear multigrid methods are applicable to engineering challenges, mathematical biology, and data science. Furthermore, the project played a vital role in educating the next generation of computational mathematicians through its integration into the iFEM package and a course on numerical methods. The research was also promoted through social media platforms, increasing public awareness of the role of computational mathematics in advancing physical and data sciences.\n\n\n\t\t\t\t\tLast Modified: 08/15/2024\n\n\t\t\t\t\tSubmitted by: LongChen\n"
 }
}