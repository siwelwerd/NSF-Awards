{
 "awd_id": "2015411",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Bayesian and Semi-Bayesian Methods for Detecting Relationships in High Dimensions",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yulia Gel",
 "awd_eff_date": "2020-08-15",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 120000.0,
 "awd_amount": 120000.0,
 "awd_min_amd_letter_date": "2020-08-04",
 "awd_max_amd_letter_date": "2020-08-04",
 "awd_abstract_narration": "In this big-data era, massive data sets are being generated routinely and we are seeing a growing need for powerful, reliable, and interpretable statistical learning tools to help understand these data.  The main ideas and approaches in this projectl focus on developing effective statistical learning tools to learn about complex and heterogeneous structures, such as those changing in time or varying among different groups of individuals, in high-dimensions. The activities will have a significant impact on high dimensional Bayesian analysis and modeling of nonlinear relationships. While most current efforts for high-dimensional Bayesian analyses have been focused on linear models, this project focuses on two ways of generalizing standard linear models to meet certain practical challenges: one is a generalized form of mixture modeling, termed as individualized variable selection, which enables each individual observation to have its own set of dependent variables through the employment of neuronized priors. Another extension is the Bayesian inference of index models that form a mixture structure. The project will lead to useful tools (or customized software) for discovering interpretable nonlinear and interactive patterns among a large number of potential variables. Various aspects of statistical modeling, design, and learning strategies integrated in our algorithms are  broadly applicable to problems involving  signal discovery in complex systems and high-dimensional data.  The project will also provide both educational and interdisciplinary research opportunities for graduate students, and will result in software useful to biomedical researchers, economists, social scientists, and many other practitioners. \r\n\r\nIn a vast number of regression problems, especially under high-dimensional settings, the structure of the association between covariates in hand and the target quantity of interest might be heterogeneous over observations, which calls for effective methods to detect such non-trivial structures. Standard procedures, including traditional variable selections, commonly overlook the existence of interplays of these heterogeneous factors. This research project aims to develop statistical procedures that identify the complicated relationship between response Y and a set of covariates X in flexible and computationally efficient ways.  Project 1 focuses on Bayesian individualized variable selection (BIVS), which generalizes standard linear regression models to quantify  heterogeneous effects among individual observations that differ in their  dependent variables with different magnitudes. The PIs will investigate its theoretical properties, including model selection consistency and its robustness when the model assumption is violated. Project 2 is devoted to the development of an efficient Bayesian method to infer the semi-parametric relationship between the response and covariates through general index models. The PIs will explore its computational feasibility and theoretical properties such as the posterior contraction rate on the estimation of the sufficient dimension reduction space. Project 3 focuses on a fast tuning parameter selection procedure by employing a generative process via neural networks. By using this procedure,  the cross-validation can be efficiently implemented for general models, such as the BIVS and Bayesian index models,  regularized variable selection, and nonparametric function estimation.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Liu",
   "pi_email_addr": "jliu@stat.harvard.edu",
   "nsf_id": "000193769",
   "pi_start_date": "2020-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "1 Oxford St, 715 Science Center",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 120000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As massive data are being generated routinely these days, we expect to see growing&nbsp;need of powerful and reliable statistical learning tools to discover patterns in these data.&nbsp;In the same time, as we have witnessed in the past decade, neural-network-based over-parameterized large models (including large-language models) are becoming more and more powerful and dominating.&nbsp; These developments all point to some fundamental questions: what are the roles of statistical models and statistical thinking, and how to quantify uncertainties in the prediction and inference results under these complex systems? Also, how reliable and powerful are these complex DNN-based methods in high-noise settings? We feel that Bayesian thinking is a right principle in sorting out these issues.</p>\n<p>The proposal focuses on the development of systematic and practical tools, based on the principled Bayesian thinking, for&nbsp;discovering/modeling nonlinear relationships and quantifying uncertainties, which are important steps and tasks in many scientific areas. We start with high-dimensional linear models, developing practical Bayesian solutions to the selection of variables or group of variables. We then gradually build up complexities by allowing one to conveniently test out different priors simultaneously. Our application of the methodology on polygenic risk predictions have shown significant and uniform improvements over existing approaches, as shown in the first figure. Our new heteroscedastic Gaussian process model provides us a nice estimation of the calibration curve, with proper uncertainty quantification represented by the confidence band, to infer sea surface temperature of ancient times. The work has been presented at the main conference of t<span>he American Geophysical Union</span>, AGU 2022.<a class=\"gsc_oci_title_link\" href=\"https://academic.oup.com/bioinformatics/article-abstract/38/7/1938/6502278\"></a></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/12/2023<br>\nModified by: Jun&nbsp;Liu</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2015411/2015411_10692456_1702420920067_new_result1210--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2015411/2015411_10692456_1702420920067_new_result1210--rgov-800width.jpg\" title=\"Comparison of prediction accuracy among NeuPred and other twelve methods\"><img src=\"/por/images/Reports/POR/2023/2015411/2015411_10692456_1702420920067_new_result1210--rgov-66x44.jpg\" alt=\"Comparison of prediction accuracy among NeuPred and other twelve methods\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">pmb{a}, Predictive R2 on four diseases (CD, CEL, RA, and T2D) with large-scale GWAS studies.  The AUC was evaluated based on independent test datasets. (b) Predictive R2 for the four diseases (ATH, HT, BMI, and HGT) with GWAS studies from UKBB. The AUC was evaluated based on independent test data.</div>\n<div class=\"imageCredit\">Shuang Song</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jun&nbsp;Liu\n<div class=\"imageTitle\">Comparison of prediction accuracy among NeuPred and other twelve methods</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2023/2015411/2015411_10692456_1702420388685_figure_3_1_5--rgov-214x142.png\" original=\"/por/images/Reports/POR/2023/2015411/2015411_10692456_1702420388685_figure_3_1_5--rgov-800width.png\" title=\"Sea surface temperature calibration\"><img src=\"/por/images/Reports/POR/2023/2015411/2015411_10692456_1702420388685_figure_3_1_5--rgov-66x44.png\" alt=\"Sea surface temperature calibration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The calibration model for UK37 based on Heteroscedastic Gaussian Processes. The shaded region is the 95% confidence band of the calibration model, and the black curve is medians of the posterior predictive distribution of the underlying hidden function.</div>\n<div class=\"imageCredit\">Taehee Lee</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jun&nbsp;Liu\n<div class=\"imageTitle\">Sea surface temperature calibration</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nAs massive data are being generated routinely these days, we expect to see growingneed of powerful and reliable statistical learning tools to discover patterns in these data.In the same time, as we have witnessed in the past decade, neural-network-based over-parameterized large models (including large-language models) are becoming more and more powerful and dominating. These developments all point to some fundamental questions: what are the roles of statistical models and statistical thinking, and how to quantify uncertainties in the prediction and inference results under these complex systems? Also, how reliable and powerful are these complex DNN-based methods in high-noise settings? We feel that Bayesian thinking is a right principle in sorting out these issues.\n\n\nThe proposal focuses on the development of systematic and practical tools, based on the principled Bayesian thinking, fordiscovering/modeling nonlinear relationships and quantifying uncertainties, which are important steps and tasks in many scientific areas. We start with high-dimensional linear models, developing practical Bayesian solutions to the selection of variables or group of variables. We then gradually build up complexities by allowing one to conveniently test out different priors simultaneously. Our application of the methodology on polygenic risk predictions have shown significant and uniform improvements over existing approaches, as shown in the first figure. Our new heteroscedastic Gaussian process model provides us a nice estimation of the calibration curve, with proper uncertainty quantification represented by the confidence band, to infer sea surface temperature of ancient times. The work has been presented at the main conference of the American Geophysical Union, AGU 2022.\n\n\n\t\t\t\t\tLast Modified: 12/12/2023\n\n\t\t\t\t\tSubmitted by: JunLiu\n"
 }
}