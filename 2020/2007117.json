{
 "awd_id": "2007117",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Towards Optimal and Adaptive Reinforcement Learning with Offline Data and Limited Adaptivity",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 449976.0,
 "awd_amount": 449976.0,
 "awd_min_amd_letter_date": "2020-08-25",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Reinforcement learning (RL) is one of the fastest-growing research areas in machine learning. RL-based techniques have led to several recent breakthroughs in artificial intelligence, such as beating human champions in the game of Go.  The application of RL to real life problems, however, remains limited, even in areas where a large amount of data has already been collected. The crux of the problem is that most existing RL methods require an environment for the agent to interact with, but in real-life applications, it is rarely possible to have access to such an environment \u2014 deploying an algorithm that learns by trial-and-errors may have serious legal, ethical and safety issues. This project aims to address this conundrum by developing algorithms that learn from offline data. The outcome of the research could significantly reduce the overhead of using RL techniques in real-life sequential decision-making problems such as those in power transmission, personalized medicine, scientific discoveries, computer networking and public policy.\r\n\r\nThe project focuses on two settings that aim at addressing the aforementioned challenge of limited access to an environment. In the first setting, the agent is given only the historical data from logged interactions with the environment. In the second setting, the agent is able to change how it interacts with the environment only a few times. The investigators will develop mathematical theory that describes the difficulty of the problem and ensures that the developed algorithms are robust and optimal in the sense that they use the least possible resources (data, energy, computation). Using techniques such as marginalized importance sampling, uniform convergence and batched exploration, the project will generalize the recent line of work in ``breaking the curse of horizon'' to allow function approximations and establish the much-needed statistical learning theory for offline and low-adaptive reinforcement learning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu-Xiang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu-Xiang Wang",
   "pi_email_addr": "yuxiangw@ucsd.edu",
   "nsf_id": "000785032",
   "pi_start_date": "2020-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931065110",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 449976.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Reinforcement learning (RL) is one of the fastest-growing research area in machine learning. RL-based techniques have led to several recent breakthroughs in artificial intelligence, such as beating human champions in the game of Go.&nbsp; The application of RL to real life problems however, remains limited, even in the areas where a large amount of data has already been collected. The crux of the problem is that most existing RL methods require an environment where the agent can interact with, but in real-life applications, it is rarely possible to have access to such an environment &mdash; deploying an algorithm that freely trial-and-errors may have serious legal, ethical and safety issues. This project aims at addressing this conundrum by developing algorithms that use no or very few experiments.</p>\r\n<p>On the intellectual merits front, the project focuses on two settings that aim at addressing the aforementioned challenge of limited access to an environment. In the first setting, the agent is given only the historical data from logged interactions with the environment. In the second setting, the agent is able to change how it interacts with the environment only a few times. After four years of fundamental research we have satisfactorily addressed the problems in the aforementioned tasks. &nbsp;We now have a comprehensive algorithmic and mathematical foundation for both settings through 22 top-venue scientific papers and two PhD Theses which not only identifies the most data-efficient algorithms but also uncovered the general principles of learning decision making policies when you have limited historical information through what we now know as &ldquo;pessimism in the face of uncertainty&rdquo;.&nbsp; We also now understand that strategic exploration, such as those in drug-discovery, material discovery and clinical trials or can be carried out in just a few batches while retaining the efficiency, even if the best decision policy requires deep and long-term thinking.</p>\r\n<p>On the broader impacts front. The outcome of the project significantly reduces the overhead of using RL techniques in real-life sequential decision-making problems and c in power transmission, personalized medicine, scientific discoveries, computer networking and public policy. The research results were disseminated widely in not only the machine learning community at conferences but also be made available fields of statistics, causal inference, operations research through our outreach efforts, as well as to an even broader audience through lay-person-targeted posts on social media.&nbsp; The project provided numerous training and education activities for students with diverse background through the UCSB Early Research Scholar program, UCSB Data Science Club, and through the undergraduate and graduate courses that we created using materials we developed in this project. Both the undergraduate and graduate level course materials are now public available and being widely used by other instructors teaching similar courses.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/23/2025<br>\nModified by: Yu-Xiang&nbsp;Wang</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737614040087_IOLB--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737614040087_IOLB--rgov-800width.jpg\" title=\"Adaptive bounds in offline RL\"><img src=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737614040087_IOLB--rgov-66x44.jpg\" alt=\"Adaptive bounds in offline RL\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We obtained a highly adaptive learning bound for offline RL that unifies all known special cases.</div>\n<div class=\"imageCredit\">Ming Yin</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu-Xiang&nbsp;Wang\n<div class=\"imageTitle\">Adaptive bounds in offline RL</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737613863517_low_adaptiveRL_illus--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737613863517_low_adaptiveRL_illus--rgov-800width.jpg\" title=\"Illustration of low-adaptive reinforcement learning\"><img src=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737613863517_low_adaptiveRL_illus--rgov-66x44.jpg\" alt=\"Illustration of low-adaptive reinforcement learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">When the next experiment to run depends on the outcome of the current experiment, progress is slow. We show that one can solve the problem just as well while running the experiments nearly completely in parallel. The number K can be nearly a constant.</div>\n<div class=\"imageCredit\">yu-xiang wang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu-Xiang&nbsp;Wang\n<div class=\"imageTitle\">Illustration of low-adaptive reinforcement learning</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737614088127_offlineRL_illustration--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737614088127_offlineRL_illustration--rgov-800width.jpg\" title=\"Illustration of the problem of Offline Reinforcement Learning\"><img src=\"/por/images/Reports/POR/2025/2007117/2007117_10701610_1737614088127_offlineRL_illustration--rgov-66x44.jpg\" alt=\"Illustration of the problem of Offline Reinforcement Learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of the problem of Offline Reinforcement Learning.</div>\n<div class=\"imageCredit\">Yu-Xiang Wang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Yu-Xiang&nbsp;Wang\n<div class=\"imageTitle\">Illustration of the problem of Offline Reinforcement Learning</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nReinforcement learning (RL) is one of the fastest-growing research area in machine learning. RL-based techniques have led to several recent breakthroughs in artificial intelligence, such as beating human champions in the game of Go. The application of RL to real life problems however, remains limited, even in the areas where a large amount of data has already been collected. The crux of the problem is that most existing RL methods require an environment where the agent can interact with, but in real-life applications, it is rarely possible to have access to such an environment  deploying an algorithm that freely trial-and-errors may have serious legal, ethical and safety issues. This project aims at addressing this conundrum by developing algorithms that use no or very few experiments.\r\n\n\nOn the intellectual merits front, the project focuses on two settings that aim at addressing the aforementioned challenge of limited access to an environment. In the first setting, the agent is given only the historical data from logged interactions with the environment. In the second setting, the agent is able to change how it interacts with the environment only a few times. After four years of fundamental research we have satisfactorily addressed the problems in the aforementioned tasks. We now have a comprehensive algorithmic and mathematical foundation for both settings through 22 top-venue scientific papers and two PhD Theses which not only identifies the most data-efficient algorithms but also uncovered the general principles of learning decision making policies when you have limited historical information through what we now know as pessimism in the face of uncertainty. We also now understand that strategic exploration, such as those in drug-discovery, material discovery and clinical trials or can be carried out in just a few batches while retaining the efficiency, even if the best decision policy requires deep and long-term thinking.\r\n\n\nOn the broader impacts front. The outcome of the project significantly reduces the overhead of using RL techniques in real-life sequential decision-making problems and c in power transmission, personalized medicine, scientific discoveries, computer networking and public policy. The research results were disseminated widely in not only the machine learning community at conferences but also be made available fields of statistics, causal inference, operations research through our outreach efforts, as well as to an even broader audience through lay-person-targeted posts on social media. The project provided numerous training and education activities for students with diverse background through the UCSB Early Research Scholar program, UCSB Data Science Club, and through the undergraduate and graduate courses that we created using materials we developed in this project. Both the undergraduate and graduate level course materials are now public available and being widely used by other instructors teaching similar courses.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 01/23/2025\n\n\t\t\t\t\tSubmitted by: Yu-XiangWang\n"
 }
}