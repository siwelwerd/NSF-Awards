{
 "awd_id": "2041759",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Interpreting Black-Box Predictive Models Through Causal Attribution",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2020-08-15",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-08-04",
 "awd_max_amd_letter_date": "2020-08-04",
 "awd_abstract_narration": "Our ability to acquire and annotate increasingly large amounts of data together with rapid advances in machine learning have made predictive models trained using machine learning ubiquitous in virtually all areas of human endeavor. In high-stakes applications such as healthcare, finance, criminal justice, scientific discovery, education, and others, the resulting predictive models are complex, and in many cases, black-boxes. Consider for example, a medical decision making scenario where a predictive model, e.g., a deep neural network, trained on a large database of labeled data, is to assist physicians in diagnosing patients. In this setting, it is important that the clinical decision support system be able to explain the output of the deep neural network to the physician, who may not have a deep understanding of machine learning. For example, the physician might want to understand the subset of patient characteristics that contribute to the diagnosis; or the reason as to why diagnoses were different for two different patients, etc. In high stakes applications of machine learning, the ability to explain the machine learned model is a prerequisite for establishing trust in the model\u2019s predictions. Satisfactory explanations have to provide answers to questions such as: \"What features of the input are responsible for the predictions?\"; \"Why are the model\u2019s outputs different for two individuals?\" (e.g., Why did John\u2019s loan application get approved when Sarah\u2019s was not?). Hence, satisfactory explanations have to be fundamentally causal in nature. This project will develop a theoretically sound, yet practical approach to causal attribution, that is, apportioning the responsibility for a black-box predictive model\u2019s outputs among the model\u2019s inputs.\r\n\r\nThe model interpretation question \"Why did the predictive model generate the output Y for input X?\" will be reduced to the following equivalent question: \"How are the features of the model input X causally related to the model output Y?\" In other words, the task of interpreting a black-box predictive model is reduced to the task of estimating, from observations of the inputs and the corresponding outputs of the model, the causal effect of each input variable or feature on the output variable. The planned methods do not require knowledge of the internal structure or parameters of the black-box model, or of the objective function or the algorithm used to train the model. Hence, the resulting methods can be applied, in principle, to any black-box predictive model, so long as it is possible probe the model and observe the model\u2019s response to any supplied input data sample. Advances in causal attribution methods will help broaden the application of machine learned black-box predictive models in high-stakes applications across many areas of human endeavor. The project offers enhanced opportunities for research-based training of graduate and undergraduate students in Informatics, Data Sciences, and Artificial Intelligence. The investigator will develop a new course on Foundations and Applications of Causal Inference as well as modules on Causal Attribution that for possible inclusion in undergraduate and graduate courses in Machine Learning. The broad and free dissemination of open source library of causal attribution methods, course materials, data, research results will ease their adoption and use by AI researchers, educators, and practitioners.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vasant",
   "pi_last_name": "Honavar",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Vasant G Honavar",
   "pi_email_addr": "vhonavar@ist.psu.edu",
   "nsf_id": "000205986",
   "pi_start_date": "2020-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "E335 Westgate Building",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168026823",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span><strong>Background.</strong> Our ability to acquire and annotate increasingly large amounts of data together with rapid advances in machine learning have made predictive models trained using machine learning ubiquitous in virtually all areas of human endeavor. In high-stakes applications such as healthcare, finance, criminal justice, scientific discovery, education, and others, the resulting predictive models are complex, and in many cases, black-boxes. Consider for example, a medical decision making scenario where a predictive model, e.g., a deep neural network, trained on a large database of labeled data, is to assist physicians in diagnosing patients. In this setting, it is important that the clinical decision support system be able to explain the output of the deep neural network to the physician, who may not have a deep understanding of machine learning. For example, the physician might want to understand the subset of patient characteristics that contribute to the diagnosis; or the reason as to why diagnoses were different for two different patients, etc. In high stakes applications of machine learning, the ability to explain the machine learned model is a prerequisite for establishing trust in the model&rsquo;s predictions. Satisfactory explanations have to provide answers to questions such as: \"What features of the input are responsible for the predictions?\"; \"Why are the model&rsquo;s outputs different for two individuals?\" (e.g., Why did John&rsquo;s loan application get approved when Sarah&rsquo;s was not?). Hence, satisfactory explanations have to be fundamentally causal in nature. </span></p>\r\n<p><span><strong>Intellectual Merit.</strong> The research on causal attribution was based on the following insight: The model interpretation question \"Why did the predictive model generate the a specific output (Y) for a given input (X)?\" can be reduced to the following equivalent question: \"How are the features of the model input X causally related to the model output Y?\" In other words, the task of interpreting a black-box predictive model is reduced to the task of estimating, from observations of the inputs and the corresponding outputs of the model, the causal effect of each input variable or feature on the output variable. The resulting causal attribution methods do not require knowledge of the internal structure or parameters of the black-box model, or of the objective function or the algorithm used to train the model. Hence, the resulting methods can be applied, in principle, to any black-box predictive model, so long as it is possible probe the model and observe the model's response to any supplied input data sample. The resulting advances in causal attribution methods are expected to broaden the application of machine learned black-box predictive models in high-stakes applications across many areas of human endeavor, including health care, public policy, education.</span></p>\r\n<p><span><strong>Broader Impacts</strong>. The project provided enhanced opportunities for research-based training of graduate and undergraduate students in Informatics, Data Sciences, and Artificial Intelligence. The investigator developed a new course on Foundations and Applications of Causal Inference as well as modules on Causal Inference that for possible inclusion in undergraduate and graduate courses in Machine Learning. He also taught short courses on causal inference to clinical researchers. The causal inference and causal attributioon methods resulting from the project are being used to shed light on drivers of rural health disparities in collaboration with public health researchers. &nbsp;The broad and free dissemination of open source software, &nbsp;course materials, data, research results enable their use by AI researchers, educators, and practitioners.</span><br /><br /></p><br>\n<p>\n Last Modified: 12/19/2024<br>\nModified by: Vasant&nbsp;G&nbsp;Honavar</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nBackground. Our ability to acquire and annotate increasingly large amounts of data together with rapid advances in machine learning have made predictive models trained using machine learning ubiquitous in virtually all areas of human endeavor. In high-stakes applications such as healthcare, finance, criminal justice, scientific discovery, education, and others, the resulting predictive models are complex, and in many cases, black-boxes. Consider for example, a medical decision making scenario where a predictive model, e.g., a deep neural network, trained on a large database of labeled data, is to assist physicians in diagnosing patients. In this setting, it is important that the clinical decision support system be able to explain the output of the deep neural network to the physician, who may not have a deep understanding of machine learning. For example, the physician might want to understand the subset of patient characteristics that contribute to the diagnosis; or the reason as to why diagnoses were different for two different patients, etc. In high stakes applications of machine learning, the ability to explain the machine learned model is a prerequisite for establishing trust in the models predictions. Satisfactory explanations have to provide answers to questions such as: \"What features of the input are responsible for the predictions?\"; \"Why are the models outputs different for two individuals?\" (e.g., Why did Johns loan application get approved when Sarahs was not?). Hence, satisfactory explanations have to be fundamentally causal in nature. \r\n\n\nIntellectual Merit. The research on causal attribution was based on the following insight: The model interpretation question \"Why did the predictive model generate the a specific output (Y) for a given input (X)?\" can be reduced to the following equivalent question: \"How are the features of the model input X causally related to the model output Y?\" In other words, the task of interpreting a black-box predictive model is reduced to the task of estimating, from observations of the inputs and the corresponding outputs of the model, the causal effect of each input variable or feature on the output variable. The resulting causal attribution methods do not require knowledge of the internal structure or parameters of the black-box model, or of the objective function or the algorithm used to train the model. Hence, the resulting methods can be applied, in principle, to any black-box predictive model, so long as it is possible probe the model and observe the model's response to any supplied input data sample. The resulting advances in causal attribution methods are expected to broaden the application of machine learned black-box predictive models in high-stakes applications across many areas of human endeavor, including health care, public policy, education.\r\n\n\nBroader Impacts. The project provided enhanced opportunities for research-based training of graduate and undergraduate students in Informatics, Data Sciences, and Artificial Intelligence. The investigator developed a new course on Foundations and Applications of Causal Inference as well as modules on Causal Inference that for possible inclusion in undergraduate and graduate courses in Machine Learning. He also taught short courses on causal inference to clinical researchers. The causal inference and causal attributioon methods resulting from the project are being used to shed light on drivers of rural health disparities in collaboration with public health researchers. The broad and free dissemination of open source software, course materials, data, research results enable their use by AI researchers, educators, and practitioners.\n\n\t\t\t\t\tLast Modified: 12/19/2024\n\n\t\t\t\t\tSubmitted by: VasantGHonavar\n"
 }
}