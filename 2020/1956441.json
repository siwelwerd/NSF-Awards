{
 "awd_id": "1956441",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: RI: AF: Medium: Exchanging Knowledge Beyond Data Between Human and Machine Learner",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928318",
 "po_email": "vpavlovi@nsf.gov",
 "po_sign_block_name": "Vladimir Pavlovic",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 498930.0,
 "awd_amount": 498930.0,
 "awd_min_amd_letter_date": "2020-09-10",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Recent advances in deep learning have made dramatic progress in solving basic perceptual tasks such as speech recognition and object detection. To pave the way for the many human-centered applications that these advances might enable, in healthcare for instance, it is important to move beyond classification problems: to think of machine learning systems as producing not just category predictions, but also the reasons for them. Moreover, these patterns of reasoning need to be comprehensible to humans. To enable this, this project will focus on the exchange of knowledge between humans and machine learning systems and how such exchange of knowledge beyond data can lead to better predictions that are also human-interpretable. The project will result in technological advances that will have the potential to significantly impact the usability of machine learning in human-facing applications.\r\n\r\nThe technical aims of this project are developed along two broad themes. The first addresses the question, \"How can we involve human feedback in the machine learning process to create succinct models that are interpretable and generate predictions that are explainable?\" By enabling humans to provide rich feedback in the form of rules-of-thumb as relational knowledge, the project aims to derive succinct interpretable machine learning models that are amenable to simple explanations that are more compatible with the causal world-view of humans. To enhance the interpretability of machine learning, the project will further explore how human feedback based on relational knowledge can be leveraged to reduce the size of data sets required to train accurate models. The second addresses the question, \"How can we encode and exploit relational information in deriving interpretable and explainable models for reasoning?\" The project will explore the encoding of relational knowledge in both vector spaces and logical models and further investigate how relational knowledge can be used for analogical reasoning, semantic understanding, and relational queries.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guy",
   "pi_last_name": "Van den Broeck",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guy Van den Broeck",
   "pi_email_addr": "guyvdb@cs.ucla.edu",
   "nsf_id": "000711612",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hongjing",
   "pi_last_name": "Lu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hongjing Lu",
   "pi_email_addr": "hongjing@ucla.edu",
   "nsf_id": "000498799",
   "pi_start_date": "2020-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "UCLA, Computer Science Dept.",
  "perf_str_addr": "404 Westwood Plaza, 368 Engr VI.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951596",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 498930.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-f1c34264-7fff-1f3f-ba10-8b074805893a\"> </span></p>\n<h3 dir=\"ltr\"><span>In this project, we have made progress in bridging between machine learning models learned from data and knowledge that comes from human users. Concretely, we have developed novel neurosymbolic architectures that can take symbolic knowledge as input to the machine learning pipeline, in addition to the data that machine learning models are usually trained on. We have also studied the ability of modern machine learning architectures (i.e., transformers) to process such symbolic information correctly and reason according to the rules of logic. Here, we found that standard machine learning practices are unable to robustly learn how to do logical reasoning from data. Finally, we have studied when machine learning models can efficiently provide explanations for the decisions, in the form of SHAP explanations, and found a clear taxonomy of cases where it is and is not possible to do so.</span></h3>\n<h3 dir=\"ltr\"><span>Beyond these algorithmic studies of machine learning with symbolic information, we have studied the connection to human reasoning, and in particular whether human cognitive capacities might emerge in the machine learning process. For example, whether learned large language models can reason about novel problems without having seen much data as an example. Here we find that modern large language models indeed show strong performance on such tasks, especially in analogical reasoning. Their ability is limited when requiring physical understanding of the world and analogical reasoning based on causal relations.</span></h3>\n<h3 dir=\"ltr\"><span>In terms of broader impacts, our findings have the potential to significantly impact the usability of machine learning in human-facing applications going forward. Moreover, we have been able to work towards this goal combining insights from machine learning, statistics, and psychology, building a broader community and interdisciplinary expertise around this topic. Finally, this project has been a springboard for course development, educational, and outreach activities on the technical topics we have studied.</span></h3>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 03/18/2024<br>\nModified by: Guy&nbsp;Van Den Broeck</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \nIn this project, we have made progress in bridging between machine learning models learned from data and knowledge that comes from human users. Concretely, we have developed novel neurosymbolic architectures that can take symbolic knowledge as input to the machine learning pipeline, in addition to the data that machine learning models are usually trained on. We have also studied the ability of modern machine learning architectures (i.e., transformers) to process such symbolic information correctly and reason according to the rules of logic. Here, we found that standard machine learning practices are unable to robustly learn how to do logical reasoning from data. Finally, we have studied when machine learning models can efficiently provide explanations for the decisions, in the form of SHAP explanations, and found a clear taxonomy of cases where it is and is not possible to do so.\nBeyond these algorithmic studies of machine learning with symbolic information, we have studied the connection to human reasoning, and in particular whether human cognitive capacities might emerge in the machine learning process. For example, whether learned large language models can reason about novel problems without having seen much data as an example. Here we find that modern large language models indeed show strong performance on such tasks, especially in analogical reasoning. Their ability is limited when requiring physical understanding of the world and analogical reasoning based on causal relations.\nIn terms of broader impacts, our findings have the potential to significantly impact the usability of machine learning in human-facing applications going forward. Moreover, we have been able to work towards this goal combining insights from machine learning, statistics, and psychology, building a broader community and interdisciplinary expertise around this topic. Finally, this project has been a springboard for course development, educational, and outreach activities on the technical topics we have studied.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 03/18/2024\n\n\t\t\t\t\tSubmitted by: GuyVan Den Broeck\n"
 }
}