{
 "awd_id": "2024790",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Coordinating and Incorporating Trust in Teams of Humans and Robots with Multi-Robot Reinforcement Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 646999.0,
 "awd_amount": 646999.0,
 "awd_min_amd_letter_date": "2020-09-08",
 "awd_max_amd_letter_date": "2020-10-13",
 "awd_abstract_narration": "The decreasing cost and increasing sophistication of robot hardware has created new opportunities for applications where teams of robots can be deployed in combination with skilled humans to automate labor-intensive tasks. However, if such systems are to become widely deployable, they must be able to appropriately reason about human teamwork. Therefore, this project will create new methods for generating and solving models for teams of multiple humans and robots working together to solve complex problems. These approaches will be able to learn quickly from limited interactions and consider the dynamic and uncertain nature of coordinating teams of robots and humans. Furthermore, the project will develop methods that allows for communication between the robots and humans and incorporates models of trust to permit humans and robots to appropriately establish trust in each other.\r\n\r\nIn particular, this project will produce several novel methods for modeling and learning solutions for teams of robots interacting with multiple people. The approaches will leverage the strengths of POMDPs to consider the dynamic and uncertain nature of coordinating teams of robots and humans. Because sample efficiency is of utmost importance when dealing with humans and real-world tasks when the number of interactions will be limited, the project will develop Bayesian reinforcement learning methods that scale by exploiting hierarchy and deep learning. The project will also develop methods for communication and shared mental models to allow the humans and robots to have confidence in what each other is doing. These methods will allow tight cooperation between the humans and robots. Furthermore, humans will not want to use our system if they cannot trust the robots. Therefore, the project will develop methods that model and incorporate trust into the approach while generating interpretable POMDP models and solutions that can be shared with humans during or after execution. These advances will produce high-quality solutions for mixed human-robot teams in realistic scenarios.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Amato",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Amato",
   "pi_email_addr": "c.amato@northeastern.edu",
   "nsf_id": "000677651",
   "pi_start_date": "2020-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stacy",
   "pi_last_name": "Marsella",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stacy Marsella",
   "pi_email_addr": "stacymarsella@gmail.com",
   "nsf_id": "000110217",
   "pi_start_date": "2020-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 646999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project produced several novel and significant methods for robot learning in partially observable settings and when interacting with people. Reinforcement learning (RL) is a powerful approach that can allow robots to learn to optimize their behavior from experience but typically requires a large amount of data to learn, and struggles with partial observability---when information is limited such as with noisy and limited sensors or interacting with humans. This project developed new approaches to perform efficient partially observable reinforcement learning directly on robots, leverage privileged information from simulators, and explore new adaptive mental models.&nbsp;</p>\r\n<p>RL methods typically use a simulator to generate data but it is difficult to accurately simulate human behavior. As a result, RL methods that interact with humans must learn from a few interactions. We developed extremely sample efficient methods that are also scalable by leveraging ideas from Bayesian RL and deep learning. We developed a general framework and showed a specialized method that performs on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes.&nbsp;</p>\r\n<p>RL in partially observable environments presents unique challenges due to limited state information. However, offline training in simulators, where privileged state information is available, provides an opportunity to address this limitation. We developed a principled framework for learning with privileged simulator data (e.g., ground truth state) in partially observable settings as well as several methods that can exploit different information. For example, we developed simple extensions to popular actor-critic and action-value (e.g., DQN) RL methods that can use state and history information during training but only history information during execution. We also showed that using fully observable policies enhances offline training to improve partially observable online performance by balancing imitation and exploration under partial observability. We later incorporated mutual information to learn better history representations based on the states and observations seen during training. Furthermore, we showed how to exploit symmetry in these problems with the development of equivariant learning methods. Our methods were validated across robotics domains (such as robotic manipulation tasks), demonstrating improved performance over traditional RL techniques.</p>\r\n<p>A series of human subject studies evaluated approaches to co-adaptive coordination between human subjects and a virtual robot, each with different capabilities and where each could adapt its goals to the other. The robot could decide to decompose the task across robot and human but also adapt the decomposition based on its observations of the human&rsquo;s actions. It would also pursue its current subtask in a fashion where it would select low level behaviors in order to effectively implicitly communicate to the human what the robot's current subtask was. Results revealed that adaptive conditions and the robot's implicit communications&nbsp; via action selection provided human participants eventually with a clearer understanding of the robot&rsquo;s capabilities and a stronger perception of the robot&rsquo;s commitment to shared goals. However, despite behavioral implicit communication encouraging human leadership, the highest task performance occurs when the robot maintains a leading disposition, often reducing co-adaptation to the human adapting. These results revealed the utility of behavior based implicit communication and also underscored that implementing co-adaptation effectively required the human acquiring a better model of the robot&rsquo;s capabilities.</p>\r\n<p>The methods developed in this project could allow reinforcement learning methods to be significantly more sample efficient, apply in cases with significant sensor limitations/noise (partial observability) and fit better with settings involving humans. The proposed methods could find broad application as teams of humans and robots become more common. Examples include search and rescue, manufacturing settings, disaster response and autonomous vehicles.</p><br>\n<p>\n Last Modified: 01/07/2025<br>\nModified by: Christopher&nbsp;Amato</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2024790/2024790_10705410_1736270116924_intro--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2024790/2024790_10705410_1736270116924_intro--rgov-800width.jpg\" title=\"Rotationally symmetric example of a drawer opening problem\"><img src=\"/por/images/Reports/POR/2025/2024790/2024790_10705410_1736270116924_intro--rgov-66x44.jpg\" alt=\"Rotationally symmetric example of a drawer opening problem\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Equivariant Reinforcement Learning under Partial Observability. Hai Nguyen, Andrea Baisero, David Klee, Dian Wang, Robert Platt and Christopher Amato. In the Proceedings of the 2023 Conference on Robot Learning (CoRL-23), November 2023.</div>\n<div class=\"imageCredit\">Hai Nguyen</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Christopher&nbsp;Amato\n<div class=\"imageTitle\">Rotationally symmetric example of a drawer opening problem</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2024790/2024790_10705410_1736269646770_workspace--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2024790/2024790_10705410_1736269646770_workspace--rgov-800width.jpg\" title=\"Workspace for on-robot learning\"><img src=\"/por/images/Reports/POR/2025/2024790/2024790_10705410_1736269646770_workspace--rgov-66x44.jpg\" alt=\"Workspace for on-robot learning\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">On-Robot Bayesian Reinforcement Learning for POMDPs. Hai Nguyen, Sammie Katt, Yuchen Xiao and Christopher Amato. In the Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS-23), October 2023.</div>\n<div class=\"imageCredit\">Hai Nguyen</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Christopher&nbsp;Amato\n<div class=\"imageTitle\">Workspace for on-robot learning</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project produced several novel and significant methods for robot learning in partially observable settings and when interacting with people. Reinforcement learning (RL) is a powerful approach that can allow robots to learn to optimize their behavior from experience but typically requires a large amount of data to learn, and struggles with partial observability---when information is limited such as with noisy and limited sensors or interacting with humans. This project developed new approaches to perform efficient partially observable reinforcement learning directly on robots, leverage privileged information from simulators, and explore new adaptive mental models.\r\n\n\nRL methods typically use a simulator to generate data but it is difficult to accurately simulate human behavior. As a result, RL methods that interact with humans must learn from a few interactions. We developed extremely sample efficient methods that are also scalable by leveraging ideas from Bayesian RL and deep learning. We developed a general framework and showed a specialized method that performs on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes.\r\n\n\nRL in partially observable environments presents unique challenges due to limited state information. However, offline training in simulators, where privileged state information is available, provides an opportunity to address this limitation. We developed a principled framework for learning with privileged simulator data (e.g., ground truth state) in partially observable settings as well as several methods that can exploit different information. For example, we developed simple extensions to popular actor-critic and action-value (e.g., DQN) RL methods that can use state and history information during training but only history information during execution. We also showed that using fully observable policies enhances offline training to improve partially observable online performance by balancing imitation and exploration under partial observability. We later incorporated mutual information to learn better history representations based on the states and observations seen during training. Furthermore, we showed how to exploit symmetry in these problems with the development of equivariant learning methods. Our methods were validated across robotics domains (such as robotic manipulation tasks), demonstrating improved performance over traditional RL techniques.\r\n\n\nA series of human subject studies evaluated approaches to co-adaptive coordination between human subjects and a virtual robot, each with different capabilities and where each could adapt its goals to the other. The robot could decide to decompose the task across robot and human but also adapt the decomposition based on its observations of the humans actions. It would also pursue its current subtask in a fashion where it would select low level behaviors in order to effectively implicitly communicate to the human what the robot's current subtask was. Results revealed that adaptive conditions and the robot's implicit communications via action selection provided human participants eventually with a clearer understanding of the robots capabilities and a stronger perception of the robots commitment to shared goals. However, despite behavioral implicit communication encouraging human leadership, the highest task performance occurs when the robot maintains a leading disposition, often reducing co-adaptation to the human adapting. These results revealed the utility of behavior based implicit communication and also underscored that implementing co-adaptation effectively required the human acquiring a better model of the robots capabilities.\r\n\n\nThe methods developed in this project could allow reinforcement learning methods to be significantly more sample efficient, apply in cases with significant sensor limitations/noise (partial observability) and fit better with settings involving humans. The proposed methods could find broad application as teams of humans and robots become more common. Examples include search and rescue, manufacturing settings, disaster response and autonomous vehicles.\t\t\t\t\tLast Modified: 01/07/2025\n\n\t\t\t\t\tSubmitted by: ChristopherAmato\n"
 }
}