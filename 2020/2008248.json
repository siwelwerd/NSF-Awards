{
 "awd_id": "2008248",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CNS Core: Small: Toward Globally-Optimal Resource Distribution and Computation Acceleration in Multi-Tenant and Heterogeneous Machine Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jason Hallstrom",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 499910.0,
 "awd_amount": 499910.0,
 "awd_min_amd_letter_date": "2020-06-30",
 "awd_max_amd_letter_date": "2020-06-30",
 "awd_abstract_narration": "In the era of large-scale deep learning (DL) and massive data, existing hardware systems have struggled to effectively accommodate heavy and complex computing workload due to difficulties in scheduling highly dynamic, heterogeneous, and competing tasks from many users over many machines in a cluster or data-center environment. This project aims to develop a \"1-click\" demand-aware and responsive software system capable of simultaneously training a wide spectrum of DL tasks, using a new resource management architecture that automatically and adaptively chooses the most effective distributed training/serving techniques and their hyperparameters to achieve best overall efficiency of multiple tasks in such environment.\r\n\r\nThis interdisciplinary project innovates in distributed systems design, DL algorithm design, and related industrial applications and theoretical analyses, with the following thrusts:  1: Develop a framework for \"ML-aware\" resource management and scheduling of multiple simultaneously running training tasks. 2: Develop principled strategies for resource management and scheduling for serving, streaming, and heterogeneous-task settings. 3: Optimize memory resources for training large-parameter models by developing holistic approaches to maximize computation throughput subject to device memory bounds. A limited-scope but rigorous and practical theoretical analysis of some of the proposed architectures will also be performed. \r\n\r\nThis project addresses the needs from the academic and industrial communities and will have a broad impact on both. It will provide easy-to-use tools that reduce the time to set-up and facilitate large-scale experimentation, while reducing the required costs, whether measured in cluster access quotas or dollars spent on cloud services. The impact on commercial practitioners will be even greater, by improving their productivity by an order of magnitude or more, as they must contend with heterogeneous computing and network resources that are shared among many users as well as the need to run many jobs on a regular basis.\r\n\r\nThe team will release and/or open-source the code at  http://sailing-lab.wixsite.com/sailing-pmls to benefit researchers and practitioners, to share their lessons learned to advocate more research in machine learning (ML) systems problems, and also to democratize high-performance ML systems and make them accessible to non-ML-educated software developers and society at large, such as industrial and manufacturing, healthcare, biology, social science, and finance, where results may have a catalytic impact. The team will publish results at a variety of top tier conferences, including machine learning (NIPS, ICML), systems (OSDI, SOSP, USENIX), and data mining (KDD, WWW).\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Xing",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Eric P Xing",
   "pi_email_addr": "epxing@cs.cmu.edu",
   "nsf_id": "000195787",
   "pi_start_date": "2020-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 499910.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our main goal in this awarded project is to develop a user-friendly software system that enables one-click setup for distributed machine learning and deep learning across large-scale and foundation ML models. This system intelligently manages resources, choosing optimal techniques and settings to train a variety of models efficiently in a sophisticated, multi-user computing setting.<br /> <br /> In completing the proposed research goal, we developed two major systems: Alpa and Pollux, both presented at OSDI, the top conference in computing systems. Notably, Pollux won the OSDI 2021 Best Paper Award. Alpa is a system that manages the training and serving of large-scale ML models by categorizing parallelisms into inter-operator and intra-operator types. It constructs a hierarchical structure for vast model-parallel execution plans and designs several compilation passes to automatically create computation and memory-efficient parallel execution strategies at each level of parallelism. Pollux enhances the scheduling performance in GPU clusters by adaptively optimizing interdependent factors at both the individual job level and across the entire cluster. Unlike most schedulers that merely allocate resources based on user requests&mdash;potentially leading to inefficient use&mdash;Pollux and some of the more recent schedulers determine the allocation of job resources with an eye toward optimization. Pollux takes this further by monitoring each job's performance during training to project how the &lsquo;goodput&rsquo; (a term we coined to reflect the combination of system throughput and statistical efficiency) might vary with changes in resource allocation. Pollux dynamically reassigns or adjusts resources to enhance cluster-wide goodput, balancing fairness and the continuous optimization of each large-scale ML job to make better use of allocated resources.<br /> <br /> Building on Alpa and Pollux, we have developed additional algorithms and systems. At NeurIPS 2022, we introduced AMP, a framework that automates the identification and application of effective model parallelism strategies. AMP explores a vast array of strategies and selects the most suitable ones by leveraging a cost model attuned to the diversity of the model structures and the specifications of the computing cluster. This framework excels at managing complex models featuring unevenly structured layers and clusters that incorporate a mix of GPU generations. Furthermore, at MLSys 2023, we presented AlpaComm, a library engineered to optimize communication across the different segments of a computational mesh, which is vital in distributed computing. AlpaComm segments cross-mesh communication into discrete units, enabling each part to transfer a segment of the data tensor from any set of potential senders to several receivers, while ensuring the data at all sender nodes is identical. It utilizes a streaming primitive for each task and employs a search-based algorithm to orchestrate these tasks efficiently. AlpaComm also proposes a novel pipeline scheduling technique that enhances the potential to concurrently optimize communication with computation tasks, thereby improving overall system efficiency.<br /> <br /> To enhance user experience, we have designed and implemented Redco, a tool that simplifies setting up distributed training for large language models, reducing the necessity for deep technical knowledge. Key among Redco's features are pre-set rules for model distribution and customizable functions that streamline machine learning workflows. Redco will be presented at the upcoming MLSys workshop at NeurIPS 2024.<br /> <br /> Moreover, our team has developed low-rank approximation algorithms that enhance the speed and efficiency of both pre-training and fine-tuning for foundation models, as well as software that tackles specific challenges such as private model inference and intricate meta-learning problems. These research outcomes were presented at top-tier ML and systems conferences, including MLSys and ICLR. Notably, our papers on the meta-learning library and private inference were selected for Oral (top 5%) and Spotlight (top 25%) presentations, respectively, at ICLR 2023.<br /> <br /></p>\n<p>Ultimately, the outcomes of our funded project have significantly streamlined and enhanced the efficiency of training and deploying cutting-edge large-scale ML and foundation models, catering to the increasing complexity of both the models and their associated data. Moreover, we have released all our software, algorithms, and datasets to benefit researchers, developers, and students across the entire systems and ML community.</p><br>\n<p>\n Last Modified: 11/04/2023<br>\nModified by: Eric&nbsp;P&nbsp;Xing</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nOur main goal in this awarded project is to develop a user-friendly software system that enables one-click setup for distributed machine learning and deep learning across large-scale and foundation ML models. This system intelligently manages resources, choosing optimal techniques and settings to train a variety of models efficiently in a sophisticated, multi-user computing setting.\n \n In completing the proposed research goal, we developed two major systems: Alpa and Pollux, both presented at OSDI, the top conference in computing systems. Notably, Pollux won the OSDI 2021 Best Paper Award. Alpa is a system that manages the training and serving of large-scale ML models by categorizing parallelisms into inter-operator and intra-operator types. It constructs a hierarchical structure for vast model-parallel execution plans and designs several compilation passes to automatically create computation and memory-efficient parallel execution strategies at each level of parallelism. Pollux enhances the scheduling performance in GPU clusters by adaptively optimizing interdependent factors at both the individual job level and across the entire cluster. Unlike most schedulers that merely allocate resources based on user requestspotentially leading to inefficient usePollux and some of the more recent schedulers determine the allocation of job resources with an eye toward optimization. Pollux takes this further by monitoring each job's performance during training to project how the goodput (a term we coined to reflect the combination of system throughput and statistical efficiency) might vary with changes in resource allocation. Pollux dynamically reassigns or adjusts resources to enhance cluster-wide goodput, balancing fairness and the continuous optimization of each large-scale ML job to make better use of allocated resources.\n \n Building on Alpa and Pollux, we have developed additional algorithms and systems. At NeurIPS 2022, we introduced AMP, a framework that automates the identification and application of effective model parallelism strategies. AMP explores a vast array of strategies and selects the most suitable ones by leveraging a cost model attuned to the diversity of the model structures and the specifications of the computing cluster. This framework excels at managing complex models featuring unevenly structured layers and clusters that incorporate a mix of GPU generations. Furthermore, at MLSys 2023, we presented AlpaComm, a library engineered to optimize communication across the different segments of a computational mesh, which is vital in distributed computing. AlpaComm segments cross-mesh communication into discrete units, enabling each part to transfer a segment of the data tensor from any set of potential senders to several receivers, while ensuring the data at all sender nodes is identical. It utilizes a streaming primitive for each task and employs a search-based algorithm to orchestrate these tasks efficiently. AlpaComm also proposes a novel pipeline scheduling technique that enhances the potential to concurrently optimize communication with computation tasks, thereby improving overall system efficiency.\n \n To enhance user experience, we have designed and implemented Redco, a tool that simplifies setting up distributed training for large language models, reducing the necessity for deep technical knowledge. Key among Redco's features are pre-set rules for model distribution and customizable functions that streamline machine learning workflows. Redco will be presented at the upcoming MLSys workshop at NeurIPS 2024.\n \n Moreover, our team has developed low-rank approximation algorithms that enhance the speed and efficiency of both pre-training and fine-tuning for foundation models, as well as software that tackles specific challenges such as private model inference and intricate meta-learning problems. These research outcomes were presented at top-tier ML and systems conferences, including MLSys and ICLR. Notably, our papers on the meta-learning library and private inference were selected for Oral (top 5%) and Spotlight (top 25%) presentations, respectively, at ICLR 2023.\n \n\n\n\nUltimately, the outcomes of our funded project have significantly streamlined and enhanced the efficiency of training and deploying cutting-edge large-scale ML and foundation models, catering to the increasing complexity of both the models and their associated data. Moreover, we have released all our software, algorithms, and datasets to benefit researchers, developers, and students across the entire systems and ML community.\t\t\t\t\tLast Modified: 11/04/2023\n\n\t\t\t\t\tSubmitted by: EricPXing\n"
 }
}