{
 "awd_id": "1947307",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: RI: Opening the black box of neural natural language processing models using machine-behavioral methods",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2023-06-30",
 "tot_intn_awd_amt": 173405.0,
 "awd_amount": 173405.0,
 "awd_min_amd_letter_date": "2020-02-28",
 "awd_max_amd_letter_date": "2020-02-28",
 "awd_abstract_narration": "In our daily lives, we are increasingly interacting with computer applications using natural language. The goal of this project is to understand and improve the neural network technology that underlies these applications. A neural network is an artificial intelligence system that is trained to behave in a certain way by showing it many examples of how it should behave. While neural networks are the best way currently known for building natural language applications, they have a major drawback: Once they appear to have learned how to do some natural language task, we don\u2019t know exactly what they have learned, and consequently we can\u2019t be certain how they will act in all circumstances. In order to improve this technology to be more robust, and also to make it more comprehensible and controllable, we need to develop ways of determining exactly what patterns a neural network has learned once it has been trained. In this project, methods from experimental psychology are adapted and used to reveal the inner workings of neural network systems that have been trained to perform natural language tasks. \r\n\r\nThis research focuses on determining how well neural networks that are trained to do natural language processing (NLP) tasks have learned representations of the syntactic structure of natural language: the grammatical relationships among words in sentences. It does so by adopting the recently proposed \u201cmachine behavior\u201d approach in which neural networks are studied by subjecting them to behavioral tests. The project as a whole has three major components. The first is to develop and train a large reference set of neural network NLP systems that vary in their architecture; all subsequent experiments are to be carried out on all of these models, so that they can be ranked in terms of the quality of their representations of syntactic structure. The second is to carry out a number of behavioral tests, by constructing carefully crafted sentences designed to elicit certain responses from the neural networks. The third is to analyze the actual information inside of the neural networks using structural probing, a newly developed technique, in order to detect representations of syntactic structure directly. The results of this research can be useful for guiding future NLP research in the development of more comprehensible neural network systems for natural language applications.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Futrell",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Richard Futrell",
   "pi_email_addr": "rfutrell@uci.edu",
   "nsf_id": "000699693",
   "pi_start_date": "2020-02-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926975100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 173405.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As language models such as ChatGPT play an increasingly large role in our lives, it is crucially important that we understand how these models work and how they will behave. Language models such as ChatGPT are machine learning systems, which means that they are not programmed to produce particular outputs---rather they are <em>trained</em>&nbsp;to produce helpful outputs based on many examples. The fact that they are trained in this way makes them very powerful and flexible, but it comes at a cost because it means we don't always understand exactly what they have learned from their training data, nor how they have learned it, nor exactly how they will behave if we give them input that is very different from anything in the training data. The goal of the research in this grant has been to \"open the black box\" of language models using a variety of methods, in order to determine what they know and how they will behave.</p>\n<p>Under the grant, we have discovered that language models often have knowledge of the structure of language that is similar to what humans have. For example, in a sentence like \"the chef chopped the onion\", they know that the \"chef\" is the one doing the chopping, not the one being chopped. Furthermore, we found out that they derive this information in a way that is similar to humans---they consider the <em>order</em> of words (although previous word indicated that they do not) in addition to their meanings. Furthermore, we discovered that when a language model is trained on multiple languages, the <em>way</em>&nbsp;that it understands which word of a sentence is the subject (the \"chopper\" in the example) is shared between the languages, in the same way that two languages are activated in a shared way in the brain of a human bilingual. We also discovered that language models can predict new word forms as humans can: for example if I tell you a new word \"wug\", you know its plural form would be \"wugs\". In addition to these empirical studies, we also developed some new methods and theories for understanding language models, including a theory that predicts how a language model will generalize to new inputs, and some new methods for detecting information stored inside language models about word meaning.</p>\n<p>As language models form a larger and larger part of the economy and affect more and more aspects of everyday life, it is vital that we understand the actual contents of these models. The research carried out under this grant has increased our understanding of how language models work, and has increased our confidence that language models' understanding of language is at least somewhat similar to how humans understand it---which means that the way language models interpret sentences will be similar to how humans do. The grant also funded training for five graduate students and two high school students in all the techniques for carrying out this kind of research. Overall, the work has increased the chances that language models make an positive impact, and has decreased some of the risk involved in deploying them.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/24/2023<br>\n\t\t\t\t\tModified by: Richard&nbsp;Futrell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs language models such as ChatGPT play an increasingly large role in our lives, it is crucially important that we understand how these models work and how they will behave. Language models such as ChatGPT are machine learning systems, which means that they are not programmed to produce particular outputs---rather they are trained to produce helpful outputs based on many examples. The fact that they are trained in this way makes them very powerful and flexible, but it comes at a cost because it means we don't always understand exactly what they have learned from their training data, nor how they have learned it, nor exactly how they will behave if we give them input that is very different from anything in the training data. The goal of the research in this grant has been to \"open the black box\" of language models using a variety of methods, in order to determine what they know and how they will behave.\n\nUnder the grant, we have discovered that language models often have knowledge of the structure of language that is similar to what humans have. For example, in a sentence like \"the chef chopped the onion\", they know that the \"chef\" is the one doing the chopping, not the one being chopped. Furthermore, we found out that they derive this information in a way that is similar to humans---they consider the order of words (although previous word indicated that they do not) in addition to their meanings. Furthermore, we discovered that when a language model is trained on multiple languages, the way that it understands which word of a sentence is the subject (the \"chopper\" in the example) is shared between the languages, in the same way that two languages are activated in a shared way in the brain of a human bilingual. We also discovered that language models can predict new word forms as humans can: for example if I tell you a new word \"wug\", you know its plural form would be \"wugs\". In addition to these empirical studies, we also developed some new methods and theories for understanding language models, including a theory that predicts how a language model will generalize to new inputs, and some new methods for detecting information stored inside language models about word meaning.\n\nAs language models form a larger and larger part of the economy and affect more and more aspects of everyday life, it is vital that we understand the actual contents of these models. The research carried out under this grant has increased our understanding of how language models work, and has increased our confidence that language models' understanding of language is at least somewhat similar to how humans understand it---which means that the way language models interpret sentences will be similar to how humans do. The grant also funded training for five graduate students and two high school students in all the techniques for carrying out this kind of research. Overall, the work has increased the chances that language models make an positive impact, and has decreased some of the risk involved in deploying them.\n\n\t\t\t\t\tLast Modified: 10/24/2023\n\n\t\t\t\t\tSubmitted by: Richard Futrell"
 }
}