{
 "awd_id": "1948503",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: TongueWrite: An efficient tongue-based text-entry method using Multifunctional intraORal Assistive technology (MORA)",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2023-08-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 175000.0,
 "awd_min_amd_letter_date": "2020-04-24",
 "awd_max_amd_letter_date": "2023-03-29",
 "awd_abstract_narration": "Individuals with high-level of paralysis can enhance their independence and quality-of-life by adding or replacing modes of input/control with the individuals' available voluntary motions. The input modes of many current assistive technologies, however, are limited. They can mostly interface with specific tasks such as computer mouse control, wheelchair driving, or text entry, but not for multiple of these purposes. Even if these technologies interfaces with multiple modalities, they are not very user-friendly, and a disabled individual may choose not to use the inefficient modality or switch from one technology to another for different tasks. One of the most powerful candidates in the human body to interact with assistive technologies is the tongue. The human tongue is able to harness voluntary movements above the neck in individuals with severe disabilities. Existing tongue-based assistive technologies perform cursor navigation and wheelchair driving with the same level of comfort as using a finger, but their performance in text entry is not good. This project will explore how the tongue-based interfaces can improve the performance of text entry (apart from tasks such as navigation and other discrete commands) by developing intuitive and easy-to-learn tongue commands. Ultimately, this multimodality will enhance both the independence of those with limited hand motions (e.g., Tetraplegia, stroke, Parkinson's disease, and age-related neurological disorders) and the quality-of-life of their caregivers.\r\n\r\nFor text entry, a user should be able to unambiguously specify the position and sequence of keys that should be considered as input. In order to use the human tongue for text entry, the associated algorithm should be able to handle the continuous tongue motion (involving multiple degrees of freedom), identify the movements related to text entry, and detect and ignore \u201cnormal\u201d tongue movements such as swallowing saliva, etc. To address these challenges, this project will model the performance of users on tongue-based commands by the number of commands to be carried out and investigate efficient text entry methods along with various modes/mechanisms of text entry (such as a touchscreen). The research team has developed a new tongue-based assistive technology, Multifunctional intraORal Assistive technology (MORA), which employs advanced sensor technology and a smart data fusion algorithm while taking advantage of the power of the tongue. Designed as a customized wireless headset/retainer, MORA uses an array of four three-axis magnetic sensors located near user\u2019s cheek / upper pallet. MORA can differentiate user-defined tongue movements from other natural tongue movements, especially those involved in speaking and swallowing without any tracer attachment. To model the usability of the tongue commands using MORA, the research team will first evaluate tongue performance by the number of commands: five, seven, and nine. To evaluate tongue performance by the number of commands and their learning effects, the research team will implement a random command task and a Fitts' law-based multidirectional tapping task. Then, the team will explore various text entry methods (such as H4-Writer, OPTI II, Hex-O-Spell, Metropolis II, EdgeWrite, Multitap) based on three mechanisms: multi-stroke, touchscreen, and gesture recognition, to investigate efficient text entry methods based on discrete tongue commands. MORA will be introduced to users through focus groups in medical facilities such as The Texas Brain and Spine Institute, Bryan, Texas, and the Center for Excellence in Aging Services and Long-Term Care, in the University of Texas at Austin.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kiju",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kiju Lee",
   "pi_email_addr": "kiju.lee@tamu.edu",
   "nsf_id": "000547312",
   "pi_start_date": "2023-03-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Jeonghee",
   "pi_last_name": "Kim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeonghee Kim",
   "pi_email_addr": "jeonghee.kim@tamu.edu",
   "nsf_id": "000785263",
   "pi_start_date": "2020-04-24",
   "pi_end_date": "2023-03-29"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Kiju",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kiju Lee",
   "pi_email_addr": "kiju.lee@tamu.edu",
   "nsf_id": "000547312",
   "pi_start_date": "2023-02-07",
   "pi_end_date": "2023-03-29"
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "Fermier Hall 008A, 106 Ross Stre",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433367",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 175000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In 2019, the PI and research team at Texas A&amp;M University received a $175k NSF Computer and&nbsp;Information Science and Engineering (CISE) Research Initiation Initiative (CRII) award entitled&nbsp;&ldquo;TongueWrite: An efficient tongue text entry method using Multifunctional intraORal Assistive&nbsp;technology (MORA).&rdquo;&nbsp;To efficiently support individuals with high-level of paralysis, the PI and the research team have&nbsp;developed a new tracer-free Multifunctional intraORal Assistive technology (MORA) that intuitively&nbsp;maps intraoral movements to user-defined commands to control multiple devices/functions without any&nbsp;tracer in daily activities. To maximize the usability of this new tongue-based AT, the research team&nbsp;implemented various types of text-entry methods with the intuitive and easy-to-learn tongue-based&nbsp;commands of the MORA to improve the performance of &ldquo;text entry.&rdquo;&nbsp;To model the usability of the tongue commands, we evaluated tongue performance by the number of the&nbsp;commands: five and seven command version of personalized hardware that was implemented on the&nbsp;flexible electronics. To evaluate tongue performance by the number of commands and their learning&nbsp;effects, we implemented a random command task and a Fitts&rsquo; law-based multidirectional tapping task.&nbsp;Based on the tasks, we analyzed the following metrics: Correctly completed commands (%), Information&nbsp;Transfer Rate (bits per minute), Throughput (bits per second), Error rate (%), Completion Time (seconds),&nbsp;and Path Efficiency (%). We further explored the performance evaluation on the electric powered&nbsp;wheelchair using the custom-designed wireless interface and virtual reality environment.&nbsp;To investigate efficient text entry methods that can maximize the power of tongue movements, we&nbsp;explored various command-based text entry methods that use the tongue-based AT, MORA. We&nbsp;implemented the OPTI II, Hex-O-Spell, Metropolis II, and EdgeWrite text entry methods using the five&nbsp;and seven commands of MORA in different human subject groups. The performance of the text entry&nbsp;methods were analyzed using the following metrics: Keystroke per character (KSPC), Correct words per&nbsp;minute (cwpm), Accuracy, and Completion Time (seconds). The performance was analyzed in different&nbsp;interventions (the number of commands, different devices, different text entry layouts) as well as their&nbsp;learning effects.&nbsp;The findings have been introduced in two journal papers, two conference papers, and two dissertations&nbsp;(the project supported three PhD students (one full-time and two part time) and two master students).&nbsp;Four journal papers are ready to be submitted with further analysis. From this information, we could find&nbsp;the most efficient text entry method by harnessing the usability of the tongue-based assistive technology.&nbsp;Ultimately, with further implementation on the larger study based on these findings, we can contribute to&nbsp;maximize the power of tongue movements for multi-modalities in a single input device. In addition, along&nbsp;with these efficient multi-input modalities, we believe we can establish a universal interface with existing&nbsp;technologies such as smartphones, wheelchairs, home automation systems.</p><br>\n<p>\n Last Modified: 12/29/2023<br>\nModified by: Kiju&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nIn 2019, the PI and research team at Texas A&M University received a $175k NSF Computer andInformation Science and Engineering (CISE) Research Initiation Initiative (CRII) award entitledTongueWrite: An efficient tongue text entry method using Multifunctional intraORal Assistivetechnology (MORA).To efficiently support individuals with high-level of paralysis, the PI and the research team havedeveloped a new tracer-free Multifunctional intraORal Assistive technology (MORA) that intuitivelymaps intraoral movements to user-defined commands to control multiple devices/functions without anytracer in daily activities. To maximize the usability of this new tongue-based AT, the research teamimplemented various types of text-entry methods with the intuitive and easy-to-learn tongue-basedcommands of the MORA to improve the performance of text entry.To model the usability of the tongue commands, we evaluated tongue performance by the number of thecommands: five and seven command version of personalized hardware that was implemented on theflexible electronics. To evaluate tongue performance by the number of commands and their learningeffects, we implemented a random command task and a Fitts law-based multidirectional tapping task.Based on the tasks, we analyzed the following metrics: Correctly completed commands (%), InformationTransfer Rate (bits per minute), Throughput (bits per second), Error rate (%), Completion Time (seconds),and Path Efficiency (%). We further explored the performance evaluation on the electric poweredwheelchair using the custom-designed wireless interface and virtual reality environment.To investigate efficient text entry methods that can maximize the power of tongue movements, weexplored various command-based text entry methods that use the tongue-based AT, MORA. Weimplemented the OPTI II, Hex-O-Spell, Metropolis II, and EdgeWrite text entry methods using the fiveand seven commands of MORA in different human subject groups. The performance of the text entrymethods were analyzed using the following metrics: Keystroke per character (KSPC), Correct words perminute (cwpm), Accuracy, and Completion Time (seconds). The performance was analyzed in differentinterventions (the number of commands, different devices, different text entry layouts) as well as theirlearning effects.The findings have been introduced in two journal papers, two conference papers, and two dissertations(the project supported three PhD students (one full-time and two part time) and two master students).Four journal papers are ready to be submitted with further analysis. From this information, we could findthe most efficient text entry method by harnessing the usability of the tongue-based assistive technology.Ultimately, with further implementation on the larger study based on these findings, we can contribute tomaximize the power of tongue movements for multi-modalities in a single input device. In addition, alongwith these efficient multi-input modalities, we believe we can establish a universal interface with existingtechnologies such as smartphones, wheelchairs, home automation systems.\t\t\t\t\tLast Modified: 12/29/2023\n\n\t\t\t\t\tSubmitted by: KijuLee\n"
 }
}