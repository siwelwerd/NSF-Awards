{
 "awd_id": "2016178",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: A Just-in-Time, Cross-Layer Instrumentation Framework for Diagnosing Performance Problems in Distributed Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jason Hallstrom",
 "awd_eff_date": "2019-08-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 397906.0,
 "awd_amount": 397906.0,
 "awd_min_amd_letter_date": "2020-01-27",
 "awd_max_amd_letter_date": "2020-01-27",
 "awd_abstract_narration": "Distributed applications running in data centers are critical to society (e.g., for shopping, banking).  Engineers must diagnose and fix problems observed in data centers quickly; however, doing so is extremely challenging.  A significant hurdle is that engineers must spend significant time and effort exploring what instrumentation (e.g., log messages about specific application behaviors) is needed to provide visibility into a new problem.  To assist in this front, this project will develop an instrumentation framework that, in response to a new problem, will automatically search the space of possible instrumentation choices and enable the instrumentation needed to provide insight into it.\r\n\r\nThis project addresses fundamental challenges associated with creating an automatic instrumentation framework: (a) What algorithms and heuristics are suited for automatically and efficiently exploring the instrumentation search space?  (b) What architectural support is needed within the framework to enable automatic exploration?  (c) How can the search space be explored without significantly impacting application performance?  The proposal will explore the utility of algorithms based on operator knowledge, statistics, and machine learning to explore the search space.  It will build on end-to-end tracing, as this will enable the framework to work for problems that affect different sets of requests.\r\n\r\nThis project will inform the architecture of next-generation instrumentation frameworks, which are needed to keep pace with the ever-increasing complexity of distributed applications.  The critical issues identified in popular open-source distributed applications while evaluating the framework will improve their robustness.  Researchers will be able to leverage the software artifacts released by this project to create novel distributed-application-management tools that leverage the framework's unique capabilities.  They will be able to deploy the framework in research clouds to obtain valuable workload traces from them.  The project will generate course modules on diagnosis practices for distributed applications.\r\n\r\nThe artifacts produced by this project, including framework source code, workload traces, instrumented applications, and research results, will be freely disseminated online at: https://massopen.cloud and https://www.rajasambasivan.com.   All software artifacts will be stored in Github as well.  All artifacts will be available for a minimum of seven years from the start of the project.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raja",
   "pi_last_name": "Sambasivan",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Raja R Sambasivan",
   "pi_email_addr": "raja@cs.tufts.edu",
   "nsf_id": "000762158",
   "pi_start_date": "2020-01-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Tufts University",
  "inst_street_address": "80 GEORGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "MEDFORD",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6176273696",
  "inst_zip_code": "021555519",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "TRUSTEES OF TUFTS COLLEGE",
  "org_prnt_uei_num": "WL9FLBRVPJJ7",
  "org_uei_num": "WL9FLBRVPJJ7"
 },
 "perf_inst": {
  "perf_inst_name": "Tufts University",
  "perf_str_addr": "136 Harrison Ave.",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021111817",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 397906.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Society relies on distributed applications for all aspects of modern society. For example, we rely on them to shop (Amazon), collaborate at work (Google Docs), and watch movies (Netflix). So, performance problems in these applications must be diagnosed and fixed quickly. Today, developers use logs---i.e., printouts inserted into distributed applications codebases---to diagnose problems. But, it is difficult to know<em>&nbsp;a priori&nbsp;</em>where logs must be inserted in the codebase to diagnose future problems. Instrumenting all areas of distributed applications to obtain very fine-grained logs is infeasible due to the resulting overheads. As a result of these issues, developers spend time-consuming cycles exploring whether existing logs are sufficient to provide visibility into a new problem, finding that they aren?t, and then exploring where additional logs are needed.&nbsp; Only when developers have gained sufficient visibility can they diagnose &amp; fix the problem.</p>\n<p>This project explored the research question:&nbsp;<em>Is it possible to create an instrumentation framework that automatically explores where logs must be enabled in a distributed application to localize---i.e., identify areas of the codebase most affected by---newly-observed runtime performance problems?&nbsp;&nbsp;</em>We hypothesized that two observations would enable such a framework.&nbsp;&nbsp;</p>\n<p>The first observation is that in many distributed applications, requests that have similar workflows---i.e., are processed similarly within and among the application nodes---should perform similarly. If they do not perform similarly---i.e., they exhibit high response-time variance---there is something unknown that differentiates their workflows. This unknown behavior may be performance problems, such as slow functions, resource contention, or load imbalances. For example, there is a common expectation that requests that retrieve file attributes in a distributed storage system should perform similarly because the same basic processing is involved for all of them. RetrieveAttribute requests that do not perform similarly may indicate problems, such as problematic hash tables that suffer from increased collisions for some file keys or lock contention that results in some requests being slow. &nbsp;</p>\n<p>The second observation is the power of&nbsp;<em>workflow graphs</em>&nbsp;that show: 1) the order of log statements executed by individual requests and 2) latencies incurred between log executions. Using the first insight, if groups of requests with identical workflow graphs exhibit high response-time variance, we can conclude that the log statements they execute are insufficient to differentiate their workflows. Identifying edges of the graphs that contribute most to response-time variance gives insight into where in the codebase additional log statements are needed to differentiate them. Of course, some problems may manifest as groups with consistently-slow response times instead of high variance. For these groups, identifying high-latency edges of their graphs provides insight into where additional logs are needed to provide additional resolution.&nbsp; &nbsp;</p>\n<p><strong>Intellectual merit:&nbsp;</strong>We developed VAIF, an instrumentation framework that builds on the above insights. When developers notice certain requests are slow, they initialize VAIF to explore what log statements are needed to localize code areas most affected by them. VAIF starts by creating workflow graphs using a set of logs that developers have chosen to be always on. It then operates in a continuous cycle. It 1) groups together identical workflow graphs collected for the problematic requests; 2) identifies groups that exhibit high response-time variance or consistently-slow behavior; 3) locates areas in the groups' graphs that contribute most to variance or latency; 5) uses simple strategies to chose which log statements are needed in those areas; 5) inserts or enables them either via dynamic instrumentation or control signals sent to disabled log statements.&nbsp; This loop continues until all groups exhibit low response-time variance or no more logs can be enabled in high-variance or high-latency areas.&nbsp;&nbsp;</p>\n<p>We showed that VAIF can constructed by&nbsp;<em>building on existing widely-available distributed-tracing infrastructures.&nbsp;&nbsp;</em>These infrastructures provide the infrastructure needed to capture graphs of request workflows using logging points (called trace points).&nbsp;</p>\n<p>We demonstrated VAIF?s efficacy in diagnosing seven real performance problems in popular open-source distributed applications: OpenStack, HDFS, and DeathStarBench. For all of them, VAIF successfully localized areas of the codebase most affected by the problem while only enabling 3-34% of the instrumentation that would be enabled using their default tracing infrastructures. We discovered many of these problems w/o a priori knowledge of their existence. Work on VAIF resulted in five conference papers on top systems conferences and journals, five posters, and five talks at academic and industrial conferences.</p>\n<p><strong>Broader impacts</strong>: This project in-part funded the training of five PhD students and has (or will) constitute a significant portion of two dissertations. Two Master&rsquo;s students and three undergraduates completed projects or theses related to VAIF. Insights from VAIF informed projects in a course that provides research experience to undergraduates and projects for six-high school students. VAIF has drawn interest from the open-source distributed-tracing community, and its source code is publicly available at: https://github.com/docc-lab/pythia.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/18/2023<br>\n\t\t\t\t\tModified by: Raja&nbsp;R&nbsp;Sambasivan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2016178/2016178_10565387_1676575910238_fig_staif_design--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2016178/2016178_10565387_1676575910238_fig_staif_design--rgov-800width.jpg\" title=\"VAIF's architecture\"><img src=\"/por/images/Reports/POR/2023/2016178/2016178_10565387_1676575910238_fig_staif_design--rgov-66x44.jpg\" alt=\"VAIF's architecture\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">VAIF includes a control & data plane.  The former consists of logic that chooses which log points to enable and supporting elements.  The latter consists of a distributed application instrumented with distributed tracing.  The tracing system is modified to allow log points to be enabled/disabled.</div>\n<div class=\"imageCredit\">Darby Huye, Mert Toslali, and Raja Sambasivan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Raja&nbsp;R&nbsp;Sambasivan</div>\n<div class=\"imageTitle\">VAIF's architecture</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2016178/2016178_10565387_1676569578223_fig_end_to_end_trace_modified--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2016178/2016178_10565387_1676569578223_fig_end_to_end_trace_modified--rgov-800width.jpg\" title=\"Two example request workflows\"><img src=\"/por/images/Reports/POR/2023/2016178/2016178_10565387_1676569578223_fig_end_to_end_trace_modified--rgov-66x44.jpg\" alt=\"Two example request workflows\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">he first request\ufffds workflow involves retrieving data from a cache within the table-store client\ufffds cache.  The second request\ufffds workflow involves retrieving data from a backing storage system.  Ovals represent work done within nodes for requests, which may or not be recorded by logs.</div>\n<div class=\"imageCredit\">Ilari Shafer and Raja Sambasivan</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Raja&nbsp;R&nbsp;Sambasivan</div>\n<div class=\"imageTitle\">Two example request workflows</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nSociety relies on distributed applications for all aspects of modern society. For example, we rely on them to shop (Amazon), collaborate at work (Google Docs), and watch movies (Netflix). So, performance problems in these applications must be diagnosed and fixed quickly. Today, developers use logs---i.e., printouts inserted into distributed applications codebases---to diagnose problems. But, it is difficult to know a priori where logs must be inserted in the codebase to diagnose future problems. Instrumenting all areas of distributed applications to obtain very fine-grained logs is infeasible due to the resulting overheads. As a result of these issues, developers spend time-consuming cycles exploring whether existing logs are sufficient to provide visibility into a new problem, finding that they aren?t, and then exploring where additional logs are needed.  Only when developers have gained sufficient visibility can they diagnose &amp; fix the problem.\n\nThis project explored the research question: Is it possible to create an instrumentation framework that automatically explores where logs must be enabled in a distributed application to localize---i.e., identify areas of the codebase most affected by---newly-observed runtime performance problems?  We hypothesized that two observations would enable such a framework.  \n\nThe first observation is that in many distributed applications, requests that have similar workflows---i.e., are processed similarly within and among the application nodes---should perform similarly. If they do not perform similarly---i.e., they exhibit high response-time variance---there is something unknown that differentiates their workflows. This unknown behavior may be performance problems, such as slow functions, resource contention, or load imbalances. For example, there is a common expectation that requests that retrieve file attributes in a distributed storage system should perform similarly because the same basic processing is involved for all of them. RetrieveAttribute requests that do not perform similarly may indicate problems, such as problematic hash tables that suffer from increased collisions for some file keys or lock contention that results in some requests being slow.  \n\nThe second observation is the power of workflow graphs that show: 1) the order of log statements executed by individual requests and 2) latencies incurred between log executions. Using the first insight, if groups of requests with identical workflow graphs exhibit high response-time variance, we can conclude that the log statements they execute are insufficient to differentiate their workflows. Identifying edges of the graphs that contribute most to response-time variance gives insight into where in the codebase additional log statements are needed to differentiate them. Of course, some problems may manifest as groups with consistently-slow response times instead of high variance. For these groups, identifying high-latency edges of their graphs provides insight into where additional logs are needed to provide additional resolution.   \n\nIntellectual merit: We developed VAIF, an instrumentation framework that builds on the above insights. When developers notice certain requests are slow, they initialize VAIF to explore what log statements are needed to localize code areas most affected by them. VAIF starts by creating workflow graphs using a set of logs that developers have chosen to be always on. It then operates in a continuous cycle. It 1) groups together identical workflow graphs collected for the problematic requests; 2) identifies groups that exhibit high response-time variance or consistently-slow behavior; 3) locates areas in the groups' graphs that contribute most to variance or latency; 5) uses simple strategies to chose which log statements are needed in those areas; 5) inserts or enables them either via dynamic instrumentation or control signals sent to disabled log statements.  This loop continues until all groups exhibit low response-time variance or no more logs can be enabled in high-variance or high-latency areas.  \n\nWe showed that VAIF can constructed by building on existing widely-available distributed-tracing infrastructures.  These infrastructures provide the infrastructure needed to capture graphs of request workflows using logging points (called trace points). \n\nWe demonstrated VAIF?s efficacy in diagnosing seven real performance problems in popular open-source distributed applications: OpenStack, HDFS, and DeathStarBench. For all of them, VAIF successfully localized areas of the codebase most affected by the problem while only enabling 3-34% of the instrumentation that would be enabled using their default tracing infrastructures. We discovered many of these problems w/o a priori knowledge of their existence. Work on VAIF resulted in five conference papers on top systems conferences and journals, five posters, and five talks at academic and industrial conferences.\n\nBroader impacts: This project in-part funded the training of five PhD students and has (or will) constitute a significant portion of two dissertations. Two Master\u2019s students and three undergraduates completed projects or theses related to VAIF. Insights from VAIF informed projects in a course that provides research experience to undergraduates and projects for six-high school students. VAIF has drawn interest from the open-source distributed-tracing community, and its source code is publicly available at: https://github.com/docc-lab/pythia.\n\n \n\n\t\t\t\t\tLast Modified: 02/18/2023\n\n\t\t\t\t\tSubmitted by: Raja R Sambasivan"
 }
}