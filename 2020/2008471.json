{
 "awd_id": "2008471",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: EVE: Ephemeral Vector Engines",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2020-06-30",
 "awd_max_amd_letter_date": "2020-06-30",
 "awd_abstract_narration": "Data-parallel kernels dominate the computational workload in a wide variety of demanding application domains, including graphics rendering, computer vision, audio processing, physical simulation, machine learning, and graph processing. Since it is no longer possible to rely on technology scaling for inevitable improvements in transistor performance and energy efficiency, there is renewed interest in specialized hardware to improve performance and efficiency compared to general-purpose processors for codes with significant amounts of data-level parallelism (DLP). Unfortunately, this specialized hardware lies idle when these computing systems are executing the many other interesting workloads that lack DLP. This project is exploring a new approach that is able to create specialized hardware \"on-demand\" by reconfiguring the memory already contained within modern computer systems to handle both storage and computation. The project's broader significance and importance are rooted in the need for computer architects to creatively mitigate the challenges imposed by the looming end of Moore's law, and the potential transformative impact of a software/hardware co-design approach.\r\n\r\nTwo popular styles of hardware accelerators for exploiting DLP include data-parallel acceleration (DPA), which focuses on moving data in main memory to the compute hardware, and processing-in-memory (PIM), which focuses on moving compute hardware to the data in memory. In-situ processing-in-memory (PIM) is a recently proposed approach that attempts to significantly reduce the area overhead associated with exploiting data-level parallelism while at the same time reaping most of the benefit. In-situ PIM uses bit-line computation to perform basic bit-wise logical operations in a single read of a traditional memory array. Each memory column can be further transformed into a bit-serial ALU by adding extra logic, multiplexing, and state elements in the peripheral circuitry. This project makes two key observations about prior work on in-situ PIM: (1) in-situ PIM lacks compelling programming models; and (2) in-situ PIM requires massive parallelism to outweigh bit-serial execution overheads.\r\n\r\nThis project is exploring ephemeral vector engines (EVE) as a new approach to address these challenges. EVE enables dynamically repurposing one or more private L2 cache ways to serve as on-demand (i.e., ephemeral) vector engines implemented using a novel reconfigurable bit-serial/bit-parallel in-situ processing-in-SRAM. EVE supports the unmodified RISC-V vector instruction set and can be rapidly reconfigured to use either bit-serial or bit-parallel execution. Bit-serial execution provides higher throughput but longer latencies, while bit-parallel execution provides lower throughput but shorter latencies. This project is using a vertically integrated research methodology spanning circuits, microarchitecture, architecture, and applications to explore three research thrusts. Thrust 1: EVE Circuits is exploring how to implement reconfigurable bit-serial/bit-parallel compute logic in the periphery of the SRAM array so as to minimize area, energy, and timing overhead. Thrust 2: EVE Microarchitecture is exploring how to efficiently map higher-level vector instructions into lower-level micro-operations. Thrust 3: EVE Architecture is exploring how to integrate the EVE microarchitecture into a complete system accelerating applications with regular DLP, while adding little to no area, energy, performance overhead for applications without regular DLP.\r\n\r\nThis project is also pursuing two broader impact initiatives. The first is an ambitious yet concrete effort to increase participation of women in computer engineering by organizing a week-long computer engineering design experience for high-school girls. The second is an initiative to better preparing Cornell graduates for the post-Moore's law era, by integrating open-source CAD tools and the emerging PIM architectural approach into the undergraduate and graduate curriculum.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Batten",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Batten",
   "pi_email_addr": "cbatten@cornell.edu",
   "nsf_id": "000539036",
   "pi_start_date": "2020-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "323 Rhodes Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148533801",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Data-parallel kernels dominate the computational workload in a wide variety of demanding application domains, including graphics rendering, computer vision, audio processing, physical simulation, machine learning, and graph processing. Since it is no longer possible to rely on technology scaling for inevitable improvements in transistor performance and energy efficiency, there is renewed interest in specialized hardware to improve performance and efficiency compared to general-purpose processors for codes with significant amounts of data-level parallelism (DLP). Unfortunately, this specialized hardware lies idle when these computing systems are executing the many other interesting workloads that lack DLP. This project is exploring a new approach that is able to create specialized hardware \"on-demand\" by reconfiguring the memory already contained within modern computer systems to handle both storage and computation. The project's broader significance and importance are rooted in the need for computer architects to creatively mitigate the challenges imposed by the looming end of Moore's law, and the potential transformative impact of a software/hardware co-design approach.</p>\n<p>Two popular styles of hardware accelerators for exploiting DLP include data-parallel acceleration (DPA), which focuses on moving data in main memory to the compute hardware, and processing-in-memory (PIM), which focuses on moving compute hardware to the data in memory. In-situ processing-in-memory (PIM) is a recently proposed approach that attempts to significantly reduce the area overhead associated with exploiting data-level parallelism while at the same time reaping most of the benefit. In-situ PIM uses bit-line computation to perform basic bit-wise logical operations in a single read of a traditional memory array. Each memory column can be further transformed into a bit-serial ALU by adding extra logic, multiplexing, and state elements in the peripheral circuitry. This project made two key observations about prior work on in-situ PIM: (1) in-situ PIM lacks compelling programming models; and (2) in-situ PIM requires massive parallelism to outweigh bit-serial execution overheads.</p>\n<p>This project accomplished four key research achievements to address these two challenges. First, a novel bit-hybrid circuit implementation was proposed and evaluated which better balances throughput vs. latency compared to exclusively using bit-serial or bit-parallel execution; bit-hybrid execution helps address the massive parallelism challenge. Second, a novel ephemeral vector engine (EVE) architecture was proposed and evaluated which dynamically repurposes one or more private L2 cache ways to serve as on-demand vector engines implemented using in-situ processing-in-SRAM; EVE architectures support the unmodified RISC-V vector instruction set to address the programmability challenge. Third, the project completed the first microbenchmarking work in the computer architecture community evaluating a commercial processing-in-SRAM accelerator. Fourth, the original vision for the project was extended to include other kinds of on-demand vector acceleration beyond processing-in-memory. The project resulted in four conference papers, one journal paper, two PhD theses, and two workshop papers. Taken together, these research achievements have pushed the frontier of research on in-situ processing-in-memory.</p>\n<p>As part of this project's broader impact initiatives, the PI led a week-long design experience for 43 high-school girls titled \"Computing at the Edge\" as part of the CURIE Academy. The CURIE Academy is a summer program organized by the Cornell Office of Diversity Programming in Engineering. This version of CURIE Academy was held virtually due to the COVID-19 pandemic. CURIE scholars had an opportunity to learn about various fields within engineering, and to focus on a hands-on design project. Each scholar was shipped a kit containing a variety of electronics prototyping components including breadboards, transistors, LEDs, resistors, ICs, a Particle Argon IoT device, and various sensors. The week began with two laboratory sessions: one on digital logic design with Boolean logic gates and one on microcontroller programming in C. After these laboratory sessions, scholars worked in groups of three on an IoT system of their own design. Even though the scholars were remote and many scholars began the week with little knowledge of computer engineering, by the end, 14 groups completed unique and interesting IoT systems. The PI effectively turned the remote learning environment from a disadvantage into an advantage by demonstrating the power of building truly integrated IoT systems interconnected through the cloud.</p><br>\n<p>\n Last Modified: 08/04/2024<br>\nModified by: Christopher&nbsp;Batten</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nData-parallel kernels dominate the computational workload in a wide variety of demanding application domains, including graphics rendering, computer vision, audio processing, physical simulation, machine learning, and graph processing. Since it is no longer possible to rely on technology scaling for inevitable improvements in transistor performance and energy efficiency, there is renewed interest in specialized hardware to improve performance and efficiency compared to general-purpose processors for codes with significant amounts of data-level parallelism (DLP). Unfortunately, this specialized hardware lies idle when these computing systems are executing the many other interesting workloads that lack DLP. This project is exploring a new approach that is able to create specialized hardware \"on-demand\" by reconfiguring the memory already contained within modern computer systems to handle both storage and computation. The project's broader significance and importance are rooted in the need for computer architects to creatively mitigate the challenges imposed by the looming end of Moore's law, and the potential transformative impact of a software/hardware co-design approach.\n\n\nTwo popular styles of hardware accelerators for exploiting DLP include data-parallel acceleration (DPA), which focuses on moving data in main memory to the compute hardware, and processing-in-memory (PIM), which focuses on moving compute hardware to the data in memory. In-situ processing-in-memory (PIM) is a recently proposed approach that attempts to significantly reduce the area overhead associated with exploiting data-level parallelism while at the same time reaping most of the benefit. In-situ PIM uses bit-line computation to perform basic bit-wise logical operations in a single read of a traditional memory array. Each memory column can be further transformed into a bit-serial ALU by adding extra logic, multiplexing, and state elements in the peripheral circuitry. This project made two key observations about prior work on in-situ PIM: (1) in-situ PIM lacks compelling programming models; and (2) in-situ PIM requires massive parallelism to outweigh bit-serial execution overheads.\n\n\nThis project accomplished four key research achievements to address these two challenges. First, a novel bit-hybrid circuit implementation was proposed and evaluated which better balances throughput vs. latency compared to exclusively using bit-serial or bit-parallel execution; bit-hybrid execution helps address the massive parallelism challenge. Second, a novel ephemeral vector engine (EVE) architecture was proposed and evaluated which dynamically repurposes one or more private L2 cache ways to serve as on-demand vector engines implemented using in-situ processing-in-SRAM; EVE architectures support the unmodified RISC-V vector instruction set to address the programmability challenge. Third, the project completed the first microbenchmarking work in the computer architecture community evaluating a commercial processing-in-SRAM accelerator. Fourth, the original vision for the project was extended to include other kinds of on-demand vector acceleration beyond processing-in-memory. The project resulted in four conference papers, one journal paper, two PhD theses, and two workshop papers. Taken together, these research achievements have pushed the frontier of research on in-situ processing-in-memory.\n\n\nAs part of this project's broader impact initiatives, the PI led a week-long design experience for 43 high-school girls titled \"Computing at the Edge\" as part of the CURIE Academy. The CURIE Academy is a summer program organized by the Cornell Office of Diversity Programming in Engineering. This version of CURIE Academy was held virtually due to the COVID-19 pandemic. CURIE scholars had an opportunity to learn about various fields within engineering, and to focus on a hands-on design project. Each scholar was shipped a kit containing a variety of electronics prototyping components including breadboards, transistors, LEDs, resistors, ICs, a Particle Argon IoT device, and various sensors. The week began with two laboratory sessions: one on digital logic design with Boolean logic gates and one on microcontroller programming in C. After these laboratory sessions, scholars worked in groups of three on an IoT system of their own design. Even though the scholars were remote and many scholars began the week with little knowledge of computer engineering, by the end, 14 groups completed unique and interesting IoT systems. The PI effectively turned the remote learning environment from a disadvantage into an advantage by demonstrating the power of building truly integrated IoT systems interconnected through the cloud.\t\t\t\t\tLast Modified: 08/04/2024\n\n\t\t\t\t\tSubmitted by: ChristopherBatten\n"
 }
}