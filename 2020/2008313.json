{
 "awd_id": "2008313",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Understanding and Synthesizing People in 3D Scenes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 498622.0,
 "awd_amount": 498622.0,
 "awd_min_amd_letter_date": "2020-07-17",
 "awd_max_amd_letter_date": "2020-07-17",
 "awd_abstract_narration": "This project advances fundamental research into understanding people in images, and how they interact with their environment. This goal is important because people are crucial in many real-world applications that involve imagery. For example, vehicles and robots should perceive people, predict their behavior, and operate safely around them. In the realm of computer graphics, algorithms that generate content for movies and games should depict people with realistic appearance and who interact with their environment in realistic ways. Developing a computational understanding of people sufficient for such applications will involve, crucially, understanding the connection between people and the scenes they inhabit. For instance, in a typical street scene, people will tend to appear walking on sidewalks or crosswalks -- but if it suddenly starts raining, they might look and act differently, for instance, by carrying umbrellas, hurrying for shelter, etc. This project seeks to build such a computational understanding of people in scenes via new human-centric methods for perceiving the world. The research will also be coupled with educational activities, including efforts to broaden participation in computing.\r\n\r\nThe technical goals of the project fall into two main thrusts corresponding to a computer vision and a computer graphics goal: (1) human-centric understanding of images, and (2) realistic synthesis of people into images. Both of these goals will be driven by new machine learning methods that will answer the following series of questions: given an image of a scene, where might a person appear, what would they be doing, and how would they look? These questions are distinct from the typical computer vision task of identifying where humans actually are in an image, and instead involve reasoning about where they could be. This reasoning is highly dependent on the scene depicted in the image -- are there benches, is it raining, etc. -- and also on the 3D geometry of the scene -- e.g., whether a particular surface is horizontal or vertical. Hence, the project will explore scene- and 3D-aware learning of people and their interactions with the world.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Snavely",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Noah Snavely",
   "pi_email_addr": "snavely@cs.cornell.edu",
   "nsf_id": "000533237",
   "pi_start_date": "2020-07-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell Tech",
  "perf_str_addr": "2 West Loop Rd",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100441501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 498622.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-2916621c-7fff-c0d5-9c31-d8accdb8299c\">\r\n<p dir=\"ltr\"><span>This National Science Foundation award supported research that drove major advances in the fields of computer vision and computer graphics, particularly in human-centered understanding of images, improved methods for 3D reconstruction of human-centered scenes, and new datasets for large-scale 3D learning. This funding:</span></p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>supported research that led to eight research papers published at top computer vision venues, including an award-winning paper, work that has been cited over 700 times to date</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>led to the creation of Doppelgangers and MegaScenes, two large-scale datasets for use in pushing the boundaries of 3D learning</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>helped train five PhD students and undergraduate students in vision, graphics, and machine learning research</span></p>\r\n</li>\r\n</ul>\r\n<p dir=\"ltr\"><span>Our research:</span></p>\r\n<ul>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>created new methods and new datasets for reconstructing 3D scenes in the presence of visual ambiguities, significantly advancing the state-of-the-art in image-based 3D reconstruction (</span><a href=\"https://doppelgangers-3d.github.io/\"><span>Doppelgangers</span></a><span>)</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>devised new approaches to long-range, dense correspondence in images, including for human motion, with applications in many areas that involve motion estimation from video, and helping spur renewed interest in this problem in the computer vision community (</span><a href=\"https://omnimotion.github.io/\"><span>OmniMotion</span></a><span>)</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>created new inverse rendering methods that can estimate 3D shape, material parameters, and illumination conditions from multiple images (</span><a href=\"https://kai-46.github.io/PhySG-website/\"><span>PhySG</span></a><span>, </span><a href=\"https://kai-46.github.io/IRON-website/\"><span>IRON</span></a><span>)</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>led to new techniques for reasoning about humans in images, by associating references in captions to humans detected in an image (</span><a href=\"https://whoswaldo.github.io/\"><span>Who&rsquo;s Waldo</span></a><span>)</span></p>\r\n</li>\r\n<li dir=\"ltr\">\r\n<p dir=\"ltr\"><span>built new datasets for 3D learning containing multimodal data types, including images, text, and 3D geometry (</span><a href=\"https://www.cs.cornell.edu/projects/babel/\"><span>WikiScenes</span></a><span>, </span><a href=\"https://megascenes.github.io/\"><span>MegaScenes</span></a><span>)</span></p>\r\n</li>\r\n</ul>\r\n<br />\r\n<p dir=\"ltr\"><span>We have published papers, code, and data for all of the work carried out under this award, which are helping researchers and practitioners in a range of areas benefit from and build on our work. Thanks to this NSF award, we have made significant progress towards better reconstruction and understanding of human-centric scenes. The impact of this work is also furthered by the training provided to the many students involved in the work. The results of this work have high relevance to the furthering of AI and computer vision methods, as well as downstream applications, through new algorithms and datasets.</span></p>\r\n</span></p><br>\n<p>\n Last Modified: 04/01/2025<br>\nModified by: Noah&nbsp;Snavely</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2025/2008313/2008313_10686547_1743557921792_MegaScenes_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2025/2008313/2008313_10686547_1743557921792_MegaScenes_teaser--rgov-800width.jpg\" title=\"MegaScenes: Scene-Level View Synthesis at Scale\"><img src=\"/por/images/Reports/POR/2025/2008313/2008313_10686547_1743557921792_MegaScenes_teaser--rgov-66x44.jpg\" alt=\"MegaScenes: Scene-Level View Synthesis at Scale\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The MegaScenes Dataset is an extensive collection of structure-from-motion reconstructions and internet images, from a diverse collections of scenes. MegaScenes have help advance large-scale 3D learning and foundation models.</div>\n<div class=\"imageCredit\">Noah Snavely</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Noah&nbsp;Snavely\n<div class=\"imageTitle\">MegaScenes: Scene-Level View Synthesis at Scale</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2025/2008313/2008313_10686547_1743516822426_doppelgangers_teaser_512--rgov-214x142.gif\" original=\"/por/images/Reports/POR/2025/2008313/2008313_10686547_1743516822426_doppelgangers_teaser_512--rgov-800width.gif\" title=\"Doppelgangers: Learning to Disambiguate Images of Similar Structures\"><img src=\"/por/images/Reports/POR/2025/2008313/2008313_10686547_1743516822426_doppelgangers_teaser_512--rgov-66x44.gif\" alt=\"Doppelgangers: Learning to Disambiguate Images of Similar Structures\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Illustration of our Doppelgangers method for visual scene disambiguation within 3D reconstruction.</div>\n<div class=\"imageCredit\">Noah Snavely</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Noah&nbsp;Snavely\n<div class=\"imageTitle\">Doppelgangers: Learning to Disambiguate Images of Similar Structures</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nThis National Science Foundation award supported research that drove major advances in the fields of computer vision and computer graphics, particularly in human-centered understanding of images, improved methods for 3D reconstruction of human-centered scenes, and new datasets for large-scale 3D learning. This funding:\r\n\r\n\r\n\n\nsupported research that led to eight research papers published at top computer vision venues, including an award-winning paper, work that has been cited over 700 times to date\r\n\r\n\r\n\n\nled to the creation of Doppelgangers and MegaScenes, two large-scale datasets for use in pushing the boundaries of 3D learning\r\n\r\n\r\n\n\nhelped train five PhD students and undergraduate students in vision, graphics, and machine learning research\r\n\r\n\r\n\n\nOur research:\r\n\r\n\r\n\n\ncreated new methods and new datasets for reconstructing 3D scenes in the presence of visual ambiguities, significantly advancing the state-of-the-art in image-based 3D reconstruction (Doppelgangers)\r\n\r\n\r\n\n\ndevised new approaches to long-range, dense correspondence in images, including for human motion, with applications in many areas that involve motion estimation from video, and helping spur renewed interest in this problem in the computer vision community (OmniMotion)\r\n\r\n\r\n\n\ncreated new inverse rendering methods that can estimate 3D shape, material parameters, and illumination conditions from multiple images (PhySG, IRON)\r\n\r\n\r\n\n\nled to new techniques for reasoning about humans in images, by associating references in captions to humans detected in an image (Whos Waldo)\r\n\r\n\r\n\n\nbuilt new datasets for 3D learning containing multimodal data types, including images, text, and 3D geometry (WikiScenes, MegaScenes)\r\n\r\n\r\n\n\r\n\n\nWe have published papers, code, and data for all of the work carried out under this award, which are helping researchers and practitioners in a range of areas benefit from and build on our work. Thanks to this NSF award, we have made significant progress towards better reconstruction and understanding of human-centric scenes. The impact of this work is also furthered by the training provided to the many students involved in the work. The results of this work have high relevance to the furthering of AI and computer vision methods, as well as downstream applications, through new algorithms and datasets.\r\n\t\t\t\t\tLast Modified: 04/01/2025\n\n\t\t\t\t\tSubmitted by: NoahSnavely\n"
 }
}