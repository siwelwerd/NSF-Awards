{
 "awd_id": "2033792",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Quantifying the Unknown Unknowns for Data Integration",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2020-04-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 334537.0,
 "awd_amount": 334537.0,
 "awd_min_amd_letter_date": "2020-07-28",
 "awd_max_amd_letter_date": "2023-06-01",
 "awd_abstract_narration": "As the amount and variety of data available online explodes, it is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) is the integrated data set complete and (2) what is the impact of any unknown (i.e., unobserved) data on query results? In this work, this project will develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) for analytical queries. This will help to better understand answers in the presence of incomplete information across fields ranging from business and the military to medical applications.\r\n\r\nThis project will develop and exploit the following paradoxical statistical phenomenon: the ability to see certain data items more than once (across multiple data sets) enables one to estimate parameters of data items that have never been seen at all. This project will therefore develop new statistical techniques which take advantage of overlapping datasets, and software backed by both theory and experiments. This will enable users with overlapping incomplete data sets to actively \"see the unseen,\" and in many cases perform as though they had access to missing information not represented in any of their data sources.  The project will also focus on data validation, and how to use multiple unreliable data sources to correct each other.  Further, as the proposed analysis is nuanced and novel, the project will also explore how to best convey valuable insights to the user, via interactive visualizations of the predictions. For further information see the project web site at: http://unknown-unknowns.cs.brown.edu",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tim",
   "pi_last_name": "Kraska",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Tim K Kraska",
   "pi_email_addr": "kraska@mit.edu",
   "nsf_id": "000635999",
   "pi_start_date": "2020-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 97016.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 237521.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The goal of this project was to make data exploration &ldquo;safer&rdquo; over unreliable data sources and data modalities. We aimed to develop theoretically-sound techniques to estimate the data quality &nbsp;(e.g., in the number of missing data items, or remaining data cleaning errors) of data sets and, when possible, develop techniques to automatically correct the query results over imperfect data sources.&nbsp;&nbsp;</span></p>\n<p>In the following we list some of the key results from this project:&nbsp;&nbsp;</p>\n<p><strong>Estimating the Impact of Unknown Unknowns on Aggregate Query Results</strong></p>\n<p>It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) is the integrated data set complete and (2) what is the impact of any unknown (i.e., unobserved) data on query results? We developed techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.</p>\n<p>&nbsp;</p>\n<p><strong>Investigation of the Effect of the Multiple Comparisons Problem in Visual Analysis</strong></p>\n<p>Comparing a visualization to a mental image is akin to performing a statistical test, thus repeated interpretation of visualizations is susceptible to the MCP. By means of a user study we showed that by not accounting for all visual comparisons made during visual data exploration, false discovery rates will be inflated even after validating user insights with further statistical testing. Furthermore, we demonstrated that a confirmatory approach can provide similar statistical guarentees to one that uses a validation dataset.</p>\n<p>&nbsp;</p>\n<p><strong>QUDE Controlling False Discoveries During Interactive Data Exploration</strong></p>\n<p>We developed the first automatic approach to controlling the multiple hypothesis problem during data exploration, called QUDE. We showed how the QUDE systems integrates user feedback and presented several multiple hypothesis control techniques based on &alpha;-investing, which controls mFDR, and are especially suited for controlling the error for interactive data exploration sessions. Finally, our evaluation showed that the techniques are indeed capable of controlling the number of false discoveries using synthetic and real world datasets. We consider this work as an important first step towards more sustainable discoveries in a time where the importance of data analysis is more pervasive than ever.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Lower bounds for using crowdsourced workers to produce a data quality metric (DQM)</strong></p>\n<p>We developed an adaptive algorithm to estimate the quality of a data set. In contrast to many prior approaches which queried workers non-adaptively&mdash;e.g. by asking 5 randomly chosen workers about each data item&mdash;here we allow for future queries to depend on the responses to prior querier. Such adaptive algorithms open up many more possibilities for improving performance; further, from a theoretical perspective, analyzing the entire space of adaptive algorithms to show that our algorithm is optimal in this rich class is a significant step which has implications for understanding the role of adaptivity across many other algorithmic settings. We classify adaptivity into 1) same-data adaptivity, where previous worker responses about the same data point can affect whether an algorithm continues to query workers about this data point, and 2) between-data adaptivity, where worker responses about other data points may affect whether an algorithm queries a different data point. We show, essentially, that our data quality metric makes near-optimal use of the first kind of adaptivity, while the second kind of adaptivity provably cannot improve performance.</p>\n<p>&nbsp;</p>\n<p><strong>SlicerFinder</strong></p>\n<p>We developed Slice Finder, a tool to efficiently discover large possibly-overlapping slices that are both interpretable and problematic. A slice is defined as a conjunction of feature-value pairs where having fewer features is considered more interpretable. A problematic slice is identified based on testing of a significant difference of model performance metrics (e.g., loss function) of the slice and its counterpart. That is, we treat each problematic slice as a hypothesis and check that the difference is statistically significant, and the magnitude of the difference is large enough according to the effect size. In comparison, clustering similar examples has the drawback of not being interpretable. In addition to testing, the slices found by Slice Finder can be used to evaluate model fairness or in applications such as fraud detection, business analytics, and anomaly detection.</p><br>\n<p>\n Last Modified: 05/01/2024<br>\nModified by: Tim&nbsp;K&nbsp;Kraska</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project was to make data exploration safer over unreliable data sources and data modalities. We aimed to develop theoretically-sound techniques to estimate the data quality (e.g., in the number of missing data items, or remaining data cleaning errors) of data sets and, when possible, develop techniques to automatically correct the query results over imperfect data sources.\n\n\nIn the following we list some of the key results from this project:\n\n\nEstimating the Impact of Unknown Unknowns on Aggregate Query Results\n\n\nIt is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) is the integrated data set complete and (2) what is the impact of any unknown (i.e., unobserved) data on query results? We developed techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.\n\n\n\n\n\nInvestigation of the Effect of the Multiple Comparisons Problem in Visual Analysis\n\n\nComparing a visualization to a mental image is akin to performing a statistical test, thus repeated interpretation of visualizations is susceptible to the MCP. By means of a user study we showed that by not accounting for all visual comparisons made during visual data exploration, false discovery rates will be inflated even after validating user insights with further statistical testing. Furthermore, we demonstrated that a confirmatory approach can provide similar statistical guarentees to one that uses a validation dataset.\n\n\n\n\n\nQUDE Controlling False Discoveries During Interactive Data Exploration\n\n\nWe developed the first automatic approach to controlling the multiple hypothesis problem during data exploration, called QUDE. We showed how the QUDE systems integrates user feedback and presented several multiple hypothesis control techniques based on -investing, which controls mFDR, and are especially suited for controlling the error for interactive data exploration sessions. Finally, our evaluation showed that the techniques are indeed capable of controlling the number of false discoveries using synthetic and real world datasets. We consider this work as an important first step towards more sustainable discoveries in a time where the importance of data analysis is more pervasive than ever.\n\n\n\n\n\nLower bounds for using crowdsourced workers to produce a data quality metric (DQM)\n\n\nWe developed an adaptive algorithm to estimate the quality of a data set. In contrast to many prior approaches which queried workers non-adaptivelye.g. by asking 5 randomly chosen workers about each data itemhere we allow for future queries to depend on the responses to prior querier. Such adaptive algorithms open up many more possibilities for improving performance; further, from a theoretical perspective, analyzing the entire space of adaptive algorithms to show that our algorithm is optimal in this rich class is a significant step which has implications for understanding the role of adaptivity across many other algorithmic settings. We classify adaptivity into 1) same-data adaptivity, where previous worker responses about the same data point can affect whether an algorithm continues to query workers about this data point, and 2) between-data adaptivity, where worker responses about other data points may affect whether an algorithm queries a different data point. We show, essentially, that our data quality metric makes near-optimal use of the first kind of adaptivity, while the second kind of adaptivity provably cannot improve performance.\n\n\n\n\n\nSlicerFinder\n\n\nWe developed Slice Finder, a tool to efficiently discover large possibly-overlapping slices that are both interpretable and problematic. A slice is defined as a conjunction of feature-value pairs where having fewer features is considered more interpretable. A problematic slice is identified based on testing of a significant difference of model performance metrics (e.g., loss function) of the slice and its counterpart. That is, we treat each problematic slice as a hypothesis and check that the difference is statistically significant, and the magnitude of the difference is large enough according to the effect size. In comparison, clustering similar examples has the drawback of not being interpretable. In addition to testing, the slices found by Slice Finder can be used to evaluate model fairness or in applications such as fraud detection, business analytics, and anomaly detection.\t\t\t\t\tLast Modified: 05/01/2024\n\n\t\t\t\t\tSubmitted by: TimKKraska\n"
 }
}