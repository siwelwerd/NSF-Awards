{
 "awd_id": "1952735",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FRG: Collaborative Research: Randomized Algorithms for Solving Linear Systems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922113",
 "po_email": "ygorb@nsf.gov",
 "po_sign_block_name": "Yuliya Gorb",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-06-30",
 "tot_intn_awd_amt": 677023.0,
 "awd_amount": 677023.0,
 "awd_min_amd_letter_date": "2020-04-24",
 "awd_max_amd_letter_date": "2020-04-24",
 "awd_abstract_narration": "The objective of this project is to develop faster and more energy-efficient algorithms for one of the most fundamental tasks in computational science: solving large systems of coupled linear equations. Faster algorithms will both accelerate computations that can already be performed, and enable computations that are beyond the reach of existing methods. More energy efficient algorithms will help to reduce the power consumption of data centers, and to extend the battery life of mobile devices such as cell phones and tablet computers. The fundamental innovation behind our approach is to harness mathematical properties of large collections of random numbers to build new stochastic algorithms that dramatically outperform existing deterministic ones. In a nutshell, the idea is to use randomized sampling, and randomized averaging, to reduce the effective dimensionality of the problems to be processed. In addition the project provides research training opportunities for postdoctoral fellows and graduate students.\r\n\r\nWe seek to develop computationally efficient methods for solving linear systems of equations involving large numbers of variables, both in terms of asymptotic complexity, and in terms of practical speed at realistic problem sizes. Such systems of equations arise ubiquitously in science and engineering, and solving them is often the bottleneck in terms of time that decides how large of a problem can be handled. In particular, this is what limits how large of a data set can be analyzed, or how realistic a computational simulation can be when modelling some physical phenomenon. By developing faster and more efficient algorithms, we will accelerate computations that are done today, and enable many others that are outside the reach of currently existing methods.  The project is premised on the recent development of new randomized algorithms for solving linear algebraic problems. Such methods have proven to dramatically outperform classical deterministic methods for certain tasks such as computing low rank factorizations to matrices - the crucial computational step in e.g. Principal Component Analysis, the PageRank algorithm by Larry Page and Sergey Brin, numerical coarse graining when modeling complex multiscale systems, and many more. Randomized algorithms have also been used to build faster solvers for linear systems. However, while the theoretical results obtained at this point are extremely encouraging, it remains to develop randomized linear solvers that are decisively faster in practical applications. To achieve this goal, the project will support a research group that brings together four researchers with complementary skills in numerical linear algebra, random matrix theory, computational harmonic analysis, optimization, and high performance computing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Per-Gunnar",
   "pi_last_name": "Martinsson",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Per-Gunnar J Martinsson",
   "pi_email_addr": "pgm@ices.utexas.edu",
   "nsf_id": "000257904",
   "pi_start_date": "2020-04-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rachel",
   "pi_last_name": "Ward",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rachel Ward",
   "pi_email_addr": "rward@math.utexas.edu",
   "nsf_id": "000512743",
   "pi_start_date": "2020-04-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787595316",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1616",
   "pgm_ref_txt": "FOCUSED RESEARCH GROUPS IN MATH SCIENCES"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 677023.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of the project was to develop more computationally efficient methods for solving linear systems of equations involving large numbers of variables. Such systems of equations arise ubiquitously in data science as well as in classical science and engineering. The time required to solve such systems is often the factor that decides how large of a problem can be handled. This in turn is what limits how realistically a computational simulation can model some physical phenomenon, or how large of a data set can be processed.<br /><br />The work was premised on the recent development of new randomized algorithms for solving linear algebraic problems. Such methods have proven to dramatically outperform classical deterministic methods for certain tasks such as computing low rank factorizations to matrices. (Computing low rank factorizations is the crucial computational step in a wide range problems; it is the essence of Principal Component Analysis in statistics, of the PageRank algorithm by Larry Page and Sergey Brin and many other more modern techniques in data analysis, of numerical coarse graining when modeling complex systems, and many more.)<br /><br />The key contribution of the work was to take some of the ideas that have successfully been applied to the problem of data compression (\"low rank approximation\") and apply them to solve the harder problem of solving large systems of coupled linear equations.<br /><br />A key success of the project was the development of a new class of algorithms for solving certain linear systems that arise in scientific computing, for instance in the context of modeling elastic deformations of bodies, or wave propagation problems such as modeling imaging devices or computing radar cross sections. We used randomization to build a highly accurate representation of the linear system by examining input-output pairs for randomized \"probes\" of the system. We coupled this probing mechanism with newly designed algorithms (also randomized!) for solving structured linear systems to achieve new solvers that are both faster and more accurate than previously existing method. We demonstrated the efficiency of these methods by applying them to wave propagation problems that are known to be highly challenging to previously existing methodologies.<br /><br />The project also supported the development of new methods for assessing the error in any given instantiation of a randomized algorithm. This is a dramatic step forward since previously existing analysis of the methods expressed the errors in terms of quantities that are generally not known at the time of computation. In contrast, our new \"a posteriori error estimators\" rely exclusively on information that is available to a user, and can be evaluated as the computation proceeds. Such estimators are essential in order for users to feel confidence in using randomized techniques in environments such as engineering design and control, medical imaging, and other situations where larger than expected errors cannot be tolerated.<br /><br />Another consequential outcome of the project was a set of techniques for resolving certain data fitting problems that occur ubiquitously in machine learning and data science. We managed to build new randomized algorithms that combine the exceptional computational efficiency of randomized sampling methods such as Monte Carlo, with the high accuracy and reliability of traditional deterministic techniques.<br /><br />Finally, the project supported the training and mentoring of several brilliant doctoral students and postdoctoral research fellows at UT-Austin. Their training was greatly enhanced by the opportunities to interact with the teams at Yale and at Caltech. Five of the supported students have graduated, and one has already secured a tenure-track faculty position at a research university. In addition, four of the former postdoctoral fellows at UT-Austin have secured tenure-track positions.<br /><br /></p><br>\n<p>\n Last Modified: 02/20/2025<br>\nModified by: Per-Gunnar&nbsp;J&nbsp;Martinsson</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe objective of the project was to develop more computationally efficient methods for solving linear systems of equations involving large numbers of variables. Such systems of equations arise ubiquitously in data science as well as in classical science and engineering. The time required to solve such systems is often the factor that decides how large of a problem can be handled. This in turn is what limits how realistically a computational simulation can model some physical phenomenon, or how large of a data set can be processed.\n\nThe work was premised on the recent development of new randomized algorithms for solving linear algebraic problems. Such methods have proven to dramatically outperform classical deterministic methods for certain tasks such as computing low rank factorizations to matrices. (Computing low rank factorizations is the crucial computational step in a wide range problems; it is the essence of Principal Component Analysis in statistics, of the PageRank algorithm by Larry Page and Sergey Brin and many other more modern techniques in data analysis, of numerical coarse graining when modeling complex systems, and many more.)\n\nThe key contribution of the work was to take some of the ideas that have successfully been applied to the problem of data compression (\"low rank approximation\") and apply them to solve the harder problem of solving large systems of coupled linear equations.\n\nA key success of the project was the development of a new class of algorithms for solving certain linear systems that arise in scientific computing, for instance in the context of modeling elastic deformations of bodies, or wave propagation problems such as modeling imaging devices or computing radar cross sections. We used randomization to build a highly accurate representation of the linear system by examining input-output pairs for randomized \"probes\" of the system. We coupled this probing mechanism with newly designed algorithms (also randomized!) for solving structured linear systems to achieve new solvers that are both faster and more accurate than previously existing method. We demonstrated the efficiency of these methods by applying them to wave propagation problems that are known to be highly challenging to previously existing methodologies.\n\nThe project also supported the development of new methods for assessing the error in any given instantiation of a randomized algorithm. This is a dramatic step forward since previously existing analysis of the methods expressed the errors in terms of quantities that are generally not known at the time of computation. In contrast, our new \"a posteriori error estimators\" rely exclusively on information that is available to a user, and can be evaluated as the computation proceeds. Such estimators are essential in order for users to feel confidence in using randomized techniques in environments such as engineering design and control, medical imaging, and other situations where larger than expected errors cannot be tolerated.\n\nAnother consequential outcome of the project was a set of techniques for resolving certain data fitting problems that occur ubiquitously in machine learning and data science. We managed to build new randomized algorithms that combine the exceptional computational efficiency of randomized sampling methods such as Monte Carlo, with the high accuracy and reliability of traditional deterministic techniques.\n\nFinally, the project supported the training and mentoring of several brilliant doctoral students and postdoctoral research fellows at UT-Austin. Their training was greatly enhanced by the opportunities to interact with the teams at Yale and at Caltech. Five of the supported students have graduated, and one has already secured a tenure-track faculty position at a research university. In addition, four of the former postdoctoral fellows at UT-Austin have secured tenure-track positions.\n\n\t\t\t\t\tLast Modified: 02/20/2025\n\n\t\t\t\t\tSubmitted by: Per-GunnarJMartinsson\n"
 }
}