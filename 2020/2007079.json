{
 "awd_id": "2007079",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Metric Information Theory, Online Learning, and Competitive Analysis",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2020-07-22",
 "awd_max_amd_letter_date": "2020-07-22",
 "awd_abstract_narration": "There will always be limitations on human ability to collect and process data\r\nabout systems, environments, and populations.  Algorithmic agents act in real\r\ntime based on partial, noisy information about the world, and the elimination\r\nof uncertainty through \"learning\" must often be balanced against the\r\noptimization of some objective.  The cost of choices made in the present must\r\nbe weighed against the risk that those choices might incur further costs in\r\nthe future.  Information theory provides a rich and fertile framework for quantifying and\r\nmanaging uncertainty.  The situation becomes more subtle when distinct pieces\r\nof information carry differing costs. Consider one's 401(k) balance at\r\nretirement.  While the least significant digit is most uncertain, there is\r\nsubstantially more cost associated with incorrectly predicting the value of the\r\nmost significant digit.\r\n\r\nThis project concerns probability spaces endowed with a metric that describes\r\nthe associated cost of uncertainty.  Designing algorithms to optimize in such a\r\nframework is intimately connected to having a robust theory of metric\r\ninformation.  Moreover, the settings of online optimization and competitive\r\nanalysis provide a deep and varied set of formal models in which to apply these\r\nmethods and test their efficacy.  In an area where algorithm design and \r\nanalysis have often been seen as ad-hoc and unstructured, the framework \r\nunderlying this work contends that both algorithms and their analysis can be\r\nderived readily from the right set of underlying definitions.  Indeed, many \r\nproblems in this area have been researched for 30-40 years, and\r\nyet preliminary application of algorithms and analysis tools from online convex\r\noptimization--in the context of metric probability spaces--has already achieved\r\na sequence of breakthroughs.  The team of researchers will develop the \r\ncorresponding theory, guided by a collection of prominent open problems, with\r\nthe ultimate goal of understanding in what circumstances, and to what extent, \r\none can limit the detrimental effects of uncertainty on optimization.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Lee",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "James R Lee",
   "pi_email_addr": "jrl@cs.washington.edu",
   "nsf_id": "000482524",
   "pi_start_date": "2020-07-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "185 Stevens Way CSE101",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The rise of machine learning in recent decades has generated a renewed interest in online decision-making, where algorithms are equipped only with partial information about the world, and must take actions in the face of uncertainty about the future. There are various ways to analyze how much the presence of uncertainty degrades optimization.</p>\n<p>This project studied mathematical notions of uncertainty in metric environments where there is a natural notion of distance, and uncertainty exists at many different scales. These measures can be thought of in terms of multiscale \"information\" or \"entropy.\" One significant application is in algorithmic reasoning in the presence of uncertainty, where decisions need to be made that locally optimize a given cost function while also hedging against the future effects of those decisions.</p>\n<p>We develop new algorithms in such settings, where the algorithms themselves are derived in an automated way from the&nbsp;<strong>definition</strong>&nbsp;of entropy. This is a powerful method where the definiton leads both to algorithms and the method of rigorous analysis. In short, the definition of entropy puts a (Riemannian) geometric structure on the state space, and the derived algorithms are simply a form of gradient descent in this geometry.</p>\n<p>Another notion of metrical entropy occurs in the study of high-dimensional sparsification problems. Here, one is given a function f written as a sum of many simpler functions {f_i} and the goal is to approximate f=f_1+...+f_m using only a sparse subset of terms. We show that sparsification is possible in settings that are vastly more general than previously understood. For instance, we show how to do nearly-optimal sparsification when each f_i is a symmetric submodular function. (This is a large class of functions that appear in a vast array of settings, like economics, data anlaysis, and machine learning.)</p>\n<p>The sparsification is achieved by down sampling the terms, but choosing the correct probability distribution for sampling is essential and relies on geometric insights related to high-dimensional metric entropy calculations.</p>\n<p>The benefit of sparsification is that one can sparsify upfront and then use the sparse approximation to speed up computations involving the original function. As one application of this paradigm, we give the fastest known algorithms for linear l_p regression when 1 &lt; p &lt; 2. This algorithms are able to reduce l_p regression to the easier problem of solving a very small number of instances of sparse linear systems, where efficient algorithms are known in both theory and practice.</p><br>\n<p>\n Last Modified: 12/29/2023<br>\nModified by: James&nbsp;R&nbsp;Lee</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2007079/2007079_10687938_1703911406261_deepnet--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2007079/2007079_10687938_1703911406261_deepnet--rgov-800width.jpg\" title=\"Metric entropy DAG\"><img src=\"/por/images/Reports/POR/2023/2007079/2007079_10687938_1703911406261_deepnet--rgov-66x44.jpg\" alt=\"Metric entropy DAG\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A directed acyclic graph capturing the multiscale entropic structure on a path</div>\n<div class=\"imageCredit\">PI</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;R&nbsp;Lee\n<div class=\"imageTitle\">Metric entropy DAG</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe rise of machine learning in recent decades has generated a renewed interest in online decision-making, where algorithms are equipped only with partial information about the world, and must take actions in the face of uncertainty about the future. There are various ways to analyze how much the presence of uncertainty degrades optimization.\n\n\nThis project studied mathematical notions of uncertainty in metric environments where there is a natural notion of distance, and uncertainty exists at many different scales. These measures can be thought of in terms of multiscale \"information\" or \"entropy.\" One significant application is in algorithmic reasoning in the presence of uncertainty, where decisions need to be made that locally optimize a given cost function while also hedging against the future effects of those decisions.\n\n\nWe develop new algorithms in such settings, where the algorithms themselves are derived in an automated way from thedefinitionof entropy. This is a powerful method where the definiton leads both to algorithms and the method of rigorous analysis. In short, the definition of entropy puts a (Riemannian) geometric structure on the state space, and the derived algorithms are simply a form of gradient descent in this geometry.\n\n\nAnother notion of metrical entropy occurs in the study of high-dimensional sparsification problems. Here, one is given a function f written as a sum of many simpler functions {f_i} and the goal is to approximate f=f_1+...+f_m using only a sparse subset of terms. We show that sparsification is possible in settings that are vastly more general than previously understood. For instance, we show how to do nearly-optimal sparsification when each f_i is a symmetric submodular function. (This is a large class of functions that appear in a vast array of settings, like economics, data anlaysis, and machine learning.)\n\n\nThe sparsification is achieved by down sampling the terms, but choosing the correct probability distribution for sampling is essential and relies on geometric insights related to high-dimensional metric entropy calculations.\n\n\nThe benefit of sparsification is that one can sparsify upfront and then use the sparse approximation to speed up computations involving the original function. As one application of this paradigm, we give the fastest known algorithms for linear l_p regression when 1 \t\t\t\t\tLast Modified: 12/29/2023\n\n\t\t\t\t\tSubmitted by: JamesRLee\n"
 }
}