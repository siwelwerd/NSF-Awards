{
 "awd_id": "1948887",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research in DRMS: Judgments of Answerers and the Answers They Give",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032924710",
 "po_email": "clagonza@nsf.gov",
 "po_sign_block_name": "Claudia Gonzalez-Vallejo",
 "awd_eff_date": "2020-02-15",
 "awd_exp_date": "2022-01-31",
 "tot_intn_awd_amt": 23420.0,
 "awd_amount": 23420.0,
 "awd_min_amd_letter_date": "2020-01-24",
 "awd_max_amd_letter_date": "2020-01-24",
 "awd_abstract_narration": "A basic feature of human social life is asking and answering questions, a complex task that requires one to decide whom to ask, how to phrase the question, and how to interpret the answer and judge its worth. Despite the centrality of the activity, however, little is understood about the general principles involved in each step of the process, making it difficult to design better ways to help people ask questions and find the information they need. The problem becomes even more pressing in the information age: people ask more questions than ever before, on a wider range of topics, and they take those questions not only to friends and family, but also to strangers and acquaintances on the wider world of online bulletin boards and social media. This dissertation work addresses gaps in our understanding, using a combination of methods from psychology and computer science. Tools from machine learning and artificial intelligence will be used to analyze large-scale collections of questions and answers that people produce on online bulletin board systems and results will be used to build general theories that explain and predict how and when people find good answers to their questions. In laboratory experiments to test these theories, participants will be shown questions and answers with varying underlying properties, and the results will identify features that are particularly crucial for information gathering. The large-scale nature of the data science investigation affords the development of theories based on subtle patterns in the relationship between question and answer, while the laboratory work allows investigation into the causal nature of the problem: what, for the question-asker, makes an answer good?\r\n\r\nOne of the most common ways people explore and learn about the world is by seeking information from others through asking questions. This behavior requires two kinds of judgments: (1) from whom one should seek answers and (2) how should one judge whether the answer satisfies the question? These two judgments are inter-dependent and require that people make decisions under uncertainty. This two-pronged investigation into how such judgments are made includes not only the lab-based experiments that are a traditional strength of social and decision sciences, but also a data science component that looks at how these decisions are made in the wild. This mix of methodologies will be used to examine the defining features of good answers and those who give them. The research focuses on questions and answers that are given in human-to-human linguistic dialogue. Techniques from data science and computational linguistics will be used to analyze collections of questions and answers produced on online discussion forums, ranging from requests for technical help on Stackexchange to more nuanced, social questions asked by parents on Mumsnet. Two key properties of the relationship between questions and answers\u2014the extent to which they overlap, and the degree to which answers focus the solution space posed by the question\u2014are hypothesized to correspond to how answers are evaluated. Controlled laboratory experiments will be used to elucidate preferences for those who answer questions. The types of preferences a questioner has for potential answerers has direct consequences for the relevance and quality of information received, and in turn affects the utility of subsequent answers and further decisions. A Bayesian framework is used to formalize the task facing the questioner: to infer what type of answerer an individual is, given their past answering behavior. Behavioral experiments will test key predictions of the model.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Simon",
   "pi_last_name": "DeDeo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Simon DeDeo",
   "pi_email_addr": "sdedeo@andrew.cmu.edu",
   "nsf_id": "000742148",
   "pi_start_date": "2020-01-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gretchen",
   "pi_last_name": "Chapman",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Gretchen B Chapman",
   "pi_email_addr": "gchapman@andrew.cmu.edu",
   "nsf_id": "000208911",
   "pi_start_date": "2020-01-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christina",
   "pi_last_name": "Boyce-Jacino",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christina Boyce-Jacino",
   "pi_email_addr": "cboyceja@andrew.cmu.edu",
   "nsf_id": "000806669",
   "pi_start_date": "2020-01-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132100",
   "pgm_ele_name": "Decision, Risk & Mgmt Sci"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 23420.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>If someone asks you a question, providing them a good answer requires much more than simply saying the things you know -- it requires a complex series of judgements and decisions about what the other person already knows, and how to provide the new information in a way that they can assimilate. If someone's question suggests, for example, that they are missing crucial aspects of the problem they're facing, a good answer ought not only to provide the correct answer, but lead them to a better framing of the problem in which that answer makes sense.</p>\n<p>Our work, in the course of this grant, examined this problem in a new way. It used tools from machine learning to look at the words used by people when they ask, and answer questions. It showed how these tools can be used to automatically infer the underlying \"semantic space\" of the question-asker -- meaning, the kinds of ideas they think are relevant in their question -- and then to look at the relationship between this space, and the constellation of ideas that appear in the answer.</p>\n<p>Approaching the problem this way has two major benefits. First, it allows us to study question-asking \"in the wild\": we can now, in principle, pull in the hundreds of millions of questions and answers that people provide (for example) on websites such as Stack Exchange, Quora, and on the innumerable bulletin boards where people go online to help each other out.</p>\n<p>Second, it allows us to quantify -- put numbers on -- key ideas that previously were understood in a more qualitative, descriptive fashion. For example, we might say that a good answer is one that provides \"new information\". But this can mean many different things, and our mathematical tools now allow us to distinguish, in a rigorous fashion, between answers that \"shift\" the domain of the question asker, by introducing new material, from answers that \"focus\" down the domain of the question asker, by selectively drawing attention to the ideas the question-asker already has in play. Both \"shift\" and \"focus\" can be thought of as providing new information: \"shift\" provides new information by simple addition, while \"focus\" provides information by \"subtraction\", i.e., by informating the person what doesn't matter.</p>\n<p>This quantification we discovered, it turns out, has a direct bearing on how people actually evaluate the quality of answers. We took, as a test case, the question-answer pairs that were released by governments, universities, media outlets, and professional organizations about the COVID-19 crisis &mdash; the so-called \"FAQs\", or \"frequently asked questions\" lists that aimed to help people understand the complex challenges of getting through the pandemic safely. Even though they all contained verifiably accurate information, all FAQs are equally successful, and our methods were able to predict a great deal of the variation in the perceived quality of the question-answer pairs.</p>\n<p>These discoveries, and their validation on data sets like the COVID-19 FAQs, have now led to new work in the related task of making arguments. When I make an argument to you, i.e., when I try to convince you of something, I am faced with many of the same problems with when I ask you a question. The grant has supported pilot studies that extend to the evaluation of arguments, as can be found in large-scale datasets of people making arguments to each other online. These immediate spill-on discoveries show the power and generality of the work that this grant enabled.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/02/2022<br>\n\t\t\t\t\tModified by: Simon&nbsp;Dedeo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIf someone asks you a question, providing them a good answer requires much more than simply saying the things you know -- it requires a complex series of judgements and decisions about what the other person already knows, and how to provide the new information in a way that they can assimilate. If someone's question suggests, for example, that they are missing crucial aspects of the problem they're facing, a good answer ought not only to provide the correct answer, but lead them to a better framing of the problem in which that answer makes sense.\n\nOur work, in the course of this grant, examined this problem in a new way. It used tools from machine learning to look at the words used by people when they ask, and answer questions. It showed how these tools can be used to automatically infer the underlying \"semantic space\" of the question-asker -- meaning, the kinds of ideas they think are relevant in their question -- and then to look at the relationship between this space, and the constellation of ideas that appear in the answer.\n\nApproaching the problem this way has two major benefits. First, it allows us to study question-asking \"in the wild\": we can now, in principle, pull in the hundreds of millions of questions and answers that people provide (for example) on websites such as Stack Exchange, Quora, and on the innumerable bulletin boards where people go online to help each other out.\n\nSecond, it allows us to quantify -- put numbers on -- key ideas that previously were understood in a more qualitative, descriptive fashion. For example, we might say that a good answer is one that provides \"new information\". But this can mean many different things, and our mathematical tools now allow us to distinguish, in a rigorous fashion, between answers that \"shift\" the domain of the question asker, by introducing new material, from answers that \"focus\" down the domain of the question asker, by selectively drawing attention to the ideas the question-asker already has in play. Both \"shift\" and \"focus\" can be thought of as providing new information: \"shift\" provides new information by simple addition, while \"focus\" provides information by \"subtraction\", i.e., by informating the person what doesn't matter.\n\nThis quantification we discovered, it turns out, has a direct bearing on how people actually evaluate the quality of answers. We took, as a test case, the question-answer pairs that were released by governments, universities, media outlets, and professional organizations about the COVID-19 crisis &mdash; the so-called \"FAQs\", or \"frequently asked questions\" lists that aimed to help people understand the complex challenges of getting through the pandemic safely. Even though they all contained verifiably accurate information, all FAQs are equally successful, and our methods were able to predict a great deal of the variation in the perceived quality of the question-answer pairs.\n\nThese discoveries, and their validation on data sets like the COVID-19 FAQs, have now led to new work in the related task of making arguments. When I make an argument to you, i.e., when I try to convince you of something, I am faced with many of the same problems with when I ask you a question. The grant has supported pilot studies that extend to the evaluation of arguments, as can be found in large-scale datasets of people making arguments to each other online. These immediate spill-on discoveries show the power and generality of the work that this grant enabled.\n\n\t\t\t\t\tLast Modified: 06/02/2022\n\n\t\t\t\t\tSubmitted by: Simon Dedeo"
 }
}