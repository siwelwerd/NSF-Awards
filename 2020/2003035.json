{
 "awd_id": "2003035",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MLWiNS: Optimization and Coding Theory for Fast and Robust Wireless Distributed Learning",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925139",
 "po_email": "haliu@nsf.gov",
 "po_sign_block_name": "Hang Liu",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2020-08-27",
 "awd_max_amd_letter_date": "2020-10-15",
 "awd_abstract_narration": "Wireless distributed learning systems can enable a variety of new applications including industrial automation, semantic learning, autonomous driving, health-care applications, etc. While wireless distributed learning brings about new opportunities, it faces two major challenges that severely limit its efficiency, reliability, and scalability: (1) Network heterogeneity, which is due to varying computational capabilities of edge devices. This challenge, also known as Straggler bottleneck, incurs large delays and failures due to computing nodes that are significantly slower than the rest; and (2) Communication Bottleneck, which is due to the massive amounts of raw or processed data that must be moved around the network. To tackle these bottlenecks, this project proposes techniques from coding theory and optimization theory to develop distributed learning algorithms with strong theoretical guarantees and empirical performance. \r\n\r\nWireless distributed learning systems are driven by scaling out computations across many wireless edge nodes. There are, however, two major systems bottlenecks that arise: (1) Straggler Delay Bottleneck, which is due to the latency in waiting for slowest nodes to finish their tasks; (2) Data Shuffling Bottleneck, which is due to the massive amounts of data that must be moved among nodes. Moreover, there are privacy concerns about sharing sensitive local data, as well as vulnerabilities to adversarial attacks. This proposal aims to develop novel techniques from coding theory and optimization theory to tackle the mentioned bottlenecks and concerns. The project develops new \"coded computing\" algorithms for robust gradient aggregation, as well as new optimization algorithms for distributed learning. These algorithms are then used in two network settings to develop communication-efficient, straggler-resilient, and robust distributed learning frameworks: (i) a collaborative setting where a learning task is allocated to multiple edge nodes of the network. In this setting, data points can be encoded and offloaded to the edge nodes to provide resiliency against system bottlenecks; (ii) a federated setting where data points are gathered locally at edge devices and have to remain local due to privacy concerns.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ramtin",
   "pi_last_name": "Pedarsani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ramtin Pedarsani",
   "pi_email_addr": "ramtin@ece.ucsb.edu",
   "nsf_id": "000741613",
   "pi_start_date": "2020-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "Harold Frank Hall",
  "perf_city_name": "Santa Barbara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931069560",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "021Z",
   "pgm_ref_txt": "Industry Partnerships"
  },
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "8585",
   "pgm_ref_txt": "NSF/Intel Partnership Projects"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\r\n<div class=\"layoutArea\">\r\n<div class=\"column\">\r\n<p>The machine learning paradigm has undergone a significant shift from a traditional centralized approach to a more distributed setting, where a collection of devices or computing nodes collaborate to perform learning or inference tasks. This shift has been driven by the growing need to process data locally, reducing dependency on centralized servers and enhancing privacy. In particular, federated learning has emerged as a prominent distributed framework in which a machine learning model is trained across multiple devices while keeping data private and localized on each device. This decentralized approach allows for learning from diverse datasets without requiring data aggregation, making it particularly appealing for privacy-sensitive applications.</p>\r\n<p>However, due to its inherently distributed nature, federated learning introduces several key challenges that must be addressed to ensure its efficiency and reliability. These challenges include communication bottlenecks caused by limited network bandwidth, the presence of slow computing nodes (or stragglers) that hinder overall progress, the need to provide strong privacy guarantees to protect user data, and ensuring robustness in the face of adversarial attacks or compromised nodes that attempt to disrupt learning.</p>\r\n<p>In this project, we tackled these critical challenges to enable fast, scalable, private, and robust distributed learning. To achieve this, we adopted two complementary approaches: (i) a coding-theoretic approach to mitigate the impact of stragglers, ensuring that distributed computation can proceed efficiently even when some nodes are slow or unreliable; and (ii) an optimization-theoretic approach to solving the distributed empirical risk minimization problem under various settings, with a particular focus on enhancing communication efficiency and robustness. Additionally, we explored scenarios where client devices exhibit heterogeneous data distributions and have varying privacy requirements, making federated learning more adaptable to real-world conditions.</p>\r\n<p>The outcomes of the award are publications that appeared in several machine learning and IEEE conferences and journals.</p>\r\n</div>\r\n</div>\r\n</div>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/31/2025<br>\nModified by: Ramtin&nbsp;Pedarsani</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\r\n\r\n\r\n\n\nThe machine learning paradigm has undergone a significant shift from a traditional centralized approach to a more distributed setting, where a collection of devices or computing nodes collaborate to perform learning or inference tasks. This shift has been driven by the growing need to process data locally, reducing dependency on centralized servers and enhancing privacy. In particular, federated learning has emerged as a prominent distributed framework in which a machine learning model is trained across multiple devices while keeping data private and localized on each device. This decentralized approach allows for learning from diverse datasets without requiring data aggregation, making it particularly appealing for privacy-sensitive applications.\r\n\n\nHowever, due to its inherently distributed nature, federated learning introduces several key challenges that must be addressed to ensure its efficiency and reliability. These challenges include communication bottlenecks caused by limited network bandwidth, the presence of slow computing nodes (or stragglers) that hinder overall progress, the need to provide strong privacy guarantees to protect user data, and ensuring robustness in the face of adversarial attacks or compromised nodes that attempt to disrupt learning.\r\n\n\nIn this project, we tackled these critical challenges to enable fast, scalable, private, and robust distributed learning. To achieve this, we adopted two complementary approaches: (i) a coding-theoretic approach to mitigate the impact of stragglers, ensuring that distributed computation can proceed efficiently even when some nodes are slow or unreliable; and (ii) an optimization-theoretic approach to solving the distributed empirical risk minimization problem under various settings, with a particular focus on enhancing communication efficiency and robustness. Additionally, we explored scenarios where client devices exhibit heterogeneous data distributions and have varying privacy requirements, making federated learning more adaptable to real-world conditions.\r\n\n\nThe outcomes of the award are publications that appeared in several machine learning and IEEE conferences and journals.\r\n\r\n\r\n\r\n\n\n\t\t\t\t\tLast Modified: 01/31/2025\n\n\t\t\t\t\tSubmitted by: RamtinPedarsani\n"
 }
}