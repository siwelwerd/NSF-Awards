{
 "awd_id": "1955357",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Medium: Spatial Sound Scene Description",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2020-07-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 999875.0,
 "awd_amount": 1056155.0,
 "awd_min_amd_letter_date": "2020-05-19",
 "awd_max_amd_letter_date": "2024-04-15",
 "awd_abstract_narration": "Sound is rich with information about the surrounding environment. If you stand on a city sidewalk with your eyes closed and listen, you will hear the sounds of events happening around you: birds chirping, squirrels scurrying, people talking, doors opening, an ambulance speeding, a truck idling. In addition, you will also likely be able to perceive the location of each sound source, where it\u2019s going, and how fast it\u2019s moving. This project will build innovative technologies to allow computers to extract this rich information out of sound. By not only identifying which sound sources are present but also estimating the spatial location and movement of each sound source, sound sensing technology will be able to better describe our environments with microphone-enabled everyday devices, e.g. smartphones, headphones, smart speakers, hearing-aids, home camera, and mixed-reality headsets. For hearing impaired individuals, the developed technologies have the potential to alert them to dangerous situations in urban or domestic environments. For city agencies, acoustic sensors will be able to more accurately quantify traffic, construction, and other activities in urban environments. For ecologists, this technology can help them more accurately monitor and study wildlife. In addition, this information complements what computer vision can sense, as sound can include information about events that are not easily visible, such as sources that are small (e.g., insects), far away (e.g., a distant jackhammer), or simply hidden behind another object (e.g., an incoming ambulance around a building's corner). This project also includes outreach activities involving over 100 public school students and teachers, as well as the training and mentoring of postdoctoral, graduate and undergraduate students.\r\n \r\nThis project will develop computational models for spatial sound scene description: that is, estimating the class, spatial location, direction and speed of movement of living beings and objects in real environments by the sounds they make. The investigators aim for their solutions to be robust across a wide range of sound scenes and sensing conditions: noisy, sparse, natural, urban, indoors, outdoors, with varying compositions of sources, with unknown sources, with moving sources, with moving sensors, etc. While current approaches show promise, they are still far from robust in real-world conditions and thus unable to support any of the above scenarios. These shortcomings stem from important data issues such as a lack of spatially annotated real-world audio data, and an over-reliance on poor quality, unrealistic synthesized data; as well as methodological issues such as excessive dependence on supervised learning and a failure to capture the structure of the solution space. This project plans an approach mixing innovative data collection strategies with cutting-edge machine learning solutions. First, it advances a novel framework for the probabilistic synthesis of soundscape datasets using physical and generative models. The goal is to substantially increase the amount, realism and diversity of strongly-labeled spatial audio data. Second, it collects and annotates new datasets of real sound scenes via a combination of high-quality field recordings, crowdsourcing, novel VR/AR multimodal annotation strategies and large-scale annotation by citizen scientists. Third, it puts forward novel deep self-supervised representation learning strategies trained on vast quantities of unlabeled audio data. Fourth, these representation modules are paired with hierarchical predictive models, where the top/bottom levels of the hierarchy correspond to coarser/finer levels of scene description. Finally, the project includes collaborations with three industrial partners to explore applications enabled by the proposed solutions. The project will result in novel methods and open source software libraries for spatial sound scene generation, annotation, representation learning, and sound event detection/localization/tracking; and new open datasets of spatial audio recordings, spatial sound scene annotations, synthesized isolated sounds, and synthesized spatial soundscapes.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Juan",
   "pi_last_name": "Bello",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Juan P Bello",
   "pi_email_addr": "jpbello@nyu.edu",
   "nsf_id": "000496889",
   "pi_start_date": "2020-05-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Agnieszka",
   "pi_last_name": "Roginska",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Agnieszka Roginska",
   "pi_email_addr": "roginska@nyu.edu",
   "nsf_id": "000304392",
   "pi_start_date": "2020-05-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "McFee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brian McFee",
   "pi_email_addr": "brian.mcfee@nyu.edu",
   "nsf_id": "000811220",
   "pi_start_date": "2020-05-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Cartwright",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Mark B Cartwright",
   "pi_email_addr": "mark.cartwright@njit.edu",
   "nsf_id": "000870328",
   "pi_start_date": "2020-05-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "70 Washington Square South",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002425DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 999875.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 22680.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2024,
   "fund_oblg_amt": 17600.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-70b6872e-7fff-ce47-b9b4-1fbc3063305a\"> </span></p>\r\n<p dir=\"ltr\"><span>The Spatial Sound Scene Description (S3D) project focused on developing computational approaches for describing living beings and objects in real environments by the sounds they make. In particular the project&rsquo;s efforts centered around (1) developing solutions for the synthesis of spatial soundscapes at scale; (2) collecting and annotating datasets of real spatial sounds scenes; (3) proposing novel representation learning solutions for spatial sound; and (4) advancing scene description solutions that identify and localize sound events in 3D.</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Notably, the project produced the first, open framework for the large-scale generation of spatial soundscapes, Spatial Scaper; advanced the state of the art in audio representation learning with novel multimodal and audio-only approaches; and created a first of its kind self-supervised acoustic imaging approach, Latent Acoustic Mapping (LAM), combining classical spatial signal processing methods with the scalability and robustness of modern machine learning solutions. In addition it advanced our understanding of biases and robustness in audio representation learning, proposed new evaluation paradigms for sound event detection and localization, augmented audiovisual machine learning research by increasing our understanding of the challenges and limitations of vision-centric models, and advanced research in machine learning for acoustic modeling and room parameter estimation. These contributions hold the potential to speed up machine learning research into spatial audio, from modeling and simulation, to representation, classification, localization and reasoning, and create opportunities for technology transfer, with the project already exploring applications of these techniques in urban and industrial monitoring, media production and extended reality.&nbsp;&nbsp;</span></p>\r\n<p>&nbsp;</p>\r\n<p dir=\"ltr\"><span>Importantly, the project made important contributions to the development of human resources in science, engineering and technology. We provided mentoring and training opportunities for 13 post-doctoral researchers and doctoral students specializing in STEM areas including acoustics, machine learning, signal processing, and sound technologies. It is worth mentioning that nearly half of these scholars self-identify as women or non-binary, and a third as BIPOC. Eight of them started full time jobs in industry and academia within the duration of the project. In addition the project provided training opportunities for over a dozen master's, undergraduate and high school students, and collaborated to provide STEM training for 70+ middle school students from the NYC public school system.&nbsp;</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/20/2024<br>\nModified by: Juan&nbsp;P&nbsp;Bello</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThe Spatial Sound Scene Description (S3D) project focused on developing computational approaches for describing living beings and objects in real environments by the sounds they make. In particular the projects efforts centered around (1) developing solutions for the synthesis of spatial soundscapes at scale; (2) collecting and annotating datasets of real spatial sounds scenes; (3) proposing novel representation learning solutions for spatial sound; and (4) advancing scene description solutions that identify and localize sound events in 3D.\r\n\n\n\r\n\n\nNotably, the project produced the first, open framework for the large-scale generation of spatial soundscapes, Spatial Scaper; advanced the state of the art in audio representation learning with novel multimodal and audio-only approaches; and created a first of its kind self-supervised acoustic imaging approach, Latent Acoustic Mapping (LAM), combining classical spatial signal processing methods with the scalability and robustness of modern machine learning solutions. In addition it advanced our understanding of biases and robustness in audio representation learning, proposed new evaluation paradigms for sound event detection and localization, augmented audiovisual machine learning research by increasing our understanding of the challenges and limitations of vision-centric models, and advanced research in machine learning for acoustic modeling and room parameter estimation. These contributions hold the potential to speed up machine learning research into spatial audio, from modeling and simulation, to representation, classification, localization and reasoning, and create opportunities for technology transfer, with the project already exploring applications of these techniques in urban and industrial monitoring, media production and extended reality.\r\n\n\n\r\n\n\nImportantly, the project made important contributions to the development of human resources in science, engineering and technology. We provided mentoring and training opportunities for 13 post-doctoral researchers and doctoral students specializing in STEM areas including acoustics, machine learning, signal processing, and sound technologies. It is worth mentioning that nearly half of these scholars self-identify as women or non-binary, and a third as BIPOC. Eight of them started full time jobs in industry and academia within the duration of the project. In addition the project provided training opportunities for over a dozen master's, undergraduate and high school students, and collaborated to provide STEM training for 70+ middle school students from the NYC public school system.\r\n\n\n\t\t\t\t\tLast Modified: 12/20/2024\n\n\t\t\t\t\tSubmitted by: JuanPBello\n"
 }
}