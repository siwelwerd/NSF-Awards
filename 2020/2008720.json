{
 "awd_id": "2008720",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: A Novel Framework for Informed Manipulation Planning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 425000.0,
 "awd_amount": 441000.0,
 "awd_min_amd_letter_date": "2020-09-14",
 "awd_max_amd_letter_date": "2023-08-08",
 "awd_abstract_narration": "Humans use many types of manipulation to accomplish daily tasks and easily sequence and execute pertinent actions. Robots are confined to very simple tasks that are often painstakingly broken down to an excruciating level of detail by humans who operate those robots. Today, more than ever, there is an urgent need to develop robots that reason about manipulation as seamlessly as humans do. Manipulation planning finds a sequence of actions that move the robot and the objects it interacts with from a start state to a goal state. In doing so, contacts are formed and broken, limits are imposed, and some high-level reasoning occurs:. Grasping, placing, regrasping, and rearranging objects often take place. The combination of manipulation planning with classical AI planning and formal methods will lead to the kind of behavior that is expected from a robot capable of cleaning a hospital room, performing tasks in an ICU, or assisting a recovering patient.\r\n\r\nThis project advocates a novel, unifying framework for manipulation planning. It adopts a constraint-centric view, in particular using manifold constraints which define lower-dimensional subspaces, or modes, among which the robot must transition. The definition of transitions is also constraint-centric, defined by the combination of the constraints which define modes, and is only possible within the unified approach used to consider modes. The work depends on constructs from differential geometry and the use of multi-resolution search schemes. First, the project will generalize sampling-based algorithms for robot motion planning by decoupling the planning strategy from the satisfaction of constraints often found in manipulation. Second, the concept of a transition graph between modes will be defined and exploited during search to automatically find viable transitions between modes and produce manipulation plans. The work will start with a specific but general type of constraint, manifold constraints, and later expand to other types. The proposed research will investigate optimality and robustness guarantees provided by the proposed framework and will identify the limits of using constraints as a unifying construct in manipulation planning. The work will be supported by extensive experimentation in both simulation and realistic scenarios that involve several objects, cabinets, shelves, a mobile manipulator, and two stationary state-of-the-art manipulators.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lydia",
   "pi_last_name": "Kavraki",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lydia Kavraki",
   "pi_email_addr": "kavraki@cs.rice.edu",
   "nsf_id": "000489055",
   "pi_start_date": "2020-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 425000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\r\n<p>In contrast to humans who use many types of manipulation to accomplish daily tasks and can easily sequence and execute pertinent actions, robots are confined to very simple tasks that are often painstakingly broken down to an excruciating level of detail by the humans who operate those robots. A robot that will be capable of cleaning a home or assisting an elderly person will possess superior sensing and planning capabilities. This proposal focused on extending the capabilities of manipulation planners. For the purposes of this proposal, manipulation planning is the domain in-between classical motion planning and what is often called task and motion planning (TAMP), where temporal constraints are considered. Manipulation planning is more than motion planning. The latter specifies an initial and a goal configuration and computes a valid motion, assuming that the solution space of the problem does not change (e.g., there are no contacts that are formed or broken during the motion, no new constraints added on the robot, etc.) Manipulation planning&nbsp;finds a sequence of actions that move the robot and the objects it interacts with from a start state to a goal state. In doing so, contacts may be formed and broken, new constraints may be imposed, and high-level reasoning occurs: through different mechanisms manipulation planners may decide to perform multiple actions, such as grasping, placing down, regrasping objects, etc. Manipulation planners may invoke motion planners multiple times.&nbsp;</p>\r\n<p>Our work pushed the boundaries of manipulation planners. The topics that were investigated include among others:&nbsp;&nbsp;developing ultra-fast motion planners that exploit vectorized operations; dealing with sensing uncertainty both in the case where uncertainty is modeled and in the case where it is learned through an implicit representation; incorporating dynamic motion primitives typically learned from human demonstrations; developing an optimization-based approach for grounding to solve cluttered manipulation problems that have many constraints that arise from geometry; and utilizing inverse reinforcement learning to derive motion-level guidance from human critiques in the case of partially observed environments. All of the above allowed us to develop a general manipulation planning framework. Manipulation typically has an underlying discrete structure, such as if an object is grasped. Many problems, such as pick-and-place tasks, are multi-modal, where every object is grasped and placed in a mode. To find a sequence of transitions between modes&mdash;for example, a particular sequence of object picks and placements &ndash; we developed an experience-based planning framework that reuses experience from similar modes both online and from training data. For task satisfaction, we presented a layered planning approach that uses a discrete lead to bias search towards useful mode transitions, informed by weights over mode transitions. Together, our contributions enable multi-modal planners to tackle complex manipulation tasks that were previously infeasible or inefficient and provide significant improvements in scenes with high-dimensional robots.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/07/2025<br>\nModified by: Lydia&nbsp;Kavraki</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nIn contrast to humans who use many types of manipulation to accomplish daily tasks and can easily sequence and execute pertinent actions, robots are confined to very simple tasks that are often painstakingly broken down to an excruciating level of detail by the humans who operate those robots. A robot that will be capable of cleaning a home or assisting an elderly person will possess superior sensing and planning capabilities. This proposal focused on extending the capabilities of manipulation planners. For the purposes of this proposal, manipulation planning is the domain in-between classical motion planning and what is often called task and motion planning (TAMP), where temporal constraints are considered. Manipulation planning is more than motion planning. The latter specifies an initial and a goal configuration and computes a valid motion, assuming that the solution space of the problem does not change (e.g., there are no contacts that are formed or broken during the motion, no new constraints added on the robot, etc.) Manipulation planningfinds a sequence of actions that move the robot and the objects it interacts with from a start state to a goal state. In doing so, contacts may be formed and broken, new constraints may be imposed, and high-level reasoning occurs: through different mechanisms manipulation planners may decide to perform multiple actions, such as grasping, placing down, regrasping objects, etc. Manipulation planners may invoke motion planners multiple times.\r\n\n\nOur work pushed the boundaries of manipulation planners. The topics that were investigated include among others:developing ultra-fast motion planners that exploit vectorized operations; dealing with sensing uncertainty both in the case where uncertainty is modeled and in the case where it is learned through an implicit representation; incorporating dynamic motion primitives typically learned from human demonstrations; developing an optimization-based approach for grounding to solve cluttered manipulation problems that have many constraints that arise from geometry; and utilizing inverse reinforcement learning to derive motion-level guidance from human critiques in the case of partially observed environments. All of the above allowed us to develop a general manipulation planning framework. Manipulation typically has an underlying discrete structure, such as if an object is grasped. Many problems, such as pick-and-place tasks, are multi-modal, where every object is grasped and placed in a mode. To find a sequence of transitions between modesfor example, a particular sequence of object picks and placements  we developed an experience-based planning framework that reuses experience from similar modes both online and from training data. For task satisfaction, we presented a layered planning approach that uses a discrete lead to bias search towards useful mode transitions, informed by weights over mode transitions. Together, our contributions enable multi-modal planners to tackle complex manipulation tasks that were previously infeasible or inefficient and provide significant improvements in scenes with high-dimensional robots.\r\n\n\n\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 01/07/2025\n\n\t\t\t\t\tSubmitted by: LydiaKavraki\n"
 }
}