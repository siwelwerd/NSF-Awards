{
 "awd_id": "2010265",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Assessing College-Ready Computational Thinking",
 "cfda_num": "47.076",
 "org_code": "11090000",
 "po_phone": "7032922255",
 "po_email": "tfrank@nsf.gov",
 "po_sign_block_name": "Toya frank",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 238653.0,
 "awd_amount": 238653.0,
 "awd_min_amd_letter_date": "2020-07-16",
 "awd_max_amd_letter_date": "2022-06-30",
 "awd_abstract_narration": "Because of the growing need for students to be college and career ready, high-quality assessments of college readiness skills are in high demand. To realize the goal of preparing students for college and careers, assessments must measure important competencies and provide rapid feedback to teachers. It is necessary to go beyond the limits of multiple-choice testing and foster the skills and thinking that lie at the core of college and career ready skills, such as computational thinking. Computational thinking is a set of valuable skills that can be used to solve problems, design systems, and understand human behavior, and is thus essential to developing a more STEM-literate public. Computational thinking is increasingly seen as a fundamental analytical skill that everyone, not just computer scientists, can use. The goal of this project is to develop learning progressions and assessment items targeting computational thinking. The items will be used for a test of college-ready critical reasoning skills and will be integrated into an existing online assessment system, the Berkeley Assessment System Software.\r\n\r\nThe project will address a set of research questions focused on 1) clarifying computational thinking constructs, 2) usability, reliability of validity of assessment items and the information they provide, 3) teachers' use of assessments, and 4) relationships to student performance. The study sample of 2,700 used for the pilot and field tests will include all levels of students in 10th through 12th grade and first year college students (both community college and university level). The target population is students in schools which are implementing the College Readiness Program (CRP) of the National Mathematics and Science Institute. In the 2020-21 academic year 54 high schools across 11 states (CA, GA, FL, ID, LA, NC, NM, OH, TX, VA, and WA) will participate. This will include high school students in Advanced Placement classes as well as non-Advanced Placement classes.  The team will use the BEAR Assessment System to develop and refine assessment materials. This system is an integrated approach to developing assessments that seeks to provide meaningful interpretations of student work relative to cognitive and developmental goals. The researchers will gather empirical evidence to develop and improve the assessment materials, and then gather reliability and validity evidence to support their use. In total, item response data will be collected from several thousand students. Student response data will be analyzed using multidimensional item response theory models.\r\n\r\nThe Discovery Research preK-12 program (DRK-12) seeks to significantly enhance the learning and teaching of science, technology, engineering and mathematics (STEM) by preK-12 students and teachers, through research and development of innovative resources, models and tools. Projects in the DRK-12 program build on fundamental research in STEM education and prior research and development efforts that provide theoretical and empirical justification for proposed projects.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DRL",
 "org_div_long_name": "Division of Research on Learning in Formal and Informal Settings (DRL)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brenda",
   "pi_last_name": "Neuman-Sheldon",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brenda Neuman-Sheldon",
   "pi_email_addr": "partnerships@nms.org",
   "nsf_id": "000779521",
   "pi_start_date": "2020-07-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "National Math and Science Initiative, Inc.",
  "inst_street_address": "8080 N CENTRAL EXPY STE 1703",
  "inst_street_address_2": "",
  "inst_city_name": "DALLAS",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "2146652500",
  "inst_zip_code": "752061838",
  "inst_country_name": "United States",
  "cong_dist_code": "32",
  "st_cong_dist_code": "TX32",
  "org_lgl_bus_name": "NATIONAL MATH AND SCIENCE INITIATIVE, INC.",
  "org_prnt_uei_num": "DVBLS8XWTDB9",
  "org_uei_num": "DVBLS8XWTDB9"
 },
 "perf_inst": {
  "perf_inst_name": "National Math and Science Initiative, Inc.",
  "perf_str_addr": "8350 N Central Expy Ste M-2200",
  "perf_city_name": "Dallas",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "752061600",
  "perf_ctry_code": "US",
  "perf_cong_dist": "32",
  "perf_st_cong_dist": "TX32",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "764500",
   "pgm_ele_name": "Discovery Research K-12"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0420",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002021DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0422",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002223DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 159714.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 78939.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span style=\"text-decoration: underline;\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Comprehensive Report on Activities and Findings</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><strong><em><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Reporting and Feedback from Advisors</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></em></strong></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">We developed a comprehensive report analyzing data from sample respondents for the Critical Reasoning for College Readiness (CR4CR) Computational Thinking (CoT) assessment. </span><span class=\"NormalTextRun SCXW179056431 BCX0\">This report was shared with external advisors, including researchers and professionals in the fields of computer science and computational thinking, to gather feedback and refine our work.</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> During several Zoom meetings with these advisors, we received critical insights on our analyses and findings, which informed </span><span class=\"NormalTextRun SCXW179056431 BCX0\">subsequent</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> refinements to the assessment and its associated frameworks.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Teacher Resources and Module Development</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></strong></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">As part of our collaboration with our partners at UC Berkeley we refined a learning module designed to help teachers set up class accounts in the BASS platform and enroll students to access the assessment seamlessly. To support teachers further, we developed prototype progress reports that </span><span class=\"NormalTextRun SCXW179056431 BCX0\">provide</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> actionable insights into student performance. These reports are instrumental in helping educators use the assessment results effectively to inform instruction.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Scoring Models and Framework Refinement</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></strong></em></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">To enhance the scoring process, we explored </span><span class=\"NormalTextRun SCXW179056431 BCX0\">additional</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> scoring models, including a composite modeling approach for score estimation. This approach, along with a multidimensional model that aligns with individual constructs, was further developed and refined by our colleagues. These models enable the assessment to serve multiple purposes: formative use by </span><span class=\"NormalTextRun SCXW179056431 BCX0\">providing</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> construct-level insights, summative use by aggregating results into composite scores, and evaluative use by offering data for schools and districts. This dual-model approach provides flexibility, allowing teachers to </span><span class=\"NormalTextRun SCXW179056431 BCX0\">utilize</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> results in meaningful ways while supporting broader educational evaluation needs.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Construct Framework and Item Feedback</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></strong></em></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">We authored a white paper outlining the framework for the College-ready Computational Thinking (CoT) Assessment. This document was shared with external advisors, who provided detailed feedback during two Zoom meetings. The discussions focused on refining construct definitions and improving prototype assessment items. In addition, we received valuable input from four NMSI computer science coaches on a panel reviewing </span><span class=\"NormalTextRun SCXW179056431 BCX0\">an initial</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> set of 48 assessment items. Their feedback was overwhelmingly positive and highlighted the importance of incorporating </span><span class=\"NormalTextRun SCXW179056431 BCX0\">appropriate scaffolding</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> in prompts to support student success.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Data Analysis and Reporting</span></strong></em><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">Together with our Berkeley collaborators, we conducted preliminary pretest and posttest analyses for the 2022-23 data. Additionally, we provided student reports to </span><span class=\"NormalTextRun SCXW179056431 BCX0\">participating</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> teachers for the Fall 2022 and Spring 2023 cohorts. These reports, reviewed with teachers and trainers, offered calculated </span><span class=\"NormalTextRun SCXW179056431 BCX0\">proficiency</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> levels and actionable insights into student performance. By ensuring </span><span class=\"NormalTextRun SCXW179056431 BCX0\">timely</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> delivery&mdash;within two weeks of the testing window&rsquo;s conclusion&mdash;these reports supported instructional planning and program evaluation for our NMSI </span><span class=\"NormalTextRun SCXW179056431 BCX0\">teachers</span><span class=\"NormalTextRun SCXW179056431 BCX0\">.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Measurement Models and Theoretical Insights</span></strong></em><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">We evaluated the structure of computational thinking by modeling various theoretical frameworks of the construct. This included exploring measurement invariance across different student sub-populations to ensure fairness and accuracy in the assessment. The findings from this analysis were documented in collaborative research publications (e.g., Brown &amp; Brown, 2023). Our modeling efforts further solidified the robustness of the assessment, enabling it to capture the multifaceted nature of computational thinking.</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Innovations in Scoring with Machine Learning</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></strong></em></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">To improve scoring efficiency, we explored machine-learning-based approaches to evaluate student-written responses to assessment tasks. Using decision-tree algorithms guided by GPT-4, we emulated human raters&rsquo; decision-making processes. This approach achieved moderate to high accuracy (0.60&ndash;0.89) and </span><span class=\"NormalTextRun SCXW179056431 BCX0\">identified</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> scenarios where human rater intervention may still be necessary. Furthermore, we investigated the need for balanced training data to enhance the accuracy and reliability of auto-grading algorithms, with findings presented in recent studies (Liu, Liu, Fernandez, Zhang, &amp; Xiang, 2024; Liu, Feng, &amp; Toyama, 2024).</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Continuing Data Analysis and Reporting for 2023-24</span></strong></em><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">Building on our earlier work, we conducted pretest and posttest analyses for the 2023-24 data, providing detailed findings for program evaluation. We also delivered student reports for Fall 2023 and Spring 2024, incorporating </span><span class=\"NormalTextRun SCXW179056431 BCX0\">proficiency</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> estimates derived from look-up tables. These reports, delivered promptly within two weeks of the testing period, helped teachers and program coordinators make </span><span class=\"NormalTextRun SCXW179056431 BCX0\">timely</span><span class=\"NormalTextRun SCXW179056431 BCX0\">, data-driven decisions.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><em><strong><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\">Advancing the Composite Model Approach</span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></strong></em></p>\r\n</div>\r\n<div class=\"OutlineElement Ltr SCXW179056431 BCX0\">\r\n<p class=\"Paragraph SCXW179056431 BCX0\"><span class=\"TextRun SCXW179056431 BCX0\" lang=\"EN-US\" xml:lang=\"EN-US\"><span class=\"NormalTextRun SCXW179056431 BCX0\">Our composite modeling approach and associated standard-setting procedure (Toyama, Gochyyev, &amp; Wilson, 2023) continued to evolve. This approach estimates student abilities on the overall CoT construct while supporting a multidimensional perspective for finer-grained analysis. Teachers </span><span class=\"NormalTextRun SCXW179056431 BCX0\">benefit</span><span class=\"NormalTextRun SCXW179056431 BCX0\"> from both construct-level insights for formative assessment and composite scores for summative evaluation. At the same time, the composite results provide valuable evaluative data for schools and districts, ensuring the assessment serves multiple stakeholders effectively.</span></span><span class=\"EOP SCXW179056431 BCX0\">&nbsp;</span></p>\r\n</div>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/20/2024<br>\nModified by: Brenda&nbsp;Neuman-Sheldon</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715586981_Screenshot_2024_12_20_112608--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715586981_Screenshot_2024_12_20_112608--rgov-800width.png\" title=\"Wright Map for Partial Credit Model\"><img src=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715586981_Screenshot_2024_12_20_112608--rgov-66x44.png\" alt=\"Wright Map for Partial Credit Model\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Wright Map for Partial Credit Model</div>\n<div class=\"imageCredit\">Richard Brown</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brenda&nbsp;Neuman-Sheldon\n<div class=\"imageTitle\">Wright Map for Partial Credit Model</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715630888_Screenshot_2024_12_20_112645--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715630888_Screenshot_2024_12_20_112645--rgov-800width.png\" title=\"Wright Map for Rasch Model\"><img src=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715630888_Screenshot_2024_12_20_112645--rgov-66x44.png\" alt=\"Wright Map for Rasch Model\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Wright Map for Rasch Model</div>\n<div class=\"imageCredit\">Richard Brown</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brenda&nbsp;Neuman-Sheldon\n<div class=\"imageTitle\">Wright Map for Rasch Model</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715673160_Screenshot_2024_12_20_112727--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715673160_Screenshot_2024_12_20_112727--rgov-800width.png\" title=\"Correlation between scoring models\"><img src=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715673160_Screenshot_2024_12_20_112727--rgov-66x44.png\" alt=\"Correlation between scoring models\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Correlation between scoring models</div>\n<div class=\"imageCredit\">Richard Brown</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brenda&nbsp;Neuman-Sheldon\n<div class=\"imageTitle\">Correlation between scoring models</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715951180_Screenshot_2024_12_20_113157--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715951180_Screenshot_2024_12_20_113157--rgov-800width.png\" title=\"Using Wright Maps and Learning Progressions to Engage Teachers and Trainers\"><img src=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715951180_Screenshot_2024_12_20_113157--rgov-66x44.png\" alt=\"Using Wright Maps and Learning Progressions to Engage Teachers and Trainers\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using Wright Maps and Learning Progressions to Engage Teachers and Trainers</div>\n<div class=\"imageCredit\">Yukie Toyama, Richard Brown, Mark Wilson, Karen Draney, Perman Gochyyev, University of California Berkeley, West Coast Analytics, National Math and Science Initiative</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brenda&nbsp;Neuman-Sheldon\n<div class=\"imageTitle\">Using Wright Maps and Learning Progressions to Engage Teachers and Trainers</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715798062_Screenshot_2024_12_20_112834--rgov-214x142.png\" original=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715798062_Screenshot_2024_12_20_112834--rgov-800width.png\" title=\"Measuring and reporting change for classroom use in the context of formative college-readiness assessments\"><img src=\"/por/images/Reports/POR/2024/2010265/2010265_10686350_1734715798062_Screenshot_2024_12_20_112834--rgov-66x44.png\" alt=\"Measuring and reporting change for classroom use in the context of formative college-readiness assessments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Measuring and reporting change for classroom use in the context of formative college-readiness assessments</div>\n<div class=\"imageCredit\">Yukie Toyama, Richard Brown, Perman Gochyyev, University of California Berkeley, West Coast Analytics</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Brenda&nbsp;Neuman-Sheldon\n<div class=\"imageTitle\">Measuring and reporting change for classroom use in the context of formative college-readiness assessments</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\r\n\n\nComprehensive Report on Activities and Findings\r\n\r\n\r\n\n\nReporting and Feedback from Advisors\r\n\r\n\r\n\n\nWe developed a comprehensive report analyzing data from sample respondents for the Critical Reasoning for College Readiness (CR4CR) Computational Thinking (CoT) assessment. This report was shared with external advisors, including researchers and professionals in the fields of computer science and computational thinking, to gather feedback and refine our work. During several Zoom meetings with these advisors, we received critical insights on our analyses and findings, which informed subsequent refinements to the assessment and its associated frameworks.\r\n\r\n\r\n\n\nTeacher Resources and Module Development\r\n\r\n\r\n\n\nAs part of our collaboration with our partners at UC Berkeley we refined a learning module designed to help teachers set up class accounts in the BASS platform and enroll students to access the assessment seamlessly. To support teachers further, we developed prototype progress reports that provide actionable insights into student performance. These reports are instrumental in helping educators use the assessment results effectively to inform instruction.\r\n\r\n\r\n\n\nScoring Models and Framework Refinement\r\n\r\n\r\n\n\nTo enhance the scoring process, we explored additional scoring models, including a composite modeling approach for score estimation. This approach, along with a multidimensional model that aligns with individual constructs, was further developed and refined by our colleagues. These models enable the assessment to serve multiple purposes: formative use by providing construct-level insights, summative use by aggregating results into composite scores, and evaluative use by offering data for schools and districts. This dual-model approach provides flexibility, allowing teachers to utilize results in meaningful ways while supporting broader educational evaluation needs.\r\n\r\n\r\n\n\nConstruct Framework and Item Feedback\r\n\r\n\r\n\n\nWe authored a white paper outlining the framework for the College-ready Computational Thinking (CoT) Assessment. This document was shared with external advisors, who provided detailed feedback during two Zoom meetings. The discussions focused on refining construct definitions and improving prototype assessment items. In addition, we received valuable input from four NMSI computer science coaches on a panel reviewing an initial set of 48 assessment items. Their feedback was overwhelmingly positive and highlighted the importance of incorporating appropriate scaffolding in prompts to support student success.\r\n\r\n\r\n\n\nData Analysis and Reporting\r\n\r\n\r\n\n\nTogether with our Berkeley collaborators, we conducted preliminary pretest and posttest analyses for the 2022-23 data. Additionally, we provided student reports to participating teachers for the Fall 2022 and Spring 2023 cohorts. These reports, reviewed with teachers and trainers, offered calculated proficiency levels and actionable insights into student performance. By ensuring timely deliverywithin two weeks of the testing windows conclusionthese reports supported instructional planning and program evaluation for our NMSI teachers.\r\n\r\n\r\n\n\nMeasurement Models and Theoretical Insights\r\n\r\n\r\n\n\nWe evaluated the structure of computational thinking by modeling various theoretical frameworks of the construct. This included exploring measurement invariance across different student sub-populations to ensure fairness and accuracy in the assessment. The findings from this analysis were documented in collaborative research publications (e.g., Brown & Brown, 2023). Our modeling efforts further solidified the robustness of the assessment, enabling it to capture the multifaceted nature of computational thinking.\r\n\r\n\r\n\n\nInnovations in Scoring with Machine Learning\r\n\r\n\r\n\n\nTo improve scoring efficiency, we explored machine-learning-based approaches to evaluate student-written responses to assessment tasks. Using decision-tree algorithms guided by GPT-4, we emulated human raters decision-making processes. This approach achieved moderate to high accuracy (0.600.89) and identified scenarios where human rater intervention may still be necessary. Furthermore, we investigated the need for balanced training data to enhance the accuracy and reliability of auto-grading algorithms, with findings presented in recent studies (Liu, Liu, Fernandez, Zhang, & Xiang, 2024; Liu, Feng, & Toyama, 2024).\r\n\r\n\r\n\n\nContinuing Data Analysis and Reporting for 2023-24\r\n\r\n\r\n\n\nBuilding on our earlier work, we conducted pretest and posttest analyses for the 2023-24 data, providing detailed findings for program evaluation. We also delivered student reports for Fall 2023 and Spring 2024, incorporating proficiency estimates derived from look-up tables. These reports, delivered promptly within two weeks of the testing period, helped teachers and program coordinators make timely, data-driven decisions.\r\n\r\n\r\n\n\nAdvancing the Composite Model Approach\r\n\r\n\r\n\n\nOur composite modeling approach and associated standard-setting procedure (Toyama, Gochyyev, & Wilson, 2023) continued to evolve. This approach estimates student abilities on the overall CoT construct while supporting a multidimensional perspective for finer-grained analysis. Teachers benefit from both construct-level insights for formative assessment and composite scores for summative evaluation. At the same time, the composite results provide valuable evaluative data for schools and districts, ensuring the assessment serves multiple stakeholders effectively.\r\n\r\n\n\n\t\t\t\t\tLast Modified: 12/20/2024\n\n\t\t\t\t\tSubmitted by: BrendaNeuman-Sheldon\n"
 }
}