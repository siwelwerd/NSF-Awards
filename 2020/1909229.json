{
 "awd_id": "1909229",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: TICE - Telematic Immersive Classroom Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2020-04-01",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2020-04-01",
 "awd_max_amd_letter_date": "2020-04-01",
 "awd_abstract_narration": "Since their inception, telecommunication systems have targeted the exchange between exactly two people. This holds true for the telegraph, the telephone, and typical video conference systems. Meetings for larger groups of people are usually conducted using the same personal interfaces \u2013 take, for example, a telephone conference call. This practice contrasts with a traditional group meeting, for example, or a classroom scenario. In the latter, students and teachers can communicate with each other while having an awareness of the spatial location of each participant and the ability to identify each speaker easily. The goal of this project is to develop technology that enables two distant classrooms to be joined by telecommunication devices without the need for personal communication interfaces. An electronically adjustable microphone array will be designed that can virtually focus on an active speaker. This way, echoes that would otherwise occur in a collaborative telecommunication scenario with loudspeakers will be avoided. The system will be designed such that the sounds of a speaker will emanate from the position at which they are seen on a live video feed. Project outcomes are expected to have broad impact by providing new educational opportunities for classroom students by virtually connecting them to other classrooms for collaborative learning experiences.  \r\n \r\nThis project will develop a spatially correct, long-distance audio connection between collaborative spaces, for example two classrooms with groups of people at each site. The missing link needed to enable such remote collaboration is an audio tracking system that can follow all participants on both sides of the connection. The approach will use two 16-channel spherical microphones as the main audio tracking devices. The key idea is that each spherical microphone will receive continuous information from additional body-worn microphones, which is utilized to ensure echo-free bidirectional communication. The body-worn microphone signals will inform the tracking system who among the participants is speaking at a given time interval and in a given frequency range. The spherical microphone is then used to track each sound source during the time intervals that the source is active and in those frequency bands that are not disturbed by other sources. Using machine-learning tools, including Bayesian methods, Kalman filters, and Deep Neural Networks, the tracking system will implement a joint bottom-up/top-down architecture. Two existing sites are equipped with immersive loudspeaker systems, each of which will enable accurate local reproduction of spatialized audio from the remote site. These sites are also equipped with collaborative virtual reality systems with immersive video capabilities, and they will serve as laboratories for this project, enabling a system to transmit bidirectional audio/video streams with spatially congruent audio/video images. As an extensions of the project, the audio-tracking system will be paired with an existing 6-camera tracking system that is mounted on the laboratory ceiling and technologies will be developed to replace the body-worn microphones with virtual microphones using the beam-forming capabilities of a spherical microphone array.  In another extension, visual gestures of participants will be considered to further facilitate the telematic experience. An important component of this research will be the development of an affordable and deployable version of the collaborative virtual reality system that can be easily used in schools, for example as a ceiling-mounted system hosted in a school's gymnasium.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonas",
   "pi_last_name": "Braasch",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jonas Braasch",
   "pi_email_addr": "braasj@rpi.edu",
   "nsf_id": "000186127",
   "pi_start_date": "2020-04-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rensselaer Polytechnic Institute",
  "inst_street_address": "110 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "TROY",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5182766000",
  "inst_zip_code": "121803590",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "NY20",
  "org_lgl_bus_name": "RENSSELAER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "U5WBFKEBLMX3"
 },
 "perf_inst": {
  "perf_inst_name": "Rensselaer Polytechnic Institute",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "121103522",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "NY20",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>\n<div>The core of the TICE project centered around telematically connecting two collaborative virtual environments to allow realtime communication with spatially preserved audio-visual cues as if all participant met in the same physical location. We define a collaborative virtual environment as as synthetic world that can host multiple human participants without being obstructed by technical devices such as 3D goggles. Systems like Rensselaer&rsquo;s CRAIVE-Lab and the EMPAC cognitive immersive room, allow up to 49 participants to use their natural cues to communicate between each other while have a shared experience in the virtual world consisting of a 360-deg projected image, high-fidelity multichannel audio sound, and cognitive computing to support the participants.&nbsp;</div>\n<div></div>\n<div>For the TICE project, an audiovisual tracking system was developed to capture the trajectories of multiple participants in realtime using joint camera tracking, acoustic beamforming techniques and advanced modelling of mechanisms inspired by the auditory system to extract sound spatialization and room cues.. The system is essential to spatially embed users at two-remote ends into a joint virtual environment using a shared game engine that was developed.</div>\n<div><br />The <strong>Intellectual Merit</strong>&nbsp;of the project is the completion of a framework to connect two or more collaborative groups telematically in a way that all participants are projected audiovisually congruently at the remote end in a bidirectional scenario to facilitate group discussions. The system developments include (i) an audiovisual tracking system to track participants in real time, (ii) a sound spatialization system to render participants correctly at the remote end using the tracking data, and (iii) a game-engine-based framework to render virtual worlds across two or more embedded collaborative virtual environments spatially correctly using the game engines raytracing capabilities for audio and visuals alike.</div>\n<div></div>\n<div>The project team also developed and implemented general immersive teaching resources that can be used in a telematic or single-site context, including (i) a rapid prototyping system for auralizing historic acoustics venues based on existing floorplans using large loudspeaker arrays, (ii) a framework to easily create high-spatial density sonifications from any data source using traditional music synthesis methods, and (iii) a java-based to share and distribute content on the immersive systems for teachers and students. All these technologies are targeted for rapid content creation as required by teachers and students in a classroom environment, and conducted user studies start to show the benefits of these technologies.</div>\n<div class=\"x_elementToProof\"></div>\n<div class=\"x_elementToProof\">The&nbsp;<strong>Broader Impact</strong>&nbsp;of the project lies in the practical implementation of the developed technologies into immersive learning classrooms. Immersive learning technologies have started to become a cornerstone of Rensselaer's signature education in classes ranging from architectural design, environmental and civil engineering, geology, and music.</div>\n</p><br>\n<p>\n Last Modified: 08/05/2024<br>\nModified by: Jonas&nbsp;Braasch</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\nThe core of the TICE project centered around telematically connecting two collaborative virtual environments to allow realtime communication with spatially preserved audio-visual cues as if all participant met in the same physical location. We define a collaborative virtual environment as as synthetic world that can host multiple human participants without being obstructed by technical devices such as 3D goggles. Systems like Rensselaers CRAIVE-Lab and the EMPAC cognitive immersive room, allow up to 49 participants to use their natural cues to communicate between each other while have a shared experience in the virtual world consisting of a 360-deg projected image, high-fidelity multichannel audio sound, and cognitive computing to support the participants.\n\nFor the TICE project, an audiovisual tracking system was developed to capture the trajectories of multiple participants in realtime using joint camera tracking, acoustic beamforming techniques and advanced modelling of mechanisms inspired by the auditory system to extract sound spatialization and room cues.. The system is essential to spatially embed users at two-remote ends into a joint virtual environment using a shared game engine that was developed.\n\nThe Intellectual Meritof the project is the completion of a framework to connect two or more collaborative groups telematically in a way that all participants are projected audiovisually congruently at the remote end in a bidirectional scenario to facilitate group discussions. The system developments include (i) an audiovisual tracking system to track participants in real time, (ii) a sound spatialization system to render participants correctly at the remote end using the tracking data, and (iii) a game-engine-based framework to render virtual worlds across two or more embedded collaborative virtual environments spatially correctly using the game engines raytracing capabilities for audio and visuals alike.\n\nThe project team also developed and implemented general immersive teaching resources that can be used in a telematic or single-site context, including (i) a rapid prototyping system for auralizing historic acoustics venues based on existing floorplans using large loudspeaker arrays, (ii) a framework to easily create high-spatial density sonifications from any data source using traditional music synthesis methods, and (iii) a java-based to share and distribute content on the immersive systems for teachers and students. All these technologies are targeted for rapid content creation as required by teachers and students in a classroom environment, and conducted user studies start to show the benefits of these technologies.\n\nTheBroader Impactof the project lies in the practical implementation of the developed technologies into immersive learning classrooms. Immersive learning technologies have started to become a cornerstone of Rensselaer's signature education in classes ranging from architectural design, environmental and civil engineering, geology, and music.\n\t\t\t\t\tLast Modified: 08/05/2024\n\n\t\t\t\t\tSubmitted by: JonasBraasch\n"
 }
}