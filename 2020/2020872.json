{
 "awd_id": "2020872",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC: Towards Accounting for the Human in Emotion Recognition Technologies",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2020-05-01",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 239054.0,
 "awd_amount": 239054.0,
 "awd_min_amd_letter_date": "2020-03-12",
 "awd_max_amd_letter_date": "2020-03-12",
 "awd_abstract_narration": "Emotions are powerful, mediate human experiences with their surroundings, and impact decision-making and attention online and off. Privacy and emotion are related concepts; online and off, emotions are often deemed private. Sharing and signaling one's emotions to other humans can be beneficial, but involves privacy calculations and complex decision-making processes. Despite the deeply personal nature of human emotion, artificial intelligence (AI) algorithms are being built to recognize and infer emotions using data sources such as social media behavior, streaming service use, voice, facial expressions, biometrics, and body language in ways often unknown to users. Emotion recognition\u2019s emerging market is expected to grow to $3.8 billion by 2025. This project posits that emotion recognition technologies can prioritize the privacy and preferences of the humans they impact. This project takes steps toward addressing users' privacy and other concerns by investigating people's attitudes towards emotion recognition and how users and technologists conceive of this technology. This project deepens intellectual conversations about emerging technologies' privacy and safety; interactions between ethics and AI, data science, and research; and social media design. Technologists, policy makers, and researchers in fields such as computing, economics, and medicine can use this project\u2019s resulting framework to evaluate systems' potential implications. Furthermore, the project\u2019s topic provides an important context to challenge students to think deeply about ethics, privacy, social responsibility, and technology in teaching and research \u2014 crucial for generating a responsible and thoughtful next generation of information and computing professionals.\r\n\r\nThis work will focus on emotions and users' attitudes towards emotion recognition technologies to contribute to our knowledge about socially and ethically responsible use and treatment of data in algorithmic decision-making that impacts personal lives. Using a human-centered lens, this work will contribute a framework of guidelines within which emotion recognition technologies can be evaluated for compatibility with people's values, needs, and concerns. The research team will 1) conduct a series of critical analyses of artifacts, including patents, to interrogate the ways technologists and academics articulate emotion recognition technology's importance across application domains, and its use-cases, visions, and implications (e.g., ethical, privacy, personal, inter-personal, social); and 2) use interviews, vignettes, and focus groups to examine social media users' attitudes towards emotion recognition and its perceived impacts on their privacy and their lives, highlighting factors contributing to a trustworthy or untrustworthy cyberspace that may utilize emotion recognition.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nazanin",
   "pi_last_name": "Andalibi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nazanin Andalibi",
   "pi_email_addr": "andalibi@umich.edu",
   "nsf_id": "000788669",
   "pi_start_date": "2020-03-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "105 South State Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091285",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "065Z",
   "pgm_ref_txt": "Human factors for security research"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 239054.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-25103254-7fff-58b5-6fc5-3daf1faf0698\">\n<p dir=\"ltr\"><span>This project examined the societal implications of emotion recognition or emotion AI technologies in several contexts: social media, workplace monitoring, and employment interviews. A key driving argument for this research program was that it is important to understand the perspectives of the individuals who are most impacted by emotion AI and whose data is essential for the technology&rsquo;s existence to begin with.&nbsp;</span></p>\n<p dir=\"ltr\"><span>First, using interviews with social media users we found that they see accurate inferences about their emotional states as uncomfortable and as threatening their agency, pointing to privacy and ambiguity as desired design principles for social media platforms. This included examining how people feel about well-being interventions on social media platforms that are enabled by emotion AI techniques. We found that people&rsquo;s attitudes toward these interventions were predominantly negative. Negative attitudes were largely shaped by how participants compared their conceptualizations of AI to the humans that traditionally deliver wellbeing support. We conclude that the interventions imagined by participants are incompatible with aims to promote trustworthy, socially aware, and responsible AI technologies in the current practical and regulatory landscape in the US.&nbsp;</span></p>\n<p dir=\"ltr\"><span>We then became interested in high-stakes contexts where emotion AI is and may be used such as the workplace and employment interviews. We began preliminary research through investigating how technology providers/inventors legitimize their products/inventions. First, we turned to vendors that provide emotion AI services for organizations to use in their employment interviews. We find that these services claim to solve problems such as hiring fit, accuracy, and bias through the techno-solution of emotion AI and show why these problems are indeed not addressable using emotion AI. However, these services continue to exist and be used without much regulation and oversight. We discuss implications for design and policy including Federal Trade Commission (FTC) enforcement against unfair and deceptive emotion AI hiring service practices.</span></p>\n<p dir=\"ltr\"><span>Second, we turned to patent applications that propose emotion AI technologies to be used in workplaces. We found </span><span>that these technologies scope data collection broadly; claim to reveal not only targets&rsquo; emotional expressions, but also their internal states; and take or prompt a wide range of actions, many of which impact workers&rsquo; employment and livelihoods status. Importantly, we noticed that many of the purported benefits of these technologies were aimed at employers and not employees. However, the inventors claimed benefits to employees such as improved mental health and wellbeing. This made us wonder what employees' attitudes towards these technologies might be, as they are a group highly impacted by emotion AI use in the workplace without much power and choice. So we designed further studies to this end. We found that employees have deep privacy violation concerns over their emotion data; that emotion AI re-enforce certain emotional labor expectations (e.g., people changing how they feel or express themselves), and that they may engage in emotional labor as a way to preserve their emotional privacy; and that employees may experience a wide range of harms as a result of being subject to emotion AI in the workplace.&nbsp; Employees expressed concerns regarding the potential for emotion AI use to harm their wellbeing, work environment and employment status, and create and amplify bias and stigma against them, especially the most marginalized (e.g., along dimensions of race, gender, mental health status, disability). These findings reveal the need to recognize and define an individual right to what we theorize as emotional privacy. We argue that emotion AI may magnify, rather than alleviate, existing challenges employees face in the workplace and suggest that some emotion AI-inflicted harms would persist even if concerns of the technology&rsquo;s accuracy and bias concerns are addressed.</span></p>\n<p dir=\"ltr\"><span>Results from this award were published in seven top-tier venues. This line of work also formed the basis for the PI&rsquo;s awarded CAREER grant that aims to take a more in-depth look at the implications of emotion AI in the future of work. It has also supported a PhD student advisee to develop their highly successful research portfolio as well as the professional development of a postdoctoral fellow and several undergraduate and graduate research assistants. Additionally, the work has garnered attention from the press as well as other institutions nationally and internationally (via PI&rsquo;s invited talks) and the Federal Trades Commission (FTC) via PI&rsquo;s participation as part of a panel during their Privacy Con event.&nbsp;</span></p>\n<br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/02/2023<br>\n\t\t\t\t\tModified by: Nazanin&nbsp;Andalibi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThis project examined the societal implications of emotion recognition or emotion AI technologies in several contexts: social media, workplace monitoring, and employment interviews. A key driving argument for this research program was that it is important to understand the perspectives of the individuals who are most impacted by emotion AI and whose data is essential for the technology\u2019s existence to begin with. \nFirst, using interviews with social media users we found that they see accurate inferences about their emotional states as uncomfortable and as threatening their agency, pointing to privacy and ambiguity as desired design principles for social media platforms. This included examining how people feel about well-being interventions on social media platforms that are enabled by emotion AI techniques. We found that people\u2019s attitudes toward these interventions were predominantly negative. Negative attitudes were largely shaped by how participants compared their conceptualizations of AI to the humans that traditionally deliver wellbeing support. We conclude that the interventions imagined by participants are incompatible with aims to promote trustworthy, socially aware, and responsible AI technologies in the current practical and regulatory landscape in the US. \nWe then became interested in high-stakes contexts where emotion AI is and may be used such as the workplace and employment interviews. We began preliminary research through investigating how technology providers/inventors legitimize their products/inventions. First, we turned to vendors that provide emotion AI services for organizations to use in their employment interviews. We find that these services claim to solve problems such as hiring fit, accuracy, and bias through the techno-solution of emotion AI and show why these problems are indeed not addressable using emotion AI. However, these services continue to exist and be used without much regulation and oversight. We discuss implications for design and policy including Federal Trade Commission (FTC) enforcement against unfair and deceptive emotion AI hiring service practices.\nSecond, we turned to patent applications that propose emotion AI technologies to be used in workplaces. We found that these technologies scope data collection broadly; claim to reveal not only targets\u2019 emotional expressions, but also their internal states; and take or prompt a wide range of actions, many of which impact workers\u2019 employment and livelihoods status. Importantly, we noticed that many of the purported benefits of these technologies were aimed at employers and not employees. However, the inventors claimed benefits to employees such as improved mental health and wellbeing. This made us wonder what employees' attitudes towards these technologies might be, as they are a group highly impacted by emotion AI use in the workplace without much power and choice. So we designed further studies to this end. We found that employees have deep privacy violation concerns over their emotion data; that emotion AI re-enforce certain emotional labor expectations (e.g., people changing how they feel or express themselves), and that they may engage in emotional labor as a way to preserve their emotional privacy; and that employees may experience a wide range of harms as a result of being subject to emotion AI in the workplace.  Employees expressed concerns regarding the potential for emotion AI use to harm their wellbeing, work environment and employment status, and create and amplify bias and stigma against them, especially the most marginalized (e.g., along dimensions of race, gender, mental health status, disability). These findings reveal the need to recognize and define an individual right to what we theorize as emotional privacy. We argue that emotion AI may magnify, rather than alleviate, existing challenges employees face in the workplace and suggest that some emotion AI-inflicted harms would persist even if concerns of the technology\u2019s accuracy and bias concerns are addressed.\nResults from this award were published in seven top-tier venues. This line of work also formed the basis for the PI\u2019s awarded CAREER grant that aims to take a more in-depth look at the implications of emotion AI in the future of work. It has also supported a PhD student advisee to develop their highly successful research portfolio as well as the professional development of a postdoctoral fellow and several undergraduate and graduate research assistants. Additionally, the work has garnered attention from the press as well as other institutions nationally and internationally (via PI\u2019s invited talks) and the Federal Trades Commission (FTC) via PI\u2019s participation as part of a panel during their Privacy Con event. \n\n\n\n \n\n\t\t\t\t\tLast Modified: 08/02/2023\n\n\t\t\t\t\tSubmitted by: Nazanin Andalibi"
 }
}