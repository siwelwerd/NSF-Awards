{
 "awd_id": "2039445",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: SaTC-EDU: Identifying Educational Conceptions and Challenges in  Cybersecurity and Artificial Intelligence",
 "cfda_num": "47.076",
 "org_code": "11010000",
 "po_phone": "7032928182",
 "po_email": "asiraj@nsf.gov",
 "po_sign_block_name": "Ambareen Siraj",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2020-07-27",
 "awd_max_amd_letter_date": "2020-07-27",
 "awd_abstract_narration": "Artificial intelligence (AI) has significant applications to many data-intensive emerging domains such as automated vehicles, computer-assisted medical imaging, behavior analysis, user authentication, cybersecurity, and embedded systems for smart infrastructures.  However, there are unanswered questions relating to  trust in AI systems.  There is increasing evidence that machine learning algorithms can be maliciously manipulated to cause misclassification and false detection of objects and speech. With the growing adoption of AI-based techniques, it is therefore important to teach students the skills needed to analyze vulnerabilities in AI-based systems and how such systems may fail, as well as how to mitigate such issues to help create more trustworthy AI-based systems.  This project brings together experts from the areas of education, AI, and cybersecurity to identify challenges and potential solutions to teaching topics in trustworthy AI with the goal of evolving coursework that will appeal to, and engage, a diverse student body. It is critical to diversify the workforce operating at the intersection of cybersecurity and AI because AI-based systems can be prone to implicit vulnerabilities and blind spots due to imbalanced datasets or training methods that focus only on the overall accuracy of available datasets. \r\n\r\nThe project team proposes to teach and study three courses at the intersection of cybersecurity and AI, including creating a new course on trustworthy AI. Coursework will address topics that will spur students to consider how segments of the population may be differentially impacted in areas such as authentication, privacy, and user safety. Learning science and educational psychology approaches (specifically focus groups and clinical interviews) will be used to identify learning and teaching challenges and to characterize conceptions and misconceptions. The project will produce five deliverables: model curricula at the crossroads of cybersecurity and AI; strategies for managing cross-disciplinarity in such curricula; characterizations of student concepts; identification of student learning challenges; and identification of new research directions in cybersecurity and AI. The findings and curricular ideas will be disseminated broadly. \r\n\r\nThis project is supported by a special initiative of the Secure and Trustworthy Cyberspace (SaTC) program to foster new, previously unexplored, collaborations between the fields of cybersecurity, artificial intelligence, and education. The SaTC program aligns with the Federal Cybersecurity Research and Development Strategic Plan and the National Privacy Research Strategy to protect and preserve the growing social and economic benefits of cyber systems while ensuring security and privacy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DGE",
 "org_div_long_name": "Division Of Graduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Atul",
   "pi_last_name": "Prakash",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Atul Prakash",
   "pi_email_addr": "aprakash@umich.edu",
   "nsf_id": "000217717",
   "pi_start_date": "2020-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Guzdial",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mark Guzdial",
   "pi_email_addr": "mjguz@umich.edu",
   "nsf_id": "000325617",
   "pi_start_date": "2020-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Emily",
   "pi_last_name": "Provost",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Emily M Provost",
   "pi_email_addr": "emilykmp@umich.edu",
   "nsf_id": "000607930",
   "pi_start_date": "2020-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan",
  "perf_str_addr": "2260 Hayward Street",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "093Z",
   "pgm_ref_txt": "AI Education/Workforce Develop"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0420",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002021DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-d2af45e8-7fff-a3ea-4a4e-bd4734a89950\">\r\n<p dir=\"ltr\"><span>Our modern society is dependent on software. We need to be able to trust our software to protect our privacy, to keep us safe, and to guarantee communications, like between us and our banks. To produce trustworthy software, we need software developers who value cybersecurity and who will apply best practices in secure software development. This need is particularly acute in the development of artificial intelligence (AI) software.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>In our research, we studied advanced Computer Science students (undergraduate seniors and early graduate students) taking an AI course that has data privacy risks: predicting emotion from user behavior. The course teaches AI techniques (including machine learning), but also spends part of the lectures on &nbsp;highlighting the privacy risks and how to mitigate them. The semester we interviewed students, the course included two guest lectures from cybersecurity experts. Given the additional security teachings in the AI course, we expected the students in the class&nbsp; to be generally aware and informed about cybersecurity risks in AI. But, our studies suggest otherwise.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>In our interviews, we posed hypothetical &nbsp;open-ended situations that highlighted particular security and privacy risks in AI software, then asked students how they would address them. For example, how could you prevent a nefarious company from using emotion prediction to determine when individuals suffering mental health illness are most susceptible to buying sprees, and then bombarding them with ads? Or, if you were building an autonomous vehicle, how could you deal with the possibility of an adversary putting defective sensors in your supply chain and thus confusing any AI in the vehicle?</span></p>\r\n<p dir=\"ltr\"><span>Some students did know how to manage these situations well. But other students did not. Some claimed that those were issues for cybersecurity experts, not them as AI practitioners. None of the students told us </span><em>when</em><span> they would call on cybersecurity experts nor </span><em>how</em><span> they would work with them.</span></p>\r\n<p dir=\"ltr\"><span>One way to interpret these results is that perhaps cybesecurity content needs to be a larger proportion of AI courses. But, one should weigh the possibility that students in AI classes are less interested in cybersecurity, viewing cybersecurity as someone else&rsquo;s problem.&nbsp; Regardless, one finding from the study is that we should train our AI students to recognize <em>when</em> they </span><span>need to collaborate</span><span> with cybersecurity experts and </span><span><em>how</em> they would collaborate</span><span> with those experts.</span></p>\r\n<p dir=\"ltr\"><span>The grant has contributed to the research by several Ph.D. graduate students, thus training our advanced technical workforce. </span></p>\r\n</span></p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/28/2025<br>\nModified by: Atul&nbsp;Prakash</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\r\n\n\nOur modern society is dependent on software. We need to be able to trust our software to protect our privacy, to keep us safe, and to guarantee communications, like between us and our banks. To produce trustworthy software, we need software developers who value cybersecurity and who will apply best practices in secure software development. This need is particularly acute in the development of artificial intelligence (AI) software.\r\n\n\nIn our research, we studied advanced Computer Science students (undergraduate seniors and early graduate students) taking an AI course that has data privacy risks: predicting emotion from user behavior. The course teaches AI techniques (including machine learning), but also spends part of the lectures on highlighting the privacy risks and how to mitigate them. The semester we interviewed students, the course included two guest lectures from cybersecurity experts. Given the additional security teachings in the AI course, we expected the students in the class to be generally aware and informed about cybersecurity risks in AI. But, our studies suggest otherwise.\r\n\n\nIn our interviews, we posed hypothetical open-ended situations that highlighted particular security and privacy risks in AI software, then asked students how they would address them. For example, how could you prevent a nefarious company from using emotion prediction to determine when individuals suffering mental health illness are most susceptible to buying sprees, and then bombarding them with ads? Or, if you were building an autonomous vehicle, how could you deal with the possibility of an adversary putting defective sensors in your supply chain and thus confusing any AI in the vehicle?\r\n\n\nSome students did know how to manage these situations well. But other students did not. Some claimed that those were issues for cybersecurity experts, not them as AI practitioners. None of the students told us when they would call on cybersecurity experts nor how they would work with them.\r\n\n\nOne way to interpret these results is that perhaps cybesecurity content needs to be a larger proportion of AI courses. But, one should weigh the possibility that students in AI classes are less interested in cybersecurity, viewing cybersecurity as someone elses problem. Regardless, one finding from the study is that we should train our AI students to recognize when they need to collaborate with cybersecurity experts and how they would collaborate with those experts.\r\n\n\nThe grant has contributed to the research by several Ph.D. graduate students, thus training our advanced technical workforce. \r\n\r\n\n\n\t\t\t\t\tLast Modified: 01/28/2025\n\n\t\t\t\t\tSubmitted by: AtulPrakash\n"
 }
}