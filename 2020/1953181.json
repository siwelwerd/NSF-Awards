{
 "awd_id": "1953181",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Interpolation Methods in Statistics and Machine Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2020-06-01",
 "awd_exp_date": "2024-05-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-03-24",
 "awd_max_amd_letter_date": "2022-06-06",
 "awd_abstract_narration": "One of the key tenets taught in courses on Statistics and Machine Learning is that data interpolation (or, data memorization) inevitably leads to overfitting and poor prediction performance. Yet, most of the modern large-scale models, including over-parametrized neural networks, are routinely optimized to achieve zero error on training data. The research objective of this project is to challenge the common wisdom and develop theoretical and algorithmic foundations for methods that interpolate the training data. \r\n\t\r\nThe project will focus on the statistical and computational aspects of interpolation methods. Consistency and finite-sample bounds will be derived for regression and classification methods in the interpolation regime, and information-theoretic limits of interpolating rules will be developed. The project will also focus on the computational aspects of interpolation. The PI aims to shed light on the relative advantages and disadvantages of over-parametrized models that have capacity to perfectly fit the data.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Rakhlin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Rakhlin",
   "pi_email_addr": "rakhlin@mit.edu",
   "nsf_id": "000537825",
   "pi_start_date": "2020-03-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 66667.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 97352.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 35981.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>\n<div>\n<div><span>The research performed under this grant has centered on uncovering the fundamental limits and potentials of statistical methods in machine learning, particularly in situations where models have more parameters than data points---a scenario known as overparameterization. We scrutinized widely used techniques like Empirical Risk Minimization (ERM) to assess their effectiveness in modern, data-rich contexts. Our findings revealed inherent biases and limitations within these traditional methods when applied to complex, high-dimensional data. This insight highlights the need for developing new approaches that can mitigate these biases, leading to more accurate predictions without excessive computational demands.</span></div>\n<br />\n<div><span>Building on this foundation, we advanced the theoretical understanding of optimization algorithms essential for training machine learning models. By developing novel mathematical tools, we enhanced the analysis of popular methods such as gradient descent and the Adam optimizer, even when dealing with non-smooth functions typical in real-world applications. Our work ensures that these algorithms can reliably and efficiently find good solutions, which is crucial for technologies ranging from speech recognition to autonomous vehicles.</span></div>\n<br />\n<div><span>We also explored how slight modifications to data&mdash;--a process known as smoothed learning&mdash;--can significantly improve the performance of learning algorithms. Our research demonstrated that smoothing enables algorithms to perform better even when each data point comes from a different source. This has practical implications for fields like personalized medicine and adaptive technologies, where data cannot be assumed to be uniform or identically distributed.</span></div>\n<br />\n<div><span>Furthermore, we contributed to the understanding of deep learning by examining why overparameterized models can still make accurate predictions. We showed that under certain conditions, these complex models behave similarly to simpler ones, providing clarity on how deep learning models generalize from data despite their complexity.</span></div>\n<br />\n<div><span>In addition, we developed efficient computational methods for tackling high-dimensional regression problems and introduced new techniques for estimating the intrinsic complexity of data. These advancements are vital for handling complex datasets in areas like genomics for improving the efficiency of generative models that create realistic images and sounds.</span></div>\n<br />\n<div><span>Overall, our research aims to deepen the theoretical foundations of statistical methods and optimization algorithms in machine learning. By addressing key challenges related to overparameterization, data variability, and high-dimensional analysis, we enhance the tools available for data analysis across various scientific and technological domains. These efforts ultimately contribute to the development of more accurate, efficient, and reliable machine learning applications that benefit society.</span></div>\n</div>\n</p><br>\n<p>\n Last Modified: 09/20/2024<br>\nModified by: Alexander&nbsp;Rakhlin</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\nThe research performed under this grant has centered on uncovering the fundamental limits and potentials of statistical methods in machine learning, particularly in situations where models have more parameters than data points---a scenario known as overparameterization. We scrutinized widely used techniques like Empirical Risk Minimization (ERM) to assess their effectiveness in modern, data-rich contexts. Our findings revealed inherent biases and limitations within these traditional methods when applied to complex, high-dimensional data. This insight highlights the need for developing new approaches that can mitigate these biases, leading to more accurate predictions without excessive computational demands.\n\n\nBuilding on this foundation, we advanced the theoretical understanding of optimization algorithms essential for training machine learning models. By developing novel mathematical tools, we enhanced the analysis of popular methods such as gradient descent and the Adam optimizer, even when dealing with non-smooth functions typical in real-world applications. Our work ensures that these algorithms can reliably and efficiently find good solutions, which is crucial for technologies ranging from speech recognition to autonomous vehicles.\n\n\nWe also explored how slight modifications to data--a process known as smoothed learning--can significantly improve the performance of learning algorithms. Our research demonstrated that smoothing enables algorithms to perform better even when each data point comes from a different source. This has practical implications for fields like personalized medicine and adaptive technologies, where data cannot be assumed to be uniform or identically distributed.\n\n\nFurthermore, we contributed to the understanding of deep learning by examining why overparameterized models can still make accurate predictions. We showed that under certain conditions, these complex models behave similarly to simpler ones, providing clarity on how deep learning models generalize from data despite their complexity.\n\n\nIn addition, we developed efficient computational methods for tackling high-dimensional regression problems and introduced new techniques for estimating the intrinsic complexity of data. These advancements are vital for handling complex datasets in areas like genomics for improving the efficiency of generative models that create realistic images and sounds.\n\n\nOverall, our research aims to deepen the theoretical foundations of statistical methods and optimization algorithms in machine learning. By addressing key challenges related to overparameterization, data variability, and high-dimensional analysis, we enhance the tools available for data analysis across various scientific and technological domains. These efforts ultimately contribute to the development of more accurate, efficient, and reliable machine learning applications that benefit society.\n\n\t\t\t\t\tLast Modified: 09/20/2024\n\n\t\t\t\t\tSubmitted by: AlexanderRakhlin\n"
 }
}