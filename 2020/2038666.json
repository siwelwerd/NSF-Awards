{
 "awd_id": "2038666",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CPS: Medium: Spatio-Temporal Logics for Analyzing and Querying Perception Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2021-01-01",
 "awd_exp_date": "2024-12-31",
 "tot_intn_awd_amt": 799936.0,
 "awd_amount": 799936.0,
 "awd_min_amd_letter_date": "2020-09-04",
 "awd_max_amd_letter_date": "2022-04-14",
 "awd_abstract_narration": "The goals of Automated Driving Systems (ADS) and Advanced Driver Assistance Systems (ADAS) include reduction in accidental deaths, enhanced mobility for differently abled people, and an overall improvement in the quality of life for the general public. Such systems typically operate in open and highly uncertain environments for which robust perception systems are essential. However, despite the tremendous theoretical and experimental progress in computer vision, machine learning, and sensor fusion, the form and conditions under which guarantees should be provided for perception components is still unclear. The state-of-the-art is to perform scenario-based evaluation of data against ground truth values, but this has only limited impact. The lack of formal metrics to analyze the quality of perception systems has already led to several catastrophic incidents and a plateau in ADS/ADAS development. This project develops formal languages for specifying and evaluating the quality and robustness of perception sub-systems within ADS and ADAS applications.   To enable broader dissemination of this technology, the project develops graduate and undergraduate curricula to train engineers in the use of such methods, and new educational modules to explain the challenges in developing safe and robust ADS for outreach and public engagement activities. To broaden participation in computing, the investigators target the inclusion of undergraduate women in research and development phases through summer internships.\r\n\r\nThe formal language developed in this project is based on a new spatio-temporal logic pioneered by the investigators. This logic allows one to simultaneously perform temporal reasoning about streaming perception data, and spatially reason about objects both within a single frame of the data and across frames.  The project also develops quantitative semantics for this logic, which provides the user with quantifiable quality metrics for perception sub-systems. These semantics enable comparisons between different perception systems and architectures. Crucially, the formal language facilitates the process of abstracting away implementation details, which in turn allows system designers and regulators to specify assumptions and guarantees for system performance at a higher-level of abstraction. An interesting benefit of this formal language is that it enables querying of databases with perception data for specific driving scenarios without the need for the highly manual process of creating ground truth annotations. Such a formal language currently does not exist, and this is a huge impediment to building a thriving marketplace for perception components used in safety-critical systems. This framework sets the foundation for a requirements language between suppliers of perception components and automotive companies. The open source and publicly available software tools developed in this project will assist with testing of perception systems by engineers and governmental agencies.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Georgios",
   "pi_last_name": "Fainekos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Georgios Fainekos",
   "pi_email_addr": "fainekos@asu.edu",
   "nsf_id": "000542660",
   "pi_start_date": "2020-09-04",
   "pi_end_date": "2022-04-14"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yezhou",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yezhou Yang",
   "pi_email_addr": "yz.yang@asu.edu",
   "nsf_id": "000733585",
   "pi_start_date": "2022-04-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Yezhou",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yezhou Yang",
   "pi_email_addr": "yz.yang@asu.edu",
   "nsf_id": "000733585",
   "pi_start_date": "2020-09-04",
   "pi_end_date": "2022-04-14"
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852816011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 799936.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Overview: This CPS project focused on enhancing the reliability and robustness of perception systems by employing formal languages to specify and evaluate their quality. By developing new logics and computational frameworks, the research aimed to improve perception accuracy in applications such as autonomous driving and robotic systems. Throughout the project, significant progress was made in formalizing and applying spatio-temporal logics to real-world scenarios.</p>\r\n<p><br />Intellectual Merit: The research introduced Spatio-Temporal Perception Logic (STPL) as an advanced method for evaluating and monitoring perception systems. This effort resulted in several major contributions, including the development of STPL as an extension of Timed Quality Temporal Logic (TQTL) with spatial operators, allowing for a more precise assessment of perception quality. Algorithmic advancements were made with the design of efficient offline and online monitoring algorithms for STPL, enabling real-time data stream analysis. The project demonstrated the practical application of these methods in both simulated and real-world driving environments, leading to improvements in object detection under adverse conditions. Another key achievement was the introduction of Latent Space Energy-based Neural ODEs (ODE-LEBM), a novel continuous-time sequence modeling technique that integrates an expressive energy-based prior to enhance perception models, improving both interpolation and extrapolation capabilities. The outcomes of this research were disseminated through high-impact journal publications, including papers presented at IJRR, WACV, EMNLP, and TMLR, which highlighted advancements in perception quality evaluation, object detection, and neural modeling.</p>\r\n<p><br />Broader Impacts: The research significantly contributed to the advancement of autonomous systems by improving perception robustness, thereby enhancing safety in dynamic environments. In the educational sphere, the project supported multiple PhD students, two of whom successfully completed their doctorates and secured influential roles in industry and academia. Principal Investigator Yezhou Yang actively engaged in educational outreach by conducting courses at Arizona State University and delivering public lectures, including an invited talk on \"AI and the Joy of Living\" at the Mirabella invited talk series. Additionally, a high school intern was involved in research efforts, focusing on the analysis of perception metrics for AI-driven applications. The project also contributed to open-source development through the release of the PerceMon Python toolbox, which enables researchers and developers to apply STPL-based monitoring techniques in their work. Collaboration with Toyota further enriched the research, as the company provided robotic platforms for testing and validation.</p>\r\n<p><br />Key Findings and Achievements: A fundamental contribution of this project was the formalization of STPL, which provided a mathematically rigorous approach to evaluating perception systems. A trust-aware control framework was developed, integrating trust-based monitoring into self-driving systems. The research led to improvements in object detection through the development of IA-YOLO, an enhanced object detection model that increases accuracy in challenging environments. The introduction of latent space ODE models significantly advanced continuous-time perception modeling by offering a novel energy-based neural ODE framework. These advancements were successfully implemented and tested in both simulated environments, such as CARLA, and real-world robotic systems, demonstrating their practical effectiveness.</p>\r\n<p>Future Directions: The research conducted in this project establishes a foundation for further advancements in the field. Future work will focus on extending STPL to multi-modal perception, combining vision and LiDAR to improve environmental understanding. Additional efforts will be directed toward increasing AI interpretability through formal logics that explain and verify deep learning-based perception models. The project also paves the way for enhancing perception in dynamic environments by applying STPL techniques to complex multi-agent systems beyond autonomous driving. These contributions will continue to influence AI-driven perception, with applications spanning autonomous navigation, robotics, and intelligent transportation systems.</p><br>\n<p>\n Last Modified: 03/23/2025<br>\nModified by: Yezhou&nbsp;Yang</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nOverview: This CPS project focused on enhancing the reliability and robustness of perception systems by employing formal languages to specify and evaluate their quality. By developing new logics and computational frameworks, the research aimed to improve perception accuracy in applications such as autonomous driving and robotic systems. Throughout the project, significant progress was made in formalizing and applying spatio-temporal logics to real-world scenarios.\r\n\n\n\nIntellectual Merit: The research introduced Spatio-Temporal Perception Logic (STPL) as an advanced method for evaluating and monitoring perception systems. This effort resulted in several major contributions, including the development of STPL as an extension of Timed Quality Temporal Logic (TQTL) with spatial operators, allowing for a more precise assessment of perception quality. Algorithmic advancements were made with the design of efficient offline and online monitoring algorithms for STPL, enabling real-time data stream analysis. The project demonstrated the practical application of these methods in both simulated and real-world driving environments, leading to improvements in object detection under adverse conditions. Another key achievement was the introduction of Latent Space Energy-based Neural ODEs (ODE-LEBM), a novel continuous-time sequence modeling technique that integrates an expressive energy-based prior to enhance perception models, improving both interpolation and extrapolation capabilities. The outcomes of this research were disseminated through high-impact journal publications, including papers presented at IJRR, WACV, EMNLP, and TMLR, which highlighted advancements in perception quality evaluation, object detection, and neural modeling.\r\n\n\n\nBroader Impacts: The research significantly contributed to the advancement of autonomous systems by improving perception robustness, thereby enhancing safety in dynamic environments. In the educational sphere, the project supported multiple PhD students, two of whom successfully completed their doctorates and secured influential roles in industry and academia. Principal Investigator Yezhou Yang actively engaged in educational outreach by conducting courses at Arizona State University and delivering public lectures, including an invited talk on \"AI and the Joy of Living\" at the Mirabella invited talk series. Additionally, a high school intern was involved in research efforts, focusing on the analysis of perception metrics for AI-driven applications. The project also contributed to open-source development through the release of the PerceMon Python toolbox, which enables researchers and developers to apply STPL-based monitoring techniques in their work. Collaboration with Toyota further enriched the research, as the company provided robotic platforms for testing and validation.\r\n\n\n\nKey Findings and Achievements: A fundamental contribution of this project was the formalization of STPL, which provided a mathematically rigorous approach to evaluating perception systems. A trust-aware control framework was developed, integrating trust-based monitoring into self-driving systems. The research led to improvements in object detection through the development of IA-YOLO, an enhanced object detection model that increases accuracy in challenging environments. The introduction of latent space ODE models significantly advanced continuous-time perception modeling by offering a novel energy-based neural ODE framework. These advancements were successfully implemented and tested in both simulated environments, such as CARLA, and real-world robotic systems, demonstrating their practical effectiveness.\r\n\n\nFuture Directions: The research conducted in this project establishes a foundation for further advancements in the field. Future work will focus on extending STPL to multi-modal perception, combining vision and LiDAR to improve environmental understanding. Additional efforts will be directed toward increasing AI interpretability through formal logics that explain and verify deep learning-based perception models. The project also paves the way for enhancing perception in dynamic environments by applying STPL techniques to complex multi-agent systems beyond autonomous driving. These contributions will continue to influence AI-driven perception, with applications spanning autonomous navigation, robotics, and intelligent transportation systems.\t\t\t\t\tLast Modified: 03/23/2025\n\n\t\t\t\t\tSubmitted by: YezhouYang\n"
 }
}