{
 "awd_id": "1955851",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Medium: Collaborative Research: Towards Scalable and Interpretable Graph Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Wei Ding",
 "awd_eff_date": "2020-07-15",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 99934.0,
 "awd_amount": 99934.0,
 "awd_min_amd_letter_date": "2020-07-15",
 "awd_max_amd_letter_date": "2020-08-12",
 "awd_abstract_narration": "Graphs are ubiquitous data structures in numerous domains, such as social science (social networks), natural science (physical systems, and protein-protein interaction networks) and knowledge graphs. As new generalizations of traditional deep neural networks to graph structured data, Graph Neural Networks (or GNNs) have demonstrated the power in graph representation learning and have permeated numerous areas of science and technology. However, GNNs also inherited the drawback of traditional deep neural networks, i.e., lacking interpretability. Moreover, the complexity of graph data introduces the scalability as a new limitation for GNNs because graph structured data are not independent. These drawbacks have raised tremendous concerns to adopt GNNs in many critical applications pertaining to fairness, privacy, and safety. Thus, this project aims to tackle the major drawbacks of GNNs and greatly enlarge their usability in critical applications. To achieve the research goal, this project systematically investigates advanced principles for scalable GNNs and new mechanisms to interpret GNNs. The proposed research extends the state-of-the-art GNNs to a new frontier, investigates original problems that entreat innovative solutions and paves the way for a new research endeavor effectively tame graph mining. As many real-world domains problems requires scalable and interpretable graph mining techniques, the project has potential to benefit many real-world applications from various disciplines such as Computer Science, Social Science, Healthcare and Bioinformatics. \r\n\r\n\r\n\r\nThis project proposes novel principles and mechanisms for scalable and interpretable graph neural networks to facilitate the adoption of GNNs on critical domains, investigates associated fundamental research issues and develops effective algorithms. The project offers the first comprehensive investigation on these directions, and the designed novel methodologies and tasks will deepen our understanding on the inner working mechanisms of GNNs and contribute to real-world applications. The success of this project will be (1) New scalable and interpretable GNNs with state-of-the-art graph representation learning and predictive performance; (2) Theoretical analysis such as convergence and complexity; and (3) Open-source implementations of all key algorithms and frameworks. Disparate means are planned to disseminate the project and its findings, such as web enabled data and software repositories, books, journal and conference publications, special purpose workshops or tutorials, and industrial collaborations. The project can be effectively integrated to undergraduate and graduate courses as well as in student research projects.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Suhang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Suhang Wang",
   "pi_email_addr": "szw494@psu.edu",
   "nsf_id": "000788548",
   "pi_start_date": "2020-07-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "The Pennsylvania State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 99934.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Graphs are ubiquitous data structures in numerous domains, such as social science (social networks), natural science (physical systems, and protein-protein interaction networks) and knowledge graphs. As new generalizations of traditional deep neural networks to graph structured data, Graph Neural Networks (GNNs) have shown great ability in graph representation learning and have permeated numerous areas of science and technology. However, GNNs also inherited the drawback of traditional deep neural networks, i.e., lacking interpretability. Moreover, the complexity of graph data introduces the scalability as a new limitation for GNNs because graph structured data are not independent. These drawbacks have raised tremendous concerns to adopt GNNs in many critical applications. Thus, this project aims to tackle the major drawbacks of GNNs and greatly enlarge their usability in critical applications. The goal of the project is to investigate novel scalable and interpretable GNNs with state-of-the-art graph representation learning and predictive performance. To achieve the goal, we developed three novel frameworks for interpretability of GNNs from three different perspectives.</p>\n<p>&nbsp;</p>\n<p>First, we proposed a novel self-explainable GNN framework named SE-GNN, which can simultaneously give prediction and provide instance-level explanations of the prediction. Specifically, SE-GNN finds K-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets show SE-GNN has state-of-the-art node classification performance and high-quality explanations on the prediction.</p>\n<p>&nbsp;</p>\n<p>Second, we proposed a novel prototype generation based self-explainable GNN framework. The proposed framework can learn prototype graphs that capture representative patterns of each class as class-level explanations. The learned prototypes are also used to simultaneously make prediction for a test instance and provide instance-level explanation. Extensive experiments on real-world and synthetic datasets show the effectiveness of the proposed framework for both prediction accuracy and explanation quality.</p>\n<p>&nbsp;</p>\n<p>Finally, we developed a novel post-hoc model agnostic GNN explainer, which can give faithful and consistent explanations for a trained GNN. Generally, existing post-hoc GNN explainers aim to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions.&nbsp; Existing work formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. We theoretically examine the predictions of GNNs from the causality perspective and identified two typical reasons of spurious ex- explanations are identified. Based on the identified reasons, we propose a simple yet effective countermeasure by aligning embeddings of the raw graph and the identified key graph. This new objective is easy to compute and can be incorporated into existing GNN explanation algorithms with no or little effort. Experiments on real-world and synthetic datasets validate its effectiveness, and theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design.</p>\n<p>&nbsp;</p>\n<p>The project has resulted in more than 15 papers in top-tier journals and conferences on machine learning and data mining and one PhD thesis on graph neural networks, and trained&nbsp;five graduate students and two undergraduate students. The findings are also integrated into undergraduate courses taught by the PI. In particular, the PI developed a new undergraduate course DS 420 Network Analytics, which covers graph neural networks as advanced topic in the course. Publications and software resulted from this project are disseminated on this website: <a href=\"https://faculty.ist.psu.edu/szw494/projects/scalable_interpretable_GNNs.html\">https://faculty.ist.psu.edu/szw494/projects/scalable_interpretable_GNNs.html</a>.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/28/2022<br>\n\t\t\t\t\tModified by: Suhang&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nGraphs are ubiquitous data structures in numerous domains, such as social science (social networks), natural science (physical systems, and protein-protein interaction networks) and knowledge graphs. As new generalizations of traditional deep neural networks to graph structured data, Graph Neural Networks (GNNs) have shown great ability in graph representation learning and have permeated numerous areas of science and technology. However, GNNs also inherited the drawback of traditional deep neural networks, i.e., lacking interpretability. Moreover, the complexity of graph data introduces the scalability as a new limitation for GNNs because graph structured data are not independent. These drawbacks have raised tremendous concerns to adopt GNNs in many critical applications. Thus, this project aims to tackle the major drawbacks of GNNs and greatly enlarge their usability in critical applications. The goal of the project is to investigate novel scalable and interpretable GNNs with state-of-the-art graph representation learning and predictive performance. To achieve the goal, we developed three novel frameworks for interpretability of GNNs from three different perspectives.\n\n \n\nFirst, we proposed a novel self-explainable GNN framework named SE-GNN, which can simultaneously give prediction and provide instance-level explanations of the prediction. Specifically, SE-GNN finds K-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets show SE-GNN has state-of-the-art node classification performance and high-quality explanations on the prediction.\n\n \n\nSecond, we proposed a novel prototype generation based self-explainable GNN framework. The proposed framework can learn prototype graphs that capture representative patterns of each class as class-level explanations. The learned prototypes are also used to simultaneously make prediction for a test instance and provide instance-level explanation. Extensive experiments on real-world and synthetic datasets show the effectiveness of the proposed framework for both prediction accuracy and explanation quality.\n\n \n\nFinally, we developed a novel post-hoc model agnostic GNN explainer, which can give faithful and consistent explanations for a trained GNN. Generally, existing post-hoc GNN explainers aim to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions.  Existing work formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. We theoretically examine the predictions of GNNs from the causality perspective and identified two typical reasons of spurious ex- explanations are identified. Based on the identified reasons, we propose a simple yet effective countermeasure by aligning embeddings of the raw graph and the identified key graph. This new objective is easy to compute and can be incorporated into existing GNN explanation algorithms with no or little effort. Experiments on real-world and synthetic datasets validate its effectiveness, and theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design.\n\n \n\nThe project has resulted in more than 15 papers in top-tier journals and conferences on machine learning and data mining and one PhD thesis on graph neural networks, and trained five graduate students and two undergraduate students. The findings are also integrated into undergraduate courses taught by the PI. In particular, the PI developed a new undergraduate course DS 420 Network Analytics, which covers graph neural networks as advanced topic in the course. Publications and software resulted from this project are disseminated on this website: https://faculty.ist.psu.edu/szw494/projects/scalable_interpretable_GNNs.html.\n\n\t\t\t\t\tLast Modified: 10/28/2022\n\n\t\t\t\t\tSubmitted by: Suhang Wang"
 }
}