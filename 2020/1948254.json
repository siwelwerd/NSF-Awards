{
 "awd_id": "1948254",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRII: CHS: Understanding Gesture User Behavior in Augmented Reality Headsets",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924420",
 "po_email": "cbethel@nsf.gov",
 "po_sign_block_name": "Cindy Bethel",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 175000.0,
 "awd_amount": 223000.0,
 "awd_min_amd_letter_date": "2020-04-01",
 "awd_max_amd_letter_date": "2023-06-01",
 "awd_abstract_narration": "Augmented Reality (AR) allows placing virtual objects in a real environment. AR glasses are the next wave of human-computer interaction technology. This technology will have a direct impact on education, manufacturing, national defense, and assembly jobs, among others. Understanding the role of human interaction with an AR system using gestures, speech, or the combination of both (gesture and speech) will provide foundational knowledge to decrease the complexity of user interaction. This research will have an impact in society by supporting underrepresented and non-traditional graduate students and developing new interdisciplinary course. The outcomes of this research will facilitate creation of improved AR applications that benefit several fields such as future workforce training. The datasets generated through this project can result in collaborative explorations between computer scientists and cognitive/learning scientists to make learning AR applications more accessible and incorporate more intuitive interactions.\r\n\r\nThis project will generate preliminary, foundational research on gestures for AR three-dimensional (3D) user interfaces, design egocentric gestures with/without speech interactions with AR headsets, construct two labeled datasets for gesture recognition, and develop a block-based, gesture-enabled application for AR headsets. This project will advance the state of the art in gesture interaction for AR headsets by: (1) Elicitation Studies: This project will conduct the largest elicitation study to date, substantially increasing the foundational knowledge base and improving elicitation methodology and gesture recognizers. This study will require creating a novel application that will generate new collaborative interaction datasets,  resulting in better foundational knowledge about user gestures for AR 3D user interfaces via gesture elicitation; (2) Dataset Generation: The elicitation studies will produce labeled and unlabeled egocentric datasets that will enable human-computer interaction and computer vision researchers to explore new recognition algorithms; (3) Multi-modal interaction (gesture and speech) interaction in AR Headsets: The preliminary research conducted as part of this project will allow research in complex interactions and collaborative tasks using AR Headsets. This research will include conducting egocentric gesture studies (with/without speech) to determine appropriate interaction for AR headset applications. The project\u2019s research activities will broaden our understanding and the use of elicitation studies beyond action/gesture mapping, as well as the multi-modal interactions (gesture and speech) in AR Headsets, either validating findings in human-to-human communication or discovering new ones. This research is applicable in domains using AR Headsets with intuitive user controls for interactive applications, such as industry, manufacturing, aviation, education, entertainment, energy, and defense.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Francisco",
   "pi_last_name": "Ortega",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Francisco R Ortega",
   "pi_email_addr": "fortega@colostate.edu",
   "nsf_id": "000715085",
   "pi_start_date": "2020-04-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado State University",
  "inst_street_address": "601 S HOWES ST",
  "inst_street_address_2": "",
  "inst_city_name": "FORT COLLINS",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "9704916355",
  "inst_zip_code": "805212807",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "COLORADO STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LT9CXX8L19G1"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado State University",
  "perf_str_addr": "200 W Lake St",
  "perf_city_name": "Fort Collins",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "805214595",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "8228",
   "pgm_ref_txt": "CISE Resrch Initiatn Initiatve"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002324DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 191000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2023,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-5020d836-7fff-ba99-d660-74cf4e004695\"> </span></p>\r\n<p dir=\"ltr\"><span>The planning award explored interaction methods in 3D user interfaces (AR/VR) to better understand user needs and improve mixed reality systems. The goal was to&nbsp; generating preliminary, foundational research on gestures for AR 3D user interfaces, and using gesture elicitation to improve gesture recognition and system design, enabling people to interact naturally with AR HMDs, particularly in applications like immersive analytics, VR sketching, and elicitation studies. Findings from this research aim to understand user needs, improve performance, decrease cognitive load, and enhance everyday experiences through better interaction techniques for different applications, games, and entertainment. The project resulted in 22 publications and released three open-source applications (hand gesture dataset, Artisan Bistro, and Spatial Markers) for testing the proposed infrastructure.</span></p>\r\n<p dir=\"ltr\"><span>The project found foundational knowledge about how humans interact with Augmented Reality head-mounted displays (AR glasses) that are set to become pervasive in the future. We found that for abstract commands (create an object), speech is better suited; for error correction and complex rotations, gesture and speech are preferred; and for simple manipulations, gestures are ideal (e.g., making a cube larger). We also found that as systems get more complex (more objects in the display), the gestures while sharing some similarities, tend to become more physical and complex. By physical, we mean that users try to grab the virtual objects and by complex is that as more objects are around the display, a typical one-hand gesture may now become a two-hand gesture. We provide a set of findings on our project website [1] and a dataset for other researchers to work with.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>Our work is very important if you consider that people in an office looking at 3D data visualizations require more complex interactions. Our work sets the foundation, and our future work includes examining those more complex interactions.&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>If further developed, this infrastructure will allow users to interact more naturally with AR/VR systems, create innovative mixed-reality applications, and improve collaboration across distances. This research contributes to the advancement of computer science, benefiting both the U.S. and global communities.</span></p>\r\n<p dir=\"ltr\"><span>Our work also provided an open-source test bed for researchers [2]. We created a simple spatial marker solution for undergraduate students who want to understand how spatial markers in augmented reality work. The project also helped create a new class for mixed reality design and a book about elicitation methodology [4].&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>This Project Outcomes Report is presented verbatim as submitted by the Principal Investigator (PI). The opinions, findings, and conclusions are those of the PI and do not necessarily reflect the National Science Foundation's (NSF) views, which have not approved or endorsed its content.<br /><br />Reference:&nbsp;</span></p>\r\n<p dir=\"ltr\"><span>[1]&nbsp; Project and Gesture Database,&nbsp;</span><a href=\"https://www.cs.colostate.edu/NSFGesturesCRII\"><span>https://www.cs.colostate.edu/NSFGesturesCRII</span></a></p>\r\n<p dir=\"ltr\"><span>[2] ARtisan Bistro,&nbsp;</span><a href=\"https://github.com/NuiLab/ARtisan-Bistro\"><span>https://github.com/NuiLab/ARtisan-Bistro</span></a></p>\r\n<p><span>[3] Spatial Markers,&nbsp;</span><a href=\"https://github.com/NuiLab/NUI-SpatialMarker\"><span>https://github.com/NuiLab/NUI-SpatialMarker<br />[</span></a>4]&nbsp;Williams, A. S., &amp; Ortega, F. R. (2021). A concise guide to elicitation methodology. arXiv preprint arXiv:2105.12865.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 11/29/2024<br>\nModified by: Francisco&nbsp;R&nbsp;Ortega</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n \r\n\n\nThe planning award explored interaction methods in 3D user interfaces (AR/VR) to better understand user needs and improve mixed reality systems. The goal was to generating preliminary, foundational research on gestures for AR 3D user interfaces, and using gesture elicitation to improve gesture recognition and system design, enabling people to interact naturally with AR HMDs, particularly in applications like immersive analytics, VR sketching, and elicitation studies. Findings from this research aim to understand user needs, improve performance, decrease cognitive load, and enhance everyday experiences through better interaction techniques for different applications, games, and entertainment. The project resulted in 22 publications and released three open-source applications (hand gesture dataset, Artisan Bistro, and Spatial Markers) for testing the proposed infrastructure.\r\n\n\nThe project found foundational knowledge about how humans interact with Augmented Reality head-mounted displays (AR glasses) that are set to become pervasive in the future. We found that for abstract commands (create an object), speech is better suited; for error correction and complex rotations, gesture and speech are preferred; and for simple manipulations, gestures are ideal (e.g., making a cube larger). We also found that as systems get more complex (more objects in the display), the gestures while sharing some similarities, tend to become more physical and complex. By physical, we mean that users try to grab the virtual objects and by complex is that as more objects are around the display, a typical one-hand gesture may now become a two-hand gesture. We provide a set of findings on our project website [1] and a dataset for other researchers to work with.\r\n\n\nOur work is very important if you consider that people in an office looking at 3D data visualizations require more complex interactions. Our work sets the foundation, and our future work includes examining those more complex interactions.\r\n\n\nIf further developed, this infrastructure will allow users to interact more naturally with AR/VR systems, create innovative mixed-reality applications, and improve collaboration across distances. This research contributes to the advancement of computer science, benefiting both the U.S. and global communities.\r\n\n\nOur work also provided an open-source test bed for researchers [2]. We created a simple spatial marker solution for undergraduate students who want to understand how spatial markers in augmented reality work. The project also helped create a new class for mixed reality design and a book about elicitation methodology [4].\r\n\n\nThis Project Outcomes Report is presented verbatim as submitted by the Principal Investigator (PI). The opinions, findings, and conclusions are those of the PI and do not necessarily reflect the National Science Foundation's (NSF) views, which have not approved or endorsed its content.\n\nReference:\r\n\n\n[1] Project and Gesture Database,https://www.cs.colostate.edu/NSFGesturesCRII\r\n\n\n[2] ARtisan Bistro,https://github.com/NuiLab/ARtisan-Bistro\r\n\n\n[3] Spatial Markers,https://github.com/NuiLab/NUI-SpatialMarker\n[4]Williams, A. S., & Ortega, F. R. (2021). A concise guide to elicitation methodology. arXiv preprint arXiv:2105.12865.\r\n\n\n\r\n\n\n\t\t\t\t\tLast Modified: 11/29/2024\n\n\t\t\t\t\tSubmitted by: FranciscoROrtega\n"
 }
}