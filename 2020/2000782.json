{
 "awd_id": "2000782",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:AI-DCL:Capture, Explain and Negotiate the Inherent Trade-offs in Machine Learning Algorithms",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2019-10-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 295713.0,
 "awd_amount": 311713.0,
 "awd_min_amd_letter_date": "2019-11-12",
 "awd_max_amd_letter_date": "2020-07-17",
 "awd_abstract_narration": "An outstanding issue with machine learning based decision-making algorithms is the inherent trade-offs between different system criteria. There is an emerging body of literature demonstrating trade-offs between fairness and accuracy, and between different fairness notions. By improving fairness, overall accuracy might decrease. Furthermore, different fairness notions are not compatible with each other: well-established results show that common statistical fairness notions are often mutually exclusive. Accurate understanding of such trade-offs is critical for stakeholders and practitioners to appropriately use these machine learning methods. The focus of this project is to take an interdisciplinary approach to study, explain, and address the inherent trade-offs between different system criteria in machine learning-based decision-making.\r\n\r\nThe researchers will develop methods to capture trade-offs between different system criteria in machine learning algorithms. They will develop visualizations and interactive interfaces to explain the trade-offs between the models to the stakeholders. Finally, the project team will explore social and technical innovations that let stakeholders navigate and negotiate the fundamental trade-offs between different system criteria.  The project will disseminate the new knowledge in part through a MOOC offering.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Haiyi",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Haiyi Zhu",
   "pi_email_addr": "haiyiz@cs.cmu.edu",
   "nsf_id": "000686407",
   "pi_start_date": "2019-11-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0757",
   "pgm_ref_txt": "COOP PLAN OPs & SERVICES"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 295713.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-f69174c2-7fff-5474-6338-c65c8558a5ce\"> </span></p>\n<p dir=\"ltr\"><span>Automated and artificially intelligent algorithmic systems are increasingly used to manage human activities and make important decisions. However, even sophisticated algorithms that optimize standard accuracy measures can fail to solve the problems they were designed to tackle when they are inconsistent with, or even harm, the important values and needs of the people and communities who use them.</span></p>\n<p dir=\"ltr\"><span><span>To address these problems, we propose the method of \"Value-Sensitive Algorithm Design\" (VSAD), which emphasizes the importance of uncovering a wide range of stakeholders' values at an early stage of the design process and incorporating and balancing those values in the creation of the algorithm. The method involves three steps: (1) understanding the values of the community stakeholders related to algorithms, (2) explaining the trade-offs between different algorithm choices to the stakeholders, and (3) enabling community members to reason about different fairness and accuracy measures, navigate trade-offs, express priorities, and negotiate with each other to find appropriate models.</span></span></p>\n<p dir=\"ltr\"><span>In this NSF project, we applied the VSAD approach to a machine learning-based quality prediction system on Wikipedia called the Objective Revision Evaluation Service (ORES). We first interviewed ORES stakeholders, including editors, patrollers, tool developers, Wikimedia Foundation employees, and researchers, to systematically identify which values matter to the community. We then designed an interactive visualization system (Wikipedia ORES Explorer) to communicate model threshold trade-offs and fairness in ORES and evaluated the system by conducting 10 in-depth interviews with potential ORES application designers. We also conducted a series of workshops in two online communities - English and Dutch Wikipedia - to negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values.</span></p>\n<p dir=\"ltr\"><span>The project made four major contributions. First, we articulated and demonstrated the effectiveness of a general method, including processes and best practices, in uncovering trade-offs, making trade-offs interpretable, and balancing trade-offs in machine learning models and applications. Second, we created techniques for visualizing and explaining the trade-offs. Third, we created, deployed, and evaluated social and technical innovations to address fundamental trade-offs between different system criteria. Fourth, we designed and implemented improvements to ORES, which will improve a wide variety of applications that rely on ORES and Wikipedia's content and community as a whole. Improving Wikipedia will positively influence billions of people who consume the Wikipedia content either directly or indirectly through other applications such as Google.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/25/2023<br>\n\t\t\t\t\tModified by: Haiyi&nbsp;Zhu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437876035_EAGER-figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437876035_EAGER-figure2--rgov-800width.jpg\" title=\"Wikipedia ORES Explorer Interface 1\"><img src=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437876035_EAGER-figure2--rgov-66x44.jpg\" alt=\"Wikipedia ORES Explorer Interface 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We designed an interactive visualization system to communicate model threshold trade-offs and fairness in ORES.</div>\n<div class=\"imageCredit\">Zining Ethan Ye, Xinran Yuan, Shaurya Gaur, Aaron Halfaker, Jodi Forlizzi,  Haiyi Zhu</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Haiyi&nbsp;Zhu</div>\n<div class=\"imageTitle\">Wikipedia ORES Explorer Interface 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437718058_EAGER-figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437718058_EAGER-figure1--rgov-800width.jpg\" title=\"Wikipedia Stakeholder Values\"><img src=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437718058_EAGER-figure1--rgov-66x44.jpg\" alt=\"Wikipedia Stakeholder Values\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We interviewed ORES stakeholders, including editors, patrollers, tool developers, Wikimedia Foundation employees, and researchers, to systematically identify which values matter to the community.</div>\n<div class=\"imageCredit\">Estelle Smith, Haiyi Zhu</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Haiyi&nbsp;Zhu</div>\n<div class=\"imageTitle\">Wikipedia Stakeholder Values</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437985102_EAGER-figure3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437985102_EAGER-figure3--rgov-800width.jpg\" title=\"Wikipedia ORES Explorer Interface 2\"><img src=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682437985102_EAGER-figure3--rgov-66x44.jpg\" alt=\"Wikipedia ORES Explorer Interface 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We designed an interactive visualization system to communicate model threshold trade-offs and fairness in ORES.</div>\n<div class=\"imageCredit\">Zining Ethan Ye, Xinran Yuan, Shaurya Gaur, Aaron Halfaker, Jodi Forlizzi,  Haiyi Zhu</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Haiyi&nbsp;Zhu</div>\n<div class=\"imageTitle\">Wikipedia ORES Explorer Interface 2</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682438082879_EAGER-figure4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682438082879_EAGER-figure4--rgov-800width.jpg\" title=\"Community Workshops\"><img src=\"/por/images/Reports/POR/2023/2000782/2000782_10640994_1682438082879_EAGER-figure4--rgov-66x44.jpg\" alt=\"Community Workshops\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We conducted a series of workshops in two online communities \ufffd English and Dutch Wikipedia \ufffd to negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values.</div>\n<div class=\"imageCredit\">Hong Shen, Wesley Deng, Haiyi Zhu</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Haiyi&nbsp;Zhu</div>\n<div class=\"imageTitle\">Community Workshops</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nAutomated and artificially intelligent algorithmic systems are increasingly used to manage human activities and make important decisions. However, even sophisticated algorithms that optimize standard accuracy measures can fail to solve the problems they were designed to tackle when they are inconsistent with, or even harm, the important values and needs of the people and communities who use them.\nTo address these problems, we propose the method of \"Value-Sensitive Algorithm Design\" (VSAD), which emphasizes the importance of uncovering a wide range of stakeholders' values at an early stage of the design process and incorporating and balancing those values in the creation of the algorithm. The method involves three steps: (1) understanding the values of the community stakeholders related to algorithms, (2) explaining the trade-offs between different algorithm choices to the stakeholders, and (3) enabling community members to reason about different fairness and accuracy measures, navigate trade-offs, express priorities, and negotiate with each other to find appropriate models.\nIn this NSF project, we applied the VSAD approach to a machine learning-based quality prediction system on Wikipedia called the Objective Revision Evaluation Service (ORES). We first interviewed ORES stakeholders, including editors, patrollers, tool developers, Wikimedia Foundation employees, and researchers, to systematically identify which values matter to the community. We then designed an interactive visualization system (Wikipedia ORES Explorer) to communicate model threshold trade-offs and fairness in ORES and evaluated the system by conducting 10 in-depth interviews with potential ORES application designers. We also conducted a series of workshops in two online communities - English and Dutch Wikipedia - to negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values.\nThe project made four major contributions. First, we articulated and demonstrated the effectiveness of a general method, including processes and best practices, in uncovering trade-offs, making trade-offs interpretable, and balancing trade-offs in machine learning models and applications. Second, we created techniques for visualizing and explaining the trade-offs. Third, we created, deployed, and evaluated social and technical innovations to address fundamental trade-offs between different system criteria. Fourth, we designed and implemented improvements to ORES, which will improve a wide variety of applications that rely on ORES and Wikipedia's content and community as a whole. Improving Wikipedia will positively influence billions of people who consume the Wikipedia content either directly or indirectly through other applications such as Google.\n\n \n\n\t\t\t\t\tLast Modified: 04/25/2023\n\n\t\t\t\t\tSubmitted by: Haiyi Zhu"
 }
}