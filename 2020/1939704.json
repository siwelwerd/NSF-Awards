{
 "awd_id": "1939704",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FAI: Auditing and Ensuring Fairness in Hard-to-Identify Settings",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2020-05-15",
 "awd_exp_date": "2023-04-30",
 "tot_intn_awd_amt": 381838.0,
 "awd_amount": 381838.0,
 "awd_min_amd_letter_date": "2019-12-23",
 "awd_max_amd_letter_date": "2019-12-23",
 "awd_abstract_narration": "The spread of artificial intelligence (AI) systems for high-stakes decision making gives rise to an urgent ethical and legal imperative to avoid discrimination and guarantee fairness with respect to protected classes. Example application domains include credit decisioning, personalized medicine, targeted policymaking, and sentence and bail setting. In these settings, fairness is often quantified by the decisions' disparate impacts on different groups. However, fundamental limits in the data available make both auditing disparities and eliminating them difficult or impossible. For instance, both in credit and insurance claims data, sensitive labels such as race are missing, and one never knows if a defendant detained pretrial would have fled. In both cases this missingness renders disparities unidentifiable. These identification issues not only arise in nearly every application where fairness is a concern, they also break most existing methods for fair AI and create an urgent gap between the theory and practice of fair AI.\r\n\r\nAddressing this gap, this project develops a robust theory and methodology for assessing and ensuring fairness in settings where fairness metrics are hard or impossible to pin down. Specifically, the project will develop: (a) fairness assessment methods that can reliably support credible conclusions in the face of fundamental identification limitations; (b) learning algorithms that robustly enforce fairness at the design stage even if fairness is unmeasurable. A key approach is recognizing the limits of identification and addressing it by considering the possible ranges of disparities that an algorithm may and may not induce in practice. The project identifies several discrete settings where fairness is hard or impossible to measure and that require separate treatment: unobserved protected class membership, personalized interventions, non-binary algorithmic outputs, decision-support algorithms, and non-ignorable selective labeling. By tackling these, the project stands to transform the assessment of disparity in real practical settings, where fairness is actually statistically difficult to pin down, and correspondingly impact how we ensure AI systems are fair and benefit everyone equally. This impact will also be directly achieved through direct collaborations with practitioners.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nathan",
   "pi_last_name": "Kallus",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nathan Kallus",
   "pi_email_addr": "kallus@cornell.edu",
   "nsf_id": "000727375",
   "pi_start_date": "2019-12-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "2 West Loop Rd",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100441501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "114Y00",
   "pgm_ele_name": "Fairness in Artificial Intelli"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 381838.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"group final-completion w-full text-token-text-primary border-b border-black/10 gizmo:border-0 dark:border-gray-900/50 gizmo:dark:border-0 bg-gray-50 gizmo:bg-transparent dark:bg-[#444654] gizmo:dark:bg-transparent\">\n<div class=\"p-4 justify-center text-base md:gap-6 md:py-6 m-auto\">\n<div class=\"flex flex-1 gap-4 text-base mx-auto md:gap-6 gizmo:gap-3 gizmo:md:px-5 gizmo:lg:px-1 gizmo:xl:px-5 md:max-w-2xl lg:max-w-[38rem] gizmo:md:max-w-3xl gizmo:lg:max-w-[40rem] gizmo:xl:max-w-[48rem] xl:max-w-3xl }\">\n<div class=\"relative flex w-[calc(100%-50px)] flex-col gizmo:w-full lg:w-[calc(100%-115px)] agent-turn\">\n<div class=\"flex-col gap-1 md:gap-3\">\n<div class=\"flex flex-grow flex-col gap-3 max-w-full\">\n<div class=\"min-h-[20px] flex flex-col items-start gap-3 whitespace-pre-wrap break-words overflow-x-auto\">\n<div class=\"markdown prose w-full break-words dark:prose-invert dark\">\n<p class=\"p1\">The goal of this project is to assess fairness in settings where fundamental limitations to what can be measured make exact measure impossible. The importance of this stems from the confluence of two issues: (1) AI systems are increasingly relied upon for making high-stakes decisions with material impact on people's lives and (2) the data available in these settings fundamentally fails to characterize this impact and its distribution. The outcomes of the project illuminate these fundamental limitations as well as equip stakeholders with tools to make credible assessments despite these and drive better, more trustworthy decisions.</p>\n<p class=\"p1\"><strong>Auditing and ensuring fairness in AI systems when protected class membership is unobserved</strong>. One major impediment to the assessment of disparate impact on different protected groups is the fact that group membership is often not<em> </em>observed. There may be many reasons for this missingness in practice, both legal, operational, and behavioral. For example, in the US financial service industry, lenders are not<em> </em>permitted to collect race and ethnicity information for non-mortgage products. A common stopgap is to impute group membership by matching information such as last name and zip code to external data such as the US Census. However, this is at best a guess. The project outcomes characterize exactly what are potential sources of bias in such assessments and what is exactly identifiable from the combination of such data. Moreover, the project outcomes provide tools to measure the tightest-possible bounds on disparities that are supported by the data, allowing for reliable conclusions about disparities.</p>\n<p class=\"p1\"><strong>Auditing and ensuring fairness in AI systems that maximize individual-level causal effects</strong>. Personalized targeting of limited resources are increasingly common, including specifically targeted interventions to prevent homelessness or combat disease and allocation of case management resources in child protection and in medicine. Both ethics and law compel such personalized interventions to be fair and avoid disparities in how they impact different groups. The project outcomes show how notions of equality of opportunity extend to this setting, allowing us to conceive quantitatively of such disparities. The project outcomes also show, however, that these disparities cannot be identified from data. This is because, while we can optimally target to maximize total benefit, we cannot observe who are the specific individuals who benefit from the intervention because we can only observe their outcome under either the intervention or its absence, but not both. The project outcomes provide tools to, nonetheless, make credible assessments and reliable conclusions by measuring the tightest-possible bounds on disparities that are supported by the data.&nbsp;</p>\n<p class=\"p1\"><strong>Auditing and ensuring fairness in AI systems that produce continuous risk scores used for downstream decision-making</strong>. Predictive risk scores support decision-making in high-stakes settings such as bail sentencing, triage and preventive care, and lending decisions. Since the binary decision is not determined by the algorithm itself, the question of how to measure fairness of the algorithm is less clear. The project outcomes phrase fairness in such settings in the context of bipartite ranking, allowing quantitative assessment. For example, in the context of credit decisioning, this focuses the question on how often individuals from one protected group who would default on a loan if credited are ranked as more credit worthy than individuals from another protected group who would not default, and vice versa. The project outcomes provide tools to measure, assess, visualize, and control such disparities.&nbsp;</p>\n<p class=\"p1\"><strong>Auditing and ensuring fairness in AI systems when data are censored</strong>. In many of the same settings where fairness is a concern, systematic censoring may make the training data unrepresentative of the whole population due to the fact that data is often collected under an existing decision policy. For example, in credit decisioning, data on loan default can only be collected on approved loans, but learned policies will be used for all incoming applications. The project outcomes characterizes precisely how such missingness affects standard fairness-enforcing algorithms as well as tools to measure and adjust for such missingness.</p>\n<p class=\"p1\"><strong>Auditing and targeting worst-affected groups</strong>. Experimentation can be used to assess average treatment effects of interventions. However, even when averages are positive, there is a risk that a significant subpopulation experiences a negative treatment effect. Since this group may not be observable, the project outcomes characterize the tightest bounds that experimental data can provide both on the average effect on the 10%-worst-affected group (or, 20% etc.) and on the fraction that experience a negative effect. The project outcomes provide tools to conduct robust inference on these bounds as well as on summaries of effects on multiple observable groups, especially when there are many, possibly infinitely many, groups. Beyond assessment, the project outcomes provide methods for efficiently learning automated decision-making systems that optimize for such worst-affected groups, especially in sequential decision settings where challenges arise because membership in a worst-affected group depends on the realization of a whole trajectory.</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/18/2023<br>\n\t\t\t\t\tModified by: Nathan&nbsp;Kallus</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\n\n\n\n\nThe goal of this project is to assess fairness in settings where fundamental limitations to what can be measured make exact measure impossible. The importance of this stems from the confluence of two issues: (1) AI systems are increasingly relied upon for making high-stakes decisions with material impact on people's lives and (2) the data available in these settings fundamentally fails to characterize this impact and its distribution. The outcomes of the project illuminate these fundamental limitations as well as equip stakeholders with tools to make credible assessments despite these and drive better, more trustworthy decisions.\nAuditing and ensuring fairness in AI systems when protected class membership is unobserved. One major impediment to the assessment of disparate impact on different protected groups is the fact that group membership is often not observed. There may be many reasons for this missingness in practice, both legal, operational, and behavioral. For example, in the US financial service industry, lenders are not permitted to collect race and ethnicity information for non-mortgage products. A common stopgap is to impute group membership by matching information such as last name and zip code to external data such as the US Census. However, this is at best a guess. The project outcomes characterize exactly what are potential sources of bias in such assessments and what is exactly identifiable from the combination of such data. Moreover, the project outcomes provide tools to measure the tightest-possible bounds on disparities that are supported by the data, allowing for reliable conclusions about disparities.\nAuditing and ensuring fairness in AI systems that maximize individual-level causal effects. Personalized targeting of limited resources are increasingly common, including specifically targeted interventions to prevent homelessness or combat disease and allocation of case management resources in child protection and in medicine. Both ethics and law compel such personalized interventions to be fair and avoid disparities in how they impact different groups. The project outcomes show how notions of equality of opportunity extend to this setting, allowing us to conceive quantitatively of such disparities. The project outcomes also show, however, that these disparities cannot be identified from data. This is because, while we can optimally target to maximize total benefit, we cannot observe who are the specific individuals who benefit from the intervention because we can only observe their outcome under either the intervention or its absence, but not both. The project outcomes provide tools to, nonetheless, make credible assessments and reliable conclusions by measuring the tightest-possible bounds on disparities that are supported by the data. \nAuditing and ensuring fairness in AI systems that produce continuous risk scores used for downstream decision-making. Predictive risk scores support decision-making in high-stakes settings such as bail sentencing, triage and preventive care, and lending decisions. Since the binary decision is not determined by the algorithm itself, the question of how to measure fairness of the algorithm is less clear. The project outcomes phrase fairness in such settings in the context of bipartite ranking, allowing quantitative assessment. For example, in the context of credit decisioning, this focuses the question on how often individuals from one protected group who would default on a loan if credited are ranked as more credit worthy than individuals from another protected group who would not default, and vice versa. The project outcomes provide tools to measure, assess, visualize, and control such disparities. \nAuditing and ensuring fairness in AI systems when data are censored. In many of the same settings where fairness is a concern, systematic censoring may make the training data unrepresentative of the whole population due to the fact that data is often collected under an existing decision policy. For example, in credit decisioning, data on loan default can only be collected on approved loans, but learned policies will be used for all incoming applications. The project outcomes characterizes precisely how such missingness affects standard fairness-enforcing algorithms as well as tools to measure and adjust for such missingness.\nAuditing and targeting worst-affected groups. Experimentation can be used to assess average treatment effects of interventions. However, even when averages are positive, there is a risk that a significant subpopulation experiences a negative treatment effect. Since this group may not be observable, the project outcomes characterize the tightest bounds that experimental data can provide both on the average effect on the 10%-worst-affected group (or, 20% etc.) and on the fraction that experience a negative effect. The project outcomes provide tools to conduct robust inference on these bounds as well as on summaries of effects on multiple observable groups, especially when there are many, possibly infinitely many, groups. Beyond assessment, the project outcomes provide methods for efficiently learning automated decision-making systems that optimize for such worst-affected groups, especially in sequential decision settings where challenges arise because membership in a worst-affected group depends on the realization of a whole trajectory.\n\n\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 10/18/2023\n\n\t\t\t\t\tSubmitted by: Nathan Kallus"
 }
}