{
 "awd_id": "1953813",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SaTC: CORE: Small: Attack-Agnostic Defenses against Adversarial Inputs in Learning Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2019-08-15",
 "awd_exp_date": "2022-01-31",
 "tot_intn_awd_amt": 386219.0,
 "awd_amount": 386219.0,
 "awd_min_amd_letter_date": "2019-10-25",
 "awd_max_amd_letter_date": "2021-06-04",
 "awd_abstract_narration": "Deep learning technologies hold great promise to revolutionize the way people live and work. However, deep learning systems are inherently vulnerable to adversarial inputs, which are maliciously crafted samples to trigger deep neural networks to misbehave, leading to disastrous consequences in security-critical applications. The fundamental challenges of defending against such attacks stem from their adaptive and variable nature: adversarial inputs are tailored to target deep neural networks, while crafting strategies vary greatly with concrete attacks. This project develops EagleEye, a universal, attack-agnostic defense framework that (i) works effectively against unseen attack variants, (ii) preserves predictive power of deep neural networks, (iii) complements existing defense mechanisms, and (iv) provides comprehensive diagnosis about potential risks in deep learning outputs.\r\n\r\nIn particular, EagleEye leverages a set of invariant properties underlying most attacks, including the \"minimality principle\": to maximize attack evasiveness, an adversarial input is generated by applying the minimum possible distortion to a legitimate input. By exploiting such properties in a principled manner, EagleEye effectively discriminates adversarial inputs (integrity checking) and even uncovers their correct outputs (truth recovery). The specific research tasks include: (i) identifying inherently distinct properties (differentiators) of legitimate and adversarial inputs, (ii) developing attack-agnostic adversarial input detection methods based on these differentiators, and (iii) analyzing possible countermeasures by adversaries to evade such defenses. This research not only facilitates the adoption of deep learning-powered systems and services, but also enlightens designing and implementing robust machine learning systems in general. New theories and systems developed in this project are integrated into undergraduate and graduate education and used to raise public awareness of the importance of machine learning security. More information about this project can be found at the project homepage: http://x-machine.github.io/project/eagleeye",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ting",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ting Wang",
   "pi_email_addr": "inbox.ting@gmail.com",
   "nsf_id": "000702128",
   "pi_start_date": "2019-10-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "110 Technology Center Building",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "025Z",
   "pgm_ref_txt": "SaTC: Secure and Trustworthy Cyberspace"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 386219.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The recent advances in deep learning techniques hold great promise for revolutionizing many sectors of our society. Yet, deep learning-powered systems are inherently vulnerable to adversarial attacks that craft malicious inputs to trigger neural network models to misbehave, which significantly hinders their use in security-critical domains. The challenges of defending against such attacks stem from their adaptive and variable nature: the adversarial inputs are tailored to target models, while the crafting strategies vary greatly with concrete attacks. This project aims to develop a universal, attack-agnostic defense framework that works effectively against unseen attack variants, preserves the predictive power of neural networks, complements existing defenses, and provides a comprehensive assessment of potential risks. Specifically, the project has produced the following intellectual products.</p>\n<p>1) In-depth understanding of fundamental vulnerabilities of neural network models ? We have designed a set of novel adversarial attacks, which complement existing ones, to evaluate the vulnerabilities of neural network models in various domains (e.g., images, natural language, graphs) and realistic settings (e.g., lane detection of Tesla Model S). We have found two interesting phenomena: (i) \"No free lunch\" - the improvement of attack resilience is often achieved at the cost of clean accuracy and (ii) \"No silver bullet\" - none of the existing defenses is robust against all the attacks. Further, we have found that using interpretability as a defense, which was previously believed to provide security assurance by involving humans in the loop, is also vulnerable to adversarial manipulations, due to its data-driven nature.</p>\n<p>2) Theoretical advances in invariant properties of adversarial attacks ? We have identified the \"minimality principle\" as one common property underlying many attacks: to maximize the attack evasiveness, the adversary seeks the minimum possible distortion to convert clean inputs to adversarial ones. If disobeying this principle, the attack may incur excessive perturbation, thereby reducing its evasiveness. In studying the adversary's possible countermeasure of combining adversarial attacks with other attacks, we have also revealed that there exists an intricate \"mutual reinforcement\" effect between adversarial attack and the trojaning attack, that is, leveraging one attack vector significantly amplifies the effectiveness of the other. Thus, to design effective defenses, it is crucial to account for such mutual reinforcement effects.</p>\n<p>3) Technical innovations for enforcing universal, attack-agnostic protection ? Leveraging our theoretical studies, we have developed a set of novel defenses applicable to different attack scenarios. In the black-box setting, in which the adversary only has query access to the target model, we have developed the first class of estimation models that infer the adversary's intent of adversarial attacks in a robust and prompt manner. In the white-box setting, in which the adversary has direct access to the target model, we have developed the first certification method for recurrent neural networks, which provides provable protection against adversarial attacks. We have further improved existing certification methods by considering the inherent asymmetry of adversarial perturbation and providing stronger robustness guarantees than the conventional symmetric counterparts.</p>\n<p>4) Development of attack-defense evaluation benchmarks ? We have developed DeepSec and TrojanZoo, two comprehensive platforms designed for evaluating adversarial and trojaning attacks/defenses, which are open-sourced and serve to facilitate future research.</p>\n<p>To date, the project has produced twenty-seven peer-reviewed publications in top-tier venues, with more papers in development, and has won two best paper awards. Four graduate and two undergraduate students have been trained in security and privacy, machine learning, and human-computer interactions. One Ph.D. student has finished his dissertation based on his work in this project. The outcomes of this project have been incorporated into the course and seminar materials and disseminated through conference presentations, invited talks, and media coverage.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/10/2022<br>\n\t\t\t\t\tModified by: Ting&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe recent advances in deep learning techniques hold great promise for revolutionizing many sectors of our society. Yet, deep learning-powered systems are inherently vulnerable to adversarial attacks that craft malicious inputs to trigger neural network models to misbehave, which significantly hinders their use in security-critical domains. The challenges of defending against such attacks stem from their adaptive and variable nature: the adversarial inputs are tailored to target models, while the crafting strategies vary greatly with concrete attacks. This project aims to develop a universal, attack-agnostic defense framework that works effectively against unseen attack variants, preserves the predictive power of neural networks, complements existing defenses, and provides a comprehensive assessment of potential risks. Specifically, the project has produced the following intellectual products.\n\n1) In-depth understanding of fundamental vulnerabilities of neural network models ? We have designed a set of novel adversarial attacks, which complement existing ones, to evaluate the vulnerabilities of neural network models in various domains (e.g., images, natural language, graphs) and realistic settings (e.g., lane detection of Tesla Model S). We have found two interesting phenomena: (i) \"No free lunch\" - the improvement of attack resilience is often achieved at the cost of clean accuracy and (ii) \"No silver bullet\" - none of the existing defenses is robust against all the attacks. Further, we have found that using interpretability as a defense, which was previously believed to provide security assurance by involving humans in the loop, is also vulnerable to adversarial manipulations, due to its data-driven nature.\n\n2) Theoretical advances in invariant properties of adversarial attacks ? We have identified the \"minimality principle\" as one common property underlying many attacks: to maximize the attack evasiveness, the adversary seeks the minimum possible distortion to convert clean inputs to adversarial ones. If disobeying this principle, the attack may incur excessive perturbation, thereby reducing its evasiveness. In studying the adversary's possible countermeasure of combining adversarial attacks with other attacks, we have also revealed that there exists an intricate \"mutual reinforcement\" effect between adversarial attack and the trojaning attack, that is, leveraging one attack vector significantly amplifies the effectiveness of the other. Thus, to design effective defenses, it is crucial to account for such mutual reinforcement effects.\n\n3) Technical innovations for enforcing universal, attack-agnostic protection ? Leveraging our theoretical studies, we have developed a set of novel defenses applicable to different attack scenarios. In the black-box setting, in which the adversary only has query access to the target model, we have developed the first class of estimation models that infer the adversary's intent of adversarial attacks in a robust and prompt manner. In the white-box setting, in which the adversary has direct access to the target model, we have developed the first certification method for recurrent neural networks, which provides provable protection against adversarial attacks. We have further improved existing certification methods by considering the inherent asymmetry of adversarial perturbation and providing stronger robustness guarantees than the conventional symmetric counterparts.\n\n4) Development of attack-defense evaluation benchmarks ? We have developed DeepSec and TrojanZoo, two comprehensive platforms designed for evaluating adversarial and trojaning attacks/defenses, which are open-sourced and serve to facilitate future research.\n\nTo date, the project has produced twenty-seven peer-reviewed publications in top-tier venues, with more papers in development, and has won two best paper awards. Four graduate and two undergraduate students have been trained in security and privacy, machine learning, and human-computer interactions. One Ph.D. student has finished his dissertation based on his work in this project. The outcomes of this project have been incorporated into the course and seminar materials and disseminated through conference presentations, invited talks, and media coverage.\n\n \n\n\t\t\t\t\tLast Modified: 04/10/2022\n\n\t\t\t\t\tSubmitted by: Ting Wang"
 }
}