{
 "awd_id": "2007041",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Enabling Accessibility of Virtual Reality for Persons with Balance Impairments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2024-07-31",
 "tot_intn_awd_amt": 497121.0,
 "awd_amount": 497121.0,
 "awd_min_amd_letter_date": "2020-08-03",
 "awd_max_amd_letter_date": "2020-08-03",
 "awd_abstract_narration": "Virtual Reality (VR) renders 3D computer graphics and sound in head mounted displays (HMDs) to make users feel as though they are in another place. Although consumer level HMDs are affordable enough to benefit a large user base, these devices are not accessible for many persons with balance impairments, such as elderly persons, persons with multiple sclerosis, Parkinson's, or stroke. Currently, immersive VR applications, such as education, physical fitness, rehabilitation, and entertainment, are not accessible to users with balance impairments. To address this need, the team of researchers will use a combination of visual, audio, and touch feedback to improve balance in VR. Although these methods have been effectively used in reality, they have minimally been tested in VR and it is unknown what the most effective combination of feedbacks are. If VR imbalance issues can be mitigated, persons with balance impairments may be able to more readily benefit from consumer VR. The project will also impact community outreach for persons with disabilities in that the investigators plan to give lectures at support groups and symposia on disabilities that will further educate persons with disabilities about VR. The project will also increase involvement of underrepresented minority students in research, because more than 59% of The University of Texas at San Antonio's students come from groups underrepresented in higher education. \r\n\r\nThe overall objective of this project is to investigate multimodal techniques to improve balance in VR for persons with balance impairments, such as elderly persons and persons with neurological disorders. Based on the research team's preliminary studies, the central hypothesis is: sensory cues from a combination of multimodal feedback techniques, such as a visual static rest frame, auditory feedback on posture, and tactile feedback on posture from a vibro-tactile belt, will help to mitigate the imbalance effects, depending upon the cause of the imbalance. The idea will be to separately group the target populations due to several factors (e.g., clinical condition, age) and interpret the balance impaired human participants' experiences in VR based on that grouping. Ultimately, this research will result in a set of validated, gait-in-VR data and open-source tools that will make VR more accessible for persons with balance impairments, improving their quality of life. The project is creative and original because there has been minimal previous research into why VR causes imbalance and how it affects persons with balance impairments and hence, few works have explored solutions to this problem. The project is potentially transformative because it may disrupt accepted theories and perspectives of user interaction in VR, especially for persons with balance impairments.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Quarles",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "John P Quarles",
   "pi_email_addr": "John.Quarles@utsa.edu",
   "nsf_id": "000559237",
   "pi_start_date": "2020-08-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alberto",
   "pi_last_name": "Cordova",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alberto Cordova",
   "pi_email_addr": "alberto.cordova@utsa.edu",
   "nsf_id": "000760798",
   "pi_start_date": "2020-08-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at San Antonio",
  "inst_street_address": "1 UTSA CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SAN ANTONIO",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "2104584340",
  "inst_zip_code": "782491644",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "TX20",
  "org_lgl_bus_name": "THE UNIVERSITY OF TEXAS AT SAN ANTONIO",
  "org_prnt_uei_num": "U44ZMVYU52U6",
  "org_uei_num": "U44ZMVYU52U6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at San Antonio",
  "perf_str_addr": "One UTSA Circle",
  "perf_city_name": "San Antonio",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "782491644",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "TX20",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 497121.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research increased the accessibility of immersive virtual reality (VR) using several types of sensory feedback - audio, visual, and touch (vibration) on a person&rsquo;s balance. VR users often experience imbalance and walking patterns disturbances, which is a major barrier to accessibility of VR for all, especially for people with balance impairments (BI). To solve this issue, this research developed and evaluated sixteen novel assistive feedback techniques (four audio, four vibration-based, four visual, and four techniques that&nbsp; combined multiple senses) to improve balance and walking &nbsp;while immersed in VR. The research team conducted a series of different studies with a total of 431 participants (178 with BI due to multiple sclerosis, 75 with BI due to diabetes, and 178 without BI) d to evaluate the developed techniques. Participants performed three types of tasks - standing visual exploration, standing reach and grasp, and timed walking tasks in the real world and identical tasks in virtual environments. Results showed that combinations of sensory feedback improved balance and walking significantly more than any single sensory feedback technique for all participants. However, mental load and fatigue levels were significantly higher (p &lt;.001) with combined feedback.</p>\n<p>These results contribute to a deeper understanding of the efficacy of different assistive feedback methods (auditory, vibrotactile, visual, and combined) for maintaining balance and walking in a virtual environment with head mounted displays. VR researchers and developers could apply these techniques to design more accessible virtual environments for people with and without balance impairments and make VR universally usable.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 08/01/2024<br>\nModified by: John&nbsp;P&nbsp;Quarles</p></div>\n<div class=\"porSideCol\"\n><div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2024/2007041/2007041_10691987_1722538071306_pg1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2007041/2007041_10691987_1722538071306_pg1--rgov-800width.jpg\" title=\"Reach and grab task in VR\"><img src=\"/por/images/Reports/POR/2024/2007041/2007041_10691987_1722538071306_pg1--rgov-66x44.jpg\" alt=\"Reach and grab task in VR\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A participant reaches and grabs cubes in VR.</div>\n<div class=\"imageCredit\">John Quarles</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">John&nbsp;P&nbsp;Quarles\n<div class=\"imageTitle\">Reach and grab task in VR</div>\n</div>\n</li><li>\n<a href=\"/por/images/Reports/POR/2024/2007041/2007041_10691987_1722537996721_reach_bi--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2024/2007041/2007041_10691987_1722537996721_reach_bi--rgov-800width.jpg\" title=\"Results of reaching task in VR\"><img src=\"/por/images/Reports/POR/2024/2007041/2007041_10691987_1722537996721_reach_bi--rgov-66x44.jpg\" alt=\"Results of reaching task in VR\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This shows the effect of several types of audio feedback. The most effective types scored a lower Center of Pressure velocity.</div>\n<div class=\"imageCredit\">John Quarles</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">John&nbsp;P&nbsp;Quarles\n<div class=\"imageTitle\">Results of reaching task in VR</div>\n</div>\n</li></ul>\n</div>\n</div></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis research increased the accessibility of immersive virtual reality (VR) using several types of sensory feedback - audio, visual, and touch (vibration) on a persons balance. VR users often experience imbalance and walking patterns disturbances, which is a major barrier to accessibility of VR for all, especially for people with balance impairments (BI). To solve this issue, this research developed and evaluated sixteen novel assistive feedback techniques (four audio, four vibration-based, four visual, and four techniques that combined multiple senses) to improve balance and walking while immersed in VR. The research team conducted a series of different studies with a total of 431 participants (178 with BI due to multiple sclerosis, 75 with BI due to diabetes, and 178 without BI) d to evaluate the developed techniques. Participants performed three types of tasks - standing visual exploration, standing reach and grasp, and timed walking tasks in the real world and identical tasks in virtual environments. Results showed that combinations of sensory feedback improved balance and walking significantly more than any single sensory feedback technique for all participants. However, mental load and fatigue levels were significantly higher (p \n\n\nThese results contribute to a deeper understanding of the efficacy of different assistive feedback methods (auditory, vibrotactile, visual, and combined) for maintaining balance and walking in a virtual environment with head mounted displays. VR researchers and developers could apply these techniques to design more accessible virtual environments for people with and without balance impairments and make VR universally usable.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 08/01/2024\n\n\t\t\t\t\tSubmitted by: JohnPQuarles\n"
 }
}