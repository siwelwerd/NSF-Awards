{
 "awd_id": "2015363",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Theoretical Guarantees of Statistical Methodologies Involving Nonconvex Objectives and the Difference-Of-Convex-Functions Algorithms",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2020-08-01",
 "awd_exp_date": "2023-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2020-06-16",
 "awd_max_amd_letter_date": "2020-06-16",
 "awd_abstract_narration": "This project will extend the statistical literature that involves nonconvex optimization to contemporary models. In many contemporary machine learning and/or artificial intelligence applications, deep learning and relevant neural network models are utilized. Extending these theories to other contemporary frameworks can potentially lead to a theoretical foundation for modern techniques such as deep learning. The research project has great potential to make a significant impact on the broad scientific community, who have the needs of performing inferences for their enormous data. Besides scholarly publications and presentations, the research will lead to new teaching modules in statistics and machine learning courses. Ph.D. students will be supported and exposed to asymptotic theory and computational algorithms. New toolboxes will be developed and made available online. Packages are developed so that engineering students (including undergraduates) at Georgia Tech and other universities can use them in their course projects (for example, the undergraduate senior design projects at the School of Industrial and Systems Engineering at Georgia Tech). The PI has organized many influential workshops in the past, including one on the foundation of deep learning, and will continue doing so. \r\n\r\nSpecific aims include the following. The research work will extend the theory on the statistical properties of potentially fully neural network models to some other neural network models under different structures, such as the convolutional neural networks, to explore the relation between the inferential property and the neural network architecture. The project is to derive the theoretical guarantees of statistical estimators that are based on nonconvex optimization in more general settings. The PI will explore the possibility to carrying out similar analysis in neural network-based models. Statistical model selection can be utilized in identification of partial differential equations. The project is to establish the corresponding statistical theory and uncover the related practical implication.  A set of open-source software products along with related documentation will be generated, to make our work conveniently reproducible. Existing tools (such as GitHub.com or equivalents) will be utilized to disseminate these tools. The applicability and need of the new methods will be explored in a wide spectrum of application domains. Inference techniques with nonconvex objective functions is a fundamental problem in many contemporary techniques, including the neural network based deep learning methodology. This project will contribute to this research. There are evident societal needs for inference from large datasets, and the results of this project can have many applications. The project will contribute to the statistical literature by exploring a new research frontier in statistical sciences. Our work is interdisciplinary and can bridge the communities of optimization and statistics.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Xiaoming",
   "pi_last_name": "Huo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiaoming Huo",
   "pi_email_addr": "xiaoming@isye.gatech.edu",
   "nsf_id": "000276730",
   "pi_start_date": "2020-06-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue, NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project extends the statistical literature involving nonconvex optimization to contemporary models. Many contemporary machine learning and/or artificial intelligence applications utilize deep learning and relevant neural network models. These models are typically trained via some (potentially stochastic) greed algorithms to optimize nonconvex objective functions. Despite significant recent developments, the relevant statistical methodology and theory for solutions that are based on nonconvex optimization are underdeveloped. At the same time, nonconvex penalties in specific forms are well-studied in the literature for sparse estimation (particularly in regression). At the same time, recent work has revealed that nearly all existing non-convex penalties can be represented as difference-of-convex (DC) functions, which are the differences between two convex functions, while the function itself may be nonconvex. There is a large amount of existing literature on the corresponding optimization problems. Efficient numerical solutions have been proposed. Under the DC framework, directional-stationary (d-stationary) solutions are considered and usually not unique. Based on some recent works of theirs, the PI shows that under some conditions, a specific subset of d-stationary solutions (in an optimization problem with a DC objective) has some ideal statistical properties, namely, estimation consistency, model selection consistency, and statistical efficiency. Their work shows that DC is an excellent framework to offer a unified approach to these existing works involving the nonconvex penalty. Extending these theories to other contemporary frameworks could provide a theoretical foundation for modern techniques such as deep learning.</p>\n<p>&nbsp;</p>\n<p>This grant partially supported multiple publications. They can be found on my publication website (https://sites.gatech.edu/xiaoming-huo/publications-2/).&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 12/06/2023<br>\nModified by: Xiaoming&nbsp;Huo</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project extends the statistical literature involving nonconvex optimization to contemporary models. Many contemporary machine learning and/or artificial intelligence applications utilize deep learning and relevant neural network models. These models are typically trained via some (potentially stochastic) greed algorithms to optimize nonconvex objective functions. Despite significant recent developments, the relevant statistical methodology and theory for solutions that are based on nonconvex optimization are underdeveloped. At the same time, nonconvex penalties in specific forms are well-studied in the literature for sparse estimation (particularly in regression). At the same time, recent work has revealed that nearly all existing non-convex penalties can be represented as difference-of-convex (DC) functions, which are the differences between two convex functions, while the function itself may be nonconvex. There is a large amount of existing literature on the corresponding optimization problems. Efficient numerical solutions have been proposed. Under the DC framework, directional-stationary (d-stationary) solutions are considered and usually not unique. Based on some recent works of theirs, the PI shows that under some conditions, a specific subset of d-stationary solutions (in an optimization problem with a DC objective) has some ideal statistical properties, namely, estimation consistency, model selection consistency, and statistical efficiency. Their work shows that DC is an excellent framework to offer a unified approach to these existing works involving the nonconvex penalty. Extending these theories to other contemporary frameworks could provide a theoretical foundation for modern techniques such as deep learning.\n\n\n\n\n\nThis grant partially supported multiple publications. They can be found on my publication website (https://sites.gatech.edu/xiaoming-huo/publications-2/).\n\n\n\t\t\t\t\tLast Modified: 12/06/2023\n\n\t\t\t\t\tSubmitted by: XiaomingHuo\n"
 }
}