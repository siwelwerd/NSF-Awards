{
 "awd_id": "2007757",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: RI: Small: Computationally Efficient Approximation of Stationary Points in Convex and Min-Max Optimization",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922182",
 "po_email": "pbrass@nsf.gov",
 "po_sign_block_name": "Peter Brass",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 350000.0,
 "awd_amount": 350000.0,
 "awd_min_amd_letter_date": "2020-07-31",
 "awd_max_amd_letter_date": "2020-07-31",
 "awd_abstract_narration": "Optimization permeates almost every aspect of life, from natural selection and evolution to technological and economic development. Within modern data science, optimization algorithms are the core engine for finding patterns in the data, creating models that explain and mimic them, and making predictions. The primary goal of this project is to advance the theoretical foundations of optimization and leverage the obtained insights to develop new algorithms that are broadly applicable, adaptive to different data models, and scalable, so that they can be applied to the ever-more ambitious data-science applications. One of the guiding principles for the development of theoretical frameworks in this project are parallels between optimization algorithms and laws, such as the principle of least action, governing the behavior of physical systems. \r\n\r\nMore concretely, the goal of this project is to further the understanding of how fast it is possible for optimization algorithms to converge to stationary points, defined as the points with small gradient norms. In convex optimization, one of the most fundamental facts is that every stationary point is also a global function minimum. However, the problem of efficiently computing near-stationary points is quite different from the problem of efficiently approximating the function minima, and methods that exhibit optimal convergence rates under one of the criteria do not in general exhibit optimal convergence rates under both. In particular, Nesterov\u2019s accelerated gradient method is iteration-complexity-optimal in terms of minimizing smooth (gradient-Lipschitz) convex functions, but suboptimal in terms of finding their near-stationary points. While the complexity of minimizing convex functions is well-understood, much less is known about the complexity of finding near-stationary points. This troubling gap in understanding causes severe algorithmic limitations not only for general-purpose optimization algorithms, but also in a number of application areas. The primary focus of this project is to close this gap by developing a general framework for the analysis of convergence to stationary points in convex optimization and its generalizations, leveraging technical tools from dynamical systems, monotone-operator theory, and fixed-point theory.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jelena",
   "pi_last_name": "Diakonikolas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jelena Diakonikolas",
   "pi_email_addr": "jdiakonikola@wisc.edu",
   "nsf_id": "000811478",
   "pi_start_date": "2020-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1210 W Dayton St",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061613",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 350000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project studied convergence of first-order (gradient or \"slope\"-based) methods in minimization and min-max optimization problems in terms of stationarity-based guarantees, focusing on rigorous (gradient oracle-based) complexity guarantees. The \"stationarity\" here can be understood as the stability property of a solution, where the cost of a solution cannot be significantly reduced by making slight moves in any direction.<br /><br />The motivation for studying such problems stemmed from (1) learning theory applications, where a need for such guarantees arises naturally from properties of the studied problems, (2) adversarial training in machine learning applications, and (3) utilizing the objective function's local growth properties to speed up the algorithm while accounting for the specific growth properties typically being unknown to the algorithm.<br /><br />The project has resulted in new mathematical techniques and frameworks for the analysis of optimization algorithms in terms of stationarity-based guarantees, addressing a broad range of problems. These new techniques helped uncover the mechanisms that drive the algorithm convergence to stationary points, based on trading off different types of convergence, exploiting certain duality properties between algorithms, or leveraging complementary growth properties in related problem formulations. These new mathematical techniques enabled the design of new optimization algorithms with rigorous convergence guarantees, which were complemented by lower bounds demonstrating that the obtained guarantees are essentially unimprovable. The project further established algorithmic connections between attaining stationarity-based guarantees in min-max optimization and solving fixed point equations (f(x) = x). As a result, the project led to new algorithmic results for both problem types, extending the knowledge of algorithmic possibilities for these problems in terms of the general problem characteristics such as monotonicity, stochasticity, and decomposability. Further, the stationarity-based guarantees studied in the project served as a guiding principle for the development of restart-based strategies that were demonstrated to speed up the convergence of algorithms both in theory and in practice, over broad classes of optimization problems. Many of the ideas developed in the project carried over to the more challenging structured nonconvex and non-monotone optimization settings. Finally, the ideas and principles developed within the project led to new results in learning theory, addressing fundamental regression and classification tasks. In summary, the project has advanced the theory of continuous, gradient-based optimization, has had impact on learning theory and machine learning, and has led to practical algorithmic speedups based on restarting strategies.<br /><br />Several graduate and undergraduate students were trained and mentored throughout the course of this project, with research typically leading to publications in top optimization, theory, and machine learning venues and various opportunities for presenting results to broad research communities. The results from the project were disseminated using public repositories and research presentations delivered by the PI and her students. Some of the ideas and results developed within this project were integrated in the PI's core graduate-level course in nonlinear optimization.</p><br>\n<p>\n Last Modified: 03/25/2024<br>\nModified by: Jelena&nbsp;Diakonikolas</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThis project studied convergence of first-order (gradient or \"slope\"-based) methods in minimization and min-max optimization problems in terms of stationarity-based guarantees, focusing on rigorous (gradient oracle-based) complexity guarantees. The \"stationarity\" here can be understood as the stability property of a solution, where the cost of a solution cannot be significantly reduced by making slight moves in any direction.\n\nThe motivation for studying such problems stemmed from (1) learning theory applications, where a need for such guarantees arises naturally from properties of the studied problems, (2) adversarial training in machine learning applications, and (3) utilizing the objective function's local growth properties to speed up the algorithm while accounting for the specific growth properties typically being unknown to the algorithm.\n\nThe project has resulted in new mathematical techniques and frameworks for the analysis of optimization algorithms in terms of stationarity-based guarantees, addressing a broad range of problems. These new techniques helped uncover the mechanisms that drive the algorithm convergence to stationary points, based on trading off different types of convergence, exploiting certain duality properties between algorithms, or leveraging complementary growth properties in related problem formulations. These new mathematical techniques enabled the design of new optimization algorithms with rigorous convergence guarantees, which were complemented by lower bounds demonstrating that the obtained guarantees are essentially unimprovable. The project further established algorithmic connections between attaining stationarity-based guarantees in min-max optimization and solving fixed point equations (f(x) = x). As a result, the project led to new algorithmic results for both problem types, extending the knowledge of algorithmic possibilities for these problems in terms of the general problem characteristics such as monotonicity, stochasticity, and decomposability. Further, the stationarity-based guarantees studied in the project served as a guiding principle for the development of restart-based strategies that were demonstrated to speed up the convergence of algorithms both in theory and in practice, over broad classes of optimization problems. Many of the ideas developed in the project carried over to the more challenging structured nonconvex and non-monotone optimization settings. Finally, the ideas and principles developed within the project led to new results in learning theory, addressing fundamental regression and classification tasks. In summary, the project has advanced the theory of continuous, gradient-based optimization, has had impact on learning theory and machine learning, and has led to practical algorithmic speedups based on restarting strategies.\n\nSeveral graduate and undergraduate students were trained and mentored throughout the course of this project, with research typically leading to publications in top optimization, theory, and machine learning venues and various opportunities for presenting results to broad research communities. The results from the project were disseminated using public repositories and research presentations delivered by the PI and her students. Some of the ideas and results developed within this project were integrated in the PI's core graduate-level course in nonlinear optimization.\t\t\t\t\tLast Modified: 03/25/2024\n\n\t\t\t\t\tSubmitted by: JelenaDiakonikolas\n"
 }
}