{
 "awd_id": "2023998",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: FND: Using Multi-Modal Data to Make Robotic Grasp Algorithms Aware of Human Preferences for Safe Collaborative Robot-Human Handover Interactions with Novel Objects",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924702",
 "po_email": "cye@nsf.gov",
 "po_sign_block_name": "Cang Ye",
 "awd_eff_date": "2020-10-15",
 "awd_exp_date": "2024-09-30",
 "tot_intn_awd_amt": 304935.0,
 "awd_amount": 310935.0,
 "awd_min_amd_letter_date": "2020-09-08",
 "awd_max_amd_letter_date": "2022-05-18",
 "awd_abstract_narration": "This project contributes advancements in promoting safe collaborative robot-to-human handovers by making robots with manipulator arms aware of human preferences for interactions with objects. In environments such as healthcare facilities, warehousing, retail, engine repair, and aircraft assembly, where robots may be expected to collaborate with humans for successful accomplishment of tasks, it is essential that robotic manipulators hand over objects such that people can optimally hold them, without fear of the object falling or the person being injured by the gripper or arm, and without the inconvenience of the object being unreachable. To enable safe handovers, the project will provide algorithms that use data on human interactions with objects captured from multiple viewpoints to automatically predict preferred locations of human grasp on objects, optimal orientation and distance of the object from the person, and safe point of release of the object by manipulator grippers. The research team will reach out to two-year and four-year colleges with limited technological opportunities in the North Country to provide research opportunities to women and students from underrepresented communities.\r\n\r\nThe project advances research in ubiquitous co-robots by providing holistic fine-grained insight through multi-modal sensing on natural behaviors of people as they interact with each other and with objects in their environments. The project accomplishes three objectives to address the gap on propagating understanding of human handover preferences to large collections of novel in-the-wild objects for customizability of co-robots to new environments. First, the research team will collect a large multi-viewpoint multi-modal dataset on two-person handovers and perform empirical analysis of the collected data to understand preferences on hold locations, end pose, and release point using subject ratings of object presentations. Modalities used will consist of depth cameras to acquire understanding on object geometry and spatial relationships, and thermal cameras to analyze locations of human contact based on heat transferred to object surfaces. This work will provide a quantitative decomposition of human preferences for handover parameters in terms of geometric form and functionality of objects. Second, the team will create perception algorithms based on probabilistic models to perform prediction of handover parameters ranked in order of preference using depth images of objects as input. This work enables equipping co-robots with human-like awareness of diversity in preferences, and the priorities that people assign to interactions. Third, the team will provide robotic manipulators that use the trained perception algorithms to perform handover manipulations on novel objects while being aware of human behavior. Successful accomplishment of the project activities will enable rapid propagation of robotic manipulators aware of human handover behavior to new objects and environments for enhanced social acceptability of co-robots.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Natasha",
   "pi_last_name": "Banerjee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Natasha Banerjee",
   "pi_email_addr": "natasha.banerjee@wright.edu",
   "nsf_id": "000703066",
   "pi_start_date": "2020-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sean",
   "pi_last_name": "Banerjee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sean Banerjee",
   "pi_email_addr": "sean.banerjee@wright.edu",
   "nsf_id": "000703063",
   "pi_start_date": "2020-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Clarkson University",
  "inst_street_address": "8 CLARKSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "POTSDAM",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "3152686475",
  "inst_zip_code": "136761401",
  "inst_country_name": "United States",
  "cong_dist_code": "21",
  "st_cong_dist_code": "NY21",
  "org_lgl_bus_name": "CLARKSON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "SL2PF6R7MRN1"
 },
 "perf_inst": {
  "perf_inst_name": "Clarkson University",
  "perf_str_addr": "8 Clarkson Avenue",
  "perf_city_name": "Potsdam",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "136761401",
  "perf_ctry_code": "US",
  "perf_cong_dist": "21",
  "perf_st_cong_dist": "NY21",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 304935.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 6000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project has been to perform a quantified analysis of how humans hand over objects to each other, so as to inform robots on conducting safe and effective human-robot handover (HRH). The project outcomes include the following:</p>\r\n<ol>\r\n<li>We have created the Human-Object-Human (HOH) Handover Dataset, that contains data from 40 participants engaging in human-to-human (H2H) handover, covering 136 objects (20 participants interact with 68 and the other 20 with the other 68). A total of 2,720 interactions were recorded. The data has been recorded using multi-view RGB-D cameras. Our team has annotated the data to contain, among other information, segments of giver hands, receiver hands, and objects, high-resolution scanned 3D models of the objects, key events of interest (giver grasp, transfer, and receiver release), and 6 degree of freedom pose of objects during the handover. The dataset is the largest in object count.</li>\r\n<li>We have used H2H data to conduct analyses on the timing relationships between giver and receiver motions, noting that most unprompted handovers tend to have receivers being proactive. Through an analysis of H2H data, we have also noted that giver and receiver agreement in grasp location is aligned with receivers mimicking the transfer orientation presented by the giver during a receiver-performed demonstration handover.</li>\r\n<li>We have provided AI algorithms to automatically generate hypotheses of giver grasp on an object's surface, receiver's grasp on the object given knowledge of the giver grasp, the orientation of the object at the point of transfer, as well as the trajectory of the receiver given the trajectory of the giver. These algorithms are foundational to inform robots where to grasp, how to pose an object at transfer, and where not to grasp for safe and effective human-to-robot or robot-to-human transfer. Our latest giver grasp hypothesis algorithm predicts multiple hypotheses of grasp to equip robots with the multiple ways in which humans are likely to interact with everyday objects.</li>\r\n<li>We have conducted multiple implementations of robot-to-human (R2H) handover using a Kinova Gen3 robot funded by this grant. Our R2H handover implementation directly mimics the transfer pose and trajectory of human givers from HOH, and we find that human receivers in human subject studies prefer HOH-provided pose over a randomly presented pose. We also provide R2H handovers for robotic manipulators where the robot's grasp is automatically driven to be more toward where human givers are likely to grasp. For this, we equipped our robot with an AI algorithm that enhanced traditional AI for robotic grasp with a prediction of the mean location of a human's grasp. Our R2H handovers address the concern of when a robot should release an object to a human receiver by using surface electromyography sensors attached to the human's wrist, to detect the amount of force applied by a person during receipt by measuring the quantity of muscle activation. The robot releases its gripper when the activation exceeds a threshold. These implementations now make it possible to have robots that engage in human-aware grasp, manipulation, and transfer of objects to human receivers. </li>\r\n<li>We have published findings, algorithms, and robotic implementations in multiple academic venues, including NeurIPS 2023, IEEE ARSO 2023, IEEE IRC 2023, IEEE AIxVR 2024, and IEEE RO-MAN 2024. We have one paper accepted to the IEEE ICRA 2025. We have conducted demonstrations of our R2H implementations at venues such as CVPR 2024, RO-MAN 2024, and IEEE HUMANOIDS 2024.</li>\r\n<li>Our work, including HOH, studies of human-human handover data, and algorithms and implementations for human-robot handover, have impact in areas where robots are expected to perform close physical interactions with humans. Knowledge of how humans hand objects to each other, where humans hold objects, and how they expect givers and receivers to behave is important for collaborative robots in industrial environments to hand tools to or receiver objects from humans, and for hospital and home healthcare robots that may need to hand items such as medicine bottles, medical equipment, or items of daily living to caregivers or clients.</li>\r\n<li>The project provided opportunities to multiple graduate and undergraduate students for professional development. The three graduate students funded by the project, and all undergraduate students supported by the project through direct sponsorship from the grant or an REU supplement, and nearly all undergraduate students who worked on the project outcomes through directed study and undergraduate research have been U.S. citizens. The project enabled students to strengthen their resumes by engaging in activities related to data collection, annotation, AI algorithm development, and robotic implementations. Students received internships and full-time opportunities at various companies, and pursued company-sponsored MS degrees. For some students, the project was critical to provide opportunities at a time when they were not able to find internships. One undergraduate student funded by the project is now pursuing a PhD in the area of AI-enabled human-robot interaction.</li>\r\n</ol><br>\n<p>\n Last Modified: 03/04/2025<br>\nModified by: Natasha&nbsp;Banerjee</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe goal of this project has been to perform a quantified analysis of how humans hand over objects to each other, so as to inform robots on conducting safe and effective human-robot handover (HRH). The project outcomes include the following:\r\n\r\nWe have created the Human-Object-Human (HOH) Handover Dataset, that contains data from 40 participants engaging in human-to-human (H2H) handover, covering 136 objects (20 participants interact with 68 and the other 20 with the other 68). A total of 2,720 interactions were recorded. The data has been recorded using multi-view RGB-D cameras. Our team has annotated the data to contain, among other information, segments of giver hands, receiver hands, and objects, high-resolution scanned 3D models of the objects, key events of interest (giver grasp, transfer, and receiver release), and 6 degree of freedom pose of objects during the handover. The dataset is the largest in object count.\r\nWe have used H2H data to conduct analyses on the timing relationships between giver and receiver motions, noting that most unprompted handovers tend to have receivers being proactive. Through an analysis of H2H data, we have also noted that giver and receiver agreement in grasp location is aligned with receivers mimicking the transfer orientation presented by the giver during a receiver-performed demonstration handover.\r\nWe have provided AI algorithms to automatically generate hypotheses of giver grasp on an object's surface, receiver's grasp on the object given knowledge of the giver grasp, the orientation of the object at the point of transfer, as well as the trajectory of the receiver given the trajectory of the giver. These algorithms are foundational to inform robots where to grasp, how to pose an object at transfer, and where not to grasp for safe and effective human-to-robot or robot-to-human transfer. Our latest giver grasp hypothesis algorithm predicts multiple hypotheses of grasp to equip robots with the multiple ways in which humans are likely to interact with everyday objects.\r\nWe have conducted multiple implementations of robot-to-human (R2H) handover using a Kinova Gen3 robot funded by this grant. Our R2H handover implementation directly mimics the transfer pose and trajectory of human givers from HOH, and we find that human receivers in human subject studies prefer HOH-provided pose over a randomly presented pose. We also provide R2H handovers for robotic manipulators where the robot's grasp is automatically driven to be more toward where human givers are likely to grasp. For this, we equipped our robot with an AI algorithm that enhanced traditional AI for robotic grasp with a prediction of the mean location of a human's grasp. Our R2H handovers address the concern of when a robot should release an object to a human receiver by using surface electromyography sensors attached to the human's wrist, to detect the amount of force applied by a person during receipt by measuring the quantity of muscle activation. The robot releases its gripper when the activation exceeds a threshold. These implementations now make it possible to have robots that engage in human-aware grasp, manipulation, and transfer of objects to human receivers. \r\nWe have published findings, algorithms, and robotic implementations in multiple academic venues, including NeurIPS 2023, IEEE ARSO 2023, IEEE IRC 2023, IEEE AIxVR 2024, and IEEE RO-MAN 2024. We have one paper accepted to the IEEE ICRA 2025. We have conducted demonstrations of our R2H implementations at venues such as CVPR 2024, RO-MAN 2024, and IEEE HUMANOIDS 2024.\r\nOur work, including HOH, studies of human-human handover data, and algorithms and implementations for human-robot handover, have impact in areas where robots are expected to perform close physical interactions with humans. Knowledge of how humans hand objects to each other, where humans hold objects, and how they expect givers and receivers to behave is important for collaborative robots in industrial environments to hand tools to or receiver objects from humans, and for hospital and home healthcare robots that may need to hand items such as medicine bottles, medical equipment, or items of daily living to caregivers or clients.\r\nThe project provided opportunities to multiple graduate and undergraduate students for professional development. The three graduate students funded by the project, and all undergraduate students supported by the project through direct sponsorship from the grant or an REU supplement, and nearly all undergraduate students who worked on the project outcomes through directed study and undergraduate research have been U.S. citizens. The project enabled students to strengthen their resumes by engaging in activities related to data collection, annotation, AI algorithm development, and robotic implementations. Students received internships and full-time opportunities at various companies, and pursued company-sponsored MS degrees. For some students, the project was critical to provide opportunities at a time when they were not able to find internships. One undergraduate student funded by the project is now pursuing a PhD in the area of AI-enabled human-robot interaction.\r\n\t\t\t\t\tLast Modified: 03/04/2025\n\n\t\t\t\t\tSubmitted by: NatashaBanerjee\n"
 }
}