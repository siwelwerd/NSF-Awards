{
 "awd_id": "1955246",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Two-dimensional Synaptic Array for Advanced Hardware Acceleration of Deep Neural Networks",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Prem Chahal",
 "awd_eff_date": "2020-09-01",
 "awd_exp_date": "2024-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2020-05-05",
 "awd_max_amd_letter_date": "2020-05-05",
 "awd_abstract_narration": "Nontechnical:\r\n\r\nThe big data revolution has created a critical need for new computing paradigms to efficiently extract valuable information from large datasets. In existing computing systems, data is constantly transferred between the computation and memory units. This so-called memory bottleneck limits their energy efficiency and speed. In contrast, computation and memory in the human brain (neurons and synapses) are closely and densely interconnected. This gives rise to the brain\u2019s extremely low power consumption at ~20W. Inspired by the brain, neuromorphic computing and artificial neural networks have recently attracted immense interest. In particular, deep neural networks (DNNs) can execute complex processing tasks such as pattern recognition and image reconstruction. However, DNNs are computationally intensive and power hungry. This makes it impractical for them to be scaled up to the level of the complexity for true artificial intelligence (AI). In this project, the team will develop a novel artificial synapse for deep neural networks. This prototypical synapse will offer low power consumption, high precision, good scalability, and great potential for large-scale integration. This work can lead to significant improvement in energy efficiency, bandwidth, and performance for deep learning algorithms. The research outcome can lead to the wide use of AI for both high-performance computing and low-power flexible electronics. This project can revolutionize society through advances in healthcare, self-driving vehicles, and autonomous manufacturing. The team will work closely with their local communities to attract students to pursue engineering careers, especially those  from underrepresented groups. Activities will include laboratory demonstrations, design projects, summer internships, and career workshops.\r\n\r\nTechnical:\r\n\r\nThe objective of this project is to develop scalable electrochemical two-dimensional (2D) synaptic arrays with high-precision and low-power for advanced hardware acceleration of deep neural networks (DNNs) with orders of magnitude improvements in energy and speed. While binary SRAM cells have shown promising performance for DNN hardware acceleration, its inherent limitations in power and area make it impractical to scale up to the complexity level required for large-scale problems and/or datasets. In this project, the team will take a holistic approach to develop scalable electrochemical 2D synaptic arrays with high precision, lower-power, good linearity, low variations, and CMOS compatibility for large-scale integration. The team will carry out the following three research tasks: (1) device-level optimization in device precision, dynamic range, and scaling; (2) array-level demonstration by building synaptic arrays, lowering device variations, and designing peripheral circuits; (3) system-level integration via building device models, implementing computing-in-memory (CIM), and demonstrating on-chip learning for pixel-to-pixel applications. This work will provide a low-power and scalable framework for the hardware acceleration of DNNs, paving the ways towards the ubiquitous use of artificial intelligence (AI) in both high-performance computers and low-power embedded systems.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yiran",
   "pi_last_name": "Chen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yiran Chen",
   "pi_email_addr": "yiran.chen@duke.edu",
   "nsf_id": "000575362",
   "pi_start_date": "2020-05-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "701 W. Main St.",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277015013",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "151700",
   "pgm_ele_name": "EPMD-ElectrnPhoton&MagnDevices"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  },
  {
   "pgm_ref_code": "086Z",
   "pgm_ref_txt": "Neuromorphic Computing"
  },
  {
   "pgm_ref_code": "100E",
   "pgm_ref_txt": "Novel devices & vacuum electronics"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of this project is to develop scalable electrochemical two-dimensional (2D) synaptic arrays with high-precision and low-power for advanced hardware acceleration of deep neural networks (DNNs) with orders of magnitude improvements in energy and speed.</p>\r\n<p>&nbsp;</p>\r\n<p>The transformer has emerged as a popular deep neural network (DNN) model for Natural Language Processing (NLP) applications and demonstrated excellent performance in neural machine translation, entity recognition, etc. However, its scaled dot-product attention mechanism in auto-regressive decoder brings performance bottleneck during inference. The transformer is also computationally and memory intensive and demands a hardware acceleration solution. Although researchers have successfully applied ReRAM-based Processing-in-Memory (PIM) to accelerate convolutional neural networks (CNNs) and recurrent neural networks (RNNs), the unique computation process of the scaled dot-product attention in Transformer makes it difficult to directly apply these designs. Besides, how to handle intermediate results in Matrix-matrix Multiplication (MatMul) and how to design a pipeline at a finer granularity of Transformer remain unsolved. We propose ReTransformer &ndash; a ReRAM-based PIM architecture for Transformer acceleration. ReTransformer can not only accelerate the scaled dot-product attention of Transformer using ReRAM-based PIM but also eliminate some data dependency by avoiding writing the intermediate results using the proposed matrix decomposition technique. Moreover, we propose a new sub-matrix pipeline design for multi-head self-attention. Experimental results show that compared to GPU and Pipelayer, ReTransformer improves computing efficiency by 23.21&times; and 3.25&times;, respectively. The corresponding overall power is reduced by 1086&times; and 2.82&times;, respectively. This research directly led to a conference publication on ICCAD 2020 and a journal publication on TCAS-I 2021, respectively.</p>\r\n<p>&nbsp;</p>\r\n<p>With the recent demand for deploying neural network models on mobile and edge devices, it is desired to improve the model&rsquo;s generalizability on unseen testing data, as well as enhance the model&rsquo;s robustness under fixed-point quantization for efficient deployment. Minimizing the training loss, however, provides few guarantees on the generalization and quantization performance, which is particularly important for emerging device platforms such as memristors. We fulfill the need of improving generalization and quantization performance simultaneously by theoretically unifying them under the framework of improving the model&rsquo;s robustness against bounded weight perturbation and minimizing the eigenvalues of the Hessian matrix with respect to model weights. We propose HERO, a Hessian-enhanced robust optimization method, to minimize the Hessian eigenvalues through a gradient-based training process, simultaneously improving the generalization and quantization performance. HERO enables up to a 3.8% gain on test accuracy, up to 30% higher accuracy under80% training label perturbation, and the best post-training quantization accuracy across a wide range of precision, including a &gt; 10% accuracy improvement over SGD-trained models for common model architectures on various datasets. The work was published in DAC 2022.</p>\r\n<p>&nbsp;</p>\r\n<p>In-sensor-processing (ISP) paradigm has been exploited in state-of-the-art vision system designs to pave the way towards power-efficient sensing and processing. Existing ISP designs suffer from limited frame rates and degraded fill factors. We introduce a low-latency in-sensor intelligence neuromorphic vision system using neuromorphic spiking neurons, namely SpikeSen. SpikeSen directly operates on the photocurrents and executes the computation in the frequency domain, reducing the long exposure time and speeding up the computation. In particular, we propose a spike-based computing pixel (SCP) and SCP string circuit that leverages the CMOS-based neuromorphic spiking neuron and capacitive synaptic weight to process the photocurrent locally within each pixel and accumulate the partial results from adjacent pixels. Based on the proposed SCP circuit substrate, we also designed SpikeSen, a low-latency ISP vision sensor with neuromorphic spiking neurons. SpikeSen executes convolution in the frequency domain with extremely low latency. Finally, we optimize the mapping scheme and control flow of SpikeSen with a novel sub-SCP structure. The parallel operations of multiple sub-SCPs further simplify the control flow and speed up the computation. Experiments show that SpikeSen can achieve more than 6.1&times; computation speedup compared to existing ISP designs with competitive energy consumption per pixel. The work was published in the IEEE TCAS-II in the issue of June 2023.</p>\r\n<p>&nbsp;</p>\r\n<p>Performing the floating-point computation in Resistive random-access memory (ReRAM) is challenging because of the high hardware cost and execution time due to the large FP value range. We proposed ReFloat, a data format and an accelerator architecture for low-cost and high-performance floating-point processing in ReRAM for iterative linear solvers. ReFloat matches the ReRAM crossbar hardware and represents a block of FP values with reduced bits and an optimized exponent base for a high range of dynamic representation. Thus, ReFloat achieves less ReRAM crossbar consumption and fewer processing cycles and overcomes the convergence issue in prior work. The evaluation on the SuiteSparse matrices shows ReFloat achieves 5.02&times; to 84.28&times;improvement in terms of solver time compared to a state-of-the-art ReRAM-based accelerator. The work was published in SC 2023.</p>\r\n<p>&nbsp;</p>\r\n<p>The project supported 2 graduate students and publications of 7 conference papers and 6 journal papers in total.</p>\r\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 01/10/2025<br>\nModified by: Yiran&nbsp;Chen</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe objective of this project is to develop scalable electrochemical two-dimensional (2D) synaptic arrays with high-precision and low-power for advanced hardware acceleration of deep neural networks (DNNs) with orders of magnitude improvements in energy and speed.\r\n\n\n\r\n\n\nThe transformer has emerged as a popular deep neural network (DNN) model for Natural Language Processing (NLP) applications and demonstrated excellent performance in neural machine translation, entity recognition, etc. However, its scaled dot-product attention mechanism in auto-regressive decoder brings performance bottleneck during inference. The transformer is also computationally and memory intensive and demands a hardware acceleration solution. Although researchers have successfully applied ReRAM-based Processing-in-Memory (PIM) to accelerate convolutional neural networks (CNNs) and recurrent neural networks (RNNs), the unique computation process of the scaled dot-product attention in Transformer makes it difficult to directly apply these designs. Besides, how to handle intermediate results in Matrix-matrix Multiplication (MatMul) and how to design a pipeline at a finer granularity of Transformer remain unsolved. We propose ReTransformer  a ReRAM-based PIM architecture for Transformer acceleration. ReTransformer can not only accelerate the scaled dot-product attention of Transformer using ReRAM-based PIM but also eliminate some data dependency by avoiding writing the intermediate results using the proposed matrix decomposition technique. Moreover, we propose a new sub-matrix pipeline design for multi-head self-attention. Experimental results show that compared to GPU and Pipelayer, ReTransformer improves computing efficiency by 23.21 and 3.25, respectively. The corresponding overall power is reduced by 1086 and 2.82, respectively. This research directly led to a conference publication on ICCAD 2020 and a journal publication on TCAS-I 2021, respectively.\r\n\n\n\r\n\n\nWith the recent demand for deploying neural network models on mobile and edge devices, it is desired to improve the models generalizability on unseen testing data, as well as enhance the models robustness under fixed-point quantization for efficient deployment. Minimizing the training loss, however, provides few guarantees on the generalization and quantization performance, which is particularly important for emerging device platforms such as memristors. We fulfill the need of improving generalization and quantization performance simultaneously by theoretically unifying them under the framework of improving the models robustness against bounded weight perturbation and minimizing the eigenvalues of the Hessian matrix with respect to model weights. We propose HERO, a Hessian-enhanced robust optimization method, to minimize the Hessian eigenvalues through a gradient-based training process, simultaneously improving the generalization and quantization performance. HERO enables up to a 3.8% gain on test accuracy, up to 30% higher accuracy under80% training label perturbation, and the best post-training quantization accuracy across a wide range of precision, including a  10% accuracy improvement over SGD-trained models for common model architectures on various datasets. The work was published in DAC 2022.\r\n\n\n\r\n\n\nIn-sensor-processing (ISP) paradigm has been exploited in state-of-the-art vision system designs to pave the way towards power-efficient sensing and processing. Existing ISP designs suffer from limited frame rates and degraded fill factors. We introduce a low-latency in-sensor intelligence neuromorphic vision system using neuromorphic spiking neurons, namely SpikeSen. SpikeSen directly operates on the photocurrents and executes the computation in the frequency domain, reducing the long exposure time and speeding up the computation. In particular, we propose a spike-based computing pixel (SCP) and SCP string circuit that leverages the CMOS-based neuromorphic spiking neuron and capacitive synaptic weight to process the photocurrent locally within each pixel and accumulate the partial results from adjacent pixels. Based on the proposed SCP circuit substrate, we also designed SpikeSen, a low-latency ISP vision sensor with neuromorphic spiking neurons. SpikeSen executes convolution in the frequency domain with extremely low latency. Finally, we optimize the mapping scheme and control flow of SpikeSen with a novel sub-SCP structure. The parallel operations of multiple sub-SCPs further simplify the control flow and speed up the computation. Experiments show that SpikeSen can achieve more than 6.1 computation speedup compared to existing ISP designs with competitive energy consumption per pixel. The work was published in the IEEE TCAS-II in the issue of June 2023.\r\n\n\n\r\n\n\nPerforming the floating-point computation in Resistive random-access memory (ReRAM) is challenging because of the high hardware cost and execution time due to the large FP value range. We proposed ReFloat, a data format and an accelerator architecture for low-cost and high-performance floating-point processing in ReRAM for iterative linear solvers. ReFloat matches the ReRAM crossbar hardware and represents a block of FP values with reduced bits and an optimized exponent base for a high range of dynamic representation. Thus, ReFloat achieves less ReRAM crossbar consumption and fewer processing cycles and overcomes the convergence issue in prior work. The evaluation on the SuiteSparse matrices shows ReFloat achieves 5.02 to 84.28improvement in terms of solver time compared to a state-of-the-art ReRAM-based accelerator. The work was published in SC 2023.\r\n\n\n\r\n\n\nThe project supported 2 graduate students and publications of 7 conference papers and 6 journal papers in total.\r\n\n\n\t\t\t\t\tLast Modified: 01/10/2025\n\n\t\t\t\t\tSubmitted by: YiranChen\n"
 }
}