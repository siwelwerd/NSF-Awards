{
 "awd_id": "1956271",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CNS: Core: Medium: Understanding and addressing device-reliability heterogeneity in large-scale distributed storage",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2024-03-31",
 "tot_intn_awd_amt": 970805.0,
 "awd_amount": 986805.0,
 "awd_min_amd_letter_date": "2020-05-26",
 "awd_max_amd_letter_date": "2022-09-13",
 "awd_abstract_narration": "This project develops new data protection approaches to reduce both data losses and cost-inefficiency in large storage systems. Modern data-centers store data in large collections (10s of thousands) of disk storage devices.  To avoid data loss in the event of a disk failure, each chunk of data is stored redundantly, such that there are multiple copies or extra code-information that allows data chunk reconstruction.  Previous approaches assume that all of the disks are the same in terms of their failure rates and aging properties.  But, while that was true for smaller storage systems in the past, it is not true for the large storage systems underlying today's data-driven world.  The result is that some data is not protected enough and other data is protected too much (wasting money) - worse, no one really knows which is which. This project develops new tools for dynamically learning the failure characteristics of different disks, automatically determining the right levels of redundancy for those characteristics, and efficiently adapting redundancy levels as circumstances change in the system.\r\n\r\nBy addressing a major flaw in how storage reliability is designed and managed, this project will make data protection stronger and less expensive.  Both are crucial.  Data protection is crucial, because data is core to the success of data-driven revolutions happening in medicine, science, politics, business, etc. Losing data hampers their success.  Data storage cost is crucial, because of the massive dollar, power, and environmental effects of the disks and data-centers used to house and operate them. For example, early analyses suggest that this project's outcomes could potentially reduce the number of disks needed by 20%, which would represent millions fewer devices in U.S. data centers. This project will include engaging with companies and national labs that operate large storage systems in order to help move the research results into practice.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Ganger",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory R Ganger",
   "pi_email_addr": "ganger@ece.cmu.edu",
   "nsf_id": "000328442",
   "pi_start_date": "2020-05-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rashmi",
   "pi_last_name": "Vinayak",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rashmi Vinayak",
   "pi_email_addr": "rvinayak@cs.cmu.edu",
   "nsf_id": "000754726",
   "pi_start_date": "2020-05-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "RMCIC 2208, 5000 Forbes Ave.",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0122",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002223DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 693153.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2022,
   "fund_oblg_amt": 277652.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-4c1009b6-7fff-79bb-e4ef-43a0ace47c25\">\n<p dir=\"ltr\"><span>This project develops new data protection approaches to reduce both data losses and cost-inefficiency in the large storage systems that are foundational parts of our modern data-driven, AI-integrating world. Modern datacenters store data in large collections (100s of thousands each) of storage devices called &ldquo;disks&rdquo;.&nbsp; To avoid data loss in the event of a disk failure, each chunk of data is stored redundantly, such that there are multiple copies or extra code-information that allows data chunk reconstruction.&nbsp; Previous approaches assume that all of the disks are the same, in terms of what percentage fail each year (failure rates) and aging properties.&nbsp; But, while that was true for smaller storage systems in the past, it is not true for the large storage systems underlying today's data-driven world. The result is that some data is not protected enough and other data is protected too much (wasting resource supplies, energy and money)---worse, no one really knows which is which. This project has developed new tools for dynamically learning the failure characteristics of different disks, automatically determining the right levels of redundancy for those characteristics, and efficiently adapting redundancy levels as circumstances change in the system.</span></p>\n<br />\n<p dir=\"ltr\"><span>This project has produced a number of critical new insights into disk deployment patterns, disk failure rate characteristics over time, and how data reliability can be dynamically tuned to significantly improve large-scale data storage.</span></p>\n<br /><ol>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Detailed analyses of multi-year disk failure logs and deployment characteristics of 5.3 million disks, covering over 60 makes/models, from Google, NetApp, and Bakcblaze exposed significant variation in failure rates among disk groups&mdash;disks in one group within the same datacenter can be twice as likely to fail as disks in another group.&nbsp; They are still worth using, but extra data redundancy is needed to avoid data loss.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Disk failure rates can be learned effectively, but the patterns in which disks are deployed and their failure rates change (akin to aging in cars) requires careful system design when tuning the level of redundancy.&nbsp; Traditionally, changing from one redundancy level to another involves significant work for the disk, but our new approaches to doing so require very little disk work.&nbsp; These new approaches include new types of data redundancy that are specially-designed to enable efficient redundancy level changing and new policies for how data is distributed among disks that further reduce data movement required for redundancy level changing. The end result is that, with our new approaches, the disk work required for redundancy changes are removed as a roadblock.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>Dynamic tuning of data redundancy can be integrated into real large-scale storage software without excessive difficulty and can provide large savings in terms of the number of disks needed to provide reliable large-scale data storage.</span></p>\n</li>\n</ol><br />\n<p dir=\"ltr\"><span>By addressing a major flaw in how storage reliability is designed and managed, this project enables data protection that is stronger and less expensive.&nbsp; Both are crucial.&nbsp; Data protection is crucial, because data is core to the success of data-driven revolutions happening in medicine, science, politics, business, etc. Losing data hampers their success.&nbsp; Data storage cost is crucial, because of the massive dollar, power, and environmental effects of the many millions of disks needed and the datacenters used to house and operate them. For example, results show that this project's new approaches would reduce the number of disks needed in Google&rsquo;s storage clusters (as a concrete example) by 20%, which would represent millions fewer devices in U.S. datacenters if applied by the various cloud and Internet service providers.&nbsp;</span></p>\n<br />\n<p dir=\"ltr\"><span>The knowledge and ideas arising from this project were published in top scientific forums for computer systems research, including top-tier conferences such as OSDI, ISIT, and SOSP. The PIs and their students also disseminated the results from the project via many talks and presentations to both industry practitioners and academic researchers, and by open-sourcing research artifacts.&nbsp; Three PhD dissertations from CMU, as well as multiple Masters student and undergraduate projects, were completed. </span><span>These students gained invaluable expertise in the areas of distributed computer systems and coding theory. The PIs also incorporated material from this project into the courses they teach at CMU for both undergraduate and graduate students, and in talks at outreach events.</span></p>\n<div><span><br /></span></div>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n Last Modified: 05/23/2024<br>\nModified by: Gregory&nbsp;R&nbsp;Ganger</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\n\n\n\nThis project develops new data protection approaches to reduce both data losses and cost-inefficiency in the large storage systems that are foundational parts of our modern data-driven, AI-integrating world. Modern datacenters store data in large collections (100s of thousands each) of storage devices called disks. To avoid data loss in the event of a disk failure, each chunk of data is stored redundantly, such that there are multiple copies or extra code-information that allows data chunk reconstruction. Previous approaches assume that all of the disks are the same, in terms of what percentage fail each year (failure rates) and aging properties. But, while that was true for smaller storage systems in the past, it is not true for the large storage systems underlying today's data-driven world. The result is that some data is not protected enough and other data is protected too much (wasting resource supplies, energy and money)---worse, no one really knows which is which. This project has developed new tools for dynamically learning the failure characteristics of different disks, automatically determining the right levels of redundancy for those characteristics, and efficiently adapting redundancy levels as circumstances change in the system.\n\n\n\n\nThis project has produced a number of critical new insights into disk deployment patterns, disk failure rate characteristics over time, and how data reliability can be dynamically tuned to significantly improve large-scale data storage.\n\n\n\n\n\nDetailed analyses of multi-year disk failure logs and deployment characteristics of 5.3 million disks, covering over 60 makes/models, from Google, NetApp, and Bakcblaze exposed significant variation in failure rates among disk groupsdisks in one group within the same datacenter can be twice as likely to fail as disks in another group. They are still worth using, but extra data redundancy is needed to avoid data loss.\n\n\n\n\nDisk failure rates can be learned effectively, but the patterns in which disks are deployed and their failure rates change (akin to aging in cars) requires careful system design when tuning the level of redundancy. Traditionally, changing from one redundancy level to another involves significant work for the disk, but our new approaches to doing so require very little disk work. These new approaches include new types of data redundancy that are specially-designed to enable efficient redundancy level changing and new policies for how data is distributed among disks that further reduce data movement required for redundancy level changing. The end result is that, with our new approaches, the disk work required for redundancy changes are removed as a roadblock.\n\n\n\n\nDynamic tuning of data redundancy can be integrated into real large-scale storage software without excessive difficulty and can provide large savings in terms of the number of disks needed to provide reliable large-scale data storage.\n\n\n\n\n\nBy addressing a major flaw in how storage reliability is designed and managed, this project enables data protection that is stronger and less expensive. Both are crucial. Data protection is crucial, because data is core to the success of data-driven revolutions happening in medicine, science, politics, business, etc. Losing data hampers their success. Data storage cost is crucial, because of the massive dollar, power, and environmental effects of the many millions of disks needed and the datacenters used to house and operate them. For example, results show that this project's new approaches would reduce the number of disks needed in Googles storage clusters (as a concrete example) by 20%, which would represent millions fewer devices in U.S. datacenters if applied by the various cloud and Internet service providers.\n\n\n\n\nThe knowledge and ideas arising from this project were published in top scientific forums for computer systems research, including top-tier conferences such as OSDI, ISIT, and SOSP. The PIs and their students also disseminated the results from the project via many talks and presentations to both industry practitioners and academic researchers, and by open-sourcing research artifacts. Three PhD dissertations from CMU, as well as multiple Masters student and undergraduate projects, were completed. These students gained invaluable expertise in the areas of distributed computer systems and coding theory. The PIs also incorporated material from this project into the courses they teach at CMU for both undergraduate and graduate students, and in talks at outreach events.\n\n\n\n\n\n\t\t\t\t\tLast Modified: 05/23/2024\n\n\t\t\t\t\tSubmitted by: GregoryRGanger\n"
 }
}