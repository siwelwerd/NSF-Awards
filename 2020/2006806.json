{
 "awd_id": "2006806",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: AF: Small: Fine-\nGrained Complexity of Approximate Problems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922095",
 "po_email": "kwimmer@nsf.gov",
 "po_sign_block_name": "Karl Wimmer",
 "awd_eff_date": "2020-10-01",
 "awd_exp_date": "2023-09-30",
 "tot_intn_awd_amt": 114602.0,
 "awd_amount": 114602.0,
 "awd_min_amd_letter_date": "2020-07-31",
 "awd_max_amd_letter_date": "2020-07-31",
 "awd_abstract_narration": "Classically, an algorithm is called \"efficient\" if its running time is polynomial in the input size (i.e., as the input size doubles, the runtime is multiplied by some constant term). As the data becomes large, however, many such algorithms are no longer efficient in practice. For example, a quadratic-time algorithm (whose runtime grows fourfold after doubling the input size) can easily take hundreds of CPU-years on inputs of gigabyte size. For even larger inputs, the running time of a practically efficient algorithm must be effectively linear in the input  size. For many  problems such algorithms exist; for others, despite decades of effort, no such algorithms have been discovered yet. A recently developed theory of \"fine-grained complexity\" attempts to provide an explanation to  this phenomenon, by identifying natural assumptions that imply that  some of the existing algorithms cannot be improved. The goal of this project is to make progress on some of the key directions in this area, by developing new algorithms where possible, and showing limitations otherwise.\r\n\r\nThe project will focus on approximate algorithms, because such algorithms are often very useful in practice, and their existence is often not precluded by the existing hardness results. On a high-level, the project will investigate the following topics: (1) approximate algorithms, limitations, and limitations-inspired algorithms, and (2) new hardness assumptions for improving existing hardness results. The specific goals include key computational problems over graphs, sequences and kernels.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Arturs",
   "pi_last_name": "Backurs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Arturs Backurs",
   "pi_email_addr": "backurs@ttic.edu",
   "nsf_id": "000786926",
   "pi_start_date": "2020-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "6045 S. Kenwood Ave",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372803",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7929",
   "pgm_ref_txt": "COMPUTATIONAL GEOMETRY"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0120",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002021DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 114602.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The main outcome of the award is the following publication:</span><br /><br /><span>Arturs Backurs, Piotr Indyk, Cameron Musco and Tal Wagner (2021). Faster Kernel Matrix Algebra via Density Estimation. 38th International Conference on Machine Learning, ICML 2021.</span><br /><br /><span>It is available at: </span><a rel=\"nofollow\" href=\"https://arxiv.org/abs/2102.08341\" target=\"_blank\">https://arxiv.org/abs/2102.08341</a><br /><br /><span>Kernels are a ubiquitous notion in statistics, machine learning, and other fields. A kernel is a function k : R^d &times; R^d &rarr; R that measures the similarity between two d-dimensional vectors. Many statistical and machine learning methods, such as support vector machines, kernel ridge regression and kernel density estimation, rely on appropriate choices of kernels. A prominent example of a kernel function is the Radial Basis Function, a.k.a. Gaussian kernel, defined as k(x, y) = exp(&minus;||x &minus; y||^2).</span><br /><span>Other popular choices include the Laplace kernel, exponential kernel, etc.</span><br /><span>We study fast algorithms for computing fundamental properties of a positive semidefinite kernel matrix K &isin; R^{n&times;n} corresponding to n points x_1, &hellip;, x_n &isin; Rd. In particular, we consider estimating the sum of kernel matrix entries, along with its top eigenvalue and eigenvector.</span><br /><span>We show that the sum of matrix entries can be estimated to 1+&#1013; relative error in time sublinear in n and linear in d for many popular kernels, including the Gaussian, exponential, and rational quadratic kernels. For these kernels, we also show that the top eigenvalue (and an approximate eigenvector) can be approximated to 1+&#1013; relative error in time subquadratic in n and linear in d.</span><br /><span>Our algorithms represent significant advances in the best known runtimes for these problems. They leverage the positive definiteness of the kernel matrix, along with a recent line of work on efficient kernel density estimation.</span></p><br>\n<p>\n Last Modified: 04/24/2024<br>\nModified by: Arturs&nbsp;Backurs</p></div>\n<div class=\"porSideCol\"\n></div>\n</div>\n",
  "por_txt_cntn": "\n\nThe main outcome of the award is the following publication:\n\nArturs Backurs, Piotr Indyk, Cameron Musco and Tal Wagner (2021). Faster Kernel Matrix Algebra via Density Estimation. 38th International Conference on Machine Learning, ICML 2021.\n\nIt is available at: https://arxiv.org/abs/2102.08341\n\nKernels are a ubiquitous notion in statistics, machine learning, and other fields. A kernel is a function k : R^d  R^d  R that measures the similarity between two d-dimensional vectors. Many statistical and machine learning methods, such as support vector machines, kernel ridge regression and kernel density estimation, rely on appropriate choices of kernels. A prominent example of a kernel function is the Radial Basis Function, a.k.a. Gaussian kernel, defined as k(x, y) = exp(||x  y||^2).\nOther popular choices include the Laplace kernel, exponential kernel, etc.\nWe study fast algorithms for computing fundamental properties of a positive semidefinite kernel matrix K  R^{nn} corresponding to n points x_1, , x_n  Rd. In particular, we consider estimating the sum of kernel matrix entries, along with its top eigenvalue and eigenvector.\nWe show that the sum of matrix entries can be estimated to 1+&#1013; relative error in time sublinear in n and linear in d for many popular kernels, including the Gaussian, exponential, and rational quadratic kernels. For these kernels, we also show that the top eigenvalue (and an approximate eigenvector) can be approximated to 1+&#1013; relative error in time subquadratic in n and linear in d.\nOur algorithms represent significant advances in the best known runtimes for these problems. They leverage the positive definiteness of the kernel matrix, along with a recent line of work on efficient kernel density estimation.\t\t\t\t\tLast Modified: 04/24/2024\n\n\t\t\t\t\tSubmitted by: ArtursBackurs\n"
 }
}